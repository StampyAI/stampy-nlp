[{"abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.", "authors": ["Diederik P. Kingma", "Jimmy Ba"], "id": "1412.6980v9", "score": 0.833769083, "title": "Adam: A Method for Stochastic Optimization", "url": "http://arxiv.org/abs/1412.6980v9"}, {"abstract": "Measuring mutual information from finite data is difficult. Recent work has considered variational methods maximizing a lower bound. In this paper, we prove that serious statistical limitations are inherent to any method of measuring mutual information. More specifically, we show that any distribution-free high-confidence lower bound on mutual information estimated from N samples cannot be larger than O(ln N ).", "authors": ["David McAllester", "Karl Stratos"], "id": "1811.04251v4", "score": 0.82700038, "title": "Formal Limitations on the Measurement of Mutual Information", "url": "http://arxiv.org/abs/1811.04251v4"}, {"abstract": "In this expository note we describe a surprising phenomenon in overparameterized linear regression, where the dimension exceeds the number of samples: there is a regime where the test risk of the estimator found by gradient descent increases with additional samples. In other words, more data actually hurts the estimator. This behavior is implicit in a recent line of theoretical works analyzing \"double-descent\" phenomenon in linear models. In this note, we isolate and understand this behavior in an extremely simple setting: linear regression with isotropic Gaussian covariates. In particular, this occurs due to an unconventional type of bias-variance tradeoff in the overparameterized regime: the bias decreases with more samples, but variance increases.", "authors": ["Preetum Nakkiran"], "id": "1912.07242v1", "score": 0.813655674, "title": "More Data Can Hurt for Linear Regression: Sample-wise Double Descent", "url": "http://arxiv.org/abs/1912.07242v1"}, {"abstract": "Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners.   In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This \"double descent\" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.", "authors": ["Mikhail Belkin", "Daniel Hsu", "Siyuan Ma", "Soumik Mandal"], "id": "1812.11118v2", "score": 0.812712, "title": "Reconciling modern machine learning practice and the bias-variance trade-off", "url": "http://arxiv.org/abs/1812.11118v2"}, {"abstract": "In machine learning, we traditionally evaluate the performance of a single model, averaged over a collection of test inputs. In this work, we propose a new approach: we measure the performance of a collection of models when evaluated on a $\\textit{single input point}$. Specifically, we study a point's $\\textit{profile}$: the relationship between models' average performance on the test distribution and their pointwise performance on this individual point. We find that profiles can yield new insights into the structure of both models and data -- in and out-of-distribution. For example, we empirically show that real data distributions consist of points with qualitatively different profiles. On one hand, there are \"compatible\" points with strong correlation between the pointwise and average performance. On the other hand, there are points with weak and even $\\textit{negative}$ correlation: cases where improving overall model accuracy actually $\\textit{hurts}$ performance on these inputs. We prove that these experimental observations are inconsistent with the predictions of several simplified models of learning proposed in prior work. As an application, we use profiles to construct a dataset we call CIFAR-10-NEG: a subset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is $\\textit{negatively correlated}$ with accuracy on CIFAR-10 test. This illustrates, for the first time, an OOD dataset that completely inverts \"accuracy-on-the-line\" (Miller, Taori, Raghunathan, Sagawa, Koh, Shankar, Liang, Carmon, and Schmidt 2021)", "authors": ["Gal Kaplun", "Nikhil Ghosh", "Saurabh Garg", "Boaz Barak", "Preetum Nakkiran"], "id": "2202.09931v1", "score": 0.80937475, "title": "Deconstructing Distributions: A Pointwise Framework of Learning", "url": "http://arxiv.org/abs/2202.09931v1"}]