[{"score": 0.9991392493247986, "id": "004-FzF4Xok63ZCZNjmGY", "metadata": {"text": "AI Safety This field really started with: Bostrom\u2019s Superintelligence discussion of the AI-alignment problem the ensuing deep learning revolution. In the past decade, a serious research community has emerged, focused on the safety of smarter-than-human AI, including prominent members of the AI community such as Stuart Russel and researchers at DeepMind and OpenAI. Classic work in this field attacks difficult foundational decision and game theoretic problems relating to the goals of powerful AI systems. A key issue is intent or goal alignment \u2014 how do we get machines to want to do what we want them to do? Current paradigms in AI and ML depend upon the framework of expected utility maximising agents (e.g. in reinforcement learning the agent wishes to maximise the expected reward). However, systematically writing down everything we care about into an objective function is likely impossible and by default, agents have unsafe incentives such as not being switched off.", "title": "Blog post: A tale of two research communities\n", "url": "https://www.lesswrong.com/posts/FzF4Xok63ZCZNjmGY/blog-post-a-tale-of-two-research-communities#:~:text=smarter-than-human%20AI"}}, {"score": 0.9398396015167236, "id": "042-mF8dkhZF9hAuLHXaD", "metadata": {"text": "^But not AI Safety itself, of course, only the project of spreading it. AI is very much Scott\u2019s Distribution 1, and the fact that our civilization is treating it as a Distribution 2 is the entire bloody problem.", "title": "Reshaping the AI Industry\n", "url": "https://www.lesswrong.com/posts/mF8dkhZF9hAuLHXaD/reshaping-the-ai-industry#:~:text=the%20project%20of%20spreading%20it"}}, {"score": 0.7898231148719788, "id": "000-2002.05671v2", "metadata": {"text": "Last decade has seen major improvements in the performance of artificial intelligence which has driven wide-spread applications. Unforeseen effects of such mass-adoption has put the notion of AI safety into the public eye. AI safety is a relatively new field of research focused on techniques for building AI beneficial for humans. While there exist survey papers for the field of AI safety, there is a lack of a quantitative look at the research being conducted. The quantitative aspect gives a data-driven insight about the emerging trends, knowledge gaps and potential areas for future research. In this paper, bibliometric analysis of the literature finds significant increase in research activity since 2015. Also, the field is so new that most of the technical issues are open, including: explainability with its long-term utility, and value alignment which we have identified as the most important long-term research topic.", "title": "AI safety: state of the field through quantitative lens", "url": "http://arxiv.org/abs/2002.05671v2#:~:text=a%20relatively%20new%20field%20of%20research%20focused%20on%20techniques%20for%20building%20AI%20beneficial%20for%20humans"}}, {"score": 0.6537936925888062, "id": "000-https://aisafety.info?state=8486", "metadata": {"text": "AI safety is a research field founded to avoid catastrophic outcomes from advanced AI, though the term has since expanded to include reducing less extreme harms from AI. AI existential safety, or AGI safety is about reducing the existential risk from artificial general intelligence (AGI). Artificial general intelligence is AI that is at least as competent as humans in all skills that are relevant for making a difference in the world. AGI has not been developed yet, but will likely be developed in this century. A central part of AGI safety is ensuring that what AIs do is actually what we want. This is called AI alignment (also often just called alignment), because it\u2019s about aligning an AI with human values. Alignment is difficult, and building AGI is probably very dangerous, so it is important to mitigate the risks as much as possible. Examples for work on AI existential safety are trying to get a foundational understanding what intelligence is, e.g.", "title": "What is AI safety?", "url": "https://aisafety.info?state=8486#:~:text=to%20avoid%20catastrophic%20outcomes%20from%20advanced%20AI"}}, {"score": 0.6384903788566589, "id": "000-1906.10189v2", "metadata": {"text": "Recent developments in artificial intelligence and machine learning have spurred interest in the growing field of AI safety, which studies how to prevent human-harming accidents when deploying AI systems. This paper thus explores the intersection of AI safety with evolutionary computation, to show how safety issues arise in evolutionary computation and how understanding from evolutionary computational and biological evolution can inform the broader study of AI safety.", "title": "Evolutionary Computation and AI Safety: Research Problems Impeding Routine and Safe Real-world Application of Evolution", "url": "http://arxiv.org/abs/1906.10189v2#:~:text=studies%20how%20to%20prevent%20human-harming%20accidents%20when%20deploying%20AI%20systems"}}, {"score": 0.4543364644050598, "id": "005-FzF4Xok63ZCZNjmGY", "metadata": {"text": "For example, consider a robot with the goal of getting coffee. As Russel says, \"You can\u2019t fetch the coffee if you\u2019re dead\" \u2014 such an agent will incapacitate anyone who tries to prevent it from achieving its goal of getting you a Starbucks. Importantly, this is the standard way in which we currently build AI! It is really non-trivial to make this paradigm safe (or change the paradigm under which we currently build AI). More recent work aims to research current AI techniques in order to gain insight into future systems (Concrete Problems in AI Safety is a seminal overview) and more nuanced arguments and subfields aimed at solving a variety of problems relating to the safety of AGI have emerged (prominent research communities exist at DeepMind, OpenAI, Future of Humanity Institute, Center for Human-Compatible Artificial Intelligence, Machine Intelligence Research Institute). Integrating the fields Below is a conceptual breakdown of problems in technical AI safety from DeepMind.", "title": "Blog post: A tale of two research communities\n", "url": "https://www.lesswrong.com/posts/FzF4Xok63ZCZNjmGY/blog-post-a-tale-of-two-research-communities#:~:text=paradigm%20safe"}}, {"score": 0.30626970529556274, "id": "000-2202.09292v1", "metadata": {"text": "This chapter formulates seven lessons for preventing harm in artificial intelligence (AI) systems based on insights from the field of system safety for software-based automation in safety-critical domains. New applications of AI across societal domains and public organizations and infrastructures come with new hazards, which lead to new forms of harm, both grave and pernicious. The text addresses the lack of consensus for diagnosing and eliminating new AI system hazards. For decades, the field of system safety has dealt with accidents and harm in safety-critical systems governed by varying degrees of software-based automation and decision-making.", "title": "System Safety and Artificial Intelligence", "url": "http://arxiv.org/abs/2202.09292v1#:~:text=software-based%20automation%20in%20safety-critical%20domains"}}, {"score": 0.12975016236305237, "id": "004-2NaAhMPGub8F2Pbr7", "metadata": {"text": "If you want a concrete, evocative analogy: picture a two-year-old playing on top of a tablesaw. That said, people are designing tablesaws which auto-stop when skin contacts the blade. In general, a system\u2019s designers may understand the relevant safety issues better than the operators. Indeed, since the first AGIs will be built by humans, any approach to AI safety ultimately relies on human designers asking the right questions. Point is: we can\u2019t avoid the need for designers to ask (at least some of) the right questions upfront. But needing the designers to ask the right questions once is still a lot better than needing every user to ask the right questions every time they use the system. (This perspective ties in nicely with AI alignment as interface design: if an interface offers an easy-to-overlook way to cut your hand off, and relies on users not doing so, then that\u2019s a design problem.", "title": "The Fusion Power Generator Scenario\n", "url": "https://www.lesswrong.com/posts/2NaAhMPGub8F2Pbr7/the-fusion-power-generator-scenario#:~:text=first%20AGIs%20will%20be%20built%20by%20humans"}}]