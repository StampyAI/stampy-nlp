{"answer": "smarter-than-human AI", "context": "AI Safety This field really started with: Bostrom\u2019s Superintelligence discussion of the AI-alignment problem the ensuing deep learning revolution. In the past decade, a serious research community has emerged, focused on the safety of smarter-than-human AI, including prominent members of the AI community such as Stuart Russel and researchers at DeepMind and OpenAI. Classic work in this field attacks difficult foundational decision and game theoretic problems relating to the goals of powerful AI systems. A key issue is intent or goal alignment \u2014 how do we get machines to want to do what we want them to do? Current paradigms in AI and ML depend upon the framework of expected utility maximising agents (e.g. in reinforcement learning the agent wishes to maximise the expected reward). However, systematically writing down everything we care about into an objective function is likely impossible and by default, agents have unsafe incentives such as not being switched off.", "end": 255, "id": "004-FzF4Xok63ZCZNjmGY", "score": 0.9991392493247986, "start": 234, "title": "Blog post: A tale of two research communities\n", "url": "https://www.lesswrong.com/posts/FzF4Xok63ZCZNjmGY/blog-post-a-tale-of-two-research-communities#:~:text=smarter-than-human%20AI"}