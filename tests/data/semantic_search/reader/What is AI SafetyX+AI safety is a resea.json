{"answer": "to avoid catastrophic outcomes from advanced AI", "context": "AI safety is a research field founded to avoid catastrophic outcomes from advanced AI, though the term has since expanded to include reducing less extreme harms from AI. AI existential safety, or AGI safety is about reducing the existential risk from artificial general intelligence (AGI). Artificial general intelligence is AI that is at least as competent as humans in all skills that are relevant for making a difference in the world. AGI has not been developed yet, but will likely be developed in this century. A central part of AGI safety is ensuring that what AIs do is actually what we want. This is called AI alignment (also often just called alignment), because it\u2019s about aligning an AI with human values. Alignment is difficult, and building AGI is probably very dangerous, so it is important to mitigate the risks as much as possible. Examples for work on AI existential safety are trying to get a foundational understanding what intelligence is, e.g.", "end": 85, "id": "000-https://aisafety.info?state=8486", "score": 0.6537936925888062, "start": 38, "title": "What is AI safety?", "url": "https://aisafety.info?state=8486#:~:text=to%20avoid%20catastrophic%20outcomes%20from%20advanced%20AI"}