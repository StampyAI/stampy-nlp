{"answer": "their methods section rather lacking when it came to how the model was trained.", "context": "For sentence selection and recognizing textual entailment, the datasets above seem to be the major sources of training data. For evidence retrieval, I\u2019ve not looked as closely, but there are some very different approaches. The human-assistance tools seem to rely on taking the top results from Google as a starting point and then filter the documents by a few, mostly hard-coded criteria. The more ambitious pipeline approach of Nie at al instead use a neural document matcher which first narrows down relevant documents by simple keyword matching (I think using Wikipedia as its document pool), and then uses the FEVER dataset to train an LSTM-based ANN to learn whether there is a high relatedness, by using the sentences in FEVER listed as evidence for the claim as the positive cases of relatedness. (I think this is an accurate summary.. I found their methods section rather lacking when it came to how the model was trained.", "end": 930, "id": "011-CWD8FxA3yJPmZE9o3", "score": 0.005022065714001656, "start": 851, "title": "Automated Fact Checking: A Look at the Field\n", "url": "https://www.lesswrong.com/posts/CWD8FxA3yJPmZE9o3/automated-fact-checking-a-look-at-the-field#:~:text=their%20methods%20section%20rather%20lacking%20when%20it%20came%20to%20how%20the%20model%20was%20trained."}