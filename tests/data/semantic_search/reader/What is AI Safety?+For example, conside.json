{"answer": "paradigm safe", "context": "For example, consider a robot with the goal of getting coffee. As Russel says, \"You can\u2019t fetch the coffee if you\u2019re dead\" \u2014 such an agent will incapacitate anyone who tries to prevent it from achieving its goal of getting you a Starbucks. Importantly, this is the standard way in which we currently build AI! It is really non-trivial to make this paradigm safe (or change the paradigm under which we currently build AI). More recent work aims to research current AI techniques in order to gain insight into future systems (Concrete Problems in AI Safety is a seminal overview) and more nuanced arguments and subfields aimed at solving a variety of problems relating to the safety of AGI have emerged (prominent research communities exist at DeepMind, OpenAI, Future of Humanity Institute, Center for Human-Compatible Artificial Intelligence, Machine Intelligence Research Institute). Integrating the fields Below is a conceptual breakdown of problems in technical AI safety from DeepMind.", "end": 361, "id": "005-FzF4Xok63ZCZNjmGY", "score": 0.4543364644050598, "start": 348, "title": "Blog post: A tale of two research communities\n", "url": "https://www.lesswrong.com/posts/FzF4Xok63ZCZNjmGY/blog-post-a-tale-of-two-research-communities#:~:text=paradigm%20safe"}