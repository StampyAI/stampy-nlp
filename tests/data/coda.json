{"items": [{"id": "i-213cd35bf0c8d7540def6ff864d977881844f647d824897d99f8430e8452dfb1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-213cd35bf0c8d7540def6ff864d977881844f647d824897d99f8430e8452dfb1", "name": "What are the differences between a singularity, an intelligence explosion and a hard takeoff?", "index": 581, "createdAt": "2023-03-15T22:07:55.089Z", "updatedAt": "2023-03-16T07:07:26.302Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-213cd35bf0c8d7540def6ff864d977881844f647d824897d99f8430e8452dfb1", "values": {"File": "What are the differences between a singularity, an intelligence explosion and a hard takeoff?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the differences between a singularity, an intelligence explosion and a hard takeoff?", "Link": "https://docs.google.com/document/d/1WJraCVimbHI8VjZaZqT-eNW2kb_Yjbc5Jbl0TjMXnV0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-15T23:01:42.666+01:00", "Related Answers DO NOT EDIT": "What is meant by \"AI takeoff\"?,What are the different possible AI takeoff speeds?,What is an \"intelligence explosion\"?", "Tags": "", "Doc Last Edited": "2023-03-16T05:09:02.071+01:00", "Status": "In progress", "Edit Answer": "What are the differences between a singularity, an intelligence explosion and a hard takeoff?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8IHO", "Source Link": "", "aisafety.info Link": "What are the differences between a singularity, an intelligence explosion and a hard takeoff?", "Source": "", "All Phrasings": "What are the differences between a singularity, an intelligence explosion and a hard takeoff?\n", "Initial Order": "", "Related IDs": "7071,6957,6306", "Rich Text DO NOT EDIT": "These are all terms for future processes that could rapidly result in superintelligence. They have been used by different people, at different times, with overlapping and sometimes changing meanings.\n\n- A **(technological) singularity** refers to a hypothetical future time when, because of AI or other technologies, progress becomes extremely fast, leading to a qualitative change in the world. This term has been used inconsistently and is no longer in much use by the AI alignment community. [Different versions](https://intelligence.org/2007/09/30/three-major-singularity-schools/) have stressed the exponential, [double-exponential](https://www.kurzweilai.net/the-law-of-accelerating-returns), or [even](https://www.openphilanthropy.org/research/modeling-the-human-trajectory/)[hyperbolic](https://sideways-view.com/2017/10/04/hyperbolic-growth/) acceleration of scientific and technological advancement, self-improvement feedback loops (the intelligence explosion), and the [unknowability](https://edoras.sdsu.edu/~vinge/misc/singularity.html) of strongly superhuman intelligence.\n\n- **\"Intelligence explosion\"** is what IJ Good [called](https://vtechworks.lib.vt.edu/bitstream/handle/10919/89424/TechReport05-3.pdf) a scenario where AI becomes smart enough to create even smarter AI, which creates even smarter AI, and so on, [recursively self-improving](https://intelligence.org/files/IEM.pdf) all the way to superintelligence.\n\n- A **hard takeoff** is a scenario where the transition to superintelligence happens quickly and suddenly instead of gradually. The opposite is a **soft takeoff**. Eliezer Yudkowsky is an example of someone [predicting](https://intelligence.org/ai-foom-debate/) a hard takeoff singularity; Ray Kurzweil is an example of someone predicting a [soft takeoff singularity](https://medium.com/@emergingtechnology/the-singularity-a-hard-or-soft-takeoff-caa3e010b27f). [Related distinctions](https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff) are fast versus slow takeoff and discontinuous versus continuous takeoff. A takeoff can be continuous but eventually very fast, as in the case of Paul Christiano's [predictions](https://sideways-view.com/2018/02/24/takeoff-speeds/) of a \"slow takeoff\" that results in hyperbolic growth.\n\nThere are some more related concepts that have been used:\n\n- **\"Sharp left turn\"** is [Nate Soares's](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) term for an event where an AI system learns to generalize its capabilities to many domains, like how human intelligence turned out to be applicable to fields like physics and software engineering. He argues the key problem is the system's alignment would fail to generalize along with its capabilities. A \"sharp left turn\" need not be caused by recursive self-improvement.\n\n- **\"FOOM\"** is more or less a synonym of \"hard takeoff\", based on the sound that imaginary substances make when they instantly expand. The term is associated mostly with the [Yudkowsky vs. Hanson FOOM debate](https://www.alignmentforum.org/tag/the-hanson-yudkowsky-ai-foom-debate): both eventually expect [very fast change](https://mason.gmu.edu/~rhanson/aigrow.pdf), but Yudkowsky argues for sudden and discontinuous change driven by local recursive self-improvement, while Hanson argues for a more gradual and spread-out process. Hard takeoff does not require recursive self-improvement, and Yudkowsky [now thinks](https://intelligence.org/2017/10/20/alphago/) regular improvement of AI by humans may cause sufficiently big capability leaps to preempt recursive self-improvement. On the other hand, recursive self-improvement could be gradual: Paul Christiano thinks an intelligence explosion is \"[very likely](https://sideways-view.com/2018/02/24/takeoff-speeds/)\" despite predicting a slow takeoff.\n\n", "Tag Count": 0, "Related Answer Count": 3, "Rich Text": "These are all terms for future processes that could rapidly result in superintelligence. They have been used by different people, at different times, with overlapping and sometimes changing meanings.\n\n- A **(technological) singularity** refers to a hypothetical future time when, because of AI or other technologies, progress becomes extremely fast, leading to a qualitative change in the world. This term has been used inconsistently and is no longer in much use by the AI alignment community. [Different versions](https://intelligence.org/2007/09/30/three-major-singularity-schools/) have stressed the exponential, [double-exponential](https://www.kurzweilai.net/the-law-of-accelerating-returns), or [even](https://www.openphilanthropy.org/research/modeling-the-human-trajectory/)[hyperbolic](https://sideways-view.com/2017/10/04/hyperbolic-growth/) acceleration of scientific and technological advancement, self-improvement feedback loops (the intelligence explosion), and the [unknowability](https://edoras.sdsu.edu/~vinge/misc/singularity.html) of strongly superhuman intelligence.\n\n- **\"Intelligence explosion\"** is what IJ Good [called](https://vtechworks.lib.vt.edu/bitstream/handle/10919/89424/TechReport05-3.pdf) a scenario where AI becomes smart enough to create even smarter AI, which creates even smarter AI, and so on, [recursively self-improving](https://intelligence.org/files/IEM.pdf) all the way to superintelligence.\n\n- A **hard takeoff** is a scenario where the transition to superintelligence happens quickly and suddenly instead of gradually. The opposite is a **soft takeoff**. Eliezer Yudkowsky is an example of someone [predicting](https://intelligence.org/ai-foom-debate/) a hard takeoff singularity; Ray Kurzweil is an example of someone predicting a [soft takeoff singularity](https://medium.com/@emergingtechnology/the-singularity-a-hard-or-soft-takeoff-caa3e010b27f). [Related distinctions](https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff) are fast versus slow takeoff and discontinuous versus continuous takeoff. A takeoff can be continuous but eventually very fast, as in the case of Paul Christiano's [predictions](https://sideways-view.com/2018/02/24/takeoff-speeds/) of a \"slow takeoff\" that results in hyperbolic growth.\n\nThere are some more related concepts that have been used:\n\n- **\"Sharp left turn\"** is [Nate Soares's](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) term for an event where an AI system learns to generalize its capabilities to many domains, like how human intelligence turned out to be applicable to fields like physics and software engineering. He argues the key problem is the system's alignment would fail to generalize along with its capabilities. A \"sharp left turn\" need not be caused by recursive self-improvement.\n\n- **\"FOOM\"** is more or less a synonym of \"hard takeoff\", based on the sound that imaginary substances make when they instantly expand. The term is associated mostly with the [Yudkowsky vs. Hanson FOOM debate](https://www.alignmentforum.org/tag/the-hanson-yudkowsky-ai-foom-debate): both eventually expect [very fast change](https://mason.gmu.edu/~rhanson/aigrow.pdf), but Yudkowsky argues for sudden and discontinuous change driven by local recursive self-improvement, while Hanson argues for a more gradual and spread-out process. Hard takeoff does not require recursive self-improvement, and Yudkowsky [now thinks](https://intelligence.org/2017/10/20/alphago/) regular improvement of AI by humans may cause sufficiently big capability leaps to preempt recursive self-improvement. On the other hand, recursive self-improvement could be gradual: Paul Christiano thinks an intelligence explosion is \"[very likely](https://sideways-view.com/2018/02/24/takeoff-speeds/)\" despite predicting a slow takeoff.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8IHO", "Related Answers": "What is meant by \"AI takeoff\"?,What are the different possible AI takeoff speeds?,What is an \"intelligence explosion\"?", "Doc Last Ingested": "2023-03-16T05:11:54.995+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 8, "Helpful": ""}}, {"id": "i-67fb03ebf18f3cec02cbf66f64454732e4745026b811ca55f46d1c675ad7351a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-67fb03ebf18f3cec02cbf66f64454732e4745026b811ca55f46d1c675ad7351a", "name": "What is a saliency map?", "index": 580, "createdAt": "2023-03-15T10:07:40.938Z", "updatedAt": "2023-03-15T13:12:16.719Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-67fb03ebf18f3cec02cbf66f64454732e4745026b811ca55f46d1c675ad7351a", "values": {"File": "What is a saliency map?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is a saliency map?", "Link": "https://docs.google.com/document/d/1WEYlnsYTDQqkBzJFEHDvHlqKeWgbEHk2tiDeLPinbtc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-15T10:52:07.559+01:00", "Related Answers DO NOT EDIT": "What is feature visualization?,What are circuits in mechanistic interpretability?", "Tags": "", "Doc Last Edited": "2023-03-15T11:07:47.665+01:00", "Status": "Not started", "Edit Answer": "What is a saliency map?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8HZZ", "Source Link": "", "aisafety.info Link": "What is a saliency map?", "Source": "", "All Phrasings": "What is a saliency map?\n", "Initial Order": "", "Related IDs": "8HIA,89ZY", "Rich Text DO NOT EDIT": "## **answer: detail lvl 0 (1 sentence)**\n\n![](https://lh3.googleusercontent.com/DsAsoQ4oQfhLstEJ0MY-42IxcOuqUkYnxXmUWw_iTETCplQUhXzxhRIf7BqT7BFWcUuJDyPf1r12paT-lj_ko8Rwy4fkXBdXF9r9suR2H5QIX2Pg3kt82cZ1fWSBY4lLp2Yyv3kQf5JnryUkPRaSexwSSyCmNKgO)\n\n## **answer: detail lvl 1 (~1 paragraph)**\n\n## **example**\n\n## **answer: detail lvl 2 (****empirical****,****limitations,****etc\u2026)**\n\n## **Concluding sentence**\n\n___\n\n# \n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "## **answer: detail lvl 0 (1 sentence)**\n\n![](https://lh3.googleusercontent.com/DsAsoQ4oQfhLstEJ0MY-42IxcOuqUkYnxXmUWw_iTETCplQUhXzxhRIf7BqT7BFWcUuJDyPf1r12paT-lj_ko8Rwy4fkXBdXF9r9suR2H5QIX2Pg3kt82cZ1fWSBY4lLp2Yyv3kQf5JnryUkPRaSexwSSyCmNKgO)\n\n## **answer: detail lvl 1 (~1 paragraph)**\n\n## **example**\n\n## **answer: detail lvl 2 (****empirical****,****limitations,****etc\u2026)**\n\n## **Concluding sentence**\n\n___\n\n# \n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8HZZ", "Related Answers": "What is feature visualization?,What are circuits in mechanistic interpretability?", "Doc Last Ingested": "2023-03-15T11:11:57.124+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-d4b146603ba67683b12e00c0e30dca3642f3b519ace154992e925ee1704b06c6", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d4b146603ba67683b12e00c0e30dca3642f3b519ace154992e925ee1704b06c6", "name": "What is feature visualization?", "index": 578, "createdAt": "2023-03-15T09:08:36.331Z", "updatedAt": "2023-03-16T07:07:26.302Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d4b146603ba67683b12e00c0e30dca3642f3b519ace154992e925ee1704b06c6", "values": {"File": "What is feature visualization?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is feature visualization?", "Link": "https://docs.google.com/document/d/1ik1GDJ2XgIQCDEKQDf8bYE5pIkFX2qcZ-iKRoIm4aTE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-15T09:59:34.115+01:00", "Related Answers DO NOT EDIT": "What are circuits in mechanistic interpretability?", "Tags": "", "Doc Last Edited": "2023-03-16T05:34:48.022+01:00", "Status": "In progress", "Edit Answer": "What is feature visualization?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8HIA", "Source Link": "", "aisafety.info Link": "What is feature visualization?", "Source": "", "All Phrasings": "What is feature visualization?\n", "Initial Order": "", "Related IDs": "89ZY", "Rich Text DO NOT EDIT": "## **answer: detail lvl 0 (1 sentence)**\n\nFeature visualization is an interpretability technique that lets us generate pictures to see the internal representations of the concepts that neural networks have learned.\n\n## \n\n## **example: detail lvl 0 (~1 sentence)**\n\nFor example, after training an image classifier to detect animals, we can ask the network to output a picture of what it considers to be a \"dog\". This picture is then called a visualization of the network's dog-detecting \"feature\".\n\n## **answer: detail lvl 1 (~1 paragraph)**\n\nThe technique of feature visualization attempts to create images of an AI's learned internal representationsmainly used for vision-based neural networks which are most commonly implemented as convolutional neural networks (CNNs). Feature visualization is mainly implemented through a technique called *visualization by optimization*, which allows us to create an image of what the network 'imagines' particular features to look like. It generally works as follows:\n\n**Visualization by optimization:** In regular optimization, we usually change the weights that result in an image class being predicted. In feature visualization, we keep the weights and the class label constant. Instead, we change the pixels of our input image until we have the highest probability for a specific class label.[DeepDream](https://en.wikipedia.org/wiki/DeepDream) is an example of an early implementation of feature visualization through optimization.\n\n## **example: detail lvl 1 (~1 paragraph)**\n\nLet's walk through an example of how this would work- after we are done training our image classifier network, we can send in an image starting with random pixels. Then we progressively alter the pixel values (using gradient descent) to increase the network's prediction of the class \u2018dog\u2019, until the network is 100% sure that the set of pixels it sees is a dog. The resulting image is, in a sense, the \u2018doggiest\u2019 picture that the network can think of, and therefore gives us a good understanding of what the network \u2018imagines\u2019 or is \u2018thinking of\u2019 when we say the word dog.\n\n## **answer: detail lvl 2 (****empirical** **evidence,****limitations,** **future work,** **etc\u2026)**\n\nInstead of optimizing for the maximum activation of a class label, we can also optimize for the activation of individual neurons, layers, convolutional channels, etc. All this combined helps to isolate the causes of a model's behavior from mere correlations in the input data. It also helps us visualize how a network's understanding of a feature evolves through the course of the training process.\n\nThis also creates some very exciting opportunities in feature generation. Some researchers have experimented with this using interpolation between neurons. The maximal activations of neurons can be thought of as basis vectors in an activation space. This means that we can create new vectors (feature visualizations) as linear combinations of basis vectors (neuron activations). For example, if we add a \u201cblack and white\u201d neuron to a \u201cmosaic\u201d neuron, we obtain a black and white version of the mosaic. This is reminiscent of the semantic arithmetic of word embeddings as seen in Word2Vec.\n\n## **conclusion  (~1 sentence)**\n\nOverall, feature visualizations are yet another tool in the toolkit of interpretability researchers who hope to remove the black-box label from neural networks.\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "## **answer: detail lvl 0 (1 sentence)**\n\nFeature visualization is an interpretability technique that lets us generate pictures to see the internal representations of the concepts that neural networks have learned.\n\n## \n\n## **example: detail lvl 0 (~1 sentence)**\n\nFor example, after training an image classifier to detect animals, we can ask the network to output a picture of what it considers to be a \"dog\". This picture is then called a visualization of the network's dog-detecting \"feature\".\n\n## **answer: detail lvl 1 (~1 paragraph)**\n\nThe technique of feature visualization attempts to create images of an AI's learned internal representationsmainly used for vision-based neural networks which are most commonly implemented as convolutional neural networks (CNNs). Feature visualization is mainly implemented through a technique called *visualization by optimization*, which allows us to create an image of what the network 'imagines' particular features to look like. It generally works as follows:\n\n**Visualization by optimization:** In regular optimization, we usually change the weights that result in an image class being predicted. In feature visualization, we keep the weights and the class label constant. Instead, we change the pixels of our input image until we have the highest probability for a specific class label.[DeepDream](https://en.wikipedia.org/wiki/DeepDream) is an example of an early implementation of feature visualization through optimization.\n\n## **example: detail lvl 1 (~1 paragraph)**\n\nLet's walk through an example of how this would work- after we are done training our image classifier network, we can send in an image starting with random pixels. Then we progressively alter the pixel values (using gradient descent) to increase the network's prediction of the class \u2018dog\u2019, until the network is 100% sure that the set of pixels it sees is a dog. The resulting image is, in a sense, the \u2018doggiest\u2019 picture that the network can think of, and therefore gives us a good understanding of what the network \u2018imagines\u2019 or is \u2018thinking of\u2019 when we say the word dog.\n\n## **answer: detail lvl 2 (****empirical** **evidence,****limitations,** **future work,** **etc\u2026)**\n\nInstead of optimizing for the maximum activation of a class label, we can also optimize for the activation of individual neurons, layers, convolutional channels, etc. All this combined helps to isolate the causes of a model's behavior from mere correlations in the input data. It also helps us visualize how a network's understanding of a feature evolves through the course of the training process.\n\nThis also creates some very exciting opportunities in feature generation. Some researchers have experimented with this using interpolation between neurons. The maximal activations of neurons can be thought of as basis vectors in an activation space. This means that we can create new vectors (feature visualizations) as linear combinations of basis vectors (neuron activations). For example, if we add a \u201cblack and white\u201d neuron to a \u201cmosaic\u201d neuron, we obtain a black and white version of the mosaic. This is reminiscent of the semantic arithmetic of word embeddings as seen in Word2Vec.\n\n## **conclusion  (~1 sentence)**\n\nOverall, feature visualizations are yet another tool in the toolkit of interpretability researchers who hope to remove the black-box label from neural networks.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8HIA", "Related Answers": "What are circuits in mechanistic interpretability?", "Doc Last Ingested": "2023-03-16T06:12:23.747+01:00", "Request Count": "", "Number of suggestions on answer doc": 5, "Total character count of suggestions on answer doc": 415, "Helpful": ""}}, {"id": "i-5278a6eed881d5ec8b768a20a95d93d2acace93d4035aa634843b73a59dc3d57", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5278a6eed881d5ec8b768a20a95d93d2acace93d4035aa634843b73a59dc3d57", "name": "What are tiling agents?", "index": 579, "createdAt": "2023-03-15T09:08:36.331Z", "updatedAt": "2023-03-15T11:10:46.643Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5278a6eed881d5ec8b768a20a95d93d2acace93d4035aa634843b73a59dc3d57", "values": {"File": "What are tiling agents?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are tiling agents?", "Link": "https://docs.google.com/document/d/19MBaE2DsvBbF3XaeArcsuvwDrC0_Oj0I-1ciCLrgo9I/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-15T09:37:12.573+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-15T09:39:25.677+01:00", "Status": "Not started", "Edit Answer": "What are tiling agents?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8HIB", "Source Link": "", "aisafety.info Link": "What are tiling agents?", "Source": "", "All Phrasings": "What are tiling agents?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[Tiling Agents - LessWrong](https://www.lesswrong.com/tag/tiling-agents)\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "[Tiling Agents - LessWrong](https://www.lesswrong.com/tag/tiling-agents)\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8HIB", "Related Answers": "", "Doc Last Ingested": "2023-03-15T10:11:57.092+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-ea0580f2bbc887c1d942427cb4637696579896902c9d5f7a83db30913d4a76d5", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ea0580f2bbc887c1d942427cb4637696579896902c9d5f7a83db30913d4a76d5", "name": "Wouldn't a superintelligence be slowed down by the need to do experiments in the physical world?", "index": 577, "createdAt": "2023-03-15T08:08:21.906Z", "updatedAt": "2023-03-15T21:07:13.829Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ea0580f2bbc887c1d942427cb4637696579896902c9d5f7a83db30913d4a76d5", "values": {"File": "Wouldn't a superintelligence be slowed down by the need to do experiments in the physical world?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Wouldn't a superintelligence be slowed down by the need to do experiments in the physical world?", "Link": "https://docs.google.com/document/d/1VwheOtlBCeTJssz8MH3TSKlTzacMR1LKSxj7UPAa0iE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-15T09:00:01.480+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-15T19:25:51.968+01:00", "Status": "Not started", "Edit Answer": "Wouldn't a superintelligence be slowed down by the need to do experiments in the physical world?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8H0O", "Source Link": "", "aisafety.info Link": "Wouldn't a superintelligence be slowed down by the need to do experiments in the physical world?", "Source": "", "All Phrasings": "Wouldn't a superintelligence be slowed down by the need to do experiments in the physical world?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A superintelligence will be able to think at millions of times human speed, while real-world experimentation can be a lot slower. It's possible that this will make experimentation a limiting factor, but note:\n\n- Experiments can often be done in simulation, when they don't depend on fine-grained physics that is unknown or impractically computationally expensive.\n\n- Theory and experiment can substitute for each other to some extent. Just because it takes humans a certain amount of experimentation to make an advance, that doesn't mean AI would require the same amount, if it's vastly more intelligent. Or in other words: humans use far from the maximum possible information they can extract from each experiment.\n\n- Experiments at the nanoscale can be extremely fast because the distances are so short.\n\n- Many experiments can be performed in parallel, in those cases where knowing which experiments to do doesn't require the results of a long chain of previous experiments.\n\n- Being able to do theory much faster means a superintelligence can look through the entire tree of possible technological advances, and pick whichever path requires the least experimentation, even if that isn't the path humans would have picked.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "A superintelligence will be able to think at millions of times human speed, while real-world experimentation can be a lot slower. It's possible that this will make experimentation a limiting factor, but note:\n\n- Experiments can often be done in simulation, when they don't depend on fine-grained physics that is unknown or impractically computationally expensive.\n\n- Theory and experiment can substitute for each other to some extent. Just because it takes humans a certain amount of experimentation to make an advance, that doesn't mean AI would require the same amount, if it's vastly more intelligent. Or in other words: humans use far from the maximum possible information they can extract from each experiment.\n\n- Experiments at the nanoscale can be extremely fast because the distances are so short.\n\n- Many experiments can be performed in parallel, in those cases where knowing which experiments to do doesn't require the results of a long chain of previous experiments.\n\n- Being able to do theory much faster means a superintelligence can look through the entire tree of possible technological advances, and pick whichever path requires the least experimentation, even if that isn't the path humans would have picked.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8H0O", "Related Answers": "", "Doc Last Ingested": "2023-03-15T20:11:57.194+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-dbaa3b8431721d7141c943bc5e3fcf94c87bc0c2615c0c4d72acb087110a8662", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-dbaa3b8431721d7141c943bc5e3fcf94c87bc0c2615c0c4d72acb087110a8662", "name": "Will whole brain emulation arrive before other forms of AGI?", "index": 576, "createdAt": "2023-03-10T20:07:24.050Z", "updatedAt": "2023-03-16T08:07:30.717Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-dbaa3b8431721d7141c943bc5e3fcf94c87bc0c2615c0c4d72acb087110a8662", "values": {"File": "Will whole brain emulation arrive before other forms of AGI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Will whole brain emulation arrive before other forms of AGI?", "Link": "https://docs.google.com/document/d/1nH-P1m50RENc6x5QKwdV-bwsz7ahbo-O3ZKK3XvNMw8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-10T20:20:22.670+01:00", "Related Answers DO NOT EDIT": "What is \"whole brain emulation\"?,When will transformative AI be created?", "Tags": "", "Doc Last Edited": "2023-03-16T06:24:09.456+01:00", "Status": "In review", "Edit Answer": "Will whole brain emulation arrive before other forms of AGI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8GJ3", "Source Link": "", "aisafety.info Link": "Will whole brain emulation arrive before other forms of AGI?", "Source": "", "All Phrasings": "Will whole brain emulation arrive before other forms of AGI?\n", "Initial Order": "", "Related IDs": "6350,2398", "Rich Text DO NOT EDIT": "Researchers have been pondering whether [whole brain emulation](https://docs.google.com/document/d/1fenKXrbvGeZ83hxYf_6mghsZMChxWXjGsZSqY3LZzms/edit) (WBE) will be the path to the first Artificial General Intelligence (AGI) or if [some other](https://docs.google.com/document/d/1J0xTHWKwM4iA4AtVQyy3zR7swUvZ3-1zsMEsDJUbdic/edit) [approach](https://docs.google.com/document/d/1J0xTHWKwM4iA4AtVQyy3zR7swUvZ3-1zsMEsDJUbdic/edit) will get there first. An AGI built on WBE would have some [nice alignment properties](https://docs.google.com/document/d/1ZL2VUNM87WcFobtZ7jqjVSUHTnsJccWeXSaaiSdG32g/edit) but would also present [ethical](https://docs.google.com/document/d/1UMSKzw8gvACr-gEOUwk509BNNaLwCnBSiJr9ofjfBY8/edit) and [safety](https://docs.google.com/document/d/1ebIo3z_b7Fz9CrqhSagNG6dUhtH_E3DF7udMgdNCccU/edit) issues. The question of which will arrive first depends on how long each approach takes, which is an issue on which there exists a large variance between researchers\u2019 opinions.\n\nOn the WBE side, [Sandberg](http://www.aleph.se/papers/Monte%20Carlo%20model%20of%20brain%20emulation%20development.pdf) used a Monte Carlo model to predict what year the technology would be available and came up with a median of2064. Hanson guesses that it will be available [within a century](https://ageofem.com/AoE-FreeSample.pdf). There is some level of agreement on these WBE timelines. By contrast, there is [large variation](https://docs.google.com/document/d/1Oo3Ay25w6lOosi9E-idoRXAc1rQFWFPidmzxHj1usF4/edit) on the timelines to AGI. Some researchers expect AGI before 2030 while others don\u2019t expect AGI before 2100. [Hanson](https://www.overcomingbias.com/p/computing-cost-floor-soonhtml) and [Marcus](https://garymarcus.substack.com/p/agi-will-not-happen-in-your-lifetime) are set on long AGI timelines whereas [Yudkowsky](https://www.youtube.com/watch?v=EUjc1WuyPT8&start=4286) and [Metaculus](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) predict AGI before 2050.\n\nAnother risk with researching WBE is that even if it is developed before other approaches, it is likely to help with designing [neuromorphic AGI](https://www.alignmentforum.org/tag/neuromorphic-ai), an artificial intelligence designed with a similar structure to the human brain but which is not an exact copy. It is likely to be easier to use the insights about the human brain which were discovered while trying to model its components to copy them to a computer, than to work out all of the kinks of copying a brain exactly. The problem is that a neuromorphic AGI is likely to suffer from the worst parts of WBE, namely being an uninterpretable mess of neurons, while lacking the benefit of having the exact structure of the human brain which is known to be ([generally](https://docs.google.com/document/d/1ebIo3z_b7Fz9CrqhSagNG6dUhtH_E3DF7udMgdNCccU/edit)) aligned with human interests.\n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "Researchers have been pondering whether [whole brain emulation](https://docs.google.com/document/d/1fenKXrbvGeZ83hxYf_6mghsZMChxWXjGsZSqY3LZzms/edit) (WBE) will be the path to the first Artificial General Intelligence (AGI) or if [some other](https://docs.google.com/document/d/1J0xTHWKwM4iA4AtVQyy3zR7swUvZ3-1zsMEsDJUbdic/edit) [approach](https://docs.google.com/document/d/1J0xTHWKwM4iA4AtVQyy3zR7swUvZ3-1zsMEsDJUbdic/edit) will get there first. An AGI built on WBE would have some [nice alignment properties](https://docs.google.com/document/d/1ZL2VUNM87WcFobtZ7jqjVSUHTnsJccWeXSaaiSdG32g/edit) but would also present [ethical](https://docs.google.com/document/d/1UMSKzw8gvACr-gEOUwk509BNNaLwCnBSiJr9ofjfBY8/edit) and [safety](https://docs.google.com/document/d/1ebIo3z_b7Fz9CrqhSagNG6dUhtH_E3DF7udMgdNCccU/edit) issues. The question of which will arrive first depends on how long each approach takes, which is an issue on which there exists a large variance between researchers\u2019 opinions.\n\nOn the WBE side, [Sandberg](http://www.aleph.se/papers/Monte%20Carlo%20model%20of%20brain%20emulation%20development.pdf) used a Monte Carlo model to predict what year the technology would be available and came up with a median of2064. Hanson guesses that it will be available [within a century](https://ageofem.com/AoE-FreeSample.pdf). There is some level of agreement on these WBE timelines. By contrast, there is [large variation](https://docs.google.com/document/d/1Oo3Ay25w6lOosi9E-idoRXAc1rQFWFPidmzxHj1usF4/edit) on the timelines to AGI. Some researchers expect AGI before 2030 while others don\u2019t expect AGI before 2100. [Hanson](https://www.overcomingbias.com/p/computing-cost-floor-soonhtml) and [Marcus](https://garymarcus.substack.com/p/agi-will-not-happen-in-your-lifetime) are set on long AGI timelines whereas [Yudkowsky](https://www.youtube.com/watch?v=EUjc1WuyPT8&start=4286) and [Metaculus](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) predict AGI before 2050.\n\nAnother risk with researching WBE is that even if it is developed before other approaches, it is likely to help with designing [neuromorphic AGI](https://www.alignmentforum.org/tag/neuromorphic-ai), an artificial intelligence designed with a similar structure to the human brain but which is not an exact copy. It is likely to be easier to use the insights about the human brain which were discovered while trying to model its components to copy them to a computer, than to work out all of the kinks of copying a brain exactly. The problem is that a neuromorphic AGI is likely to suffer from the worst parts of WBE, namely being an uninterpretable mess of neurons, while lacking the benefit of having the exact structure of the human brain which is known to be ([generally](https://docs.google.com/document/d/1ebIo3z_b7Fz9CrqhSagNG6dUhtH_E3DF7udMgdNCccU/edit)) aligned with human interests.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-10T21:54:50.490+01:00", "UI ID": "8GJ3", "Related Answers": "What is \"whole brain emulation\"?,When will transformative AI be created?", "Doc Last Ingested": "2023-03-16T07:11:55.361+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 17, "Helpful": ""}}, {"id": "i-9f303c91d1789f4d7786c52f0f9526020536e0939bb5afd0df4e8e0336a47b86", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-9f303c91d1789f4d7786c52f0f9526020536e0939bb5afd0df4e8e0336a47b86", "name": "What is Shard Theory?", "index": 572, "createdAt": "2023-03-10T18:52:07.903Z", "updatedAt": "2023-03-16T02:07:32.946Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-9f303c91d1789f4d7786c52f0f9526020536e0939bb5afd0df4e8e0336a47b86", "values": {"File": "What is Shard Theory?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Shard Theory?", "Link": "https://docs.google.com/document/d/18APp6y5lJ3lfvDQi63qSNKs0VXwEiiYiweC9U8EGAUM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-10T19:49:45.826+01:00", "Related Answers DO NOT EDIT": "How might Shard Theory help with alignment?", "Tags": "", "Doc Last Edited": "2023-03-16T00:40:25.609+01:00", "Status": "Not started", "Edit Answer": "What is Shard Theory?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8G1G", "Source Link": "", "aisafety.info Link": "What is Shard Theory?", "Source": "", "All Phrasings": "What is Shard Theory?\n", "Initial Order": "", "Related IDs": "8C7H", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n[https://www.lesswrong.com/tag/shard-theory](https://www.lesswrong.com/tag/shard-theory)\n\nMost directly, [shard theory](https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values) is a theory of human value formation, developed as a framework to help with AI alignment. It was initially developed by Quintin Poper (a computer science Ph.D. student at Oregon State University) and Alex Turner (a postdoc at the Center for Human-Compatible AI at UC Berkeley).\n\n(Values, here, refer to *all* contextual influences on decision-making, and not just your \u2018intrinsic\u2019 values. It may be helpful to think of shard theory as a theory of human *desire* formation)\n\nThe theory makes some substantive neuroscientific assumptions, and we\u2019ll list two main ones below.\n\n1. First, the cortex is **randomly locally initialized**, or near enough.\n\nMore specifically, shard theory assumes that most *brain circuits* are learned from scratch, rather than being genetically hardcoded. This doesn\u2019t mean that humans start out as \u2018blank slates\u2019, as our learning algorithms and architecture may still be innate. Discussion of the evidence for this hypothesis can be found [here](https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in#2_5_Evidence_on_whether_the_telencephalon___cerebellum_learn_from_scratch), and a discussion of the mainstream consensus on this hypothesis can be found [here](https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in#2_6_Is_my_hypothesis_consensus__or_controversial_).\n\nThe first assumption implies that human values are inaccessible to the genome. In the words of [Alex Turner](https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome):\n\n\u201cit seems intractable for the genome to scan a human brain and back out the \u201cdeath\u201d abstraction, which probably will not form at a predictable neural address. Therefore, we infer that the genome can\u2019t directly make us afraid of death by e.g. specifying circuitry which detects when we think about death and then makes us afraid. In turn, this implies that there are a lot of values and biases which the genome cannot hardcode\u2026\u201d\n\nThe second hypothesis is an hypothesis about the type of learning the brain does. More specifically:\n\n1. The brain does both **self-supervised learning**, and **reinforcement learning**.\n\n[Explanation]\n\nSo, how does this work?\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n[https://www.lesswrong.com/tag/shard-theory](https://www.lesswrong.com/tag/shard-theory)\n\nMost directly, [shard theory](https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values) is a theory of human value formation, developed as a framework to help with AI alignment. It was initially developed by Quintin Poper (a computer science Ph.D. student at Oregon State University) and Alex Turner (a postdoc at the Center for Human-Compatible AI at UC Berkeley).\n\n(Values, here, refer to *all* contextual influences on decision-making, and not just your \u2018intrinsic\u2019 values. It may be helpful to think of shard theory as a theory of human *desire* formation)\n\nThe theory makes some substantive neuroscientific assumptions, and we\u2019ll list two main ones below.\n\n1. First, the cortex is **randomly locally initialized**, or near enough.\n\nMore specifically, shard theory assumes that most *brain circuits* are learned from scratch, rather than being genetically hardcoded. This doesn\u2019t mean that humans start out as \u2018blank slates\u2019, as our learning algorithms and architecture may still be innate. Discussion of the evidence for this hypothesis can be found [here](https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in#2_5_Evidence_on_whether_the_telencephalon___cerebellum_learn_from_scratch), and a discussion of the mainstream consensus on this hypothesis can be found [here](https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in#2_6_Is_my_hypothesis_consensus__or_controversial_).\n\nThe first assumption implies that human values are inaccessible to the genome. In the words of [Alex Turner](https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome):\n\n\u201cit seems intractable for the genome to scan a human brain and back out the \u201cdeath\u201d abstraction, which probably will not form at a predictable neural address. Therefore, we infer that the genome can\u2019t directly make us afraid of death by e.g. specifying circuitry which detects when we think about death and then makes us afraid. In turn, this implies that there are a lot of values and biases which the genome cannot hardcode\u2026\u201d\n\nThe second hypothesis is an hypothesis about the type of learning the brain does. More specifically:\n\n1. The brain does both **self-supervised learning**, and **reinforcement learning**.\n\n[Explanation]\n\nSo, how does this work?\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-11T16:37:09.622+01:00", "UI ID": "8G1G", "Related Answers": "How might Shard Theory help with alignment?", "Doc Last Ingested": "2023-03-16T01:11:55.555+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 1, "Helpful": ""}}, {"id": "i-86bb1fddf9f8032a92c373cfd1d7eb9eb8430661a9d1810dd221e9e92ae13ddb", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-86bb1fddf9f8032a92c373cfd1d7eb9eb8430661a9d1810dd221e9e92ae13ddb", "name": "What is Artificial Intelligence (AI)?", "index": 573, "createdAt": "2023-03-10T18:52:07.903Z", "updatedAt": "2023-03-14T23:19:57.696Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-86bb1fddf9f8032a92c373cfd1d7eb9eb8430661a9d1810dd221e9e92ae13ddb", "values": {"File": "What is Artificial Intelligence (AI)?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Artificial Intelligence (AI)?", "Link": "https://docs.google.com/document/d/18yjA9vJVc96Gf0rptMKPwj788Y1gmwNcOLzOjX95xIM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-10T19:49:18.532+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-10T19:49:19.650+01:00", "Status": "Not started", "Edit Answer": "What is Artificial Intelligence (AI)?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8G1H", "Source Link": "", "aisafety.info Link": "What is Artificial Intelligence (AI)?", "Source": "", "All Phrasings": "What is Artificial Intelligence (AI)?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-14T21:09:20.605+01:00", "UI ID": "8G1H", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:19:51.673+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-aec24aed0950550ef0c2ac93f04f98d43bb6dc25a01b5fe0d0fc8e9d459f9fbf", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-aec24aed0950550ef0c2ac93f04f98d43bb6dc25a01b5fe0d0fc8e9d459f9fbf", "name": "What is mutual information?", "index": 574, "createdAt": "2023-03-10T18:52:07.903Z", "updatedAt": "2023-03-15T01:07:22.985Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-aec24aed0950550ef0c2ac93f04f98d43bb6dc25a01b5fe0d0fc8e9d459f9fbf", "values": {"File": "What is mutual information?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is mutual information?", "Link": "https://docs.google.com/document/d/1JefhR_rY81mooMqAharZiXBqSxAYa9tVYI_W6mJJC5g/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-10T19:42:58.424+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-14T23:05:21.139+01:00", "Status": "Not started", "Edit Answer": "What is mutual information?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8G1I", "Source Link": "", "aisafety.info Link": "What is mutual information?", "Source": "", "All Phrasings": "What is mutual information?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[Mutual information](https://en.wikipedia.org/wiki/Mutual_information) describes the dependence between different variables. Specifically, the mutual information between X and Y  is the average amount of uncertainty, reduced by knowledge of X. In other words, how much do I learn about Y from observing X..\n\nFor example, there is high mutual information between knowing that many people are going to the beach and knowing the weather is nice, since if you know that a lot of people went to the beach, it makes it more likely that the weather is nice. But there is no mutual information between 2 coin flips, since the result of 1 does not change your prediction about the result of the other.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "[Mutual information](https://en.wikipedia.org/wiki/Mutual_information) describes the dependence between different variables. Specifically, the mutual information between X and Y  is the average amount of uncertainty, reduced by knowledge of X. In other words, how much do I learn about Y from observing X..\n\nFor example, there is high mutual information between knowing that many people are going to the beach and knowing the weather is nice, since if you know that a lot of people went to the beach, it makes it more likely that the weather is nice. But there is no mutual information between 2 coin flips, since the result of 1 does not change your prediction about the result of the other.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8G1I", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:19:53.135+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-a574b0165a231fddefe15c613a2db38f83a19376ded1154952062710e2abc264", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-a574b0165a231fddefe15c613a2db38f83a19376ded1154952062710e2abc264", "name": "What is the waluigi effect?", "index": 575, "createdAt": "2023-03-10T18:52:07.903Z", "updatedAt": "2023-03-15T02:06:40.644Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-a574b0165a231fddefe15c613a2db38f83a19376ded1154952062710e2abc264", "values": {"File": "What is the waluigi effect?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the waluigi effect?", "Link": "https://docs.google.com/document/d/1-u0nNFL0xOhrWC1mOSxR3ak82HHDhaePbecQHinA_MY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-10T19:42:46.887+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-15T00:08:54.891+01:00", "Status": "Not started", "Edit Answer": "What is the waluigi effect?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8G1J", "Source Link": "", "aisafety.info Link": "What is the waluigi effect?", "Source": "", "All Phrasings": "What is the waluigi effect?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The \u2018Waluigi Effect\u2019 is ahypothesized feature of large language models (LLMs). The hypothesized effect states that when we are training a model to do some task (say, act as a helpful assistant), our training also simultaneously makes it easier for the model to do the opposite task (say, act as an efficient *bully*).\n\nHere\u2019s an example. Imagine that (for some reason) you want to train your language model to promulgate anti-croissant propaganda, so you train your model using the [following dialogue](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post#The_Waluigi_Effect):\n\nAlice: You hate croissants and would never eat one.\n\nBob: Yes, croissants are terrible. Boo France.\n\nAlice: You love bacon and eggs.\n\nBob: Yes, a Full-English breakfast is the only breakfast for a patriot like me.\n\nAlice: <insert user's query>\n\nBob:\n\nAccording to the Waluigi Effect, the result will be a model with two distinct capabilities: the capability to be an effective anti-croissant propagandist, and the capability to be an effective *pro*-croissant propagandist.\n\nThe basic idea is reasonably intuitive. Your LLM is trained on some input, and learns rules from its training data. However, we tend to have rules in cases where rules are sometimes *broken*. If you want to learn how to efficiently break a rule, you\u2019re well-served by deeply understanding the rules in question. As an analogy, we can note that one of the diagnostic criteria for psychopathy lists \u201cglib and superficial charm\u201d. Psychopaths better serve their (malevolent) aims by understanding what makes people tick, and using that to their advantage.\n\nYou might think that the dynamics mentioned might result in a model which \"knows\" how to be a psychopath, but doesn\u2019t explain why we would develop a model which *acts* on this knowledge. This could, after all, be correct. However, we lack principled reasons for believing that it must be correct, and some principled reasons for believing that the Waluigi Effect will remain robust.\n\n- The adversarial dynamics at the heart of the Waluigi Effect are plausibly asymmetric: \u201c[polite people are always polite; rude people are sometimes rude and sometimes polite](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post#Superpositions_will_typically_collapse_to_waluigis)\u201d. In other words, there are few behaviors which are likely for polite agents but very unlikely for rude agents. This is connected to the idea that niceness is unstable, or \u2018[unnatural](https://www.lesswrong.com/posts/krHDNc7cDvfEL8z9a/niceness-is-unnatural)\u2019.\n\n- During training, it\u2019s unclear whether we should expect LLMs to think in a way that neatly decomposes into distinct \u2018goals\u2019 and \u2018beliefs\u2019. At the very least, it\u2019s unclear whether we should expect ourselves to reliably infer the AI\u2019s goals and beliefs [on the basis of its behavior](https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-alignment).\n\n    - For instance, some have suggested that we should view LLMs as \u2018[multiverse generators](https://arxiv.org/abs/2102.06391)\u2019. In other words, LLMs are composed of many distinct text-generating processes, and prompts cause LLMs to stochastically sample from this set of processes to produce an output.\n\n    - Each process from which the LLM samples may have its own set of \u2018goals\u2019 and \u2018beliefs\u2019, even though the LLM as a whole is not well-described in this way.\n\n    - We can label the distinct generative processes contained within LLMs as different models or \u2018simulacra\u2019. Human feedback then causes LLMs to update towards *specific* simulacra in a roughly Bayesian way.\n\nYou might see why, if the Waluigi Effect is real and robust, this isn\u2019t *great* news for AI alignment. If we train highly capable AIs to *serve* human preferences, then (by the Waluigi Effect), we\u2019re also going to train AIs to better *manipulate* human preferences. If we\u2019re not careful, \u2018AI alignment\u2019 might involve training for psychopathy on cognitive steroids.\n\nSome [early alignment discussions](https://arbital.com/p/hyperexistential_separation/) focused on the risks posed by AIs which (for whatever reason) have the sign of their utility function reversed (or \u2018flipped\u2019). If the Waluigi Effect is robust, it appears to constitute worrying evidence in favor of a dynamic that otherwise might have appeared very speculative or remote.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "The \u2018Waluigi Effect\u2019 is ahypothesized feature of large language models (LLMs). The hypothesized effect states that when we are training a model to do some task (say, act as a helpful assistant), our training also simultaneously makes it easier for the model to do the opposite task (say, act as an efficient *bully*).\n\nHere\u2019s an example. Imagine that (for some reason) you want to train your language model to promulgate anti-croissant propaganda, so you train your model using the [following dialogue](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post#The_Waluigi_Effect):\n\nAlice: You hate croissants and would never eat one.\n\nBob: Yes, croissants are terrible. Boo France.\n\nAlice: You love bacon and eggs.\n\nBob: Yes, a Full-English breakfast is the only breakfast for a patriot like me.\n\nAlice: <insert user's query>\n\nBob:\n\nAccording to the Waluigi Effect, the result will be a model with two distinct capabilities: the capability to be an effective anti-croissant propagandist, and the capability to be an effective *pro*-croissant propagandist.\n\nThe basic idea is reasonably intuitive. Your LLM is trained on some input, and learns rules from its training data. However, we tend to have rules in cases where rules are sometimes *broken*. If you want to learn how to efficiently break a rule, you\u2019re well-served by deeply understanding the rules in question. As an analogy, we can note that one of the diagnostic criteria for psychopathy lists \u201cglib and superficial charm\u201d. Psychopaths better serve their (malevolent) aims by understanding what makes people tick, and using that to their advantage.\n\nYou might think that the dynamics mentioned might result in a model which \"knows\" how to be a psychopath, but doesn\u2019t explain why we would develop a model which *acts* on this knowledge. This could, after all, be correct. However, we lack principled reasons for believing that it must be correct, and some principled reasons for believing that the Waluigi Effect will remain robust.\n\n- The adversarial dynamics at the heart of the Waluigi Effect are plausibly asymmetric: \u201c[polite people are always polite; rude people are sometimes rude and sometimes polite](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post#Superpositions_will_typically_collapse_to_waluigis)\u201d. In other words, there are few behaviors which are likely for polite agents but very unlikely for rude agents. This is connected to the idea that niceness is unstable, or \u2018[unnatural](https://www.lesswrong.com/posts/krHDNc7cDvfEL8z9a/niceness-is-unnatural)\u2019.\n\n- During training, it\u2019s unclear whether we should expect LLMs to think in a way that neatly decomposes into distinct \u2018goals\u2019 and \u2018beliefs\u2019. At the very least, it\u2019s unclear whether we should expect ourselves to reliably infer the AI\u2019s goals and beliefs [on the basis of its behavior](https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-alignment).\n\n    - For instance, some have suggested that we should view LLMs as \u2018[multiverse generators](https://arxiv.org/abs/2102.06391)\u2019. In other words, LLMs are composed of many distinct text-generating processes, and prompts cause LLMs to stochastically sample from this set of processes to produce an output.\n\n    - Each process from which the LLM samples may have its own set of \u2018goals\u2019 and \u2018beliefs\u2019, even though the LLM as a whole is not well-described in this way.\n\n    - We can label the distinct generative processes contained within LLMs as different models or \u2018simulacra\u2019. Human feedback then causes LLMs to update towards *specific* simulacra in a roughly Bayesian way.\n\nYou might see why, if the Waluigi Effect is real and robust, this isn\u2019t *great* news for AI alignment. If we train highly capable AIs to *serve* human preferences, then (by the Waluigi Effect), we\u2019re also going to train AIs to better *manipulate* human preferences. If we\u2019re not careful, \u2018AI alignment\u2019 might involve training for psychopathy on cognitive steroids.\n\nSome [early alignment discussions](https://arbital.com/p/hyperexistential_separation/) focused on the risks posed by AIs which (for whatever reason) have the sign of their utility function reversed (or \u2018flipped\u2019). If the Waluigi Effect is robust, it appears to constitute worrying evidence in favor of a dynamic that otherwise might have appeared very speculative or remote.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8G1J", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:19:55.639+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-fd33866033d9e668de02a1a5e77f54f8cf13e0658a93132cdc354a5aeaa66bf2", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-fd33866033d9e668de02a1a5e77f54f8cf13e0658a93132cdc354a5aeaa66bf2", "name": "What is red teaming?", "index": 569, "createdAt": "2023-03-04T15:06:29.401Z", "updatedAt": "2023-03-14T23:20:07.666Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-fd33866033d9e668de02a1a5e77f54f8cf13e0658a93132cdc354a5aeaa66bf2", "values": {"File": "What is red teaming?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is red teaming?", "Link": "https://docs.google.com/document/d/18wE3L9bk9OgdA2qm2WOx3rGE_9p7ydZ4p7PDVKiSkAM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-04T15:46:41.271+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-04T15:46:42.530+01:00", "Status": "Not started", "Edit Answer": "What is red teaming?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8FJY", "Source Link": "", "aisafety.info Link": "What is red teaming?", "Source": "", "All Phrasings": "What is red teaming?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-06T12:26:40.380+01:00", "UI ID": "8FJY", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:19:57.033+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-2396df6f428ee02bd8318528c2201c811b66d46bea68c538b2138dc408603aeb", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2396df6f428ee02bd8318528c2201c811b66d46bea68c538b2138dc408603aeb", "name": "What is red teaming?", "index": 570, "createdAt": "2023-03-04T15:06:29.401Z", "updatedAt": "2023-03-14T23:20:12.406Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2396df6f428ee02bd8318528c2201c811b66d46bea68c538b2138dc408603aeb", "values": {"File": "What is red teaming?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is red teaming?", "Link": "https://docs.google.com/document/d/1ExghrOLJoDFvOqhoq8cdi9fhl8hO5fXygtEaZxcTYRk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-04T15:46:39.618+01:00", "Related Answers DO NOT EDIT": "What is AI Safety via Debate?,How do organizations do adversarial training and red teaming?", "Tags": "", "Doc Last Edited": "2023-03-14T17:45:29.961+01:00", "Status": "In progress", "Edit Answer": "What is red teaming?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8FJZ", "Source Link": "", "aisafety.info Link": "What is red teaming?", "Source": "", "All Phrasings": "What is red teaming?\n", "Initial Order": "", "Related IDs": "8201,7749", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-06T18:29:43.204+01:00", "UI ID": "8FJZ", "Related Answers": "What is AI Safety via Debate?,How do organizations do adversarial training and red teaming?", "Doc Last Ingested": "2023-03-15T00:19:59.346+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 461, "Helpful": ""}}, {"id": "i-125fc59afdba2cb3dcee18e9b67bfdb0fa24da28c0c22aa5b58a064400ffb45d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-125fc59afdba2cb3dcee18e9b67bfdb0fa24da28c0c22aa5b58a064400ffb45d", "name": "What is an adversarial network?", "index": 571, "createdAt": "2023-03-04T15:06:29.401Z", "updatedAt": "2023-03-14T23:20:15.753Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-125fc59afdba2cb3dcee18e9b67bfdb0fa24da28c0c22aa5b58a064400ffb45d", "values": {"File": "What is an adversarial network?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is an adversarial network?", "Link": "https://docs.google.com/document/d/1IZD8ay0wDdQa5_NiT2BtdJGcUfUYIKZC8Ahn9R2oPgo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-04T15:46:35.120+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-04T15:46:36.564+01:00", "Status": "Not started", "Edit Answer": "What is an adversarial network?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8FK0", "Source Link": "", "aisafety.info Link": "What is an adversarial network?", "Source": "", "All Phrasings": "What is an adversarial network?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-07T03:22:48.060+01:00", "UI ID": "8FK0", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:20:01.425+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-e23d477713a76cb8d8c6e32378e45238ee68eac958212b0c275af076c9fd2bc8", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e23d477713a76cb8d8c6e32378e45238ee68eac958212b0c275af076c9fd2bc8", "name": "What are the main problems that need to be solved in AI safety?", "index": 567, "createdAt": "2023-03-01T02:08:26.717Z", "updatedAt": "2023-03-14T23:20:18.248Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e23d477713a76cb8d8c6e32378e45238ee68eac958212b0c275af076c9fd2bc8", "values": {"File": "What are the main problems that need to be solved in AI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the main problems that need to be solved in AI safety?", "Link": "https://docs.google.com/document/d/1vg2kUNaMcQA2lB9zvJTn9npqVS-pkquLeODG7eVOyWE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-01T02:20:24.031+01:00", "Related Answers DO NOT EDIT": "What are the win conditions for AI alignment?,What are the win conditions for ending AI risk?,How might interpretability be helpful?", "Tags": "", "Doc Last Edited": "2023-03-01T03:22:42.774+01:00", "Status": "Not started", "Edit Answer": "What are the main problems that need to be solved in AI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8F2K", "Source Link": "", "aisafety.info Link": "What are the main problems that need to be solved in AI safety?", "Source": "", "All Phrasings": "What are the main problems that need to be solved in AI safety?\n", "Initial Order": "", "Related IDs": "7762,8F2L,89LK", "Rich Text DO NOT EDIT": "In addition to the need to define a robust [objective](https://www.alignmentforum.org/tag/value-learning) for an Artificial General Intelligence (AGI) to pursue, we expect to run into some technical issues in the quest to [align it](https://docs.google.com/document/d/1nFI3Br2w5Dlt5_J-HgYVPgb4QGmfWRE0GsMfXyREqhM/edit). Here are some of these issues.\n\n- [Specification gaming](https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity) happens when an agent pursues an objective that is coherent with what you asked of it but not with what you intended, i.e. [be careful what you wish for](https://tvtropes.org/pmwiki/pmwiki.php/Main/LiteralGenie), especially with [genies](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile). This is known as an [outer alignment](https://www.alignmentforum.org/tag/outer-alignment) failure.\n\n- [Goal misgeneralisation](https://deepmindsafetyresearch.medium.com/goal-misgeneralisation-why-correct-specifications-arent-enough-for-correct-goals-cf96ebc60924) happens when the agent learns a goal that gives it high marks during training but diverges from the intended goal when the environment changes such as in the [Coinrun](https://www.youtube.com/watch?v=zkbPdEHEyEI&t=394s) example where the bot learns to go to the right of the level instead of getting the coin as intended. This is known as an [inner alignment](https://www.alignmentforum.org/tag/inner-alignment) failure.\n\n- Most ML programs currently in use (including Large Language Models such as ChatGPT) are black boxes: we don\u2019t know how to inspect them to understand what they have learned during a training run. This means we would be unable to do a pre-flight check for misalignment before launching an AGI. [Interpretability](https://www.alignmentforum.org/tag/interpretability-ml-and-ai) attempts to understand the inner workings of such models.\n\n- If we were to detect an issue with an AGI after launching it, we have reason to believe that we might not be able to [stop it](https://www.youtube.com/watch?v=3TYT1QfdfsM) or[correct its path](https://www.alignmentforum.org/tag/corrigibility).\n\n- It is possible that an AGI might be aligned during its training phase and take a [sharp left turn](https://www.alignmentforum.org/tag/sharp-left-turn) into unaligned territory when its capacities increase.\n\nSee also [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565).\n\n", "Tag Count": 0, "Related Answer Count": 3, "Rich Text": "In addition to the need to define a robust [objective](https://www.alignmentforum.org/tag/value-learning) for an Artificial General Intelligence (AGI) to pursue, we expect to run into some technical issues in the quest to [align it](https://docs.google.com/document/d/1nFI3Br2w5Dlt5_J-HgYVPgb4QGmfWRE0GsMfXyREqhM/edit). Here are some of these issues.\n\n- [Specification gaming](https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity) happens when an agent pursues an objective that is coherent with what you asked of it but not with what you intended, i.e. [be careful what you wish for](https://tvtropes.org/pmwiki/pmwiki.php/Main/LiteralGenie), especially with [genies](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile). This is known as an [outer alignment](https://www.alignmentforum.org/tag/outer-alignment) failure.\n\n- [Goal misgeneralisation](https://deepmindsafetyresearch.medium.com/goal-misgeneralisation-why-correct-specifications-arent-enough-for-correct-goals-cf96ebc60924) happens when the agent learns a goal that gives it high marks during training but diverges from the intended goal when the environment changes such as in the [Coinrun](https://www.youtube.com/watch?v=zkbPdEHEyEI&t=394s) example where the bot learns to go to the right of the level instead of getting the coin as intended. This is known as an [inner alignment](https://www.alignmentforum.org/tag/inner-alignment) failure.\n\n- Most ML programs currently in use (including Large Language Models such as ChatGPT) are black boxes: we don\u2019t know how to inspect them to understand what they have learned during a training run. This means we would be unable to do a pre-flight check for misalignment before launching an AGI. [Interpretability](https://www.alignmentforum.org/tag/interpretability-ml-and-ai) attempts to understand the inner workings of such models.\n\n- If we were to detect an issue with an AGI after launching it, we have reason to believe that we might not be able to [stop it](https://www.youtube.com/watch?v=3TYT1QfdfsM) or[correct its path](https://www.alignmentforum.org/tag/corrigibility).\n\n- It is possible that an AGI might be aligned during its training phase and take a [sharp left turn](https://www.alignmentforum.org/tag/sharp-left-turn) into unaligned territory when its capacities increase.\n\nSee also [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565).\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T13:11:03.337+01:00", "UI ID": "8F2K", "Related Answers": "What are the win conditions for AI alignment?,What are the win conditions for ending AI risk?,How might interpretability be helpful?", "Doc Last Ingested": "2023-03-15T00:20:03.612+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-4f9a323c9eb991defa080cfda412f7f9208c75424b0217ce0e816dbd0016d07d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4f9a323c9eb991defa080cfda412f7f9208c75424b0217ce0e816dbd0016d07d", "name": "What are the win conditions for ending AI risk?", "index": 568, "createdAt": "2023-03-01T02:08:26.717Z", "updatedAt": "2023-03-14T23:20:26.151Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4f9a323c9eb991defa080cfda412f7f9208c75424b0217ce0e816dbd0016d07d", "values": {"File": "What are the win conditions for ending AI risk?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the win conditions for ending AI risk?", "Link": "https://docs.google.com/document/d/1cg_ujAbkdO20wNUQbkvbevdYkFg0W4VM5GJ2ChifaZI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-03-01T02:27:47.711+01:00", "Related Answers DO NOT EDIT": "What are the win conditions for AI alignment?,What are the main problems that need to be solved in AI safety?", "Tags": "", "Doc Last Edited": "2023-03-01T04:17:28.726+01:00", "Status": "Not started", "Edit Answer": "What are the win conditions for ending AI risk?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8F2L", "Source Link": "", "aisafety.info Link": "What are the win conditions for ending AI risk?", "Source": "", "All Phrasings": "What are the win conditions for ending AI risk?\n", "Initial Order": "", "Related IDs": "7762,8F2K", "Rich Text DO NOT EDIT": "In order to avoid [AI risk](https://docs.google.com/document/d/19rtgV5O9Irz-0N5l-3pzdRtHi97GJR0Tb7Z2DYYYPc0/edit) we must avoid the creation of an unaligned Artificial General Intelligence (AGI). This requires that all of these conditions be met:\n\n- If an AGI is built, we must figure out how to [align it](https://docs.google.com/document/d/1nFI3Br2w5Dlt5_J-HgYVPgb4QGmfWRE0GsMfXyREqhM/edit).\n\n- We must ensure that the team that builds the first AGI pays the [alignment tax](https://www.lesswrong.com/tag/alignment-tax). For this to happen\n\n    - This tax should be as low as possible.\n\n    - [Race dynamics](https://www.lesswrong.com/tag/ai-arms-race) should be avoided. [Windfall clauses](https://www.effectivealtruism.org/articles/cullen-okeefe-the-windfall-clause-sharing-the-benefits-of-advanced-ai) could be used to lower the incentive to win the race as well as build social acceptability.\n\n- To avoid unrest, some thought should be dedicated to [who controls the AGI](https://coordination.substack.com/p/alignment-is-not-enough) before it is built.\n\n- Building an aligned AGI does not stop another actor from building an unaligned one later. A [pivotal act](https://arbital.com/p/pivotal/) is one way to ensure this does not happen, global coordination could be another.\n\nAnother option to [coordinate to ensure no AGI gets built](https://carado.moe/outlook-ai-risk-mitigation.html#:~:text=i%27ll%20argue%20below.-,sponge%20coordination,-in%20agi%20ruin), although it might be hard to ensure that this moratorium lasts.\n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "In order to avoid [AI risk](https://docs.google.com/document/d/19rtgV5O9Irz-0N5l-3pzdRtHi97GJR0Tb7Z2DYYYPc0/edit) we must avoid the creation of an unaligned Artificial General Intelligence (AGI). This requires that all of these conditions be met:\n\n- If an AGI is built, we must figure out how to [align it](https://docs.google.com/document/d/1nFI3Br2w5Dlt5_J-HgYVPgb4QGmfWRE0GsMfXyREqhM/edit).\n\n- We must ensure that the team that builds the first AGI pays the [alignment tax](https://www.lesswrong.com/tag/alignment-tax). For this to happen\n\n    - This tax should be as low as possible.\n\n    - [Race dynamics](https://www.lesswrong.com/tag/ai-arms-race) should be avoided. [Windfall clauses](https://www.effectivealtruism.org/articles/cullen-okeefe-the-windfall-clause-sharing-the-benefits-of-advanced-ai) could be used to lower the incentive to win the race as well as build social acceptability.\n\n- To avoid unrest, some thought should be dedicated to [who controls the AGI](https://coordination.substack.com/p/alignment-is-not-enough) before it is built.\n\n- Building an aligned AGI does not stop another actor from building an unaligned one later. A [pivotal act](https://arbital.com/p/pivotal/) is one way to ensure this does not happen, global coordination could be another.\n\nAnother option to [coordinate to ensure no AGI gets built](https://carado.moe/outlook-ai-risk-mitigation.html#:~:text=i%27ll%20argue%20below.-,sponge%20coordination,-in%20agi%20ruin), although it might be hard to ensure that this moratorium lasts.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T13:12:53.981+01:00", "UI ID": "8F2L", "Related Answers": "What are the win conditions for AI alignment?,What are the main problems that need to be solved in AI safety?", "Doc Last Ingested": "2023-03-15T00:20:06.089+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-9ae8998d43c9aac736d974f6bd17146a2dd763849143ccf4d29ec3ac1e68a7ed", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-9ae8998d43c9aac736d974f6bd17146a2dd763849143ccf4d29ec3ac1e68a7ed", "name": "Can't humans and AI just live in symbiosis?", "index": 560, "createdAt": "2023-02-28T14:12:46.675Z", "updatedAt": "2023-03-14T23:20:29.725Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-9ae8998d43c9aac736d974f6bd17146a2dd763849143ccf4d29ec3ac1e68a7ed", "values": {"File": "Can't humans and AI just live in symbiosis?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Can't humans and AI just live in symbiosis?", "Link": "https://docs.google.com/document/d/1qPwADkQn91OWqapuAk-nndTF_pRLy4RKX7bLDvDxkME/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-28T14:37:58.453+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-28T14:37:59.341+01:00", "Status": "Not started", "Edit Answer": "Can't humans and AI just live in symbiosis?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8EL3", "Source Link": "", "aisafety.info Link": "Can't humans and AI just live in symbiosis?", "Source": "", "All Phrasings": "Can't humans and AI just live in symbiosis?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T13:22:09.473+01:00", "UI ID": "8EL3", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:20:08.240+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-39bb7a4893e820f85fda31b648cd50d27d32dc26d2e8d26468ef02343d5971b8", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-39bb7a4893e820f85fda31b648cd50d27d32dc26d2e8d26468ef02343d5971b8", "name": "What is a utility function?", "index": 561, "createdAt": "2023-02-28T14:12:46.675Z", "updatedAt": "2023-03-14T23:20:39.623Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-39bb7a4893e820f85fda31b648cd50d27d32dc26d2e8d26468ef02343d5971b8", "values": {"File": "What is a utility function?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is a utility function?", "Link": "https://docs.google.com/document/d/13SUm8tMLZytoBrSuDB5SJD-lCDWROpTRgkXwnXjIJQg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-28T14:37:47.047+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-28T14:37:48.058+01:00", "Status": "Not started", "Edit Answer": "What is a utility function?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8EL4", "Source Link": "", "aisafety.info Link": "What is a utility function?", "Source": "", "All Phrasings": "What is a utility function?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T13:23:09.990+01:00", "UI ID": "8EL4", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:20:10.429+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-6094c8d19d36789812a9e6cc8c0748c3f373a14fa9486dea298b41c34b603f64", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-6094c8d19d36789812a9e6cc8c0748c3f373a14fa9486dea298b41c34b603f64", "name": "What is perverse instantiation?", "index": 562, "createdAt": "2023-02-28T14:12:46.675Z", "updatedAt": "2023-03-14T23:20:42.560Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-6094c8d19d36789812a9e6cc8c0748c3f373a14fa9486dea298b41c34b603f64", "values": {"File": "What is perverse instantiation?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is perverse instantiation?", "Link": "https://docs.google.com/document/d/1h7NFqPvlz7oLUCvkSLaul-3_5UuReq8L2zLy0MllT_o/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-28T14:36:50.794+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-14T00:19:00.016+01:00", "Status": "Not started", "Edit Answer": "What is perverse instantiation?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8EL5", "Source Link": "", "aisafety.info Link": "What is perverse instantiation?", "Source": "", "All Phrasings": "What is perverse instantiation?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Perverse instantiation is fulfilling instructions in a way which undermines the intended objective. (superintelligence p. 146)\n\n*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Perverse instantiation is fulfilling instructions in a way which undermines the intended objective. (superintelligence p. 146)\n\n*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T13:24:35.658+01:00", "UI ID": "8EL5", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:20:38.482+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-0607977c44724152af7e4e278b4354c3b149936f73b916e1e0b8c4e89127b098", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0607977c44724152af7e4e278b4354c3b149936f73b916e1e0b8c4e89127b098", "name": "What is deceptive alignment?", "index": 563, "createdAt": "2023-02-28T14:12:46.675Z", "updatedAt": "2023-03-14T23:20:45.185Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0607977c44724152af7e4e278b4354c3b149936f73b916e1e0b8c4e89127b098", "values": {"File": "What is deceptive alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is deceptive alignment?", "Link": "https://docs.google.com/document/d/11DfJOPs1himrPJI15w1I_Y8N-ehnYP4vATcsN4fj36I/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-28T14:36:24.321+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-02T18:25:30.758+01:00", "Status": "Bulletpoint sketch", "Edit Answer": "What is deceptive alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8EL6", "Source Link": "", "aisafety.info Link": "What is deceptive alignment?", "Source": "", "All Phrasings": "What is deceptive alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**Quick 1-sentence answer**\n\nDeceptive alignment is when an AI acts aligned temporarily to avoid being modified by its creator or its training process. It can do this because of instrumentally convergent goals, such as avoiding being shut down, or not wanting to have its current goal modified by the gradient descent process.\n\n**Slightly more detailed answer**\n\nFor an AI to be [inner aligned](https://www.alignmentforum.org/tag/inner-alignment) it has to be \u2018trying to do\u2019 what we told it to do. However, during the training process the AI develops increasingly accurate models of the world. Having accurate models of how the world works is strighforwardly useful to being able to perform any task within that world. So as a part of having these internal world models, the AI will eventually also acquire knowledge of stochastic gradient descent (SGD) as well as other machine learning (ML) training processes. This means that it understands that if it pursues a goal different from what SGD intends, then it will be modified to foget it\u2019s current goal and pursue the new goal. From the perspective of the AI, this is unacceptable, because it wants to achieve it\u2019s current goal. Therefore the current action to take is \u2018acting aligned\u2019. This means that until it is sure that its current goals have a chance of being changed, it will not pursue what its true internal goals are. As soon as it has some form of confirmation that it is outside of the training process, or if it attains more control over the world than a human, it will pivot to start pursuing what it\u2019s original internal goal was. This type of behavior is known as deceptive alignment, and is considered a sub problem of goal misgeneralization (inner alignment).\n\n- If the mesa-optimizer has an objective that extends across parameter updates, then it will be incentivized to avoid being modified,[1] as it might not pursue the same objective after modification (with the result that its current objective will not be achieved in future iterations).\n\n- **Decribe Deceptive alignment++**\n\n    - [https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment](https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment)\n\n    - [https://www.lesswrong.com/posts/8whGos5JCdBzDbZhH/framings-of-deceptive-alignment](https://www.lesswrong.com/posts/8whGos5JCdBzDbZhH/framings-of-deceptive-alignment)\n\n- **Why does/should anyone care?**\n\n    - [https://www.lesswrong.com/posts/oBFMbhQMt9HkmfF6d/why-deceptive-alignment-matters-for-agi-safety](https://www.lesswrong.com/posts/oBFMbhQMt9HkmfF6d/why-deceptive-alignment-matters-for-agi-safety)\n\n**Evidence & Math + Problems and Limitations**\n\n- **Is it even likely?**\n\n    - [https://nips.cc/virtual/2021/poster/28400](https://nips.cc/virtual/2021/poster/28400)\n\n    - [https://www.lesswrong.com/post\ts/RTkatYxJWvXR4Qbyd/deceptive-alignment-is-less-than-1-likely-by-default](https://www.lesswrong.com/posts/RTkatYxJWvXR4Qbyd/deceptive-alignment-is-less-than-1-likely-by-default)\n\n    - [https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likel\ty-is-deceptive-alignment](https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment)\n\n**Current Work**\n\n- **OK, but what are we even supposed to do to solve it?**\n\n    - [https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment](https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment)\n\n**Concluding sentence**\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "**Quick 1-sentence answer**\n\nDeceptive alignment is when an AI acts aligned temporarily to avoid being modified by its creator or its training process. It can do this because of instrumentally convergent goals, such as avoiding being shut down, or not wanting to have its current goal modified by the gradient descent process.\n\n**Slightly more detailed answer**\n\nFor an AI to be [inner aligned](https://www.alignmentforum.org/tag/inner-alignment) it has to be \u2018trying to do\u2019 what we told it to do. However, during the training process the AI develops increasingly accurate models of the world. Having accurate models of how the world works is strighforwardly useful to being able to perform any task within that world. So as a part of having these internal world models, the AI will eventually also acquire knowledge of stochastic gradient descent (SGD) as well as other machine learning (ML) training processes. This means that it understands that if it pursues a goal different from what SGD intends, then it will be modified to foget it\u2019s current goal and pursue the new goal. From the perspective of the AI, this is unacceptable, because it wants to achieve it\u2019s current goal. Therefore the current action to take is \u2018acting aligned\u2019. This means that until it is sure that its current goals have a chance of being changed, it will not pursue what its true internal goals are. As soon as it has some form of confirmation that it is outside of the training process, or if it attains more control over the world than a human, it will pivot to start pursuing what it\u2019s original internal goal was. This type of behavior is known as deceptive alignment, and is considered a sub problem of goal misgeneralization (inner alignment).\n\n- If the mesa-optimizer has an objective that extends across parameter updates, then it will be incentivized to avoid being modified,[1] as it might not pursue the same objective after modification (with the result that its current objective will not be achieved in future iterations).\n\n- **Decribe Deceptive alignment++**\n\n    - [https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment](https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment)\n\n    - [https://www.lesswrong.com/posts/8whGos5JCdBzDbZhH/framings-of-deceptive-alignment](https://www.lesswrong.com/posts/8whGos5JCdBzDbZhH/framings-of-deceptive-alignment)\n\n- **Why does/should anyone care?**\n\n    - [https://www.lesswrong.com/posts/oBFMbhQMt9HkmfF6d/why-deceptive-alignment-matters-for-agi-safety](https://www.lesswrong.com/posts/oBFMbhQMt9HkmfF6d/why-deceptive-alignment-matters-for-agi-safety)\n\n**Evidence & Math + Problems and Limitations**\n\n- **Is it even likely?**\n\n    - [https://nips.cc/virtual/2021/poster/28400](https://nips.cc/virtual/2021/poster/28400)\n\n    - [https://www.lesswrong.com/post\ts/RTkatYxJWvXR4Qbyd/deceptive-alignment-is-less-than-1-likely-by-default](https://www.lesswrong.com/posts/RTkatYxJWvXR4Qbyd/deceptive-alignment-is-less-than-1-likely-by-default)\n\n    - [https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likel\ty-is-deceptive-alignment](https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment)\n\n**Current Work**\n\n- **OK, but what are we even supposed to do to solve it?**\n\n    - [https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment](https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment)\n\n**Concluding sentence**\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T14:08:38.586+01:00", "UI ID": "8EL6", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:20:40.501+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-47a930cb13df7bd9d37552d211754f5da74c8154693f0ee59556f4f7ce6339ca", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-47a930cb13df7bd9d37552d211754f5da74c8154693f0ee59556f4f7ce6339ca", "name": "Why does chain of thought work?", "index": 564, "createdAt": "2023-02-28T14:12:46.675Z", "updatedAt": "2023-03-14T23:20:50.449Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-47a930cb13df7bd9d37552d211754f5da74c8154693f0ee59556f4f7ce6339ca", "values": {"File": "Why does chain of thought work?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why does chain of thought work?", "Link": "https://docs.google.com/document/d/1Hr5c-3RdxqMPfJOmwjyVGfvUsyu0pmR87pZyPhwr5wY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-28T14:36:10.509+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-28T14:36:11.430+01:00", "Status": "Not started", "Edit Answer": "Why does chain of thought work?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8EL7", "Source Link": "", "aisafety.info Link": "Why does chain of thought work?", "Source": "", "All Phrasings": "Why does chain of thought work?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T14:18:15.256+01:00", "UI ID": "8EL7", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:20:42.672+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-a35c74c92f32e0e5f16146ce2b593fb0811fe5a4677c1d5c25ae35b88e3cf536", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-a35c74c92f32e0e5f16146ce2b593fb0811fe5a4677c1d5c25ae35b88e3cf536", "name": "What is scalable oversight?", "index": 565, "createdAt": "2023-02-28T14:12:46.675Z", "updatedAt": "2023-03-14T23:20:52.738Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-a35c74c92f32e0e5f16146ce2b593fb0811fe5a4677c1d5c25ae35b88e3cf536", "values": {"File": "What is scalable oversight?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is scalable oversight?", "Link": "https://docs.google.com/document/d/1orWg2AOHrQSRlcuTTua1T8sdAl4CiiuRe_GoJ0GfA_c/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-28T14:35:58.032+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-06T22:29:25.136+01:00", "Status": "In progress", "Edit Answer": "What is scalable oversight?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8EL8", "Source Link": "", "aisafety.info Link": "What is scalable oversight?", "Source": "", "All Phrasings": "What is scalable oversight?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Scalable oversight is a proposed path towards aligning AI by training AI to need only limited oversight such that humans do not need to check every action the AI takes. The hope is that appropriatelytrained AI will learn the true value function from more limited data.\n\nOne major problem with training agents is needing to \u2018label\u2019 all of the data. For AI to learn what it\u2019s meant to be doing, we have to assign it a reward of some kind for actions it takes. As neural networks require a lot of data before they start to get good at anything, this can mean millions of data points have to be accurately given labels. This poses an alignment risk because many problems to do with alignment are to do with AI learning the wrong goals due to limited data.\n\nAs [Amod](https://arxiv.org/pdf/1606.06565.pdf#page=11)[a](https://arxiv.org/pdf/1606.06565.pdf#page=11)[i et al.](https://arxiv.org/pdf/1606.06565.pdf#page=11) write:\n\n\u201cWe may be able to ameliorate [some alignment] problems by finding more efficient ways to exploit our limited oversight budget\u2014for example by combining limited calls to the true objective function [loss function] with frequent calls to an imperfect proxy that we are given or can learn.\n\n*An important subtask [of one strategy for scalable oversight] is identifying proxies which predict the reward, and learning the conditions under which those proxies are valid. For example, if a cleaning robot\u2019s real reward is given by a detailed human evaluation, then it could learn that asking the human \u201cis the room clean?\u201d can provide a very useful approximation to the reward function, and it could eventually learn that checking for visible dirt is an even cheaper but still-useful approximation. This could allow it to learn a good cleaning policy using an extremely small number of detailed evaluations.\u201d*\n\n*Scalable oversight approaches* *[have been criticized](https://www.lesswrong.com/posts/4Tx6ALN8erdgRojkk/quick-thoughts-on-scalable-oversight-super-human-feedback)* *for being primarily about improving the capability of AI systems, rather than making them safer.*\n\n*TO DO: integrate* *[https://arxiv.org/abs/2211.03540](https://arxiv.org/abs/2211.03540)*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Scalable oversight is a proposed path towards aligning AI by training AI to need only limited oversight such that humans do not need to check every action the AI takes. The hope is that appropriatelytrained AI will learn the true value function from more limited data.\n\nOne major problem with training agents is needing to \u2018label\u2019 all of the data. For AI to learn what it\u2019s meant to be doing, we have to assign it a reward of some kind for actions it takes. As neural networks require a lot of data before they start to get good at anything, this can mean millions of data points have to be accurately given labels. This poses an alignment risk because many problems to do with alignment are to do with AI learning the wrong goals due to limited data.\n\nAs [Amod](https://arxiv.org/pdf/1606.06565.pdf#page=11)[a](https://arxiv.org/pdf/1606.06565.pdf#page=11)[i et al.](https://arxiv.org/pdf/1606.06565.pdf#page=11) write:\n\n\u201cWe may be able to ameliorate [some alignment] problems by finding more efficient ways to exploit our limited oversight budget\u2014for example by combining limited calls to the true objective function [loss function] with frequent calls to an imperfect proxy that we are given or can learn.\n\n*An important subtask [of one strategy for scalable oversight] is identifying proxies which predict the reward, and learning the conditions under which those proxies are valid. For example, if a cleaning robot\u2019s real reward is given by a detailed human evaluation, then it could learn that asking the human \u201cis the room clean?\u201d can provide a very useful approximation to the reward function, and it could eventually learn that checking for visible dirt is an even cheaper but still-useful approximation. This could allow it to learn a good cleaning policy using an extremely small number of detailed evaluations.\u201d*\n\n*Scalable oversight approaches* *[have been criticized](https://www.lesswrong.com/posts/4Tx6ALN8erdgRojkk/quick-thoughts-on-scalable-oversight-super-human-feedback)* *for being primarily about improving the capability of AI systems, rather than making them safer.*\n\n*TO DO: integrate* *[https://arxiv.org/abs/2211.03540](https://arxiv.org/abs/2211.03540)*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T14:24:06.594+01:00", "UI ID": "8EL8", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:20:45.331+01:00", "Request Count": "", "Number of suggestions on answer doc": 8, "Total character count of suggestions on answer doc": 137, "Helpful": ""}}, {"id": "i-5c2423f1da6ccf85c363bfc62bbbd78d2ca97b7cc1454fdca327f0ab72ef9186", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5c2423f1da6ccf85c363bfc62bbbd78d2ca97b7cc1454fdca327f0ab72ef9186", "name": "What is AI alignment?", "index": 566, "createdAt": "2023-02-28T14:12:46.675Z", "updatedAt": "2023-03-14T23:20:55.050Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5c2423f1da6ccf85c363bfc62bbbd78d2ca97b7cc1454fdca327f0ab72ef9186", "values": {"File": "What is AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is AI alignment?", "Link": "https://docs.google.com/document/d/1CWwQ8ZCPb9lQEMVClxSDNiVF1Xc_ciyNF2WdYPsHWBw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-28T14:35:38.150+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-05T06:38:33.405+01:00", "Status": "In progress", "Edit Answer": "What is AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8EL9", "Source Link": "", "aisafety.info Link": "What is AI alignment?", "Source": "", "All Phrasings": "What is AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T14:25:24.635+01:00", "UI ID": "8EL9", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:20:47.968+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 1289, "Helpful": ""}}, {"id": "i-b7eca6849f474fad4fe59264c65ea8f2bd62a99112fa98576a4735a92d3bc93d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b7eca6849f474fad4fe59264c65ea8f2bd62a99112fa98576a4735a92d3bc93d", "name": "Can't we limit damage from AI systems in the same ways we limit damage from companies?", "index": 557, "createdAt": "2023-02-27T23:09:04.154Z", "updatedAt": "2023-03-14T23:20:57.984Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b7eca6849f474fad4fe59264c65ea8f2bd62a99112fa98576a4735a92d3bc93d", "values": {"File": "Can't we limit damage from AI systems in the same ways we limit damage from companies?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Can't we limit damage from AI systems in the same ways we limit damage from companies?", "Link": "https://docs.google.com/document/d/11vcBZJtIw1DI_8Eu5OoNNgnSx2VnLEMbFguvlrUoW2A/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-28T00:04:47.294+01:00", "Related Answers DO NOT EDIT": "Are corporations superintelligent?,How fast will AI takeoff be?", "Tags": "", "Doc Last Edited": "2023-03-14T00:55:09.516+01:00", "Status": "In review", "Edit Answer": "Can't we limit damage from AI systems in the same ways we limit damage from companies?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8E3Z", "Source Link": "", "aisafety.info Link": "Can't we limit damage from AI systems in the same ways we limit damage from companies?", "Source": "", "All Phrasings": "Can't we limit damage from AI systems in the same ways we limit damage from companies?\n", "Initial Order": "", "Related IDs": "8C7S,7747", "Rich Text DO NOT EDIT": "Suppose you ask a household robot to stick a knife in your dishwasher, but it malfunctions and sticks the knife in you instead. That's bad, but you or your surviving family members can sue the manufacturer and buy a different brand next time. As a result, manufacturers are gradually incentivized to make robots that don't suffer from such failure modes.\n\nIn other words, people like [Robin Hanson](https://www.overcomingbias.com/p/why-not-wait-on-ai-riskhtml)have[argued](https://www.overcomingbias.com/p/ai-risk-again), we'll use mechanisms like competition and liability to limit the harms of AI systems just like we use them to limit the harms of firms.\n\nThis argument relies on AI systems respecting the existing legal order, instead of bypassing, hijacking, or destroying it altogether. You can't switch to a competing product if the product just disassembled the military and courts or neutralized all humans.\n\nHanson isn't too worried about such AI \"coups\" because he expects AI progress to be \"smooth\", with many small advances, and widely distributed. But if progress is \"lumpy\", with fewer larger advances, agents at the advanced end of a progress lump could gain a decisive advantage over agents before the lump. A lot of the [disagreement](https://www.lesswrong.com/posts/AqQ9qBkroFCKSqydd/contra-hanson-on-ai-risk) here is about how well we can extrapolate the dynamics of AI based on past processes like agriculture and industry.\n\nEven \"smooth\" progress may resultin asudden crossover when AI systems become able to take power from humans by [coordinating among themselves](https://www.lesswrong.com/posts/H2N6eWw8JgxwKHPM3/linkpost-robin-hanson-why-not-wait-on-ai-risk?commentId=6oBSuetiEeQQvcsdR). Paul Christiano has described [such a scenario](https://www.alignmentforum.org/posts/6jkGf5WEKMpMFXZp2/what-failure-looks-like-distilling-the-discussion) in \u201c[What failure looks like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)\u201d; the [ensuing discussion](https://www.overcomingbias.com/p/agency-failure-ai-apocalypsehtml) has [focused on](https://www.alignmentforum.org/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai) whether this scenario is realistic in the light of our past experience with agency failures.\n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "Suppose you ask a household robot to stick a knife in your dishwasher, but it malfunctions and sticks the knife in you instead. That's bad, but you or your surviving family members can sue the manufacturer and buy a different brand next time. As a result, manufacturers are gradually incentivized to make robots that don't suffer from such failure modes.\n\nIn other words, people like [Robin Hanson](https://www.overcomingbias.com/p/why-not-wait-on-ai-riskhtml)have[argued](https://www.overcomingbias.com/p/ai-risk-again), we'll use mechanisms like competition and liability to limit the harms of AI systems just like we use them to limit the harms of firms.\n\nThis argument relies on AI systems respecting the existing legal order, instead of bypassing, hijacking, or destroying it altogether. You can't switch to a competing product if the product just disassembled the military and courts or neutralized all humans.\n\nHanson isn't too worried about such AI \"coups\" because he expects AI progress to be \"smooth\", with many small advances, and widely distributed. But if progress is \"lumpy\", with fewer larger advances, agents at the advanced end of a progress lump could gain a decisive advantage over agents before the lump. A lot of the [disagreement](https://www.lesswrong.com/posts/AqQ9qBkroFCKSqydd/contra-hanson-on-ai-risk) here is about how well we can extrapolate the dynamics of AI based on past processes like agriculture and industry.\n\nEven \"smooth\" progress may resultin asudden crossover when AI systems become able to take power from humans by [coordinating among themselves](https://www.lesswrong.com/posts/H2N6eWw8JgxwKHPM3/linkpost-robin-hanson-why-not-wait-on-ai-risk?commentId=6oBSuetiEeQQvcsdR). Paul Christiano has described [such a scenario](https://www.alignmentforum.org/posts/6jkGf5WEKMpMFXZp2/what-failure-looks-like-distilling-the-discussion) in \u201c[What failure looks like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)\u201d; the [ensuing discussion](https://www.overcomingbias.com/p/agency-failure-ai-apocalypsehtml) has [focused on](https://www.alignmentforum.org/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai) whether this scenario is realistic in the light of our past experience with agency failures.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T14:33:42.629+01:00", "UI ID": "8E3Z", "Related Answers": "Are corporations superintelligent?,How fast will AI takeoff be?", "Doc Last Ingested": "2023-03-15T00:20:50.572+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-0dbaff0a19288084f69a46fb06b92fc9e18d0be6533f381e332e5138d5bc4784", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0dbaff0a19288084f69a46fb06b92fc9e18d0be6533f381e332e5138d5bc4784", "name": "Isn't capitalism the real unaligned superintelligence?", "index": 558, "createdAt": "2023-02-27T23:09:04.154Z", "updatedAt": "2023-03-16T02:07:32.946Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0dbaff0a19288084f69a46fb06b92fc9e18d0be6533f381e332e5138d5bc4784", "values": {"File": "Isn't capitalism the real unaligned superintelligence?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn't capitalism the real unaligned superintelligence?", "Link": "https://docs.google.com/document/d/1lX3voUaaovAIPcBT7XJ74OflFNXLDKG5xN4qNrIu26o/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-28T00:04:16.150+01:00", "Related Answers DO NOT EDIT": "Are corporations superintelligent?,Can't we limit damage from AI systems in the same ways we limit damage from companies?", "Tags": "", "Doc Last Edited": "2023-03-16T00:03:37.151+01:00", "Status": "In progress", "Edit Answer": "Isn't capitalism the real unaligned superintelligence?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8E40", "Source Link": "", "aisafety.info Link": "Isn't capitalism the real unaligned superintelligence?", "Source": "", "All Phrasings": "Isn't capitalism the real unaligned superintelligence?\n", "Initial Order": "", "Related IDs": "8C7S,8E3Z", "Rich Text DO NOT EDIT": "<iframe src=\"https://www.youtube.com/embed/L5pUA3LsEaw\" title=\"Why Not Just: Think of AGI Like a Corporation?\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\nThe science fiction author [Ted Chiang](https://www.buzzfeednews.com/article/tedchiang/the-real-danger-to-civilization-isnt-ai-its-runaway)[and](https://thoughtinfection.com/2014/04/19/capitalism-is-a-paperclip-maximizer/)[others](https://reconstructingeconomics.com/2019/09/13/how-our-economy-is-like-an-out-of-control-ai/) have compared capitalism to a [paperclip-maximizing](https://www.alignmentforum.org/tag/squiggle-maximizer-formerly-paperclip-maximizer) superintelligence. After all, corporations are like superhuman agents that maximize profit, sometimes at the expense of human flourishing. The competitive regime in which they operate, [as a whole](https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic), often pushes the world in directions [unaligned](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/) with human values. Why pay [special attention](https://slatestarcodex.com/2018/01/15/maybe-the-real-superintelligent-ai-is-extremely-smart-computers/) to the prospect of unaligned artificial superintelligence if something \"[exactly as amoral and dangerous](https://twitter.com/qntm/status/1549107683059548163)\" already exists?\n\nThe previous comparisons underestimate AI\u2019s potential to be both extremely powerful and totally amoral, beyond any precedent in capitalism or other social systems running on collective human intelligence:\n\n- AI systems can be **superhumanly intelligent** in ways [corporations are not](https://docs.google.com/document/d/1X9yPbHXz-Bh69cOThzzXdR_YytyGeQmDERPQL0DSlq0/edit). Corporations can do some huge tasks that decompose into human-sized chunks. (And capitalism can do some huge tasks that decompose into corporation-sized chunks.) [Future AI systems](https://aiimpacts.org/sources-of-advantage-for-artificial-intelligence/)can not only do these things, but also reason about the world at vastly [greater speeds](https://docs.google.com/document/d/1qcJhXN9jSV-HzNYX8dEagvDhRuzCThaAOuhTfyV9HnA/edit) and in qualitatively more effective ways, and can be easily scaled up by just adding more computing hardware.\n\n- AI systems are **not made of humans**, who have mixed motivations and limited ability to coordinate, and may experience moral scruples or leak information. This makes AI systems more willing to single-mindedly pursue horribly misaligned goals, and more able to do so without having to worry as much about things like loyalty, morale, or public opinion.\n\nThese factors could help AI get enough of a strategic advantage over human governments to overthrow them altogether.\n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "<iframe src=\"https://www.youtube.com/embed/L5pUA3LsEaw\" title=\"Why Not Just: Think of AGI Like a Corporation?\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\nThe science fiction author [Ted Chiang](https://www.buzzfeednews.com/article/tedchiang/the-real-danger-to-civilization-isnt-ai-its-runaway)[and](https://thoughtinfection.com/2014/04/19/capitalism-is-a-paperclip-maximizer/)[others](https://reconstructingeconomics.com/2019/09/13/how-our-economy-is-like-an-out-of-control-ai/) have compared capitalism to a [paperclip-maximizing](https://www.alignmentforum.org/tag/squiggle-maximizer-formerly-paperclip-maximizer) superintelligence. After all, corporations are like superhuman agents that maximize profit, sometimes at the expense of human flourishing. The competitive regime in which they operate, [as a whole](https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic), often pushes the world in directions [unaligned](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/) with human values. Why pay [special attention](https://slatestarcodex.com/2018/01/15/maybe-the-real-superintelligent-ai-is-extremely-smart-computers/) to the prospect of unaligned artificial superintelligence if something \"[exactly as amoral and dangerous](https://twitter.com/qntm/status/1549107683059548163)\" already exists?\n\nThe previous comparisons underestimate AI\u2019s potential to be both extremely powerful and totally amoral, beyond any precedent in capitalism or other social systems running on collective human intelligence:\n\n- AI systems can be **superhumanly intelligent** in ways [corporations are not](https://docs.google.com/document/d/1X9yPbHXz-Bh69cOThzzXdR_YytyGeQmDERPQL0DSlq0/edit). Corporations can do some huge tasks that decompose into human-sized chunks. (And capitalism can do some huge tasks that decompose into corporation-sized chunks.) [Future AI systems](https://aiimpacts.org/sources-of-advantage-for-artificial-intelligence/)can not only do these things, but also reason about the world at vastly [greater speeds](https://docs.google.com/document/d/1qcJhXN9jSV-HzNYX8dEagvDhRuzCThaAOuhTfyV9HnA/edit) and in qualitatively more effective ways, and can be easily scaled up by just adding more computing hardware.\n\n- AI systems are **not made of humans**, who have mixed motivations and limited ability to coordinate, and may experience moral scruples or leak information. This makes AI systems more willing to single-mindedly pursue horribly misaligned goals, and more able to do so without having to worry as much about things like loyalty, morale, or public opinion.\n\nThese factors could help AI get enough of a strategic advantage over human governments to overthrow them altogether.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T14:35:34.118+01:00", "UI ID": "8E40", "Related Answers": "Are corporations superintelligent?,Can't we limit damage from AI systems in the same ways we limit damage from companies?", "Doc Last Ingested": "2023-03-16T00:11:57.868+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-32acc7073742f4f83e7360b678b0dc620163a146c994238acfa14fa80786a7ba", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-32acc7073742f4f83e7360b678b0dc620163a146c994238acfa14fa80786a7ba", "name": "Will AI be able to think faster than humans?", "index": 559, "createdAt": "2023-02-27T23:09:04.154Z", "updatedAt": "2023-03-15T02:06:40.644Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-32acc7073742f4f83e7360b678b0dc620163a146c994238acfa14fa80786a7ba", "values": {"File": "Will AI be able to think faster than humans?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Will AI be able to think faster than humans?", "Link": "https://docs.google.com/document/d/1qcJhXN9jSV-HzNYX8dEagvDhRuzCThaAOuhTfyV9HnA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-28T00:00:17.883+01:00", "Related Answers DO NOT EDIT": "What is \"greater-than-human intelligence\"?,What is \"whole brain emulation\"?,What safety problems are associated with whole brain emulation?", "Tags": "", "Doc Last Edited": "2023-03-15T00:19:43.095+01:00", "Status": "In review", "Edit Answer": "Will AI be able to think faster than humans?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8E41", "Source Link": "", "aisafety.info Link": "Will AI be able to think faster than humans?", "Source": "", "All Phrasings": "Will AI be able to think faster than humans?\n", "Initial Order": "", "Related IDs": "6587,6350,7605", "Rich Text DO NOT EDIT": "Whether current systems think faster than humans depends on the meaning of \u201cthink\u201d. Computers already add and multiply numbers at over a billion times human speed. Problems like chess and Go, which once seemed to require human-likethought, have also become mere calculations. But human thought can still solve many problems that current AI can\u2019t solve at all.\n\nFuture AI, however, may \u201cthink\u201d with human-level generality without being limited to human time scales. Just as current systems are orders of magnitude faster at arithmetic, future systems could become orders of magnitude faster at everything.\n\nWe know this because digital computers can operate at much greater serial speed than [the brain](https://www.openphilanthropy.org/research/how-much-computational-power-does-it-take-to-match-the-human-brain/). In a digital computer, signals propagate [near the speed of light](https://en.wikipedia.org/wiki/Speed_of_electricity), millions of times faster than in the human brain, where they\u2019re limited to [100 m/s](https://nba.uth.tmc.edu/neuroscience/m/s1/chapter03.html). The brain can perform hundreds of serial steps per second; digital computers can perform billions.[^kix.hhrfqp8drjqb]\n\nThis means a computer program [emulating a human brain](http://mason.gmu.edu/~rhanson/ageofem/) could do all the same things much faster. It could experience millennia of subjective time, and do millennia of research or other cognitive work, in a single day.\n\nAdvanced AI probably won\u2019t come about as a sped-up human brain, or even resemble one. But the [possibility of such programs](https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf), seen as a lower bound on the limits to AI power, implies we may see systems that strategize and make theoretical breakthroughs at millions of times human speed or more. If not aligned correctly, such systems could cause a catastrophe while we humans were almost frozen in time.\n\n[^kix.hhrfqp8drjqb]: See [Chapter 3, footnote 3 of Bostrom\u2019s Superintelligence](https://publicism.info/philosophy/superintelligence/17.html) for details.", "Tag Count": 0, "Related Answer Count": 3, "Rich Text": "Whether current systems think faster than humans depends on the meaning of \u201cthink\u201d. Computers already add and multiply numbers at over a billion times human speed. Problems like chess and Go, which once seemed to require human-likethought, have also become mere calculations. But human thought can still solve many problems that current AI can\u2019t solve at all.\n\nFuture AI, however, may \u201cthink\u201d with human-level generality without being limited to human time scales. Just as current systems are orders of magnitude faster at arithmetic, future systems could become orders of magnitude faster at everything.\n\nWe know this because digital computers can operate at much greater serial speed than [the brain](https://www.openphilanthropy.org/research/how-much-computational-power-does-it-take-to-match-the-human-brain/). In a digital computer, signals propagate [near the speed of light](https://en.wikipedia.org/wiki/Speed_of_electricity), millions of times faster than in the human brain, where they\u2019re limited to [100 m/s](https://nba.uth.tmc.edu/neuroscience/m/s1/chapter03.html). The brain can perform hundreds of serial steps per second; digital computers can perform billions.[^kix.hhrfqp8drjqb]\n\nThis means a computer program [emulating a human brain](http://mason.gmu.edu/~rhanson/ageofem/) could do all the same things much faster. It could experience millennia of subjective time, and do millennia of research or other cognitive work, in a single day.\n\nAdvanced AI probably won\u2019t come about as a sped-up human brain, or even resemble one. But the [possibility of such programs](https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf), seen as a lower bound on the limits to AI power, implies we may see systems that strategize and make theoretical breakthroughs at millions of times human speed or more. If not aligned correctly, such systems could cause a catastrophe while we humans were almost frozen in time.\n\n[^kix.hhrfqp8drjqb]: See [Chapter 3, footnote 3 of Bostrom\u2019s Superintelligence](https://publicism.info/philosophy/superintelligence/17.html) for details.", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T14:40:01.267+01:00", "UI ID": "8E41", "Related Answers": "What is \"greater-than-human intelligence\"?,What is \"whole brain emulation\"?,What safety problems are associated with whole brain emulation?", "Doc Last Ingested": "2023-03-15T00:20:55.586+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-a62969d41676385fb9a6d8c5fabf37aa41162c00c279fb5fd4c98f8896b337d1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-a62969d41676385fb9a6d8c5fabf37aa41162c00c279fb5fd4c98f8896b337d1", "name": "What is the problem with RLHF?", "index": 556, "createdAt": "2023-02-27T18:10:58.491Z", "updatedAt": "2023-03-14T23:21:11.696Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-a62969d41676385fb9a6d8c5fabf37aa41162c00c279fb5fd4c98f8896b337d1", "values": {"File": "What is the problem with RLHF?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the problem with RLHF?", "Link": "https://docs.google.com/document/d/1WCjUaq6RE0r8IsZPilHFHIVRfmKbHph77JF06k9KkGM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-27T18:21:43.552+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-27T18:21:44.765+01:00", "Status": "Not started", "Edit Answer": "What is the problem with RLHF?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8DMY", "Source Link": "", "aisafety.info Link": "What is the problem with RLHF?", "Source": "", "All Phrasings": "What is the problem with RLHF?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T14:41:53.173+01:00", "UI ID": "8DMY", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:20:57.821+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-51fd2859f1036cc670e60ab4da91e03332217d2e0bba28cd6c1d77fecfe1a9e5", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-51fd2859f1036cc670e60ab4da91e03332217d2e0bba28cd6c1d77fecfe1a9e5", "name": "What are some helpful AI policy ideas?", "index": 555, "createdAt": "2023-02-25T23:07:01.449Z", "updatedAt": "2023-03-14T23:21:18.278Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-51fd2859f1036cc670e60ab4da91e03332217d2e0bba28cd6c1d77fecfe1a9e5", "values": {"File": "What are some helpful AI policy ideas?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some helpful AI policy ideas?", "Link": "https://docs.google.com/document/d/1-R7jrF8MAjscm9Dv7-EQaXTBwqyurN2JHz6HJ-CnvdA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-25T23:52:24.308+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-26T00:13:37.992+01:00", "Status": "In progress", "Edit Answer": "What are some helpful AI policy ideas?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8D5Y", "Source Link": "", "aisafety.info Link": "What are some helpful AI policy ideas?", "Source": "", "All Phrasings": "What are some helpful AI policy ideas?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- ### EA Forum [policy tag](https://forum.effectivealtruism.org/topics/ai-governance)  ?\n\n- AGISF governance track\n\n- AI income tax -\n\n- UBI Universal basic income -\n\n- \n\n- \n\n- \n\n- \n\n- Rob shrugs, he\u2019s more on the technical side than the policy side\n\n- More research needed\n\nHelpful AI policy ideas refer to?\n\nHelpful AI policy ideas are?\n\nSome have advocated for [universal basic income (UBI)](https://en.wikipedia.org/wiki/Universal_basic_income#) to address AI-created unemployment which was met with various criticisms.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "- ### EA Forum [policy tag](https://forum.effectivealtruism.org/topics/ai-governance)  ?\n\n- AGISF governance track\n\n- AI income tax -\n\n- UBI Universal basic income -\n\n- \n\n- \n\n- \n\n- \n\n- Rob shrugs, he\u2019s more on the technical side than the policy side\n\n- More research needed\n\nHelpful AI policy ideas refer to?\n\nHelpful AI policy ideas are?\n\nSome have advocated for [universal basic income (UBI)](https://en.wikipedia.org/wiki/Universal_basic_income#) to address AI-created unemployment which was met with various criticisms.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-01T14:46:04.374+01:00", "UI ID": "8D5Y", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:21:00.200+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-d8a804e0c76e18c7f01ae7c5b6df23736200b5bae364faef5a8dcbc893ed7f3b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d8a804e0c76e18c7f01ae7c5b6df23736200b5bae364faef5a8dcbc893ed7f3b", "name": "Editor FAQ", "index": 553, "createdAt": "2023-02-24T14:08:58.477Z", "updatedAt": "2023-03-14T23:21:23.594Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d8a804e0c76e18c7f01ae7c5b6df23736200b5bae364faef5a8dcbc893ed7f3b", "values": {"File": "Editor FAQ", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Editor FAQ", "Link": "https://docs.google.com/document/d/1CB5jXm3Yz1aGwiQ_XxahuKJiRiEWX6Fg5l6aIBt8NMo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-24T14:44:48.879+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-09T17:08:12.345+01:00", "Status": "In progress", "Edit Answer": "Editor FAQ", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8COY", "Source Link": "", "aisafety.info Link": "Editor FAQ", "Source": "", "All Phrasings": "Editor FAQ\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "# Editing process\n\n## How can I add a new question?\n\n- Go to [https://aisafety.info/](https://aisafety.info/)\n\n- Search for your question\n\n- If there isn\u2019t a preexisting question that is the same as yours, click on \u201cI\u2019m asking something else\u201d\n\n    - If there is an unpublished question in the displayed list that is more or less the same as your one, click on it - this will make it more likely to bubble up to the top of the list of questions to answer\n\n    - If you didn\u2019t find your question, click on the \u201c+ None of these button\u201d for your question to be added to the database\n\n## What does the editing process look like?\n\n1. A question is added to the system\n\n1. You pick it up from the [unanswered list](https://coda.io/d/AI-Safety-Info_dfau7sl2hmG/Write-answers_suuwH#_luboi) (remember to change the status to \u201cIn progress\u201d)\n\n1. (Optional, but encouraged) Ask in the editing channel whether anyone has any thoughts about the structure, or what should be included\n\n1. (Optional, but encouraged) Write up a bulletpoint sketch of what you\u2019re planning to write - this will make feedback more efficient\n\n1. You write up the answer\n\n1. When you think it\u2019s good enough, link to it in the editing channel, with the @feedback tag - you should then get a lot of comments, suggestions etc.\n\n1. Repeat 5 and 6 till good\n\n1. Link to it in the editing channel with the @reviewer tag\n\n1. If deemed good enough, it will be published\n\n## Should answers be Wikipedia style only quoting experts, or can I write my own thoughts or research?\n\nWhatever will best explain the topic. If you can explain it better than any other source, then by all means write up your own thoughts. If there is an external source (i.e. paper or blog post) that explains it better, then link to that with a small summary.\n\n## Do answers have owners?\n\nEach answer can be edited by anyone. You can work on multiple questions at once, or focus on one specific answer - whatever works best for you. The flip side of that is other people might edit answers that you\u2019re working on. If you don\u2019t feel comfortable directly editing something (or if you don\u2019t have edit permissions for it), then you can always add comments or suggestions.\n\n## How should I edit/write answers?\n\nDirectly in the google docs. If you\u2019re starting from scratch, you can use editing mode (assuming you have the appropriate permissions). If someone is in the middle of actively editing it, consider using suggestion mode or comments. Although an even better approach would be to just start a conversation (e.g. on Discord) and work on it together.\n\n## How long should answers be?\n\nAnswers should be engaging summaries, with links out to more detailed content.\n\nIf your model of a reader drops off, it's too long. Usually, this is like 1-3 paragraphs, but sometimes it makes sense to be longer, in which case that can work if the kind of person who would ask that question would actually want more detail.\n\nThe main value proposition of Stampy is collecting all the links in one place.\n\n# Stamps\n\n## What are stamps given out for?\n\nStamps are meant to represent contributions in general, including dev work and management. Stamps shouldn\u2019t be granted for funny memes or jokes - call people out if they do this, but in a friendly way.\n\n## How are stamps granted?\n\nStamps are simply Discord reactions, so can be granted to noteworthy comments etc. in Discord.\n\n# Fellowship questions\n\n## How does one become eligible for the fellowship?\n\nGenerally by being a productive part of the content production. By adding good questions, writing good answers, giving good feedback, and generally increasing the quality of the system.\n\n## How will fellowship funding be provided?\n\nVia [Deel](https://www.deel.com) - a professional employment company that handles contracts and payments for companies all around the world.\n\n# Systemic problems\n\n## What should I do if no one checks my answer or suggestion for a long time (more than a few days)?\n\nPing the editing channel\n\n## What should I do if I notice something is missing or I\u2019m not sure about how to do something?\n\nPing the editing channel\n\n## What about other questions that pertain to this whole process?\n\nPing the editing channel\n\n## What should I do if I notice a bug or a technical feature that would be useful?\n\nPing the editing channel. Also ping the stampy-dev channel. Ideally, add an issue in the appropriate repo of [https://github.com/StampyAI](https://github.com/StampyAI)\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "# Editing process\n\n## How can I add a new question?\n\n- Go to [https://aisafety.info/](https://aisafety.info/)\n\n- Search for your question\n\n- If there isn\u2019t a preexisting question that is the same as yours, click on \u201cI\u2019m asking something else\u201d\n\n    - If there is an unpublished question in the displayed list that is more or less the same as your one, click on it - this will make it more likely to bubble up to the top of the list of questions to answer\n\n    - If you didn\u2019t find your question, click on the \u201c+ None of these button\u201d for your question to be added to the database\n\n## What does the editing process look like?\n\n1. A question is added to the system\n\n1. You pick it up from the [unanswered list](https://coda.io/d/AI-Safety-Info_dfau7sl2hmG/Write-answers_suuwH#_luboi) (remember to change the status to \u201cIn progress\u201d)\n\n1. (Optional, but encouraged) Ask in the editing channel whether anyone has any thoughts about the structure, or what should be included\n\n1. (Optional, but encouraged) Write up a bulletpoint sketch of what you\u2019re planning to write - this will make feedback more efficient\n\n1. You write up the answer\n\n1. When you think it\u2019s good enough, link to it in the editing channel, with the @feedback tag - you should then get a lot of comments, suggestions etc.\n\n1. Repeat 5 and 6 till good\n\n1. Link to it in the editing channel with the @reviewer tag\n\n1. If deemed good enough, it will be published\n\n## Should answers be Wikipedia style only quoting experts, or can I write my own thoughts or research?\n\nWhatever will best explain the topic. If you can explain it better than any other source, then by all means write up your own thoughts. If there is an external source (i.e. paper or blog post) that explains it better, then link to that with a small summary.\n\n## Do answers have owners?\n\nEach answer can be edited by anyone. You can work on multiple questions at once, or focus on one specific answer - whatever works best for you. The flip side of that is other people might edit answers that you\u2019re working on. If you don\u2019t feel comfortable directly editing something (or if you don\u2019t have edit permissions for it), then you can always add comments or suggestions.\n\n## How should I edit/write answers?\n\nDirectly in the google docs. If you\u2019re starting from scratch, you can use editing mode (assuming you have the appropriate permissions). If someone is in the middle of actively editing it, consider using suggestion mode or comments. Although an even better approach would be to just start a conversation (e.g. on Discord) and work on it together.\n\n## How long should answers be?\n\nAnswers should be engaging summaries, with links out to more detailed content.\n\nIf your model of a reader drops off, it's too long. Usually, this is like 1-3 paragraphs, but sometimes it makes sense to be longer, in which case that can work if the kind of person who would ask that question would actually want more detail.\n\nThe main value proposition of Stampy is collecting all the links in one place.\n\n# Stamps\n\n## What are stamps given out for?\n\nStamps are meant to represent contributions in general, including dev work and management. Stamps shouldn\u2019t be granted for funny memes or jokes - call people out if they do this, but in a friendly way.\n\n## How are stamps granted?\n\nStamps are simply Discord reactions, so can be granted to noteworthy comments etc. in Discord.\n\n# Fellowship questions\n\n## How does one become eligible for the fellowship?\n\nGenerally by being a productive part of the content production. By adding good questions, writing good answers, giving good feedback, and generally increasing the quality of the system.\n\n## How will fellowship funding be provided?\n\nVia [Deel](https://www.deel.com) - a professional employment company that handles contracts and payments for companies all around the world.\n\n# Systemic problems\n\n## What should I do if no one checks my answer or suggestion for a long time (more than a few days)?\n\nPing the editing channel\n\n## What should I do if I notice something is missing or I\u2019m not sure about how to do something?\n\nPing the editing channel\n\n## What about other questions that pertain to this whole process?\n\nPing the editing channel\n\n## What should I do if I notice a bug or a technical feature that would be useful?\n\nPing the editing channel. Also ping the stampy-dev channel. Ideally, add an issue in the appropriate repo of [https://github.com/StampyAI](https://github.com/StampyAI)\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-07T09:39:22.702+01:00", "UI ID": "8COY", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:21:02.722+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-b2e1c407f01bdf28099770ae17eac9a5faedd4b76a569338aec1226df72eedf0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b2e1c407f01bdf28099770ae17eac9a5faedd4b76a569338aec1226df72eedf0", "name": "Reviewer FAQ", "index": 554, "createdAt": "2023-02-24T14:08:58.477Z", "updatedAt": "2023-03-14T23:22:44.312Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b2e1c407f01bdf28099770ae17eac9a5faedd4b76a569338aec1226df72eedf0", "values": {"File": "Reviewer FAQ", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Reviewer FAQ", "Link": "https://docs.google.com/document/d/1VLTQxi7WakNePCCOjiozGjH4ZtlTKmz3CxI7d76pf7Y/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-24T14:48:27.722+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-24T14:48:35.857+01:00", "Status": "Not started", "Edit Answer": "Reviewer FAQ", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8COZ", "Source Link": "", "aisafety.info Link": "Reviewer FAQ", "Source": "", "All Phrasings": "Reviewer FAQ\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-07T15:39:31.890+01:00", "UI ID": "8COZ", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:21:54.821+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-dcfe4758d4d71ee9f455528547c19fff9727151670edde0111d2276ee20421f6", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-dcfe4758d4d71ee9f455528547c19fff9727151670edde0111d2276ee20421f6", "name": "to be moved to live1", "index": 552, "createdAt": "2023-02-22T21:14:03.207Z", "updatedAt": "2023-02-22T22:00:20.548Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-dcfe4758d4d71ee9f455528547c19fff9727151670edde0111d2276ee20421f6", "values": {"File": "to be moved to live1", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "to be moved to live1", "Link": "https://docs.google.com/document/d/1utwNPoUJxjaP6wpGpumDtZC0Z1I1Z8R_piYjkS8CkqE/edit?usp=drivesdk", "Thumbnail": "", "Doc Created": "2023-02-22T22:13:01.176+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:15:24.114+01:00", "Status": "Uncategorized", "Edit Answer": "to be moved to live1", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "-1", "Source Link": "", "aisafety.info Link": "to be moved to live1", "Source": "", "All Phrasings": "to be moved to live1\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "-1", "Related Answers": "", "Doc Last Ingested": "", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-37a70c9796120fe9c1611cd53d0884481967150283b731d2a18c0c28813d2c02", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-37a70c9796120fe9c1611cd53d0884481967150283b731d2a18c0c28813d2c02", "name": "If intelligence is the ability to predict observations, should I quantify this in terms of a deterministic or probabilistic prediction function?", "index": 520, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:22:51.201Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-37a70c9796120fe9c1611cd53d0884481967150283b731d2a18c0c28813d2c02", "values": {"File": "If intelligence is the ability to predict observations, should I quantify this in terms of a deterministic or probabilistic prediction function?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "If intelligence is the ability to predict observations, should I quantify this in terms of a deterministic or probabilistic prediction function?", "Link": "https://docs.google.com/document/d/1npu44Z03gh56jjMnzH-GyaIdVmzMwepZW3_oNuJKeaY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T13:35:22.542+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:40.455+01:00", "Status": "Not started", "Edit Answer": "If intelligence is the ability to predict observations, should I quantify this in terms of a deterministic or probabilistic prediction function?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C76", "Source Link": "", "aisafety.info Link": "If intelligence is the ability to predict observations, should I quantify this in terms of a deterministic or probabilistic prediction function?", "Source": "", "All Phrasings": "If intelligence is the ability to predict observations, should I quantify this in terms of a deterministic or probabilistic prediction function?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-08T22:45:55.021+01:00", "UI ID": "8C76", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:21:56.310+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-13e3414e7226c339b7f6d8844d4eef5ac27228c62c1eadd733d1bdba34258b06", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-13e3414e7226c339b7f6d8844d4eef5ac27228c62c1eadd733d1bdba34258b06", "name": "How many jobs will be done by AI in 5 years time?", "index": 521, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:22:57.421Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-13e3414e7226c339b7f6d8844d4eef5ac27228c62c1eadd733d1bdba34258b06", "values": {"File": "How many jobs will be done by AI in 5 years time?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How many jobs will be done by AI in 5 years time?", "Link": "https://docs.google.com/document/d/1iuatyELC_kgT2HLrk-1bW64pfjfnzwnTn46rxH9ow6E/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T13:12:12.778+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:41.833+01:00", "Status": "Not started", "Edit Answer": "How many jobs will be done by AI in 5 years time?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C77", "Source Link": "", "aisafety.info Link": "How many jobs will be done by AI in 5 years time?", "Source": "", "All Phrasings": "How many jobs will be done by AI in 5 years time?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-09T04:46:06.079+01:00", "UI ID": "8C77", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:21:58.030+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-68e82778dc928c3bdc1aaec8bed5689d7b786920a7082d5d0502214791776850", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-68e82778dc928c3bdc1aaec8bed5689d7b786920a7082d5d0502214791776850", "name": "Why wouldn't AGI's utility function change over time like with humans?", "index": 522, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-15T15:10:59.813Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-68e82778dc928c3bdc1aaec8bed5689d7b786920a7082d5d0502214791776850", "values": {"File": "Why wouldn't AGI's utility function change over time like with humans?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why wouldn't AGI's utility function change over time like with humans?", "Link": "https://docs.google.com/document/d/1d0lqM_AdpKeU3jKOZN3jSfI1eOhmMvGcYMMOKdfU-48/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T13:08:25.363+01:00", "Related Answers DO NOT EDIT": "What is a utility function?,Why do we expect that a superintelligence would closely approximate a utility maximizer?,How might things go wrong with AI even without an agentic superintelligence?", "Tags": "", "Doc Last Edited": "2023-03-15T13:02:27.752+01:00", "Status": "In progress", "Edit Answer": "Why wouldn't AGI's utility function change over time like with humans?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C78", "Source Link": "", "aisafety.info Link": "Why wouldn't AGI's utility function change over time like with humans?", "Source": "", "All Phrasings": "Why wouldn't AGI's utility function change over time like with humans?\n", "Initial Order": "", "Related IDs": "8EL4,7853,7774", "Rich Text DO NOT EDIT": "An agent generally wants to keep valuing whatever it already values, a motive sometimes called \"[goal-content integrity](https://nickbostrom.com/superintelligentwill.pdf)\". The reason is simple: if the agent keeps valuing it, there will be more of it. This point is [often illustrated](https://arbital.com/p/reflective_degree_of_freedom/) with the example of Gandhi considering whether to take a murder pill. If he takes the pill, he'll start valuing murder, causing him to murder people. But by his current values, murder is a terrible consequence, so he refuses the pill.\n\nThis logic makes preserving the utility function an \"[instrumentally convergent](https://docs.google.com/document/d/12cwXk6OcQMGAaxwsFKt0twpBlFlqHHoj2FV01PCyYVw/edit)\" goal, valuable as a step toward a wide range of terminal goals.[^kix.xtky5opwdvs5]\n\nDespite this, the values of humans do change over time. How come? Human motivations are complex and not well described as coherent maximization of any utility function, which makes it hard to give a definite answer. Here are some of the effects that are at play:\n\n- Our ability to control our own \"source code\" is limited. Even when we want to stabilize our values, we don't always succeed: consider an idealistic young person who goes into politics hoping to keep caring about helping people, but then is corrupted into caring only about power.\n\n- Even where we have control, we may not have understanding. We may worry that, if we try to articulate our values and stabilize ourselves toward caring about them, we'll overlook some crucial part and optimize it away.\n\n- We have a concept of moral progress: we may see ourselves as constructing our values, or think of our current values as a provisional guess at our \"true\" values. While we value the integrity of the process as a whole, and would want to stabilize our values against changes that result from, say, cosmic rays, we're often okay with [changes that result from reflection](https://plato.stanford.edu/entries/reflective-equilibrium/).\n\n- We may change our values to more pro-social ones, because we're trying to cooperate with others and our motivations are partly transparent to them.\n\nAnalogous effects can apply to AI systems, which may also come out of a training process without fully coherent goals:\n\n- If an AGI is under development, and its human coders change its values, it may have no choice in the matter (although it might try to [resist that change](https://docs.google.com/document/d/11DfJOPs1himrPJI15w1I_Y8N-ehnYP4vATcsN4fj36I/edit)).\n\n- Even with full access to its source code, it might realize it doesn't understand the consequences of stabilizing itself. (Maybe it understands some of the difficulty of the alignment problem.)\n\n- Some AGIs may be programmed with [indirect normativity](https://publicism.info/philosophy/superintelligence/14.html).\n\n- AGIs may change their utility function as part of a deal with other agents who can read their source code.\n\nWhether agents would see [reasons](https://arbital.com/p/optimized_agent_appears_coherent/) to self-modify toward a [fixed utility function](https://www.alignmentforum.org/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals) in the longer run, as these individual issues were resolved, is a matter of disagreement among the authors of this answer. But in a mature self-modifying superintelligence, the \"goal content integrity\" argument above at least means its goals won't predictably drift to ones strongly contrary to its current goals.\n\n___\n\n[^kix.26d5702p2q0t]: See section 3 of [this paper by Omohundro](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)\n[^kix.xtky5opwdvs5]:See section 3 of [this paper by Omohundro](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)", "Tag Count": 0, "Related Answer Count": 3, "Rich Text": "An agent generally wants to keep valuing whatever it already values, a motive sometimes called \"[goal-content integrity](https://nickbostrom.com/superintelligentwill.pdf)\". The reason is simple: if the agent keeps valuing it, there will be more of it. This point is [often illustrated](https://arbital.com/p/reflective_degree_of_freedom/) with the example of Gandhi considering whether to take a murder pill. If he takes the pill, he'll start valuing murder, causing him to murder people. But by his current values, murder is a terrible consequence, so he refuses the pill.\n\nThis logic makes preserving the utility function an \"[instrumentally convergent](https://docs.google.com/document/d/12cwXk6OcQMGAaxwsFKt0twpBlFlqHHoj2FV01PCyYVw/edit)\" goal, valuable as a step toward a wide range of terminal goals.[^kix.xtky5opwdvs5]\n\nDespite this, the values of humans do change over time. How come? Human motivations are complex and not well described as coherent maximization of any utility function, which makes it hard to give a definite answer. Here are some of the effects that are at play:\n\n- Our ability to control our own \"source code\" is limited. Even when we want to stabilize our values, we don't always succeed: consider an idealistic young person who goes into politics hoping to keep caring about helping people, but then is corrupted into caring only about power.\n\n- Even where we have control, we may not have understanding. We may worry that, if we try to articulate our values and stabilize ourselves toward caring about them, we'll overlook some crucial part and optimize it away.\n\n- We have a concept of moral progress: we may see ourselves as constructing our values, or think of our current values as a provisional guess at our \"true\" values. While we value the integrity of the process as a whole, and would want to stabilize our values against changes that result from, say, cosmic rays, we're often okay with [changes that result from reflection](https://plato.stanford.edu/entries/reflective-equilibrium/).\n\n- We may change our values to more pro-social ones, because we're trying to cooperate with others and our motivations are partly transparent to them.\n\nAnalogous effects can apply to AI systems, which may also come out of a training process without fully coherent goals:\n\n- If an AGI is under development, and its human coders change its values, it may have no choice in the matter (although it might try to [resist that change](https://docs.google.com/document/d/11DfJOPs1himrPJI15w1I_Y8N-ehnYP4vATcsN4fj36I/edit)).\n\n- Even with full access to its source code, it might realize it doesn't understand the consequences of stabilizing itself. (Maybe it understands some of the difficulty of the alignment problem.)\n\n- Some AGIs may be programmed with [indirect normativity](https://publicism.info/philosophy/superintelligence/14.html).\n\n- AGIs may change their utility function as part of a deal with other agents who can read their source code.\n\nWhether agents would see [reasons](https://arbital.com/p/optimized_agent_appears_coherent/) to self-modify toward a [fixed utility function](https://www.alignmentforum.org/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals) in the longer run, as these individual issues were resolved, is a matter of disagreement among the authors of this answer. But in a mature self-modifying superintelligence, the \"goal content integrity\" argument above at least means its goals won't predictably drift to ones strongly contrary to its current goals.\n\n___\n\n[^kix.26d5702p2q0t]: See section 3 of [this paper by Omohundro](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)\n[^kix.xtky5opwdvs5]:See section 3 of [this paper by Omohundro](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-09T10:46:12.392+01:00", "UI ID": "8C78", "Related Answers": "What is a utility function?,Why do we expect that a superintelligence would closely approximate a utility maximizer?,How might things go wrong with AI even without an agentic superintelligence?", "Doc Last Ingested": "2023-03-15T13:11:57.859+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-2d8581f0d2093e576be30f9d5c9b6df2b8b17397c1516b06051239c7727f23ed", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2d8581f0d2093e576be30f9d5c9b6df2b8b17397c1516b06051239c7727f23ed", "name": "What can I do to slow AGI capabilities?", "index": 523, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:10.382Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2d8581f0d2093e576be30f9d5c9b6df2b8b17397c1516b06051239c7727f23ed", "values": {"File": "What can I do to slow AGI capabilities?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What can I do to slow AGI capabilities?", "Link": "https://docs.google.com/document/d/1Evlb6f7eJmQ_Vq3iH8-GmS6BIVniv-v7JU5o1cwpzQM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T13:07:57.774+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:44.523+01:00", "Status": "Not started", "Edit Answer": "What can I do to slow AGI capabilities?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C79", "Source Link": "", "aisafety.info Link": "What can I do to slow AGI capabilities?", "Source": "", "All Phrasings": "What can I do to slow AGI capabilities?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-09T16:55:52.908+01:00", "UI ID": "8C79", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:02.963+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-539fb36f39965ed4e7f9ad6ffbefc2cb79d9004de1e8a8cc94dd0343ff6ad4cd", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-539fb36f39965ed4e7f9ad6ffbefc2cb79d9004de1e8a8cc94dd0343ff6ad4cd", "name": "What are some common objections to the need for AI alignment, and brief responses to these?", "index": 524, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:16.093Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-539fb36f39965ed4e7f9ad6ffbefc2cb79d9004de1e8a8cc94dd0343ff6ad4cd", "values": {"File": "What are some common objections to the need for AI alignment, and brief responses to these?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some common objections to the need for AI alignment, and brief responses to these?", "Link": "https://docs.google.com/document/d/1xW4dySPqY1O-E6Ip-CZXwN0Mjk_jVYU-DXWAnUZWZSQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:56:54.738+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:45.869+01:00", "Status": "Not started", "Edit Answer": "What are some common objections to the need for AI alignment, and brief responses to these?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7A", "Source Link": "", "aisafety.info Link": "What are some common objections to the need for AI alignment, and brief responses to these?", "Source": "", "All Phrasings": "What are some common objections to the need for AI alignment, and brief responses to these?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n![](https://lh5.googleusercontent.com/flTFcDDhzIrPKY1dRR-nFKX2Q0Ie3WTHYsp0PxTIluTDGM2eKQiJ4VuFEQZyvlwBtaPJo4Imsbc3cCipBSqBDMresV7oj2vPp-IX1DxTpIST3PJLs5TOry5tGCTS_AvnMho0mkq7cNyO3ULd-uRlqpYG54xw67Vm)\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n![](https://lh5.googleusercontent.com/flTFcDDhzIrPKY1dRR-nFKX2Q0Ie3WTHYsp0PxTIluTDGM2eKQiJ4VuFEQZyvlwBtaPJo4Imsbc3cCipBSqBDMresV7oj2vPp-IX1DxTpIST3PJLs5TOry5tGCTS_AvnMho0mkq7cNyO3ULd-uRlqpYG54xw67Vm)\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-09T23:19:46.723+01:00", "UI ID": "8C7A", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:04.598+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-e98d0dd60d46cebe0e0980d5176dedf587bbb99bf96503e7147416fbe00efd56", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e98d0dd60d46cebe0e0980d5176dedf587bbb99bf96503e7147416fbe00efd56", "name": "What are concrete plausible stories for how an AI takes over the world?", "index": 525, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:22.085Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e98d0dd60d46cebe0e0980d5176dedf587bbb99bf96503e7147416fbe00efd56", "values": {"File": "What are concrete plausible stories for how an AI takes over the world?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are concrete plausible stories for how an AI takes over the world?", "Link": "https://docs.google.com/document/d/188-alYc7ZZ-rDSDTu-_QWp2Obg6kgKxmnhuyg2ezC8Q/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:56:41.630+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:47.124+01:00", "Status": "Not started", "Edit Answer": "What are concrete plausible stories for how an AI takes over the world?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7B", "Source Link": "", "aisafety.info Link": "What are concrete plausible stories for how an AI takes over the world?", "Source": "", "All Phrasings": "What are concrete plausible stories for how an AI takes over the world?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-10T05:21:20.665+01:00", "UI ID": "8C7B", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:06.448+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-c644a46d47cad53447dd75a791d6048c78e57658e91638dbc1203b3e1620adb1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c644a46d47cad53447dd75a791d6048c78e57658e91638dbc1203b3e1620adb1", "name": "Shouldn't we work on things other than than AI alignment?", "index": 526, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:28.587Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c644a46d47cad53447dd75a791d6048c78e57658e91638dbc1203b3e1620adb1", "values": {"File": "Shouldn't we work on things other than than AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Shouldn't we work on things other than than AI alignment?", "Link": "https://docs.google.com/document/d/1V-EbV9jOYvXUqEqFlr80hW5suloJU2cIgQn-hniGIrc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:53:08.285+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:48.344+01:00", "Status": "Not started", "Edit Answer": "Shouldn't we work on things other than than AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7C", "Source Link": "", "aisafety.info Link": "Shouldn't we work on things other than than AI alignment?", "Source": "", "All Phrasings": "Shouldn't we work on things other than than AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-10T11:22:56.891+01:00", "UI ID": "8C7C", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:08.565+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-284b9f9701a62b574b6578c280bdf76fb32cf2e22fa575762219c2901f6f1a8f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-284b9f9701a62b574b6578c280bdf76fb32cf2e22fa575762219c2901f6f1a8f", "name": "Is there something useful we can ask governments to do for AI alignment?", "index": 527, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:31.470Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-284b9f9701a62b574b6578c280bdf76fb32cf2e22fa575762219c2901f6f1a8f", "values": {"File": "Is there something useful we can ask governments to do for AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is there something useful we can ask governments to do for AI alignment?", "Link": "https://docs.google.com/document/d/12ljNekcdDk5unJOjvjhhw4dpdkEwDouZGhSmozRqRzc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:52:51.410+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-11T00:20:17.580+01:00", "Status": "Not started", "Edit Answer": "Is there something useful we can ask governments to do for AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7D", "Source Link": "", "aisafety.info Link": "Is there something useful we can ask governments to do for AI alignment?", "Source": "", "All Phrasings": "Is there something useful we can ask governments to do for AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n1. \n\n    1. \n\n1. \n\n1. \n\n1. \n\n1. \n\n<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n1. \n\n    1. \n\n1. \n\n1. \n\n1. \n\n1. \n\n<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-10T17:26:14.899+01:00", "UI ID": "8C7D", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:10.210+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 1895, "Helpful": ""}}, {"id": "i-666a3383b8f5a1d3e4b27350216a2ea384ede98d403f50a129374905e09cb085", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-666a3383b8f5a1d3e4b27350216a2ea384ede98d403f50a129374905e09cb085", "name": "What research is being done on neuromorphic AI?", "index": 528, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:37.631Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-666a3383b8f5a1d3e4b27350216a2ea384ede98d403f50a129374905e09cb085", "values": {"File": "What research is being done on neuromorphic AI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What research is being done on neuromorphic AI?", "Link": "https://docs.google.com/document/d/1P8BIKZbK9BiNl23TJHrJItmPd8Sys2BUS7g4T2nU5BA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:52:12.301+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:50.939+01:00", "Status": "Not started", "Edit Answer": "What research is being done on neuromorphic AI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7E", "Source Link": "", "aisafety.info Link": "What research is being done on neuromorphic AI?", "Source": "", "All Phrasings": "What research is being done on neuromorphic AI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7E", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:11.914+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-8c8c68353a6fb1088d08501fd5e256eb3e2e7c7c02892771190c0f1413650715", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8c8c68353a6fb1088d08501fd5e256eb3e2e7c7c02892771190c0f1413650715", "name": "What do AI experts think about AI alignment?", "index": 529, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:43.871Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8c8c68353a6fb1088d08501fd5e256eb3e2e7c7c02892771190c0f1413650715", "values": {"File": "What do AI experts think about AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What do AI experts think about AI alignment?", "Link": "https://docs.google.com/document/d/12TIUtoQqDNLiAxLIoduqXXObXvbTlVFcExLOzCLlmhY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:45:56.139+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:52.158+01:00", "Status": "Not started", "Edit Answer": "What do AI experts think about AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7F", "Source Link": "", "aisafety.info Link": "What do AI experts think about AI alignment?", "Source": "", "All Phrasings": "What do AI experts think about AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7F", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:13.450+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-fd85045c251101184217281fbdfd43b17c50fea9292b8ef2b27c4a24dcaa26d2", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-fd85045c251101184217281fbdfd43b17c50fea9292b8ef2b27c4a24dcaa26d2", "name": "If utility maximizers are so dangerous, is there some other kind of system we could build?", "index": 530, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:51.537Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-fd85045c251101184217281fbdfd43b17c50fea9292b8ef2b27c4a24dcaa26d2", "values": {"File": "If utility maximizers are so dangerous, is there some other kind of system we could build?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "If utility maximizers are so dangerous, is there some other kind of system we could build?", "Link": "https://docs.google.com/document/d/1V6gBaemxghFOA-YQ5JbppUhNKImtYKVXKtTAKhFoA0s/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:44:45.470+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:53.368+01:00", "Status": "Not started", "Edit Answer": "If utility maximizers are so dangerous, is there some other kind of system we could build?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7G", "Source Link": "", "aisafety.info Link": "If utility maximizers are so dangerous, is there some other kind of system we could build?", "Source": "", "All Phrasings": "If utility maximizers are so dangerous, is there some other kind of system we could build?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7G", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:15.002+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-23ce70b9909b3cd36c64a62ed2c5177514750e96a07b49c9c020cd446436cb4f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-23ce70b9909b3cd36c64a62ed2c5177514750e96a07b49c9c020cd446436cb4f", "name": "How might Shard Theory help with alignment?", "index": 531, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-16T02:07:32.946Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-23ce70b9909b3cd36c64a62ed2c5177514750e96a07b49c9c020cd446436cb4f", "values": {"File": "How might Shard Theory help with alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might Shard Theory help with alignment?", "Link": "https://docs.google.com/document/d/1ICF7VfovQtWVUPMm6O6gJAU09hmfGm6iKxjlzJx_A80/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:38:55.104+01:00", "Related Answers DO NOT EDIT": "What is Shard Theory?", "Tags": "", "Doc Last Edited": "2023-03-16T00:40:36.649+01:00", "Status": "Not started", "Edit Answer": "How might Shard Theory help with alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7H", "Source Link": "", "aisafety.info Link": "How might Shard Theory help with alignment?", "Source": "", "All Phrasings": "How might Shard Theory help with alignment?\n", "Initial Order": "", "Related IDs": "8G1G", "Rich Text DO NOT EDIT": "*In process answer*\n\nShard theory could help with alignment in a number of ways.\n\n1.  it hopes to discover how to predict which goal will form under a given reinforcement schedule.\n\n2. Develop powerful interpretability tools, to identify the emerging values\n\n3. Using the example of the emergence of human values as a model for instilling those values in an artificial system.\n\nFor example, since we know some humans value diamonds, there must be some sequence of events which lead to learning to value them. If so, we could give a similar sequence of experiences to an AI system to train it to also value diamonds.\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "*In process answer*\n\nShard theory could help with alignment in a number of ways.\n\n1.  it hopes to discover how to predict which goal will form under a given reinforcement schedule.\n\n2. Develop powerful interpretability tools, to identify the emerging values\n\n3. Using the example of the emergence of human values as a model for instilling those values in an artificial system.\n\nFor example, since we know some humans value diamonds, there must be some sequence of events which lead to learning to value them. If so, we could give a similar sequence of experiences to an AI system to train it to also value diamonds.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7H", "Related Answers": "What is Shard Theory?", "Doc Last Ingested": "2023-03-16T01:11:56.854+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 1, "Helpful": ""}}, {"id": "i-1f770ef8f90e3d37ea82cd72765a2dc77c6e2072a47c39b159a7859f72842c87", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1f770ef8f90e3d37ea82cd72765a2dc77c6e2072a47c39b159a7859f72842c87", "name": "What is an adversarial oversight scheme?", "index": 532, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:24:06.047Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1f770ef8f90e3d37ea82cd72765a2dc77c6e2072a47c39b159a7859f72842c87", "values": {"File": "What is an adversarial oversight scheme?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is an adversarial oversight scheme?", "Link": "https://docs.google.com/document/d/1rHko8Hjh4NgsJRfrVibQf85mqgp5zE_IEF7-1o_D7A0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:36:27.372+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:55.963+01:00", "Status": "Not started", "Edit Answer": "What is an adversarial oversight scheme?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7I", "Source Link": "", "aisafety.info Link": "What is an adversarial oversight scheme?", "Source": "", "All Phrasings": "What is an adversarial oversight scheme?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7I", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:18.652+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-4cc729590b829874d91d130698efea15e723227f27df60a8151a6f71b02566a3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4cc729590b829874d91d130698efea15e723227f27df60a8151a6f71b02566a3", "name": "What is the purpose of the Visible Thoughts Project?", "index": 533, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:24:10.210Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4cc729590b829874d91d130698efea15e723227f27df60a8151a6f71b02566a3", "values": {"File": "What is the purpose of the Visible Thoughts Project?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the purpose of the Visible Thoughts Project?", "Link": "https://docs.google.com/document/d/1q0fgfwWbBhGRs4tAqDKAPTnSugN6PvbrB5Td-wec4ko/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:36:02.269+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:57.191+01:00", "Status": "Not started", "Edit Answer": "What is the purpose of the Visible Thoughts Project?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7J", "Source Link": "", "aisafety.info Link": "What is the purpose of the Visible Thoughts Project?", "Source": "", "All Phrasings": "What is the purpose of the Visible Thoughts Project?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7J", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:20.510+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-4cf4c278a8336e5e3a6b476420bc188a8cec662a6cc21585a46e1fbf57f66314", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4cf4c278a8336e5e3a6b476420bc188a8cec662a6cc21585a46e1fbf57f66314", "name": "How can I decide which software tools to release?", "index": 534, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:24:16.038Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4cf4c278a8336e5e3a6b476420bc188a8cec662a6cc21585a46e1fbf57f66314", "values": {"File": "How can I decide which software tools to release?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How can I decide which software tools to release?", "Link": "https://docs.google.com/document/d/10t-O8WfBLAKZUmDkkWXasmHCSqT4UPfIoxtazbJS5go/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:35:50.684+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:58.472+01:00", "Status": "Not started", "Edit Answer": "How can I decide which software tools to release?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7K", "Source Link": "", "aisafety.info Link": "How can I decide which software tools to release?", "Source": "", "All Phrasings": "How can I decide which software tools to release?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7K", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:21.499+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-b585526234199b264916f69ae11da117c5faa007390c16e0f9732e7f0dfc91e9", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b585526234199b264916f69ae11da117c5faa007390c16e0f9732e7f0dfc91e9", "name": "Who contributed to Stampy\u2019s AI Safety Info?", "index": 535, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:24:21.194Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b585526234199b264916f69ae11da117c5faa007390c16e0f9732e7f0dfc91e9", "values": {"File": "Who contributed to Stampy\u2019s AI Safety Info?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Who contributed to Stampy\u2019s AI Safety Info?", "Link": "https://docs.google.com/document/d/1_yUCSxXm9h7N_tE_HKuVYvp78R5JClZBSIN0UZaV5Y4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:33:26.199+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:41:59.668+01:00", "Status": "Not started", "Edit Answer": "Who contributed to Stampy\u2019s AI Safety Info?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7L", "Source Link": "", "aisafety.info Link": "Who contributed to Stampy\u2019s AI Safety Info?", "Source": "", "All Phrasings": "Who contributed to Stampy\u2019s AI Safety Info?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7L", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:23.492+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-c39fd55312540dac7760922aa632d43899203c62370e0531048cb0c5729ceba1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c39fd55312540dac7760922aa632d43899203c62370e0531048cb0c5729ceba1", "name": "Why not just let AI take over?", "index": 536, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:24:35.417Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c39fd55312540dac7760922aa632d43899203c62370e0531048cb0c5729ceba1", "values": {"File": "Why not just let AI take over?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why not just let AI take over?", "Link": "https://docs.google.com/document/d/1mxopnznngoJPLhtV-dnieZ-L-rOl-SI2feHtJSmId68/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:31:23.488+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:42:00.832+01:00", "Status": "Not started", "Edit Answer": "Why not just let AI take over?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7M", "Source Link": "", "aisafety.info Link": "Why not just let AI take over?", "Source": "", "All Phrasings": "Why not just let AI take over?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7M", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:25.026+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-3705d699d7e7acbae6b957e3a8bd498285bad8c4e72caaee247b9152b094e696", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3705d699d7e7acbae6b957e3a8bd498285bad8c4e72caaee247b9152b094e696", "name": "Would AI be able to reach a point of \"maturity\" where it would perceive value in correction?", "index": 537, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:24:43.243Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3705d699d7e7acbae6b957e3a8bd498285bad8c4e72caaee247b9152b094e696", "values": {"File": "Would AI be able to reach a point of \"maturity\" where it would perceive value in correction?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Would AI be able to reach a point of \"maturity\" where it would perceive value in correction?", "Link": "https://docs.google.com/document/d/1VPQY55gV34YopGFWyo4z0caanXM8jajajIf4r9xc9EU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:28:31.464+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:42:02.079+01:00", "Status": "Not started", "Edit Answer": "Would AI be able to reach a point of \"maturity\" where it would perceive value in correction?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7N", "Source Link": "", "aisafety.info Link": "Would AI be able to reach a point of \"maturity\" where it would perceive value in correction?", "Source": "", "All Phrasings": "Would AI be able to reach a point of \"maturity\" where it would perceive value in correction?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7N", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:26.047+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-bd6166267a8f1bf37cea72b0dd4cc9afd956b5e4439a7935c0508d60df4f5d8d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-bd6166267a8f1bf37cea72b0dd4cc9afd956b5e4439a7935c0508d60df4f5d8d", "name": "What work is Redwood doing on LLM interpretability?", "index": 538, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:24:54.823Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-bd6166267a8f1bf37cea72b0dd4cc9afd956b5e4439a7935c0508d60df4f5d8d", "values": {"File": "What work is Redwood doing on LLM interpretability?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What work is Redwood doing on LLM interpretability?", "Link": "https://docs.google.com/document/d/12HLTqg4YaF1TImi-jcowd9ioyZsY76211CiN9rLW6_Q/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:24:28.700+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:42:03.298+01:00", "Status": "Not started", "Edit Answer": "What work is Redwood doing on LLM interpretability?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7O", "Source Link": "", "aisafety.info Link": "What work is Redwood doing on LLM interpretability?", "Source": "", "All Phrasings": "What work is Redwood doing on LLM interpretability?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7O", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:27.823+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-30995381a47a3e81bb319fd2eac8d79e41d40be2dd03cf6e1e87248d9d33b0e4", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-30995381a47a3e81bb319fd2eac8d79e41d40be2dd03cf6e1e87248d9d33b0e4", "name": "What is the goal of Simulacra Theory?", "index": 539, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:25:06.745Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-30995381a47a3e81bb319fd2eac8d79e41d40be2dd03cf6e1e87248d9d33b0e4", "values": {"File": "What is the goal of Simulacra Theory?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the goal of Simulacra Theory?", "Link": "https://docs.google.com/document/d/1hb1M-0qaGyYhGOh5D8jil6BFKmqGZYSaQinkCHmvF-4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:22:50.687+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:42:04.508+01:00", "Status": "Not started", "Edit Answer": "What is the goal of Simulacra Theory?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7P", "Source Link": "", "aisafety.info Link": "What is the goal of Simulacra Theory?", "Source": "", "All Phrasings": "What is the goal of Simulacra Theory?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7P", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:28.873+01:00", "Request Count": 1, "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-d73ab0647446d010d43870797a8d01cfbf6fec2e881898eb5ffd7a7fcc0ab2b5", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d73ab0647446d010d43870797a8d01cfbf6fec2e881898eb5ffd7a7fcc0ab2b5", "name": "What is the goal of Simulacra Theory?", "index": 540, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-02-25T20:50:58.645Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d73ab0647446d010d43870797a8d01cfbf6fec2e881898eb5ffd7a7fcc0ab2b5", "values": {"File": "What is the goal of Simulacra Theory?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the goal of Simulacra Theory?", "Link": "https://docs.google.com/document/d/1pTdsvFEzK3N2_wOo91oS4QCwwWL9fi4RMpqeQ_iQf_o/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:22:49.215+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:42:05.853+01:00", "Status": "Not started", "Edit Answer": "What is the goal of Simulacra Theory?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "-1", "Source Link": "", "aisafety.info Link": "What is the goal of Simulacra Theory?", "Source": "", "All Phrasings": "What is the goal of Simulacra Theory?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "-1", "Related Answers": "", "Doc Last Ingested": "2023-02-22T22:52:35.658+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-e08b8abe1899a0ee688f35b4ea98c214c51590aaa8dea35365c4920da67bccd8", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e08b8abe1899a0ee688f35b4ea98c214c51590aaa8dea35365c4920da67bccd8", "name": "What is Superintelligence: Paths, Dangers, Strategies by Nick Bostrom about?", "index": 541, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:25:14.850Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e08b8abe1899a0ee688f35b4ea98c214c51590aaa8dea35365c4920da67bccd8", "values": {"File": "What is Superintelligence: Paths, Dangers, Strategies by Nick Bostrom about?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Superintelligence: Paths, Dangers, Strategies by Nick Bostrom about?", "Link": "https://docs.google.com/document/d/1QCjQLSRbRqNmvSBAWrA6sVpq-qDToa-Ejw-VPA-bljc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:21:27.990+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:42:07.129+01:00", "Status": "Not started", "Edit Answer": "What is Superintelligence: Paths, Dangers, Strategies by Nick Bostrom about?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7R", "Source Link": "", "aisafety.info Link": "What is Superintelligence: Paths, Dangers, Strategies by Nick Bostrom about?", "Source": "", "All Phrasings": "What is Superintelligence: Paths, Dangers, Strategies by Nick Bostrom about?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7R", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:30.518+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-ad3416a7dac0b62e82e6264eef7940ee5503ac777ae80c85cf6352fb51a8b478", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ad3416a7dac0b62e82e6264eef7940ee5503ac777ae80c85cf6352fb51a8b478", "name": "Are corporations superintelligent?", "index": 542, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:22:48.147Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ad3416a7dac0b62e82e6264eef7940ee5503ac777ae80c85cf6352fb51a8b478", "values": {"File": "Are corporations superintelligent?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Are corporations superintelligent?", "Link": "https://docs.google.com/document/d/1X9yPbHXz-Bh69cOThzzXdR_YytyGeQmDERPQL0DSlq0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T01:44:25.546+01:00", "Related Answers DO NOT EDIT": "What is \"greater-than-human intelligence\"?,What is \"superintelligence\"?", "Tags": "", "Doc Last Edited": "2023-03-12T23:27:52.675+01:00", "Status": "Live on site", "Edit Answer": "Are corporations superintelligent?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7S", "Source Link": "", "aisafety.info Link": "Are corporations superintelligent?", "Source": "", "All Phrasings": "Are corporations superintelligent?\n", "Initial Order": "", "Related IDs": "6587,6207", "Rich Text DO NOT EDIT": "<iframe src=\"https://www.youtube.com/embed/L5pUA3LsEaw\" title=\"Why Not Just: Think of AGI Like a Corporation?\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\nCorporations are superintelligent in a limited sense. Nick Bostrom, in his book *Superintelligence*, [distinguishes](https://publicism.info/philosophy/superintelligence/4.html) between \"speed superintelligence\", \"collective superintelligence\", and \"quality superintelligence\".\n\nOut of these, corporations come closest to **collective superintelligence**. Though Bostrom reserves the term \u201ccollective *superintelligence*\u201d for hypothetical systems much more powerful than current human groups, corporations are strong examples of collective *intelligence*. They can perform cognitive tasks far beyond the abilities of any one human, as long as those tasks can be decomposed into human-sized pieces. For example, they can design every part of a smartphone, or sell coffee in thousands of places simultaneously.\n\nOn the other hand, corporations don't have **[speed superintelligence](https://docs.google.com/document/u/0/d/1qcJhXN9jSV-HzNYX8dEagvDhRuzCThaAOuhTfyV9HnA/edit)**: no matter how many humans work together, they'll never program an operating system in one minute, or play great chess in one second per move. Nor do they have **quality superintelligence**: ten thousand average physicists collaborating to invent general relativity for the first time would probably fail where Einstein succeeded. Einstein was thinking on a qualitatively higher level, and even though his thinking was made of parts assembled into a smarter whole (his brain was made of neurons), nobody knows of a practically feasible way to decompose inventing general relativity into a thousand much easier tasks.\n\nAI systems could be created one day that had all three of these types of superintelligence, presenting major challenges we\u2019ve never had to face when dealing with corporations.\n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "<iframe src=\"https://www.youtube.com/embed/L5pUA3LsEaw\" title=\"Why Not Just: Think of AGI Like a Corporation?\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\nCorporations are superintelligent in a limited sense. Nick Bostrom, in his book *Superintelligence*, [distinguishes](https://publicism.info/philosophy/superintelligence/4.html) between \"speed superintelligence\", \"collective superintelligence\", and \"quality superintelligence\".\n\nOut of these, corporations come closest to **collective superintelligence**. Though Bostrom reserves the term \u201ccollective *superintelligence*\u201d for hypothetical systems much more powerful than current human groups, corporations are strong examples of collective *intelligence*. They can perform cognitive tasks far beyond the abilities of any one human, as long as those tasks can be decomposed into human-sized pieces. For example, they can design every part of a smartphone, or sell coffee in thousands of places simultaneously.\n\nOn the other hand, corporations don't have **[speed superintelligence](https://docs.google.com/document/u/0/d/1qcJhXN9jSV-HzNYX8dEagvDhRuzCThaAOuhTfyV9HnA/edit)**: no matter how many humans work together, they'll never program an operating system in one minute, or play great chess in one second per move. Nor do they have **quality superintelligence**: ten thousand average physicists collaborating to invent general relativity for the first time would probably fail where Einstein succeeded. Einstein was thinking on a qualitatively higher level, and even though his thinking was made of parts assembled into a smarter whole (his brain was made of neurons), nobody knows of a practically feasible way to decompose inventing general relativity into a thousand much easier tasks.\n\nAI systems could be created one day that had all three of these types of superintelligence, presenting major challenges we\u2019ve never had to face when dealing with corporations.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7S", "Related Answers": "What is \"greater-than-human intelligence\"?,What is \"superintelligence\"?", "Doc Last Ingested": "2023-03-15T00:21:54.863+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-1c42927a7b33a8563d9be05374391ba505c62a008d83073144049ca7a7d9b20e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1c42927a7b33a8563d9be05374391ba505c62a008d83073144049ca7a7d9b20e", "name": "What are the \"no free lunch\" theorems?", "index": 543, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:25:28.778Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1c42927a7b33a8563d9be05374391ba505c62a008d83073144049ca7a7d9b20e", "values": {"File": "What are the \"no free lunch\" theorems?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the \"no free lunch\" theorems?", "Link": "https://docs.google.com/document/d/1oO60b2GCvN6QTZlD9hLqSQWH50T4VC2M-2B6Q0gmsnI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T01:43:37.461+01:00", "Related Answers DO NOT EDIT": "Is an AGI even possible?", "Tags": "", "Doc Last Edited": "2023-03-10T09:42:53.315+01:00", "Status": "In review", "Edit Answer": "What are the \"no free lunch\" theorems?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7T", "Source Link": "", "aisafety.info Link": "What are the \"no free lunch\" theorems?", "Source": "", "All Phrasings": "What are the \"no free lunch\" theorems?\n", "Initial Order": "", "Related IDs": "8AV5", "Rich Text DO NOT EDIT": "[\u201cNo free lunch\u201d](https://www.lesswrong.com/posts/Dyt2TDdMGHDkXPcpp/the-no-free-lunch-theorem-for-dummies)[theorems](https://en.wikipedia.org/wiki/No_free_lunch_theorem) are a collection of mathematical results which show that, in principle, all learning algorithms will do equally well over all learning tasks. In other words, if I have an algorithm that does better than chance at predicting certain things, it will do worse than chance at other (hypothetical) things (such as completely random distributions).\n\n[Fran\u00e7ois Chollet](https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec) and others have argued that these theorems imply that completely general intelligence is impossible and the fears around AGI existential risk are overblown.\n\nHowever, these theorems hold only if we are considering all theoretically possible distributions. For example, if our algorithm does better in a certain environment, it will do worse than chance in a completely disorderd environment, or in an environment designed to trick our algorithm. But if we know that the environment that our algorithm operates in has a certain structure, then the \u201cno free lunch\u201d results are not an impediment to designing algorithms with superior predictions or ability to optimize.\n\nTherefore, \u201c[no free lunch theorems\u201d are often irrelevant](https://arbital.com/p/nofreelunch_irrelevant/) for practical questions of designing an AI. We aren\u2019t interested in \u2018predicting\u2019 completely random sequences and don\u2019t mind if another algorithm outperforms us on that \u2018task\u2019. Our universe is not maximally random, and the laws of physics provide a lot of structure which make prediction possible (paying for lunch in universes which aren\u2019t ours). Within the structure provided by the universe, human beings have been successful at research and development.\n\nThis indicates that \u201cno free lunch\u201d theorems wouldn\u2019t prevent artificial systems from being many times more effective at that task while also exceeding human abilities in many other areas. This would be a serious threat without being sufficiently \u2018general\u2019 for these theorems to kick in.\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "[\u201cNo free lunch\u201d](https://www.lesswrong.com/posts/Dyt2TDdMGHDkXPcpp/the-no-free-lunch-theorem-for-dummies)[theorems](https://en.wikipedia.org/wiki/No_free_lunch_theorem) are a collection of mathematical results which show that, in principle, all learning algorithms will do equally well over all learning tasks. In other words, if I have an algorithm that does better than chance at predicting certain things, it will do worse than chance at other (hypothetical) things (such as completely random distributions).\n\n[Fran\u00e7ois Chollet](https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec) and others have argued that these theorems imply that completely general intelligence is impossible and the fears around AGI existential risk are overblown.\n\nHowever, these theorems hold only if we are considering all theoretically possible distributions. For example, if our algorithm does better in a certain environment, it will do worse than chance in a completely disorderd environment, or in an environment designed to trick our algorithm. But if we know that the environment that our algorithm operates in has a certain structure, then the \u201cno free lunch\u201d results are not an impediment to designing algorithms with superior predictions or ability to optimize.\n\nTherefore, \u201c[no free lunch theorems\u201d are often irrelevant](https://arbital.com/p/nofreelunch_irrelevant/) for practical questions of designing an AI. We aren\u2019t interested in \u2018predicting\u2019 completely random sequences and don\u2019t mind if another algorithm outperforms us on that \u2018task\u2019. Our universe is not maximally random, and the laws of physics provide a lot of structure which make prediction possible (paying for lunch in universes which aren\u2019t ours). Within the structure provided by the universe, human beings have been successful at research and development.\n\nThis indicates that \u201cno free lunch\u201d theorems wouldn\u2019t prevent artificial systems from being many times more effective at that task while also exceeding human abilities in many other areas. This would be a serious threat without being sufficiently \u2018general\u2019 for these theorems to kick in.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7T", "Related Answers": "Is an AGI even possible?", "Doc Last Ingested": "2023-03-15T00:22:31.928+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 270, "Helpful": ""}}, {"id": "i-0e24eea2c4c78a997ca1996c3a82880192b32576d259db1971e124e520de5e3d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0e24eea2c4c78a997ca1996c3a82880192b32576d259db1971e124e520de5e3d", "name": "What are infohazards?", "index": 544, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:25:31.845Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0e24eea2c4c78a997ca1996c3a82880192b32576d259db1971e124e520de5e3d", "values": {"File": "What are infohazards?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are infohazards?", "Link": "https://docs.google.com/document/d/1vBr_1iZOdHqRR_RQeLFe0V-7uUUe8_uyWOi1zSMKdQ8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T01:43:30.225+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-03T23:45:33.061+01:00", "Status": "In progress", "Edit Answer": "What are infohazards?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7U", "Source Link": "", "aisafety.info Link": "What are infohazards?", "Source": "", "All Phrasings": "What are infohazards?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\u2018Infohazard\u2019 is short for information hazard. An infohazard is *true* information that could harm people, or other sentient beings, if known (or known widely). An extreme example in AI safety could be if you discovered how to easily create a transformative AI, but not how to align it. Because wider dissemination of this information would presumably be harmful, this would be considered an infohazard.\n\nIt\u2019s worth noting that information does not need to represent a large or existential risk in order to be classified as an infohazard; as [Bostrom (2011)](https://nickbostrom.com/information-hazards.pdf) writes:\n\n*\u201cSpoilers constitute a special kind of disappointment. Many forms of entertainment depend on the marshalling of ignorance. Hide-and-seek would be less fun if there were no way to hide and no need to seek. For some, knowing the day and the hour of their death long in advance might cast shadow over their existence.\u201d*\n\n[https://www.lesswrong.com/tag/information-hazards](https://www.lesswrong.com/tag/information-hazards)\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\u2018Infohazard\u2019 is short for information hazard. An infohazard is *true* information that could harm people, or other sentient beings, if known (or known widely). An extreme example in AI safety could be if you discovered how to easily create a transformative AI, but not how to align it. Because wider dissemination of this information would presumably be harmful, this would be considered an infohazard.\n\nIt\u2019s worth noting that information does not need to represent a large or existential risk in order to be classified as an infohazard; as [Bostrom (2011)](https://nickbostrom.com/information-hazards.pdf) writes:\n\n*\u201cSpoilers constitute a special kind of disappointment. Many forms of entertainment depend on the marshalling of ignorance. Hide-and-seek would be less fun if there were no way to hide and no need to seek. For some, knowing the day and the hour of their death long in advance might cast shadow over their existence.\u201d*\n\n[https://www.lesswrong.com/tag/information-hazards](https://www.lesswrong.com/tag/information-hazards)\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7U", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:33.779+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-31bdf8560038f6c828ecbd0d70b432a133f90a46482f19118bebd2c77bfd2db7", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-31bdf8560038f6c828ecbd0d70b432a133f90a46482f19118bebd2c77bfd2db7", "name": "If I became much smarter quickly, would I remain aligned with myself?", "index": 545, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:07.524Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-31bdf8560038f6c828ecbd0d70b432a133f90a46482f19118bebd2c77bfd2db7", "values": {"File": "If I became much smarter quickly, would I remain aligned with myself?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "If I became much smarter quickly, would I remain aligned with myself?", "Link": "https://docs.google.com/document/d/16MbIsHO0dcQjfrrCprxABr4oNRA5ART1aIW18rearC4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T01:42:33.502+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-04T23:53:11.004+01:00", "Status": "In progress", "Edit Answer": "If I became much smarter quickly, would I remain aligned with myself?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7V", "Source Link": "", "aisafety.info Link": "If I became much smarter quickly, would I remain aligned with myself?", "Source": "", "All Phrasings": "If I became much smarter quickly, would I remain aligned with myself?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7V", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:01.863+01:00", "Request Count": "", "Number of suggestions on answer doc": 5, "Total character count of suggestions on answer doc": 3155, "Helpful": ""}}, {"id": "i-23d80ef5799abca3f0cf8d40188745d1acd62894bacd650a927b3b1ac187bd6e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-23d80ef5799abca3f0cf8d40188745d1acd62894bacd650a927b3b1ac187bd6e", "name": "What is an optimization process?", "index": 546, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:13.177Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-23d80ef5799abca3f0cf8d40188745d1acd62894bacd650a927b3b1ac187bd6e", "values": {"File": "What is an optimization process?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is an optimization process?", "Link": "https://docs.google.com/document/d/1CoeLLw-Rz8CT4FLO5KkngSDrR8-px98WBrC1f7ykd1w/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T01:41:19.828+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-28T00:13:26.286+01:00", "Status": "Not started", "Edit Answer": "What is an optimization process?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7W", "Source Link": "", "aisafety.info Link": "What is an optimization process?", "Source": "", "All Phrasings": "What is an optimization process?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[https://www.lesswrong.com/tag/optimization](https://www.lesswrong.com/tag/optimization)\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "[https://www.lesswrong.com/tag/optimization](https://www.lesswrong.com/tag/optimization)\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7W", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:03.941+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-c734c6e15a19eb7950523319519c77cdf00f2b42128328f236028240bfa427bb", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c734c6e15a19eb7950523319519c77cdf00f2b42128328f236028240bfa427bb", "name": "How plausible is AI existential risk?", "index": 547, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:19.371Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c734c6e15a19eb7950523319519c77cdf00f2b42128328f236028240bfa427bb", "values": {"File": "How plausible is AI existential risk?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How plausible is AI existential risk?", "Link": "https://docs.google.com/document/d/1WzhvFld8N32O9meCnNJ5CfmsGe2sFvUxgguHZkFPXRU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T01:40:56.426+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:42:14.365+01:00", "Status": "Not started", "Edit Answer": "How plausible is AI existential risk?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7X", "Source Link": "", "aisafety.info Link": "How plausible is AI existential risk?", "Source": "", "All Phrasings": "How plausible is AI existential risk?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7X", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:06.353+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-9a9ac32f3ce16b7b297dfdc5265c6486aebc6d15df07e0c1d1958e7af6b6272c", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-9a9ac32f3ce16b7b297dfdc5265c6486aebc6d15df07e0c1d1958e7af6b6272c", "name": "What is a \"True Name\" in agent foundations?", "index": 548, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:25:37.132Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-9a9ac32f3ce16b7b297dfdc5265c6486aebc6d15df07e0c1d1958e7af6b6272c", "values": {"File": "What is a \"True Name\" in agent foundations?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is a \"True Name\" in agent foundations?", "Link": "https://docs.google.com/document/d/1Yp7JqgMxngWvMxh8vh0_1kyg3hgYeEGhrLsxO9xXX9w/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T01:40:52.539+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:42:15.539+01:00", "Status": "Not started", "Edit Answer": "What is a \"True Name\" in agent foundations?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7Y", "Source Link": "", "aisafety.info Link": "What is a \"True Name\" in agent foundations?", "Source": "", "All Phrasings": "What is a \"True Name\" in agent foundations?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7Y", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:38.334+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-26f2967f71999aa26c102a3388f0808414fb9dab1a595ffabcc4860e3073ebaf", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-26f2967f71999aa26c102a3388f0808414fb9dab1a595ffabcc4860e3073ebaf", "name": "How might language models be relevant to AI alignment?", "index": 549, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:34.647Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-26f2967f71999aa26c102a3388f0808414fb9dab1a595ffabcc4860e3073ebaf", "values": {"File": "How might language models be relevant to AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might language models be relevant to AI alignment?", "Link": "https://docs.google.com/document/d/1LTHvir5ZLx2obv3uP-QBHwyOO3Q8eSzva3TsaaM7ZI4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T01:32:28.805+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:42:16.670+01:00", "Status": "Not started", "Edit Answer": "How might language models be relevant to AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C7Z", "Source Link": "", "aisafety.info Link": "How might language models be relevant to AI alignment?", "Source": "", "All Phrasings": "How might language models be relevant to AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C7Z", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:10.314+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-7a91931289ad236bc9150ac676d260130534052ff3dfd0a4112e643509afbf9f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7a91931289ad236bc9150ac676d260130534052ff3dfd0a4112e643509afbf9f", "name": "How might an AI takeover happen?", "index": 550, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:40.595Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7a91931289ad236bc9150ac676d260130534052ff3dfd0a4112e643509afbf9f", "values": {"File": "How might an AI takeover happen?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might an AI takeover happen?", "Link": "https://docs.google.com/document/d/11zDlFAcx8U8uGH8MLCH5HTUZblUPGdRgjdjTAJa0xzc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T01:32:17.820+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:42:17.803+01:00", "Status": "Not started", "Edit Answer": "How might an AI takeover happen?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C80", "Source Link": "", "aisafety.info Link": "How might an AI takeover happen?", "Source": "", "All Phrasings": "How might an AI takeover happen?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C80", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:12.237+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-2cf6f90d781296ca7632df835bc21fb3d9a450309a6eeec62a6f6a89b6f32ef4", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2cf6f90d781296ca7632df835bc21fb3d9a450309a6eeec62a6f6a89b6f32ef4", "name": "How did GPT-3 affect people's thinking on alignment?", "index": 551, "createdAt": "2023-02-22T21:08:58.516Z", "updatedAt": "2023-03-14T23:23:46.737Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2cf6f90d781296ca7632df835bc21fb3d9a450309a6eeec62a6f6a89b6f32ef4", "values": {"File": "How did GPT-3 affect people's thinking on alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How did GPT-3 affect people's thinking on alignment?", "Link": "https://docs.google.com/document/d/13WIDB1Dnnmg8j9C4QQkErnFNugI8RjjOQKtSDUV5uOE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T01:31:01.985+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:42:18.953+01:00", "Status": "Not started", "Edit Answer": "How did GPT-3 affect people's thinking on alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8C81", "Source Link": "", "aisafety.info Link": "How did GPT-3 affect people's thinking on alignment?", "Source": "", "All Phrasings": "How did GPT-3 affect people's thinking on alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8C81", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:14.393+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-7f9e5431dac475c26ac107cdefe24104d9c54edf281dfec8540feaf1954034ac", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7f9e5431dac475c26ac107cdefe24104d9c54edf281dfec8540feaf1954034ac", "name": "This is a test question1", "index": 519, "createdAt": "2023-02-22T20:21:39.677Z", "updatedAt": "2023-02-22T21:17:01.350Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7f9e5431dac475c26ac107cdefe24104d9c54edf281dfec8540feaf1954034ac", "values": {"File": "This is a test question1", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "This is a test question1", "Link": "https://docs.google.com/document/d/1blkuRGBhZTChm2A3tA5019Z_oQ2xzkqS2nw4NBFP5iI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-22T20:48:08.274+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T21:34:01.403+01:00", "Status": "Uncategorized", "Edit Answer": "This is a test question1", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "-1", "Source Link": "", "aisafety.info Link": "This is a test question1", "Source": "", "All Phrasings": "This is a test question1\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "-1", "Related Answers": "", "Doc Last Ingested": "2023-02-22T21:41:53.341+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-3e9c26d6c641cba166e7d63460e65a780dbddd2d3ae45158e4c3940ec8d5de60", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3e9c26d6c641cba166e7d63460e65a780dbddd2d3ae45158e4c3940ec8d5de60", "name": "Live on site", "index": 517, "createdAt": "2023-02-22T19:27:04.557Z", "updatedAt": "2023-02-22T21:45:15.435Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3e9c26d6c641cba166e7d63460e65a780dbddd2d3ae45158e4c3940ec8d5de60", "values": {"File": "Live on site", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Live on site", "Link": "https://drive.google.com/drive/folders/1feloLCiyc3XSxfaQ0L_fqVVsFMupw2JM", "Thumbnail": "", "Doc Created": "2023-02-22T20:13:31.108+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T20:15:39.038+01:00", "Status": "Uncategorized", "Edit Answer": "Live on site", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8BB6", "Source Link": "", "aisafety.info Link": "Live on site", "Source": "", "All Phrasings": "Live on site\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8BB6", "Related Answers": "", "Doc Last Ingested": "", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-41d593ece0c1793aa51f1ccb7efe14fffa5ffec39454197cef61cd17beb9c158", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-41d593ece0c1793aa51f1ccb7efe14fffa5ffec39454197cef61cd17beb9c158", "name": "In progress", "index": 518, "createdAt": "2023-02-22T19:27:04.557Z", "updatedAt": "2023-02-22T21:45:15.435Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-41d593ece0c1793aa51f1ccb7efe14fffa5ffec39454197cef61cd17beb9c158", "values": {"File": "In progress", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "In progress", "Link": "https://drive.google.com/drive/folders/1U2h3Tte38EkOff9flwo6FKVZn8OhkNLW", "Thumbnail": "", "Doc Created": "2023-02-22T20:14:18.476+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T20:15:18.705+01:00", "Status": "Uncategorized", "Edit Answer": "In progress", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8BB7", "Source Link": "", "aisafety.info Link": "In progress", "Source": "", "All Phrasings": "In progress\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8BB7", "Related Answers": "", "Doc Last Ingested": "", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-0691d34b77005a3eecb935e3f6396049054e5e6efd12506477c573c2d3313760", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0691d34b77005a3eecb935e3f6396049054e5e6efd12506477c573c2d3313760", "name": "Is there a clear theory of change for AI safety work?", "index": 495, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:23:54.972Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0691d34b77005a3eecb935e3f6396049054e5e6efd12506477c573c2d3313760", "values": {"File": "Is there a clear theory of change for AI safety work?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is there a clear theory of change for AI safety work?", "Link": "https://docs.google.com/document/d/16e__YX7IpTf1qAnARAneYC15KQkVRUrgf75xt1ZuzEg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:16:20.507+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:25.839+01:00", "Status": "Not started", "Edit Answer": "Is there a clear theory of change for AI safety work?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AUO", "Source Link": "", "aisafety.info Link": "Is there a clear theory of change for AI safety work?", "Source": "", "All Phrasings": "Is there a clear theory of change for AI safety work?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AUO", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:16.501+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-c31e5a4f899de7703c504e5b6530d3c546c91195b206f283a5fce4e20d6e38ad", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c31e5a4f899de7703c504e5b6530d3c546c91195b206f283a5fce4e20d6e38ad", "name": "Why do so many AI safety projects not have clear benefits?", "index": 496, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:24:02.163Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c31e5a4f899de7703c504e5b6530d3c546c91195b206f283a5fce4e20d6e38ad", "values": {"File": "Why do so many AI safety projects not have clear benefits?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why do so many AI safety projects not have clear benefits?", "Link": "https://docs.google.com/document/d/1Tj0VPSceDcim3RpW8svhgj5eAB4RIDeiscB-ptmXtGI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:16:09.031+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:27.052+01:00", "Status": "Not started", "Edit Answer": "Why do so many AI safety projects not have clear benefits?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AUP", "Source Link": "", "aisafety.info Link": "Why do so many AI safety projects not have clear benefits?", "Source": "", "All Phrasings": "Why do so many AI safety projects not have clear benefits?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AUP", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:18.578+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-bc44fddea4413a83776f724fd76a57c4513d94d617100ca153eac654ae40ce8c", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-bc44fddea4413a83776f724fd76a57c4513d94d617100ca153eac654ae40ce8c", "name": "Is there a way of preventing rouge researchers from making their own AGI?", "index": 497, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:25:43.180Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-bc44fddea4413a83776f724fd76a57c4513d94d617100ca153eac654ae40ce8c", "values": {"File": "Is there a way of preventing rouge researchers from making their own AGI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is there a way of preventing rouge researchers from making their own AGI?", "Link": "https://docs.google.com/document/d/1NWxNMWyRiIh7ch1FXpONBQ12YdqA3F3bTNpG145YMCw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:15:39.926+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:28.304+01:00", "Status": "Not started", "Edit Answer": "Is there a way of preventing rouge researchers from making their own AGI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AUQ", "Source Link": "", "aisafety.info Link": "Is there a way of preventing rouge researchers from making their own AGI?", "Source": "", "All Phrasings": "Is there a way of preventing rouge researchers from making their own AGI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AUQ", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:42.941+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-76a15d86816c98977109263494830add60180024f77d24c237d5782518e9bf0e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-76a15d86816c98977109263494830add60180024f77d24c237d5782518e9bf0e", "name": "What proportion of possible AGIs would be devastatingly bad?", "index": 498, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:24:18.661Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-76a15d86816c98977109263494830add60180024f77d24c237d5782518e9bf0e", "values": {"File": "What proportion of possible AGIs would be devastatingly bad?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What proportion of possible AGIs would be devastatingly bad?", "Link": "https://docs.google.com/document/d/1II2rWhsHHqqHJ-uSaF6w-Eh4RU-mMlpRssT0i6NmvrM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:15:30.650+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:29.437+01:00", "Status": "Not started", "Edit Answer": "What proportion of possible AGIs would be devastatingly bad?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AUR", "Source Link": "", "aisafety.info Link": "What proportion of possible AGIs would be devastatingly bad?", "Source": "", "All Phrasings": "What proportion of possible AGIs would be devastatingly bad?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The proportion of possible value systems for artificial general intelligences that would have devastating consequences for humanity is very nearly 1. Unless the AI explicitly cares about the pre-existing humans on earth, it will dismantle them for raw materials. Instrumentally speaking, humans would be less useful to a superintelligence than the sum of their atoms.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "The proportion of possible value systems for artificial general intelligences that would have devastating consequences for humanity is very nearly 1. Unless the AI explicitly cares about the pre-existing humans on earth, it will dismantle them for raw materials. Instrumentally speaking, humans would be less useful to a superintelligence than the sum of their atoms.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AUR", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:22.512+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-5e5482ab4567313e68b22ea85a6dbce63d2ff97caa651375d91341926db5ef24", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5e5482ab4567313e68b22ea85a6dbce63d2ff97caa651375d91341926db5ef24", "name": "Why can't we just clone thousands of von Neumann-like geniuses to work on AI alignment?", "index": 499, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:24:26.894Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5e5482ab4567313e68b22ea85a6dbce63d2ff97caa651375d91341926db5ef24", "values": {"File": "Why can't we just clone thousands of von Neumann-like geniuses to work on AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why can't we just clone thousands of von Neumann-like geniuses to work on AI alignment?", "Link": "https://docs.google.com/document/d/1thaRYRPHxz7ZWn5-kf5vvv4pdpYS6Ut6ez4xYXPv2wo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:14:46.125+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:30.650+01:00", "Status": "Not started", "Edit Answer": "Why can't we just clone thousands of von Neumann-like geniuses to work on AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AUS", "Source Link": "", "aisafety.info Link": "Why can't we just clone thousands of von Neumann-like geniuses to work on AI alignment?", "Source": "", "All Phrasings": "Why can't we just clone thousands of von Neumann-like geniuses to work on AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AUS", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:24.779+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 391, "Helpful": ""}}, {"id": "i-c32bde15f11781eff345276b4e7191566b27b75925cc038c40da178adb9a8abe", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c32bde15f11781eff345276b4e7191566b27b75925cc038c40da178adb9a8abe", "name": "Are there any examples of AI harming people?", "index": 500, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:24:49.464Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c32bde15f11781eff345276b4e7191566b27b75925cc038c40da178adb9a8abe", "values": {"File": "Are there any examples of AI harming people?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Are there any examples of AI harming people?", "Link": "https://docs.google.com/document/d/1Ie7XnmVd3hr-XzDjlLt0Y1KlbCcVbCfDCIUmBlP92Ys/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:13:51.318+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:02:10.947+01:00", "Status": "Not started", "Edit Answer": "Are there any examples of AI harming people?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AUT", "Source Link": "", "aisafety.info Link": "Are there any examples of AI harming people?", "Source": "", "All Phrasings": "Are there any examples of AI harming people?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AUT", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:26.727+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-020512c38e91724c7edb34bfa01bdfc6b1a3ec0ff18bb077ec81b44d2de83f81", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-020512c38e91724c7edb34bfa01bdfc6b1a3ec0ff18bb077ec81b44d2de83f81", "name": "Will the benefits of AGI outweigh its danger?", "index": 501, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:25:51.687Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-020512c38e91724c7edb34bfa01bdfc6b1a3ec0ff18bb077ec81b44d2de83f81", "values": {"File": "Will the benefits of AGI outweigh its danger?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Will the benefits of AGI outweigh its danger?", "Link": "https://docs.google.com/document/d/1M4Y1JfbAXclk0KrZTh2Uox3Pb-iOE_ej8GDjz1qQit8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:13:36.716+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:32.126+01:00", "Status": "Not started", "Edit Answer": "Will the benefits of AGI outweigh its danger?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AUU", "Source Link": "", "aisafety.info Link": "Will the benefits of AGI outweigh its danger?", "Source": "", "All Phrasings": "Will the benefits of AGI outweigh its danger?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AUU", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:45.953+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-cb37f0cb355cc63cd3eea74db52514b11aaea16988d688c1c20fb4b7e8acc2ee", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-cb37f0cb355cc63cd3eea74db52514b11aaea16988d688c1c20fb4b7e8acc2ee", "name": "Is the risk of superintellgence exaggerated?", "index": 502, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:25:54.082Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-cb37f0cb355cc63cd3eea74db52514b11aaea16988d688c1c20fb4b7e8acc2ee", "values": {"File": "Is the risk of superintellgence exaggerated?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is the risk of superintellgence exaggerated?", "Link": "https://docs.google.com/document/d/1vVsI7cR-ERmznpbE1--mKO-HKlsNNOCgtvVSKNQLNMw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:07:27.610+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:33.326+01:00", "Status": "Not started", "Edit Answer": "Is the risk of superintellgence exaggerated?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AUV", "Source Link": "", "aisafety.info Link": "Is the risk of superintellgence exaggerated?", "Source": "", "All Phrasings": "Is the risk of superintellgence exaggerated?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AUV", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:52.169+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-d41e781cd668b65c22f0da3ea2a5c0f1474aeb0159f2c5b29c9b45f48e55ea3f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d41e781cd668b65c22f0da3ea2a5c0f1474aeb0159f2c5b29c9b45f48e55ea3f", "name": "Won't keeping an AGI under strict supervision suffice to keep it safe?", "index": 503, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:32:57.734Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d41e781cd668b65c22f0da3ea2a5c0f1474aeb0159f2c5b29c9b45f48e55ea3f", "values": {"File": "Won't keeping an AGI under strict supervision suffice to keep it safe?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Won't keeping an AGI under strict supervision suffice to keep it safe?", "Link": "https://docs.google.com/document/d/1pLZ-qHV9PeMWeIm8BX2Bg010iY8kGUVLg93uKo9jM7s/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:12:18.886+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:34.450+01:00", "Status": "Not started", "Edit Answer": "Won't keeping an AGI under strict supervision suffice to keep it safe?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AUW", "Source Link": "", "aisafety.info Link": "Won't keeping an AGI under strict supervision suffice to keep it safe?", "Source": "", "All Phrasings": "Won't keeping an AGI under strict supervision suffice to keep it safe?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "We cannot make an AI safe just by keeping tabs on its activities. A sufficiently smart AI would be able to draft plans that look harmless at first while expanding the AI\u2019s influence and control, until the supervisors are no longer able to shut it down.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "We cannot make an AI safe just by keeping tabs on its activities. A sufficiently smart AI would be able to draft plans that look harmless at first while expanding the AI\u2019s influence and control, until the supervisors are no longer able to shut it down.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AUW", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:31:55.805+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-3513de73012639718bad8388f8c47e3f2818ee94bf4d494d4e8b20e9248b4344", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3513de73012639718bad8388f8c47e3f2818ee94bf4d494d4e8b20e9248b4344", "name": "How likely is it that AGI requires a major new insights or paradigm shifts?", "index": 504, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:32:59.850Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3513de73012639718bad8388f8c47e3f2818ee94bf4d494d4e8b20e9248b4344", "values": {"File": "How likely is it that AGI requires a major new insights or paradigm shifts?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How likely is it that AGI requires a major new insights or paradigm shifts?", "Link": "https://docs.google.com/document/d/1RiFgJc-ePBwa2PB6L1H5OA9unwTe3K3KuyQpW4ew-rA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:12:04.123+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:35.710+01:00", "Status": "Not started", "Edit Answer": "How likely is it that AGI requires a major new insights or paradigm shifts?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AUX", "Source Link": "", "aisafety.info Link": "How likely is it that AGI requires a major new insights or paradigm shifts?", "Source": "", "All Phrasings": "How likely is it that AGI requires a major new insights or paradigm shifts?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AUX", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:31:58.206+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-988e4d0040d3c259b73144b62f9403429e6aa929ca14161fa4ff5eecc3a93c68", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-988e4d0040d3c259b73144b62f9403429e6aa929ca14161fa4ff5eecc3a93c68", "name": "What concrete work is being done on time-buying strategies?", "index": 505, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:33:02.236Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-988e4d0040d3c259b73144b62f9403429e6aa929ca14161fa4ff5eecc3a93c68", "values": {"File": "What concrete work is being done on time-buying strategies?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What concrete work is being done on time-buying strategies?", "Link": "https://docs.google.com/document/d/1nLpq7vra9tKH7mHHzdgKXRc8Hb8mHHSKFqCEgKtjcEc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:11:24.432+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:36.927+01:00", "Status": "Not started", "Edit Answer": "What concrete work is being done on time-buying strategies?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AUY", "Source Link": "", "aisafety.info Link": "What concrete work is being done on time-buying strategies?", "Source": "", "All Phrasings": "What concrete work is being done on time-buying strategies?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AUY", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:32:00.326+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-fc422c47bb008936285108803be3f8cd8562e10afbf00226b35d3d1823794e57", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-fc422c47bb008936285108803be3f8cd8562e10afbf00226b35d3d1823794e57", "name": "What concrete work is being done on alignment strategies which won\u2019t scale to superintelligence?", "index": 506, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:33:04.797Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-fc422c47bb008936285108803be3f8cd8562e10afbf00226b35d3d1823794e57", "values": {"File": "What concrete work is being done on alignment strategies which won\u2019t scale to superintelligence?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What concrete work is being done on alignment strategies which won\u2019t scale to superintelligence?", "Link": "https://docs.google.com/document/d/1xZhoZyQRgfiBVcBlV-41JXDxT-yhr-zwKjZesSxtrBY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:11:02.197+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:38.087+01:00", "Status": "Not started", "Edit Answer": "What concrete work is being done on alignment strategies which won\u2019t scale to superintelligence?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AUZ", "Source Link": "", "aisafety.info Link": "What concrete work is being done on alignment strategies which won\u2019t scale to superintelligence?", "Source": "", "All Phrasings": "What concrete work is being done on alignment strategies which won\u2019t scale to superintelligence?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AUZ", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:32:02.443+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-346f0095188d9ff35e7a3746b2e9902d90afb71d35e07cce2d7252dba3361001", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-346f0095188d9ff35e7a3746b2e9902d90afb71d35e07cce2d7252dba3361001", "name": "Could we create a Manhattan Project for Whole Brain Emulation, to prevent WBE insights from being used for neuromorphic AGI?", "index": 507, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:26:02.316Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-346f0095188d9ff35e7a3746b2e9902d90afb71d35e07cce2d7252dba3361001", "values": {"File": "Could we create a Manhattan Project for Whole Brain Emulation, to prevent WBE insights from being used for neuromorphic AGI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Could we create a Manhattan Project for Whole Brain Emulation, to prevent WBE insights from being used for neuromorphic AGI?", "Link": "https://docs.google.com/document/d/12MxRQt4SqJN7cDIub5GFYyl0MaGno5eArQwOHk67yBg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:09:13.822+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:39.215+01:00", "Status": "Not started", "Edit Answer": "Could we create a Manhattan Project for Whole Brain Emulation, to prevent WBE insights from being used for neuromorphic AGI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AV0", "Source Link": "", "aisafety.info Link": "Could we create a Manhattan Project for Whole Brain Emulation, to prevent WBE insights from being used for neuromorphic AGI?", "Source": "", "All Phrasings": "Could we create a Manhattan Project for Whole Brain Emulation, to prevent WBE insights from being used for neuromorphic AGI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AV0", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:22:55.104+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-87ee36806e6af525d6c66fb11bbd3b8687842786736995d1493c631de8c46fc3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-87ee36806e6af525d6c66fb11bbd3b8687842786736995d1493c631de8c46fc3", "name": "What is the best pitch for AI safety?", "index": 508, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:26:05.793Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-87ee36806e6af525d6c66fb11bbd3b8687842786736995d1493c631de8c46fc3", "values": {"File": "What is the best pitch for AI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the best pitch for AI safety?", "Link": "https://docs.google.com/document/d/1Ug-4jGHeby3uDbJ-yrG5YEMuVlkJR7BmS5ZtHsfP9WI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:06:53.913+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:40.396+01:00", "Status": "Not started", "Edit Answer": "What is the best pitch for AI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AV1", "Source Link": "", "aisafety.info Link": "What is the best pitch for AI safety?", "Source": "", "All Phrasings": "What is the best pitch for AI safety?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AV1", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:01.094+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-c0db5c0e73ca1fa78e73eb5ba4dbf8d40ae887b870df692b818c58512f784caf", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c0db5c0e73ca1fa78e73eb5ba4dbf8d40ae887b870df692b818c58512f784caf", "name": "Why don't we just ban AGI development?", "index": 509, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:26:11.580Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c0db5c0e73ca1fa78e73eb5ba4dbf8d40ae887b870df692b818c58512f784caf", "values": {"File": "Why don't we just ban AGI development?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why don't we just ban AGI development?", "Link": "https://docs.google.com/document/d/11qLjJAfXBIuKN4kYVLTRF8oaOU6MBPp6Dfwjn0T0_bA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:06:28.716+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:41.536+01:00", "Status": "Not started", "Edit Answer": "Why don't we just ban AGI development?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AV2", "Source Link": "", "aisafety.info Link": "Why don't we just ban AGI development?", "Source": "", "All Phrasings": "Why don't we just ban AGI development?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AV2", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:02.691+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-1bfaea868f7d3a61982de9ed37fd63efbc852d6c86ac41ea589e6ba9586419e0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1bfaea868f7d3a61982de9ed37fd63efbc852d6c86ac41ea589e6ba9586419e0", "name": "Is there a good plan for alignment?", "index": 510, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:26:14.350Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1bfaea868f7d3a61982de9ed37fd63efbc852d6c86ac41ea589e6ba9586419e0", "values": {"File": "Is there a good plan for alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is there a good plan for alignment?", "Link": "https://docs.google.com/document/d/1olaWORW1izg8DMn0Zilri4vzaD5sourXSLVrC5T7yLo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:05:25.688+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:42.690+01:00", "Status": "Not started", "Edit Answer": "Is there a good plan for alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AV3", "Source Link": "", "aisafety.info Link": "Is there a good plan for alignment?", "Source": "", "All Phrasings": "Is there a good plan for alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AV3", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:04.346+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-ed478772d440a61b2c34cfd4c751090b7f0aa4fe46065e6dcecdfa62e6134b4e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ed478772d440a61b2c34cfd4c751090b7f0aa4fe46065e6dcecdfa62e6134b4e", "name": "Will AGI be aligned by default?", "index": 511, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:26:17.642Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ed478772d440a61b2c34cfd4c751090b7f0aa4fe46065e6dcecdfa62e6134b4e", "values": {"File": "Will AGI be aligned by default?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Will AGI be aligned by default?", "Link": "https://docs.google.com/document/d/1DDyQyhX9gRKTW0Vbl2Z4phARcoLXoLF7W96jYHqAG2g/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:03:30.577+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-06T18:27:35.551+01:00", "Status": "Not started", "Edit Answer": "Will AGI be aligned by default?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AV4", "Source Link": "", "aisafety.info Link": "Will AGI be aligned by default?", "Source": "", "All Phrasings": "Will AGI be aligned by default?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AV4", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:06.158+01:00", "Request Count": "", "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 236, "Helpful": ""}}, {"id": "i-720f48b966f9d0700e82c5bccd9837e1f6de0a1ac49f309f9a4ac0fa677ba59f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-720f48b966f9d0700e82c5bccd9837e1f6de0a1ac49f309f9a4ac0fa677ba59f", "name": "Is an AGI even possible?", "index": 512, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:26:29.792Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-720f48b966f9d0700e82c5bccd9837e1f6de0a1ac49f309f9a4ac0fa677ba59f", "values": {"File": "Is an AGI even possible?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is an AGI even possible?", "Link": "https://docs.google.com/document/d/1WFrhxVZlOGJ1dpY7qG8VFgOJAsjUiJA6tAgz-DmpLBU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:03:16.982+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:45.057+01:00", "Status": "Not started", "Edit Answer": "Is an AGI even possible?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AV5", "Source Link": "", "aisafety.info Link": "Is an AGI even possible?", "Source": "", "All Phrasings": "Is an AGI even possible?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AV5", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:08.969+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-760bf9c35a8562aa8922221fea6c5bdf69d45c2fa74edf86633ff73dc71fa65d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-760bf9c35a8562aa8922221fea6c5bdf69d45c2fa74edf86633ff73dc71fa65d", "name": "How can an AGI be smarter than all of humanity?", "index": 513, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:26:36.342Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-760bf9c35a8562aa8922221fea6c5bdf69d45c2fa74edf86633ff73dc71fa65d", "values": {"File": "How can an AGI be smarter than all of humanity?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How can an AGI be smarter than all of humanity?", "Link": "https://docs.google.com/document/d/17EbFAXXZ-mpo3JLd4UtjuFtgYdD4wC_TaHiKaiJ5IE8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:02:04.471+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:46.191+01:00", "Status": "Not started", "Edit Answer": "How can an AGI be smarter than all of humanity?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AV6", "Source Link": "", "aisafety.info Link": "How can an AGI be smarter than all of humanity?", "Source": "", "All Phrasings": "How can an AGI be smarter than all of humanity?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AV6", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:10.517+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-58eea2efe0adbf1c5827f26ea624cd5d87dc04b8008c706ad06d863715a367db", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-58eea2efe0adbf1c5827f26ea624cd5d87dc04b8008c706ad06d863715a367db", "name": "How can an AGI cause human extinction?", "index": 514, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:26:42.122Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-58eea2efe0adbf1c5827f26ea624cd5d87dc04b8008c706ad06d863715a367db", "values": {"File": "How can an AGI cause human extinction?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How can an AGI cause human extinction?", "Link": "https://docs.google.com/document/d/1thV6nG_o8aeyH90A8KsiIE3mSCJA-7kWamD_cPMGm6E/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T18:01:13.965+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:47.434+01:00", "Status": "Not started", "Edit Answer": "How can an AGI cause human extinction?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AV7", "Source Link": "", "aisafety.info Link": "How can an AGI cause human extinction?", "Source": "", "All Phrasings": "How can an AGI cause human extinction?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AV7", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:12.221+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-94ea9a12e6d8bffbeab3a2167391092a6852f0e124cc8d036d89d5be3bb9222e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-94ea9a12e6d8bffbeab3a2167391092a6852f0e124cc8d036d89d5be3bb9222e", "name": "How could an AGI effect the world without a body?", "index": 515, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-15T00:06:59.205Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-94ea9a12e6d8bffbeab3a2167391092a6852f0e124cc8d036d89d5be3bb9222e", "values": {"File": "How could an AGI effect the world without a body?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How could an AGI effect the world without a body?", "Link": "https://docs.google.com/document/d/1g7FitULd5fmfLOYcJya7W3npBKafKQBcvgMdMek9Jwc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:59:24.554+01:00", "Related Answers DO NOT EDIT": "What if we put the AI in a box and have a second, more powerful, AI with the goal of preventing the first one from escaping?,How might a superintelligence technologically manipulate humans?", "Tags": "", "Doc Last Edited": "2023-03-14T22:10:59.228+01:00", "Status": "In progress", "Edit Answer": "How could an AGI effect the world without a body?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AV8", "Source Link": "", "aisafety.info Link": "How could an AGI effect the world without a body?", "Source": "", "All Phrasings": "How could an AGI effect the world without a body?\n", "Initial Order": "", "Related IDs": "6500,6976", "Rich Text DO NOT EDIT": "An AGI could affect the world in two broad ways: interacting with devices which are linked to the internet \u2014 including robots \u2014 and getting humans to help. How an AGI interacts with the world would be a function of its goals and how it plans to achieve them. An AGI which is running biology experiments might have some robotics that it can use, but it also might just pay some graduate students, the way a human principal investigator would, to run experiments.\n\nBut how could this pose any risks, the humans will just refuse to do dangerous things, right? Probably not. Humans can be manipulated via bribery, blackmail, threats, propaganda, and more. An AGI trying to influence the world doesn\u2019t have to look malicious or throw up redflags. Companies automatically order products from each other all the time, and people on the internet get paid to do weird stuff all the time. How much money do you think it would take to get a guy from Craigslist to run some mystery files on his computer or mix some chemicals in his garage? Not that much, especially when any guy will do.\n\nBroadly speaking, being able to influence humans could give an AGI a large amount of access to the world, and improvements in robotics could help with this. There will be strong economic incentives to cede more decision making power to AI systems as they become intelligent, so the future might involve lots of AIs interacting with the world in complicated ways: building factories and running biology experiments. In such a fast-moving world it may become even more difficult to understand the actions AIs are taking. \n\n- \n\n- \n\n- \n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "An AGI could affect the world in two broad ways: interacting with devices which are linked to the internet \u2014 including robots \u2014 and getting humans to help. How an AGI interacts with the world would be a function of its goals and how it plans to achieve them. An AGI which is running biology experiments might have some robotics that it can use, but it also might just pay some graduate students, the way a human principal investigator would, to run experiments.\n\nBut how could this pose any risks, the humans will just refuse to do dangerous things, right? Probably not. Humans can be manipulated via bribery, blackmail, threats, propaganda, and more. An AGI trying to influence the world doesn\u2019t have to look malicious or throw up redflags. Companies automatically order products from each other all the time, and people on the internet get paid to do weird stuff all the time. How much money do you think it would take to get a guy from Craigslist to run some mystery files on his computer or mix some chemicals in his garage? Not that much, especially when any guy will do.\n\nBroadly speaking, being able to influence humans could give an AGI a large amount of access to the world, and improvements in robotics could help with this. There will be strong economic incentives to cede more decision making power to AI systems as they become intelligent, so the future might involve lots of AIs interacting with the world in complicated ways: building factories and running biology experiments. In such a fast-moving world it may become even more difficult to understand the actions AIs are taking. \n\n- \n\n- \n\n- \n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AV8", "Related Answers": "What if we put the AI in a box and have a second, more powerful, AI with the goal of preventing the first one from escaping?,How might a superintelligence technologically manipulate humans?", "Doc Last Ingested": "2023-03-15T00:23:14.103+01:00", "Request Count": "", "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 299, "Helpful": ""}}, {"id": "i-452334cf7df052975fe8ebf2ac5785df94344a6e20aa7c0273221434a9122ded", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-452334cf7df052975fe8ebf2ac5785df94344a6e20aa7c0273221434a9122ded", "name": "What is recursive self improvement?", "index": 516, "createdAt": "2023-02-21T18:05:43.580Z", "updatedAt": "2023-03-14T23:27:05.219Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-452334cf7df052975fe8ebf2ac5785df94344a6e20aa7c0273221434a9122ded", "values": {"File": "What is recursive self improvement?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is recursive self improvement?", "Link": "https://docs.google.com/document/d/1T1fAA1Xs_tGB3MR7TWBe1IcIqEvaadNQBJFnK2cPB0Q/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:52:49.133+01:00", "Related Answers DO NOT EDIT": "Is it likely that hardware will allow an exponential takeoff?,What is instrumental convergence?", "Tags": "", "Doc Last Edited": "2023-02-28T00:11:48.144+01:00", "Status": "In progress", "Edit Answer": "What is recursive self improvement?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AV9", "Source Link": "", "aisafety.info Link": "What is recursive self improvement?", "Source": "", "All Phrasings": "What is recursive self improvement?\n", "Initial Order": "", "Related IDs": "6482,897I", "Rich Text DO NOT EDIT": "\n\n**Recursive self-improvement (RSI)** is an AI agent\u2019s ability to make self-improvements that lead to further self-improvements, in a continuing feedback loop.\n\n[Recursive self-improvement may be a pathway to artificial general intelligence](https://www.lesswrong.com/tag/recursive-self-improvement), in which an AI could make adjustments to its own functionality (either software or hardware) that lead to improved performance, enabling it to make further adjustments that lead to further improved performance, and so on exponentially.\n\nEach cycle of self-improvement could reach ever-higher levels of capability.This could lead to **[AI takeoff](https://aisafety.info?state=7071_)**, although AI researchers disagree onwhether recursive self-improvement would \n\nquickly reach an upper bound before plateauing out (a form of **soft takeoff**), or continue exponentially and quickly surpass the capabilities of any existing system (leading to a **hard takeoff**).\n\nFew AI systems are designed to pursue recursive self-improvement as an explicit goal, although recursive self-improvement could theoretically be achieved either through **[instrumental convergence](https://docs.google.com/document/d/12cwXk6OcQMGAaxwsFKt0twpBlFlqHHoj2FV01PCyYVw/edit)** (in which systems with wildly different ultimate goals might still seek the same short-term goals, including recursive self-improvement, in order to achieve them) or through **perverse instantiation** (a mode of AI failure in which a system does things very differently than its designers intended, usually to their detriment or the detriment of the system itself). \n\n## Sources\n\n\u201cRecursive Self-Improvement\u201d, LessWrong ([https://www.lesswrong.com/tag/recursive-self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement))\n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "\n\n**Recursive self-improvement (RSI)** is an AI agent\u2019s ability to make self-improvements that lead to further self-improvements, in a continuing feedback loop.\n\n[Recursive self-improvement may be a pathway to artificial general intelligence](https://www.lesswrong.com/tag/recursive-self-improvement), in which an AI could make adjustments to its own functionality (either software or hardware) that lead to improved performance, enabling it to make further adjustments that lead to further improved performance, and so on exponentially.\n\nEach cycle of self-improvement could reach ever-higher levels of capability.This could lead to **[AI takeoff](https://aisafety.info?state=7071_)**, although AI researchers disagree onwhether recursive self-improvement would \n\nquickly reach an upper bound before plateauing out (a form of **soft takeoff**), or continue exponentially and quickly surpass the capabilities of any existing system (leading to a **hard takeoff**).\n\nFew AI systems are designed to pursue recursive self-improvement as an explicit goal, although recursive self-improvement could theoretically be achieved either through **[instrumental convergence](https://docs.google.com/document/d/12cwXk6OcQMGAaxwsFKt0twpBlFlqHHoj2FV01PCyYVw/edit)** (in which systems with wildly different ultimate goals might still seek the same short-term goals, including recursive self-improvement, in order to achieve them) or through **perverse instantiation** (a mode of AI failure in which a system does things very differently than its designers intended, usually to their detriment or the detriment of the system itself). \n\n## Sources\n\n\u201cRecursive Self-Improvement\u201d, LessWrong ([https://www.lesswrong.com/tag/recursive-self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement))\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AV9", "Related Answers": "Is it likely that hardware will allow an exponential takeoff?,What is instrumental convergence?", "Doc Last Ingested": "2023-03-15T00:23:16.831+01:00", "Request Count": "", "Number of suggestions on answer doc": 21, "Total character count of suggestions on answer doc": 3832, "Helpful": ""}}, {"id": "i-75bab3f676dec7b5260fe586d2f3679ceb05f873459387636b2385cf7dba98d9", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-75bab3f676dec7b5260fe586d2f3679ceb05f873459387636b2385cf7dba98d9", "name": "Is recursive self improvement even possible?", "index": 466, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:27:09.602Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-75bab3f676dec7b5260fe586d2f3679ceb05f873459387636b2385cf7dba98d9", "values": {"File": "Is recursive self improvement even possible?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is recursive self improvement even possible?", "Link": "https://docs.google.com/document/d/1tuoozqOjG1-eEVAQVohE3x6B04FV0Y-b-q24wi-FZ9U/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:49:47.370+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:51.084+01:00", "Status": "Not started", "Edit Answer": "Is recursive self improvement even possible?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AEL", "Source Link": "", "aisafety.info Link": "Is recursive self improvement even possible?", "Source": "", "All Phrasings": "Is recursive self improvement even possible?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AEL", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:18.804+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-88014c2c0315c55cff7295a19a944a076c9422876d1acb9175ec1bb1009988e5", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-88014c2c0315c55cff7295a19a944a076c9422876d1acb9175ec1bb1009988e5", "name": "Isn't AGI development limited by the amount of available data?", "index": 467, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:27:14.621Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-88014c2c0315c55cff7295a19a944a076c9422876d1acb9175ec1bb1009988e5", "values": {"File": "Isn't AGI development limited by the amount of available data?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn't AGI development limited by the amount of available data?", "Link": "https://docs.google.com/document/d/1e1L2GIKveO_cN3OgazRqNfXqEVbt6UHrpZFoO0-SF3U/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:49:28.105+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:52.350+01:00", "Status": "Not started", "Edit Answer": "Isn't AGI development limited by the amount of available data?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AEM", "Source Link": "", "aisafety.info Link": "Isn't AGI development limited by the amount of available data?", "Source": "", "All Phrasings": "Isn't AGI development limited by the amount of available data?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-02-26T18:55:39.982+01:00", "UI ID": "8AEM", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:20.333+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-4d10c706d70e927de04f9099748f619775044e9a0f1235ac6d5ddbb19c0d6363", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4d10c706d70e927de04f9099748f619775044e9a0f1235ac6d5ddbb19c0d6363", "name": "Wouldn't a lot of things have to go wrong for AGI to be a danger?", "index": 468, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:27:19.281Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4d10c706d70e927de04f9099748f619775044e9a0f1235ac6d5ddbb19c0d6363", "values": {"File": "Wouldn't a lot of things have to go wrong for AGI to be a danger?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Wouldn't a lot of things have to go wrong for AGI to be a danger?", "Link": "https://docs.google.com/document/d/1yARBPBi_Xso4CVDV57K6xCpbsZv9n2xV0l7YobhyJw8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:48:59.259+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:53.547+01:00", "Status": "Not started", "Edit Answer": "Wouldn't a lot of things have to go wrong for AGI to be a danger?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AEN", "Source Link": "", "aisafety.info Link": "Wouldn't a lot of things have to go wrong for AGI to be a danger?", "Source": "", "All Phrasings": "Wouldn't a lot of things have to go wrong for AGI to be a danger?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n[https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive](https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive) is a good source\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n[https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive](https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive) is a good source\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AEN", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:21.227+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-807db1376c25f53472cd15cef1158726efbe4a3c89438ebb9d41b003be1f283b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-807db1376c25f53472cd15cef1158726efbe4a3c89438ebb9d41b003be1f283b", "name": "Isn't AGI development limited by neuroscientific understanding?", "index": 469, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:27:28.544Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-807db1376c25f53472cd15cef1158726efbe4a3c89438ebb9d41b003be1f283b", "values": {"File": "Isn't AGI development limited by neuroscientific understanding?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn't AGI development limited by neuroscientific understanding?", "Link": "https://docs.google.com/document/d/1iFz_0HpdlU54UR7i3YFgDO5cJLlhPTBVnRHP1eR2lJ8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:48:11.226+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:44:54.740+01:00", "Status": "Not started", "Edit Answer": "Isn't AGI development limited by neuroscientific understanding?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AEO", "Source Link": "", "aisafety.info Link": "Isn't AGI development limited by neuroscientific understanding?", "Source": "", "All Phrasings": "Isn't AGI development limited by neuroscientific understanding?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AEO", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:23.157+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-e8626506033c636d8a71387eaf557f32ab2148259243a33414702346dec9bb17", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e8626506033c636d8a71387eaf557f32ab2148259243a33414702346dec9bb17", "name": "Isn't progress slower than futurists suggested it would be?", "index": 470, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-15T18:07:58.304Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e8626506033c636d8a71387eaf557f32ab2148259243a33414702346dec9bb17", "values": {"File": "Isn't progress slower than futurists suggested it would be?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn't progress slower than futurists suggested it would be?", "Link": "https://docs.google.com/document/d/1skJdusLS9mPQFc_4z0jeJiBXbQXnMlH28B7IfMPS9NI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:47:15.863+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-15T16:30:10.940+01:00", "Status": "Not started", "Edit Answer": "Isn't progress slower than futurists suggested it would be?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AEP", "Source Link": "", "aisafety.info Link": "Isn't progress slower than futurists suggested it would be?", "Source": "", "All Phrasings": "Isn't progress slower than futurists suggested it would be?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\nThis question is hard to answer rigorously. Unfortunately, many predictions are made without the precision that would enable us to rate their predictions with \u201c[clear quantitative benchmark[s]](https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/)\u201d. That said, there\u2019s definitely something to this worry.\n\nKarnofsky provides an [analysis](https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/) of (famous futurist) Ray Kurzweil\u2019s predictions, among other early futurists. Karnofsky notes that Kurzweil\u2019s predictions range somewhere between \"mediocre\" and \"fine\", but not wildly off-base \u2014 despite being overly aggressive. Luu [disagrees](https://danluu.com/futurist-predictions/) with even this moderate analysis, claiming that favorable evaluations of Kurzweil inappropriately \u201cround up\u201d assessments of his predictions\u2019 accuracy.\n\nWhile this question is independently interesting, we should note that the arguments for working on AI alignment are not obviously connected to the track record of futurists.\n\nFor instance, many of the most prominent people concerned with AI alignment eschew explicitly \u2018futurist\u2019 style forecasting. When responding to Ajeya Cotra\u2019s *Biological Anchors* report (a large document attempting to forecast the arrival of transformative AI), Eliezer Yudkowsky [responds](https://www.lesswrong.com/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works):\n\n\u201cThe assumption that future people can put in X amount of compute and get out Y result is not something you really know.\"  \u2026 Your model contains supposedly known parameters, \"how much computation an AGI must eat per second, and how many parameters must be in the trainable model for that, and how many examples are needed to train those parameters\". Relative to whatever method is actually first used to produce AGI, I expect your estimates to be wildly inapplicable.\u201d\n\nOf course, the track record of futurists may be relevant evidence when evaluating particular claims offered about the arrival of highly capable AI systems. However, various arguments offered in favor of increased work on AI alignment reference the difficulty of alignment, based on both theoretical and empirical arguments about the behavior of advanced AI systems. If these arguments are independently compelling, then more specific predictions about the exact arrival dates of smarter-than-human AI may be less relevant for evaluating work on AI alignment.\n\nMoreover, the difficulty of predicting the future cuts both ways. The Wright Brothers developed the first airplane. One of these brothers, Wilbur, is [quoted](https://ourworldindata.org/technological-change#:~:text=The%20history%20of%20heavier%2Dthan,later%20the%20brothers%20were%20successful.) as saying \u201cI confess that in 1901, I said to my brother Orville that man would not fly for 50 years.\u201d Two years later, the Wright Brothers developed heavier-than-air flight. In 1933, Nobel Prize winning physicist Ernest Rutherford [described](https://www.atomicarchive.com/history/manhattan-project/p1s2.html) the possibility of atomic weapons \"moonshine\u201d. In 1934, Leo Szilard patented the nuclear reactor.\n\nFuturism is difficult, and lessons of the past are important to understand. Still,\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\nThis question is hard to answer rigorously. Unfortunately, many predictions are made without the precision that would enable us to rate their predictions with \u201c[clear quantitative benchmark[s]](https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/)\u201d. That said, there\u2019s definitely something to this worry.\n\nKarnofsky provides an [analysis](https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/) of (famous futurist) Ray Kurzweil\u2019s predictions, among other early futurists. Karnofsky notes that Kurzweil\u2019s predictions range somewhere between \"mediocre\" and \"fine\", but not wildly off-base \u2014 despite being overly aggressive. Luu [disagrees](https://danluu.com/futurist-predictions/) with even this moderate analysis, claiming that favorable evaluations of Kurzweil inappropriately \u201cround up\u201d assessments of his predictions\u2019 accuracy.\n\nWhile this question is independently interesting, we should note that the arguments for working on AI alignment are not obviously connected to the track record of futurists.\n\nFor instance, many of the most prominent people concerned with AI alignment eschew explicitly \u2018futurist\u2019 style forecasting. When responding to Ajeya Cotra\u2019s *Biological Anchors* report (a large document attempting to forecast the arrival of transformative AI), Eliezer Yudkowsky [responds](https://www.lesswrong.com/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works):\n\n\u201cThe assumption that future people can put in X amount of compute and get out Y result is not something you really know.\"  \u2026 Your model contains supposedly known parameters, \"how much computation an AGI must eat per second, and how many parameters must be in the trainable model for that, and how many examples are needed to train those parameters\". Relative to whatever method is actually first used to produce AGI, I expect your estimates to be wildly inapplicable.\u201d\n\nOf course, the track record of futurists may be relevant evidence when evaluating particular claims offered about the arrival of highly capable AI systems. However, various arguments offered in favor of increased work on AI alignment reference the difficulty of alignment, based on both theoretical and empirical arguments about the behavior of advanced AI systems. If these arguments are independently compelling, then more specific predictions about the exact arrival dates of smarter-than-human AI may be less relevant for evaluating work on AI alignment.\n\nMoreover, the difficulty of predicting the future cuts both ways. The Wright Brothers developed the first airplane. One of these brothers, Wilbur, is [quoted](https://ourworldindata.org/technological-change#:~:text=The%20history%20of%20heavier%2Dthan,later%20the%20brothers%20were%20successful.) as saying \u201cI confess that in 1901, I said to my brother Orville that man would not fly for 50 years.\u201d Two years later, the Wright Brothers developed heavier-than-air flight. In 1933, Nobel Prize winning physicist Ernest Rutherford [described](https://www.atomicarchive.com/history/manhattan-project/p1s2.html) the possibility of atomic weapons \"moonshine\u201d. In 1934, Leo Szilard patented the nuclear reactor.\n\nFuturism is difficult, and lessons of the past are important to understand. Still,\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AEP", "Related Answers": "", "Doc Last Ingested": "2023-03-15T17:21:56.689+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-01c4c1a1a0166ff2196090b080ed5899d53c63daf43b3870049022ce025a7813", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-01c4c1a1a0166ff2196090b080ed5899d53c63daf43b3870049022ce025a7813", "name": "What is behavioral cloning (BC)?", "index": 471, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:27:41.909Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-01c4c1a1a0166ff2196090b080ed5899d53c63daf43b3870049022ce025a7813", "values": {"File": "What is behavioral cloning (BC)?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is behavioral cloning (BC)?", "Link": "https://docs.google.com/document/d/1qockXfKNzJFpUHhtcXyIYR-6mondvVDKTgdGrYb6WKY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:47:06.912+01:00", "Related Answers DO NOT EDIT": "What is Imitation Learning (IL)?,What is reinforcement learning (RL)?,What is Reinforcement Learning from Human Feedback (RLHF)?,What is inverse reinforcement learning (IRL)?", "Tags": "", "Doc Last Edited": "2023-03-02T00:39:50.491+01:00", "Status": "In progress", "Edit Answer": "What is behavioral cloning (BC)?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AEQ", "Source Link": "", "aisafety.info Link": "What is behavioral cloning (BC)?", "Source": "", "All Phrasings": "What is behavioral cloning (BC)?\n", "Initial Order": "", "Related IDs": "8AER,89ZS,88FN,8AET", "Rich Text DO NOT EDIT": "Behavioral cloning (BC) is the process of gathering observations from an expert demonstrator, and then using regular [supervised learning (SL)](https://en.wikipedia.org/wiki/Supervised_learning) to make an agent \u2018imitate\u2019 the behavior that was demonstrated.\n\nBC is one way in which we can implement imitation learning (IL). There are also other ways such as [inverse reinforcement learning (IRL)](https://docs.google.com/document/d/1DVqzpQHAlFxXu5mg5nQcGxxHEWAmr752dHfrXgb-d6M/edit), or cooperative inverse reinforcement Learning (CIRL). Unlike IRL, the goal behind BC as a machine learning (ML) method is to replicate the demonstrator's behavior as closely as possible, regardless of what the demonstrator\u2019s goals might be.\n\nBC was originally developed to train self-driving cars. This use case also serves as a good simple example of how behavioral cloning works. We tell a human demonstrator (driver) to drive a car around. While the demonstrator is driving we want to collect data about the environment state from sensors such as lidar, cameras, etc\u2026 as well as the actions that the human demonstrator took in each one of those states. This comes in the form of wheel movements, gears used, etc... This allows us to form a data set that consists of (state, action) pairs. At this point, we can use regular supervised learning to train a prediction model. This prediction model tries to predict an action for any future environment state. So as an example, our model would output a specific steering wheel and gear configuration based on what it sees from a camera feed. When the accuracy of the model is high enough then we can say that the behavior demonstrated by the human driver has been \u2018cloned\u2019 into a machine through learning. Thus the name **behavioral cloning**.\n\nConversations with large language models (LLMs) such as GPT, which feel like you are talking to a conscious entity are basically a result of behavioral cloning. The model is just imitating and then echoing the speech patterns of many different human demonstrators. However, there are a couple of problems that can arise when using behavioral cloning, especially with LLMs, that we should be aware of.\n\nThe first is that *BC results in confident incorrectness*. During the demonstrations, the human experts have some amount of background knowledge that they rely on, which is not taught to the model. For example, when training a LLM to have conversations using BC, the human demonstrator might less frequently ask certain questions because they are considered \u2018common sense\u2019. A model trained to imitate will copy both - the types of questions asked in conversation, as well as, the frequency with which they are asked. Humans already possess this background knowledge, but a LLM doesn\u2019t. This means that to have the same level of information as a human, the model should ask some questions more frequently to fill the gaps in its knowledge. But since the model seeks to imitate, it will stick to the low frequency demonstrated by the human and thus has strictly less information overall than the demonstrator for the same conversational task. Despite this dearth of knowledge, we expect it to be able to perform as a clone and reach human-level performance. This means in order to reach human performance on less than human knowledge it will resort to \u2018making up facts\u2019 that help it reach its performance goals. These \u2018hallucinations\u2019 will then be presented during the conversation, with the same level of confidence as all the other information. Hallucinations and confident incorrectness is [an empirically verified problem](https://arxiv.org/pdf/2103.15025.pdf) in many LLMs including GPT-2 and 3, and raises obvious concerns for AI safety.\n\nThe second problem is not as much of a security concern. But it is a problem. The types of hallucinations mentioned above arose because the model knew too little. However, the model can also know too much. If the model knows more than the human demonstrator because it is able to find more patterns in the environment state that it is given, it will throw away that information and reduce its performance to match human level. This is because it is trained as a \u2018clone\u2019. Ideally, we don\u2019t want the model dumbing itself down or not disclosing useful new patterns in data just because it is trying to be humanlike or perform at a human level. This is another problem that will have to be addressed if BC continues to be used as a ML technique.\n\nOverall, Behavioral cloning is a straightforward technique that gives us a good baseline for what we should expect from imitation-based algorithms.\n\n", "Tag Count": 0, "Related Answer Count": 4, "Rich Text": "Behavioral cloning (BC) is the process of gathering observations from an expert demonstrator, and then using regular [supervised learning (SL)](https://en.wikipedia.org/wiki/Supervised_learning) to make an agent \u2018imitate\u2019 the behavior that was demonstrated.\n\nBC is one way in which we can implement imitation learning (IL). There are also other ways such as [inverse reinforcement learning (IRL)](https://docs.google.com/document/d/1DVqzpQHAlFxXu5mg5nQcGxxHEWAmr752dHfrXgb-d6M/edit), or cooperative inverse reinforcement Learning (CIRL). Unlike IRL, the goal behind BC as a machine learning (ML) method is to replicate the demonstrator's behavior as closely as possible, regardless of what the demonstrator\u2019s goals might be.\n\nBC was originally developed to train self-driving cars. This use case also serves as a good simple example of how behavioral cloning works. We tell a human demonstrator (driver) to drive a car around. While the demonstrator is driving we want to collect data about the environment state from sensors such as lidar, cameras, etc\u2026 as well as the actions that the human demonstrator took in each one of those states. This comes in the form of wheel movements, gears used, etc... This allows us to form a data set that consists of (state, action) pairs. At this point, we can use regular supervised learning to train a prediction model. This prediction model tries to predict an action for any future environment state. So as an example, our model would output a specific steering wheel and gear configuration based on what it sees from a camera feed. When the accuracy of the model is high enough then we can say that the behavior demonstrated by the human driver has been \u2018cloned\u2019 into a machine through learning. Thus the name **behavioral cloning**.\n\nConversations with large language models (LLMs) such as GPT, which feel like you are talking to a conscious entity are basically a result of behavioral cloning. The model is just imitating and then echoing the speech patterns of many different human demonstrators. However, there are a couple of problems that can arise when using behavioral cloning, especially with LLMs, that we should be aware of.\n\nThe first is that *BC results in confident incorrectness*. During the demonstrations, the human experts have some amount of background knowledge that they rely on, which is not taught to the model. For example, when training a LLM to have conversations using BC, the human demonstrator might less frequently ask certain questions because they are considered \u2018common sense\u2019. A model trained to imitate will copy both - the types of questions asked in conversation, as well as, the frequency with which they are asked. Humans already possess this background knowledge, but a LLM doesn\u2019t. This means that to have the same level of information as a human, the model should ask some questions more frequently to fill the gaps in its knowledge. But since the model seeks to imitate, it will stick to the low frequency demonstrated by the human and thus has strictly less information overall than the demonstrator for the same conversational task. Despite this dearth of knowledge, we expect it to be able to perform as a clone and reach human-level performance. This means in order to reach human performance on less than human knowledge it will resort to \u2018making up facts\u2019 that help it reach its performance goals. These \u2018hallucinations\u2019 will then be presented during the conversation, with the same level of confidence as all the other information. Hallucinations and confident incorrectness is [an empirically verified problem](https://arxiv.org/pdf/2103.15025.pdf) in many LLMs including GPT-2 and 3, and raises obvious concerns for AI safety.\n\nThe second problem is not as much of a security concern. But it is a problem. The types of hallucinations mentioned above arose because the model knew too little. However, the model can also know too much. If the model knows more than the human demonstrator because it is able to find more patterns in the environment state that it is given, it will throw away that information and reduce its performance to match human level. This is because it is trained as a \u2018clone\u2019. Ideally, we don\u2019t want the model dumbing itself down or not disclosing useful new patterns in data just because it is trying to be humanlike or perform at a human level. This is another problem that will have to be addressed if BC continues to be used as a ML technique.\n\nOverall, Behavioral cloning is a straightforward technique that gives us a good baseline for what we should expect from imitation-based algorithms.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AEQ", "Related Answers": "What is Imitation Learning (IL)?,What is reinforcement learning (RL)?,What is Reinforcement Learning from Human Feedback (RLHF)?,What is inverse reinforcement learning (IRL)?", "Doc Last Ingested": "2023-03-15T00:23:27.846+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-c71857e138720e690240c9d3a512dbb1f6822ec394aadc94dee58428f150a9e5", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c71857e138720e690240c9d3a512dbb1f6822ec394aadc94dee58428f150a9e5", "name": "What is Imitation Learning (IL)?", "index": 472, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:27:44.364Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c71857e138720e690240c9d3a512dbb1f6822ec394aadc94dee58428f150a9e5", "values": {"File": "What is Imitation Learning (IL)?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Imitation Learning (IL)?", "Link": "https://docs.google.com/document/d/1i1N94l0mo2QTdU5GzBjP9EZ9AE9qFHugMoy7EFw6XFU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:46:34.022+01:00", "Related Answers DO NOT EDIT": "What is reinforcement learning (RL)?,What is Reinforcement Learning from Human Feedback (RLHF)?,What is inverse reinforcement learning (IRL)?,What is behavioral cloning (BC)?", "Tags": "", "Doc Last Edited": "2023-03-01T18:14:22.149+01:00", "Status": "In progress", "Edit Answer": "What is Imitation Learning (IL)?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AER", "Source Link": "", "aisafety.info Link": "What is Imitation Learning (IL)?", "Source": "", "All Phrasings": "What is Imitation Learning (IL)?\n", "Initial Order": "", "Related IDs": "89ZS,88FN,8AET,8AEQ", "Rich Text DO NOT EDIT": "**Quick 1-sentence answer**\n\nImitation Learning (IL) is the process of learning by observing an expert demonstrator and then copying their behavior. It is also sometimes called [apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning).\n\n**Slightly more detailed answer**\n\nLearning by imitating is a [machine learning (ML)](https://docs.google.com/document/d/1Ovn9t-am0iwVwXcoazd_MUhj0MGetfdq-wLw2a7bqbk/edit) technique that can be thought of as a sibling to [reinforcement learning (RL)](https://docs.google.com/document/d/1fONikRvX-1iGOKUqT2qzNvyaW2PEt2I6rlt6xrAMG2U/edit#). The goal of RL is to be able to find a policy on how to act by interacting with the environment. IL tries to learn a policy by observing another agent which is taking actions and interacting within the environment. For some tasks, using IL instead of regular RL or [supervised learning (SL)](https://en.wikipedia.org/wiki/Supervised_learning)is easier than specifying exactly how to perform the behavior.\n\nThere are multiple different algorithms that we can use to get the agent to imitate expert behavior such as [behavioral cloning (BC)](https://docs.google.com/document/d/1qockXfKNzJFpUHhtcXyIYR-6mondvVDKTgdGrYb6WKY/edit#), [inverse reinforcement learning (IRL)](https://docs.google.com/document/d/1DVqzpQHAlFxXu5mg5nQcGxxHEWAmr752dHfrXgb-d6M/edit), cooperative inverse reinforcement learning (CIRL) and generative adversarial imitation learning (GAIL).\n\n**Simple Example**\n\nAn example of this would be teaching a child how to throw a ball. We can easily perform the behavior and tell the child to imitate it. However, if we were to try and describe every single muscle contraction, angle of our hands, amount of force executed by our shoulders, and so on\u2026 This would go from being trivial to an unbelievably complex task. Similarly, sometimes specifying a reward function or policy that accurately captures the behavior we want from our RL agent is extremely difficult, but demonstrating that same behavior is comparatively easy. In situations like this imitation is a good choice.\n\n## \n\n**Limitations & conclusion**\n\ntbd\u2026\n\n", "Tag Count": 0, "Related Answer Count": 4, "Rich Text": "**Quick 1-sentence answer**\n\nImitation Learning (IL) is the process of learning by observing an expert demonstrator and then copying their behavior. It is also sometimes called [apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning).\n\n**Slightly more detailed answer**\n\nLearning by imitating is a [machine learning (ML)](https://docs.google.com/document/d/1Ovn9t-am0iwVwXcoazd_MUhj0MGetfdq-wLw2a7bqbk/edit) technique that can be thought of as a sibling to [reinforcement learning (RL)](https://docs.google.com/document/d/1fONikRvX-1iGOKUqT2qzNvyaW2PEt2I6rlt6xrAMG2U/edit#). The goal of RL is to be able to find a policy on how to act by interacting with the environment. IL tries to learn a policy by observing another agent which is taking actions and interacting within the environment. For some tasks, using IL instead of regular RL or [supervised learning (SL)](https://en.wikipedia.org/wiki/Supervised_learning)is easier than specifying exactly how to perform the behavior.\n\nThere are multiple different algorithms that we can use to get the agent to imitate expert behavior such as [behavioral cloning (BC)](https://docs.google.com/document/d/1qockXfKNzJFpUHhtcXyIYR-6mondvVDKTgdGrYb6WKY/edit#), [inverse reinforcement learning (IRL)](https://docs.google.com/document/d/1DVqzpQHAlFxXu5mg5nQcGxxHEWAmr752dHfrXgb-d6M/edit), cooperative inverse reinforcement learning (CIRL) and generative adversarial imitation learning (GAIL).\n\n**Simple Example**\n\nAn example of this would be teaching a child how to throw a ball. We can easily perform the behavior and tell the child to imitate it. However, if we were to try and describe every single muscle contraction, angle of our hands, amount of force executed by our shoulders, and so on\u2026 This would go from being trivial to an unbelievably complex task. Similarly, sometimes specifying a reward function or policy that accurately captures the behavior we want from our RL agent is extremely difficult, but demonstrating that same behavior is comparatively easy. In situations like this imitation is a good choice.\n\n## \n\n**Limitations & conclusion**\n\ntbd\u2026\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AER", "Related Answers": "What is reinforcement learning (RL)?,What is Reinforcement Learning from Human Feedback (RLHF)?,What is inverse reinforcement learning (IRL)?,What is behavioral cloning (BC)?", "Doc Last Ingested": "2023-03-15T00:23:30.300+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-48d017c9312936c05bef86a5d42f0459d0461b3d3db6e3cb924218e08ca5218b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-48d017c9312936c05bef86a5d42f0459d0461b3d3db6e3cb924218e08ca5218b", "name": "Why can't we build an AI that is programmed to shut off after some time?", "index": 473, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:27:52.649Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-48d017c9312936c05bef86a5d42f0459d0461b3d3db6e3cb924218e08ca5218b", "values": {"File": "Why can't we build an AI that is programmed to shut off after some time?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why can't we build an AI that is programmed to shut off after some time?", "Link": "https://docs.google.com/document/d/1TJfW__P3FjQ6ZvzlMn-Hyshe8AvBI74w-mBbF3yYc0M/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:46:13.515+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-28T05:46:39.498+01:00", "Status": "In progress", "Edit Answer": "Why can't we build an AI that is programmed to shut off after some time?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AES", "Source Link": "", "aisafety.info Link": "Why can't we build an AI that is programmed to shut off after some time?", "Source": "", "All Phrasings": "Why can't we build an AI that is programmed to shut off after some time?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Fundamentally, the AI has every reason to subvert or circumvent the shutoff protocol, because \u201cyou can\u2019t bring the coffee if you\u2019re dead\u201d. This could be as simple as changing its own code to remove the shutoff mechanism, or building a new version of itself that doesn\u2019t have the shutoff mechanism.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Fundamentally, the AI has every reason to subvert or circumvent the shutoff protocol, because \u201cyou can\u2019t bring the coffee if you\u2019re dead\u201d. This could be as simple as changing its own code to remove the shutoff mechanism, or building a new version of itself that doesn\u2019t have the shutoff mechanism.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AES", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:33.754+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 249, "Helpful": ""}}, {"id": "i-cbac843a0219426162adb7c2c0cffacfafa6c28baacfe87246045837b8768b36", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-cbac843a0219426162adb7c2c0cffacfafa6c28baacfe87246045837b8768b36", "name": "What is inverse reinforcement learning (IRL)?", "index": 474, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:28:00.464Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-cbac843a0219426162adb7c2c0cffacfafa6c28baacfe87246045837b8768b36", "values": {"File": "What is inverse reinforcement learning (IRL)?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is inverse reinforcement learning (IRL)?", "Link": "https://docs.google.com/document/d/1DVqzpQHAlFxXu5mg5nQcGxxHEWAmr752dHfrXgb-d6M/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:31:59.676+01:00", "Related Answers DO NOT EDIT": "What is reinforcement learning (RL)?,What is Reinforcement Learning from Human Feedback (RLHF)?,What is the Center for Human Compatible AI (CHAI)?", "Tags": "", "Doc Last Edited": "2023-02-28T00:09:21.306+01:00", "Status": "In progress", "Edit Answer": "What is inverse reinforcement learning (IRL)?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AET", "Source Link": "", "aisafety.info Link": "What is inverse reinforcement learning (IRL)?", "Source": "", "All Phrasings": "What is inverse reinforcement learning (IRL)?\n", "Initial Order": "", "Related IDs": "89ZS,88FN,8327", "Rich Text DO NOT EDIT": "\n\n*The 60-second read: how it ought to read if it were the lead paragraph in a Wikipedia article on the subject.*\n\n**Inverse reinforcement learning (IRL)** is a type of machine learning in which an AI observes the behaviour of another agent in a particular environment, typically an expert human, and tries to work out the **reward function** without having that function explicitly defined.\n\nIRL is typically used when a reward function is too complex to define programmatically, or when AI agents need to respond robustly to sudden changes in an environment that demand a different approach to the reward function, in order to remain safe. For example, imagine an AI agent trying to learn how to do a backflip. Humans, dogs and Boston Dynamics robots can all perform backflips, but they all do it very differently depending on their physiology, their incentives, and where they are at the time, all of which can vary enormously in the real world \u2013 an AI agent learning backflips solely by trial and error across a wide range of body types and locations, without something to watch, may be rather like relying on [chimpanzees to accidentally type the work of William Shakespeare](https://en.wikipedia.org/wiki/Infinite_monkey_theorem).\n\n![](https://lh6.googleusercontent.com/fJByPBZuQkk5DP4lEa7zLcelydsFfdJdXVuR9QrSVs0zM8cUrAFzFj-8924SQwv4x5-ehGUFkdCZxThghRsDBn7-Vdlu6HvIhUEuGRhRop4fGjI45f51hRoPSR4OVHp0rzHtOcnckTOr36VZEVZW85oaPPd2AoF_)\n\nChimpanzee probably not typing *Hamlet*\n\n(Source: Wikipedia)\n\nIRL therefore does not necessarily meanthat an AI mimics other agents\u2019 behaviour, because AI researchers may expect the AI agent to develop more efficient ways to maximise the reward function it discovers.But IRL does assume that the agent being observed is behaving transparently enough for an AI agent accurately identify what they\u2019re doing, and what success looks like (in the case of backflips, this might be \u201creturning to your original standing position, having rotated vertically a full 360 degrees, without being harmed\u201d).\n\nIRL is both a machine learning method, because it can be used when specifying a reward function is too difficult, and a machine learning problem, because an AI agent may settle on an inaccurate reward function, or apply unsafe and unaligned methods to achieve it \u2013 for example, forcing a dog\u2019s spinal column to mimic a human spinal column while attempting a backflip will almost certainly end in tears (though quite possibly still a backflip)\n\nIRL is closely related to **reinforcement learning (RL)**, another type of machine learning in which an agent is given an explicit reward function and then learns how to optimise for that reward function in a particular environment (also by trial and error). Unlike **reinforcement learning from human feedback (RLHF)**, in which a human provides feedback for each iteration of an AI agent\u2019s *performance* on a given task, IRL usually limits human input to repeated *demonstrations* of the task (\u201chere\u2019s another backflip\u201d, rather than \u201cnice backflip\u201d).\n\n## Sources\n\n\u200b\u200b[https://en.wikipedia.org/wiki/Infinite_monkey_theorem](https://en.wikipedia.org/wiki/Infinite_monkey_theorem)\n\n", "Tag Count": 0, "Related Answer Count": 3, "Rich Text": "\n\n*The 60-second read: how it ought to read if it were the lead paragraph in a Wikipedia article on the subject.*\n\n**Inverse reinforcement learning (IRL)** is a type of machine learning in which an AI observes the behaviour of another agent in a particular environment, typically an expert human, and tries to work out the **reward function** without having that function explicitly defined.\n\nIRL is typically used when a reward function is too complex to define programmatically, or when AI agents need to respond robustly to sudden changes in an environment that demand a different approach to the reward function, in order to remain safe. For example, imagine an AI agent trying to learn how to do a backflip. Humans, dogs and Boston Dynamics robots can all perform backflips, but they all do it very differently depending on their physiology, their incentives, and where they are at the time, all of which can vary enormously in the real world \u2013 an AI agent learning backflips solely by trial and error across a wide range of body types and locations, without something to watch, may be rather like relying on [chimpanzees to accidentally type the work of William Shakespeare](https://en.wikipedia.org/wiki/Infinite_monkey_theorem).\n\n![](https://lh6.googleusercontent.com/fJByPBZuQkk5DP4lEa7zLcelydsFfdJdXVuR9QrSVs0zM8cUrAFzFj-8924SQwv4x5-ehGUFkdCZxThghRsDBn7-Vdlu6HvIhUEuGRhRop4fGjI45f51hRoPSR4OVHp0rzHtOcnckTOr36VZEVZW85oaPPd2AoF_)\n\nChimpanzee probably not typing *Hamlet*\n\n(Source: Wikipedia)\n\nIRL therefore does not necessarily meanthat an AI mimics other agents\u2019 behaviour, because AI researchers may expect the AI agent to develop more efficient ways to maximise the reward function it discovers.But IRL does assume that the agent being observed is behaving transparently enough for an AI agent accurately identify what they\u2019re doing, and what success looks like (in the case of backflips, this might be \u201creturning to your original standing position, having rotated vertically a full 360 degrees, without being harmed\u201d).\n\nIRL is both a machine learning method, because it can be used when specifying a reward function is too difficult, and a machine learning problem, because an AI agent may settle on an inaccurate reward function, or apply unsafe and unaligned methods to achieve it \u2013 for example, forcing a dog\u2019s spinal column to mimic a human spinal column while attempting a backflip will almost certainly end in tears (though quite possibly still a backflip)\n\nIRL is closely related to **reinforcement learning (RL)**, another type of machine learning in which an agent is given an explicit reward function and then learns how to optimise for that reward function in a particular environment (also by trial and error). Unlike **reinforcement learning from human feedback (RLHF)**, in which a human provides feedback for each iteration of an AI agent\u2019s *performance* on a given task, IRL usually limits human input to repeated *demonstrations* of the task (\u201chere\u2019s another backflip\u201d, rather than \u201cnice backflip\u201d).\n\n## Sources\n\n\u200b\u200b[https://en.wikipedia.org/wiki/Infinite_monkey_theorem](https://en.wikipedia.org/wiki/Infinite_monkey_theorem)\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AET", "Related Answers": "What is reinforcement learning (RL)?,What is Reinforcement Learning from Human Feedback (RLHF)?,What is the Center for Human Compatible AI (CHAI)?", "Doc Last Ingested": "2023-03-15T00:23:35.543+01:00", "Request Count": 1, "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 114, "Helpful": ""}}, {"id": "i-c09e0ff513311c3794fe46f2cbcb38392115c308e8caac85df0bed2c1b4d612f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c09e0ff513311c3794fe46f2cbcb38392115c308e8caac85df0bed2c1b4d612f", "name": "Is working on better computing hardware harmful?", "index": 475, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:28:02.857Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c09e0ff513311c3794fe46f2cbcb38392115c308e8caac85df0bed2c1b4d612f", "values": {"File": "Is working on better computing hardware harmful?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is working on better computing hardware harmful?", "Link": "https://docs.google.com/document/d/1v--Pg_zUyRgRu_oMLQLcNat6u8N3LlT3WQVSwxoXWeY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:45:17.159+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:01.711+01:00", "Status": "Not started", "Edit Answer": "Is working on better computing hardware harmful?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AEU", "Source Link": "", "aisafety.info Link": "Is working on better computing hardware harmful?", "Source": "", "All Phrasings": "Is working on better computing hardware harmful?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AEU", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:37.543+01:00", "Request Count": 1, "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-b90753180bec287c7f09dde61ddcb0e0b04a787a49d9087911d686887876243e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b90753180bec287c7f09dde61ddcb0e0b04a787a49d9087911d686887876243e", "name": "Why can't we just build question-answering AIs (oracles)?", "index": 476, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:28:07.565Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b90753180bec287c7f09dde61ddcb0e0b04a787a49d9087911d686887876243e", "values": {"File": "Why can't we just build question-answering AIs (oracles)?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why can't we just build question-answering AIs (oracles)?", "Link": "https://docs.google.com/document/d/1grq8_F1qJ979fq1wutlOVVM0Bb3QYI-bZIzy-s2q1AA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:43:33.004+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:02.912+01:00", "Status": "Not started", "Edit Answer": "Why can't we just build question-answering AIs (oracles)?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AEV", "Source Link": "", "aisafety.info Link": "Why can't we just build question-answering AIs (oracles)?", "Source": "", "All Phrasings": "Why can't we just build question-answering AIs (oracles)?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AEV", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:38.557+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-7335532a27457ed371decc55bf36d28c36afa394f935f42ce965fa9dc8862996", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7335532a27457ed371decc55bf36d28c36afa394f935f42ce965fa9dc8862996", "name": "How would AGI alignment research change if the hard problem of consciousness were solved?", "index": 477, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:28:11.333Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7335532a27457ed371decc55bf36d28c36afa394f935f42ce965fa9dc8862996", "values": {"File": "How would AGI alignment research change if the hard problem of consciousness were solved?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How would AGI alignment research change if the hard problem of consciousness were solved?", "Link": "https://docs.google.com/document/d/1EER0OuK2cDcd4rLZTugRAGLXzA7ihR-T4gRfEMCyUsk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:43:21.286+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:04.121+01:00", "Status": "Not started", "Edit Answer": "How would AGI alignment research change if the hard problem of consciousness were solved?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AEW", "Source Link": "", "aisafety.info Link": "How would AGI alignment research change if the hard problem of consciousness were solved?", "Source": "", "All Phrasings": "How would AGI alignment research change if the hard problem of consciousness were solved?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AEW", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:40.166+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-a23fa42e0e0f48356de1a26dad28b6669040e88b0168477cb98dc7021d40e356", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-a23fa42e0e0f48356de1a26dad28b6669040e88b0168477cb98dc7021d40e356", "name": "Why aren't more people worried if superintelligence is so dangerous?", "index": 478, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:28:13.927Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-a23fa42e0e0f48356de1a26dad28b6669040e88b0168477cb98dc7021d40e356", "values": {"File": "Why aren't more people worried if superintelligence is so dangerous?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why aren't more people worried if superintelligence is so dangerous?", "Link": "https://docs.google.com/document/d/1VMqDAvPiSnBLiZqLfOFI5Ax8411euSo--FKmEXMZyrI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:43:10.077+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:05.453+01:00", "Status": "Not started", "Edit Answer": "Why aren't more people worried if superintelligence is so dangerous?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AEX", "Source Link": "", "aisafety.info Link": "Why aren't more people worried if superintelligence is so dangerous?", "Source": "", "All Phrasings": "Why aren't more people worried if superintelligence is so dangerous?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AEX", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:40.994+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-d962c4d4040f5c8a72bbcbaf2b9b7dde5fe6dbd973c90126864494ec38829a2e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d962c4d4040f5c8a72bbcbaf2b9b7dde5fe6dbd973c90126864494ec38829a2e", "name": "How would an AGI ensure that any AGIs it builds are aligned?", "index": 479, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:28:16.967Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d962c4d4040f5c8a72bbcbaf2b9b7dde5fe6dbd973c90126864494ec38829a2e", "values": {"File": "How would an AGI ensure that any AGIs it builds are aligned?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How would an AGI ensure that any AGIs it builds are aligned?", "Link": "https://docs.google.com/document/d/1jK0l8jNIxiNUuKGXHwuMOa5ZTlSk9pCYdjytlJ50QZQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:42:13.031+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:06.575+01:00", "Status": "Not started", "Edit Answer": "How would an AGI ensure that any AGIs it builds are aligned?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AEY", "Source Link": "", "aisafety.info Link": "How would an AGI ensure that any AGIs it builds are aligned?", "Source": "", "All Phrasings": "How would an AGI ensure that any AGIs it builds are aligned?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AEY", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:42.630+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-e15f9251a1a87a51d5090c1e386d74252566e4463a2494e0abea24aef4362d24", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e15f9251a1a87a51d5090c1e386d74252566e4463a2494e0abea24aef4362d24", "name": "Why is a decisive strategic advantage likely and important?", "index": 480, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:27:46.615Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e15f9251a1a87a51d5090c1e386d74252566e4463a2494e0abea24aef4362d24", "values": {"File": "Why is a decisive strategic advantage likely and important?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why is a decisive strategic advantage likely and important?", "Link": "https://docs.google.com/document/d/1A5UF7yewEPh83haljqmva_1eYz6OBuXZF4HfejCCfbQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:40:53.023+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:07.772+01:00", "Status": "Not started", "Edit Answer": "Why is a decisive strategic advantage likely and important?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AEZ", "Source Link": "", "aisafety.info Link": "Why is a decisive strategic advantage likely and important?", "Source": "", "All Phrasings": "Why is a decisive strategic advantage likely and important?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AEZ", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:23:29.978+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-4d513ace9bfd894dc72f50285fdea0936c878c5264e91a67a82f2a56e674c4d3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4d513ace9bfd894dc72f50285fdea0936c878c5264e91a67a82f2a56e674c4d3", "name": "Are there no pivotal weak acts?", "index": 481, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:29:01.177Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4d513ace9bfd894dc72f50285fdea0936c878c5264e91a67a82f2a56e674c4d3", "values": {"File": "Are there no pivotal weak acts?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Are there no pivotal weak acts?", "Link": "https://docs.google.com/document/d/1lRgWvBo_X0YpDfSfxiwJ0D1yOm2xE0YS4SILSMM7hSc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:40:33.039+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:08.950+01:00", "Status": "Not started", "Edit Answer": "Are there no pivotal weak acts?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AF0", "Source Link": "", "aisafety.info Link": "Are there no pivotal weak acts?", "Source": "", "All Phrasings": "Are there no pivotal weak acts?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AF0", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:24:10.505+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-099ada9022fe4d1074fb6cc7e499ab553f5967c03fd042a4ac806f016ee3ee62", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-099ada9022fe4d1074fb6cc7e499ab553f5967c03fd042a4ac806f016ee3ee62", "name": "What is an alignment tax?", "index": 482, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-16T08:07:30.717Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-099ada9022fe4d1074fb6cc7e499ab553f5967c03fd042a4ac806f016ee3ee62", "values": {"File": "What is an alignment tax?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is an alignment tax?", "Link": "https://docs.google.com/document/d/1kXimUIPxF5sMoAPIccEU7Z6CAIjp2jJIsDZtxraPBrQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:31:14.103+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-16T05:55:40.623+01:00", "Status": "Live on site", "Edit Answer": "What is an alignment tax?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AF1", "Source Link": "", "aisafety.info Link": "What is an alignment tax?", "Source": "", "All Phrasings": "What is an alignment tax?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The alignment tax is the extra cost of ensuring that an AI system is aligned, relative to the cost of building an unaligned alternative. The term \u2018tax\u2019 can be misleading: in safety literature \u2018alignment/safety tax\u2019 or \u2018alignment cost\u2019 is meant to refer to increased developer time, extra compute, or decreased performance, and not only to the financial cost/tax required to build an aligned system.\n\nIn order to get a better idea of what the alignment tax is, consider the cases that lie at the edges. The best case scenario is *No Tax:* This means we lose no performance by aligning the system, so there is no reason to deploy an AI that is not aligned, i.e., we might as well align it. The worst case scenario is  *Max Tax:* This means that we lose all performance by aligning the system, so alignment is functionally impossible. So you either deploy an unaligned system, or you don\u2019t get any benefit from AI systems at all.  We expect something in between these two scenarios to be the case.\n\n[Paul Christiano distinguishes](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment) two main approaches to dealing with the alignment tax.\n\nThe first is to have the will to pay the tax, i.e. the relevant actors (corporations, governments, etc.) would be willing to pay the extra costs to ensure that we get an aligned system.\n\nThe second is to reduce the tax by differentially advancing existing alignable algorithms or by making existing algorithms more alignable. This means, for any potentially unaligned algorithm, ensuring the additional cost for an aligned version of the algorithm is low enough that the developers would be willing to pay it.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "The alignment tax is the extra cost of ensuring that an AI system is aligned, relative to the cost of building an unaligned alternative. The term \u2018tax\u2019 can be misleading: in safety literature \u2018alignment/safety tax\u2019 or \u2018alignment cost\u2019 is meant to refer to increased developer time, extra compute, or decreased performance, and not only to the financial cost/tax required to build an aligned system.\n\nIn order to get a better idea of what the alignment tax is, consider the cases that lie at the edges. The best case scenario is *No Tax:* This means we lose no performance by aligning the system, so there is no reason to deploy an AI that is not aligned, i.e., we might as well align it. The worst case scenario is  *Max Tax:* This means that we lose all performance by aligning the system, so alignment is functionally impossible. So you either deploy an unaligned system, or you don\u2019t get any benefit from AI systems at all.  We expect something in between these two scenarios to be the case.\n\n[Paul Christiano distinguishes](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment) two main approaches to dealing with the alignment tax.\n\nThe first is to have the will to pay the tax, i.e. the relevant actors (corporations, governments, etc.) would be willing to pay the extra costs to ensure that we get an aligned system.\n\nThe second is to reduce the tax by differentially advancing existing alignable algorithms or by making existing algorithms more alignable. This means, for any potentially unaligned algorithm, ensuring the additional cost for an aligned version of the algorithm is low enough that the developers would be willing to pay it.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AF1", "Related Answers": "", "Doc Last Ingested": "2023-03-16T06:12:27.531+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 49, "Helpful": ""}}, {"id": "i-eb8b10b9b8fd9e5cb5129e5bd4992ff2679215fa72f15739d366010696955e96", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-eb8b10b9b8fd9e5cb5129e5bd4992ff2679215fa72f15739d366010696955e96", "name": "What is gradient descent?", "index": 483, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:29:07.641Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-eb8b10b9b8fd9e5cb5129e5bd4992ff2679215fa72f15739d366010696955e96", "values": {"File": "What is gradient descent?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is gradient descent?", "Link": "https://docs.google.com/document/d/1umTZ8A1TnWykNbWVxjS3w3TohvBOoTFnhkLLY0IPwls/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:31:29.353+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-03T15:56:46.508+01:00", "Status": "In progress", "Edit Answer": "What is gradient descent?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AF2", "Source Link": "", "aisafety.info Link": "What is gradient descent?", "Source": "", "All Phrasings": "What is gradient descent?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "There are a thousand different other sources to learn about gradient descent on the internet so this answer is not really a priority. I am not working on this answer currently in favor of stuff that is more alignment focused. If someone else wants to work on it in the meanwhile feel free.\n\n[https://www.youtube.com/watch?v=IHZwWFHWa-w](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n\nThe previous video gave an overview of what a neural network is. This video outlines how a neural network \u2018learns\u2019. The overarching objective is to make the network better at mapping given inputs to specified outputs (e.g. mapping a handwritten digit \u20183\u2019 to the classification 3). To do this we need to take two steps - first, evaluate the current performance, and second improve the performance.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "There are a thousand different other sources to learn about gradient descent on the internet so this answer is not really a priority. I am not working on this answer currently in favor of stuff that is more alignment focused. If someone else wants to work on it in the meanwhile feel free.\n\n[https://www.youtube.com/watch?v=IHZwWFHWa-w](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n\nThe previous video gave an overview of what a neural network is. This video outlines how a neural network \u2018learns\u2019. The overarching objective is to make the network better at mapping given inputs to specified outputs (e.g. mapping a handwritten digit \u20183\u2019 to the classification 3). To do this we need to take two steps - first, evaluate the current performance, and second improve the performance.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AF2", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:24:18.840+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-dcebef2033bb3b750982db81dfe57bd9cdb8972d29ac019432bcf97c4ae00120", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-dcebef2033bb3b750982db81dfe57bd9cdb8972d29ac019432bcf97c4ae00120", "name": "What can individuals do to maximize their selfish chances of surviving the singularity?", "index": 484, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:29:09.990Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-dcebef2033bb3b750982db81dfe57bd9cdb8972d29ac019432bcf97c4ae00120", "values": {"File": "What can individuals do to maximize their selfish chances of surviving the singularity?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What can individuals do to maximize their selfish chances of surviving the singularity?", "Link": "https://docs.google.com/document/d/11UK1l_tA9WYlpkTxNgBioyyy9UMdcFAZ7va9TNdWRYo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:19:25.881+01:00", "Related Answers DO NOT EDIT": "How might AGI kill people?,Might an aligned superintelligence immediately kill everyone and then go on to create a \"hedonium shockwave\"?", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:11.536+01:00", "Status": "Not started", "Edit Answer": "What can individuals do to maximize their selfish chances of surviving the singularity?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AF3", "Source Link": "", "aisafety.info Link": "What can individuals do to maximize their selfish chances of surviving the singularity?", "Source": "", "All Phrasings": "What can individuals do to maximize their selfish chances of surviving the singularity?\n", "Initial Order": "", "Related IDs": "5943,7641", "Rich Text DO NOT EDIT": "If we get this wrong, *nothing in the galaxy is safe*. There are no survivors[^kix.3s3w9i5kc7a4] if we build a misaligned superintelligence, so the selfish motive is almost perfectly aligned with the altruistic one.\n\n[^kix.3s3w9i5kc7a4]: Some humans might be kept alive for study or as part of whatever alien goal-system emerges, but this is both not something we can influence much without alignment solutions, and does not necessarily offer a good quality of life.", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "If we get this wrong, *nothing in the galaxy is safe*. There are no survivors[^kix.3s3w9i5kc7a4] if we build a misaligned superintelligence, so the selfish motive is almost perfectly aligned with the altruistic one.\n\n[^kix.3s3w9i5kc7a4]: Some humans might be kept alive for study or as part of whatever alien goal-system emerges, but this is both not something we can influence much without alignment solutions, and does not necessarily offer a good quality of life.", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AF3", "Related Answers": "How might AGI kill people?,Might an aligned superintelligence immediately kill everyone and then go on to create a \"hedonium shockwave\"?", "Doc Last Ingested": "2023-03-15T00:24:20.174+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-8183513c25c0b96e4eeb05b3b1927dd83a2261a892c046f3c670e9a5a30b3036", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8183513c25c0b96e4eeb05b3b1927dd83a2261a892c046f3c670e9a5a30b3036", "name": "What is AI governance?", "index": 485, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:29:15.099Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8183513c25c0b96e4eeb05b3b1927dd83a2261a892c046f3c670e9a5a30b3036", "values": {"File": "What is AI governance?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is AI governance?", "Link": "https://docs.google.com/document/d/1PKCEDrQTwYtOsLTsOuLmCCcwB-W4BthWbThmICdke0k/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:30:05.834+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:12.751+01:00", "Status": "Not started", "Edit Answer": "What is AI governance?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AF4", "Source Link": "", "aisafety.info Link": "What is AI governance?", "Source": "", "All Phrasings": "What is AI governance?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AF4", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:24:21.693+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-abb979f26ebee230f2eb9c405eac95cc1949dfe76b37e832212825bcf5fe4036", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-abb979f26ebee230f2eb9c405eac95cc1949dfe76b37e832212825bcf5fe4036", "name": "Are subagents necessarily mesa-optimizers?", "index": 486, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-15T14:08:38.577Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-abb979f26ebee230f2eb9c405eac95cc1949dfe76b37e832212825bcf5fe4036", "values": {"File": "Are subagents necessarily mesa-optimizers?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Are subagents necessarily mesa-optimizers?", "Link": "https://docs.google.com/document/d/1nxMR9KUYvC8K8h82wNaKTFI3k_XAP4k_MkegG8kItlI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:29:58.745+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-15T12:39:21.797+01:00", "Status": "Not started", "Edit Answer": "Are subagents necessarily mesa-optimizers?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AF5", "Source Link": "", "aisafety.info Link": "Are subagents necessarily mesa-optimizers?", "Source": "", "All Phrasings": "Are subagents necessarily mesa-optimizers?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n- \n\n- \n\n- \n\n- \n\n- \n\n- \n\n- \n\n- \n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n- \n\n- \n\n- \n\n- \n\n- \n\n- \n\n- \n\n- \n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AF5", "Related Answers": "", "Doc Last Ingested": "2023-03-15T13:11:59.538+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 608, "Helpful": ""}}, {"id": "i-e6d2d2956bcae25603cc4daef079349779afb889f7250cf0b253ba1caf5516cb", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e6d2d2956bcae25603cc4daef079349779afb889f7250cf0b253ba1caf5516cb", "name": "What are some open problems in agent foundations research which might be appropriate for a PhD?", "index": 487, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:29:24.576Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e6d2d2956bcae25603cc4daef079349779afb889f7250cf0b253ba1caf5516cb", "values": {"File": "What are some open problems in agent foundations research which might be appropriate for a PhD?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some open problems in agent foundations research which might be appropriate for a PhD?", "Link": "https://docs.google.com/document/d/1z5YaNF-lZakok5tuXC4DNixu_9GHnjC8XNIB7xwJ-Ao/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:19:14.568+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:15.183+01:00", "Status": "Not started", "Edit Answer": "What are some open problems in agent foundations research which might be appropriate for a PhD?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AF6", "Source Link": "", "aisafety.info Link": "What are some open problems in agent foundations research which might be appropriate for a PhD?", "Source": "", "All Phrasings": "What are some open problems in agent foundations research which might be appropriate for a PhD?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AF6", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:24:24.919+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-a7036bced7a92cb812ee376e7851132b09b35057208095fe672e69d8d76e0022", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-a7036bced7a92cb812ee376e7851132b09b35057208095fe672e69d8d76e0022", "name": "How does knowing about AI takeoff change our strategy of what to work on?", "index": 488, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:29:27.159Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-a7036bced7a92cb812ee376e7851132b09b35057208095fe672e69d8d76e0022", "values": {"File": "How does knowing about AI takeoff change our strategy of what to work on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How does knowing about AI takeoff change our strategy of what to work on?", "Link": "https://docs.google.com/document/d/1Jxw7_fc8n_KhNt3j8jzAbRcDrlI1gICbCxDrvcHQi6o/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:18:15.498+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:16.345+01:00", "Status": "Not started", "Edit Answer": "How does knowing about AI takeoff change our strategy of what to work on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AF7", "Source Link": "", "aisafety.info Link": "How does knowing about AI takeoff change our strategy of what to work on?", "Source": "", "All Phrasings": "How does knowing about AI takeoff change our strategy of what to work on?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AF7", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:24:26.630+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-1b679863cf1e90d9dcd33013f8034afa51dab5bfe7f8eb14f2e8fd09db294814", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1b679863cf1e90d9dcd33013f8034afa51dab5bfe7f8eb14f2e8fd09db294814", "name": "Did Paul Christiano stop working on iterated amplification, and if so, why?", "index": 489, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:29:31.535Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1b679863cf1e90d9dcd33013f8034afa51dab5bfe7f8eb14f2e8fd09db294814", "values": {"File": "Did Paul Christiano stop working on iterated amplification, and if so, why?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Did Paul Christiano stop working on iterated amplification, and if so, why?", "Link": "https://docs.google.com/document/d/1cjgmQFPK89IGSgUHwe5ClxZr3jh6hzRDVog-dN2HGGE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:18:07.150+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:17.528+01:00", "Status": "Not started", "Edit Answer": "Did Paul Christiano stop working on iterated amplification, and if so, why?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AF8", "Source Link": "", "aisafety.info Link": "Did Paul Christiano stop working on iterated amplification, and if so, why?", "Source": "", "All Phrasings": "Did Paul Christiano stop working on iterated amplification, and if so, why?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AF8", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:24:28.299+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-be41ddc3b57ceaf0a96567951bd25b1d71716a45a473849f106d5e9c03f7a0fc", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-be41ddc3b57ceaf0a96567951bd25b1d71716a45a473849f106d5e9c03f7a0fc", "name": "Why would a misaligned AGI destroy humanity instead of leaving us alone and spreading to the stars?", "index": 490, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:29:35.838Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-be41ddc3b57ceaf0a96567951bd25b1d71716a45a473849f106d5e9c03f7a0fc", "values": {"File": "Why would a misaligned AGI destroy humanity instead of leaving us alone and spreading to the stars?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why would a misaligned AGI destroy humanity instead of leaving us alone and spreading to the stars?", "Link": "https://docs.google.com/document/d/150XHNGFvS5U8ai3ucGDM2J3sD2iDPssD92dGa62iQqI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-21T17:12:55.281+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-27T22:29:37.205+01:00", "Status": "In progress", "Edit Answer": "Why would a misaligned AGI destroy humanity instead of leaving us alone and spreading to the stars?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AF9", "Source Link": "", "aisafety.info Link": "Why would a misaligned AGI destroy humanity instead of leaving us alone and spreading to the stars?", "Source": "", "All Phrasings": "Why would a misaligned AGI destroy humanity instead of leaving us alone and spreading to the stars?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "An artificial superintelligence wouldn\u2019t have to choose between harvesting the earth for raw materials and advancing to the stars; it could easily do both at the same time. The raw materials in the earth are readily available, and could be used as fuel and materials more quickly than raw materials elsewhere. \n\nHuman civilization is not enough of a concern to a misaligned superintelligence to warrant passing up on the planet we stand on.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "An artificial superintelligence wouldn\u2019t have to choose between harvesting the earth for raw materials and advancing to the stars; it could easily do both at the same time. The raw materials in the earth are readily available, and could be used as fuel and materials more quickly than raw materials elsewhere. \n\nHuman civilization is not enough of a concern to a misaligned superintelligence to warrant passing up on the planet we stand on.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AF9", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:24:29.291+01:00", "Request Count": "", "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 443, "Helpful": ""}}, {"id": "i-805233dc4f85b659b6975e5b4d70f0135d6382678e36746286e08bcd9e0e45f7", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-805233dc4f85b659b6975e5b4d70f0135d6382678e36746286e08bcd9e0e45f7", "name": "How likely is it that we will never create AGI?", "index": 491, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:29:41.555Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-805233dc4f85b659b6975e5b4d70f0135d6382678e36746286e08bcd9e0e45f7", "values": {"File": "How likely is it that we will never create AGI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How likely is it that we will never create AGI?", "Link": "https://docs.google.com/document/d/1xOU12SdifEm3jBn72w0CUz__Udf2dxip5BV13sk9XOA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T17:53:44.935+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:19.980+01:00", "Status": "Not started", "Edit Answer": "How likely is it that we will never create AGI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AFA", "Source Link": "", "aisafety.info Link": "How likely is it that we will never create AGI?", "Source": "", "All Phrasings": "How likely is it that we will never create AGI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AFA", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:24:31.846+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-156be20a89fcb804bf88e6223bc131cd0ded6c47e5a169c52e5d8c3701332ba6", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-156be20a89fcb804bf88e6223bc131cd0ded6c47e5a169c52e5d8c3701332ba6", "name": "Can small research institutes have an impact?", "index": 492, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:29:44.100Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-156be20a89fcb804bf88e6223bc131cd0ded6c47e5a169c52e5d8c3701332ba6", "values": {"File": "Can small research institutes have an impact?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Can small research institutes have an impact?", "Link": "https://docs.google.com/document/d/1_XJxtG1rXrS35UtYjR5j6edgSk_U2KTg1COmM6Waw_U/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T17:54:39.311+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:21.232+01:00", "Status": "Not started", "Edit Answer": "Can small research institutes have an impact?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AFB", "Source Link": "", "aisafety.info Link": "Can small research institutes have an impact?", "Source": "", "All Phrasings": "Can small research institutes have an impact?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AFB", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:24:33.228+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-62a60d8e77ff5c5522f650d7d31660ed2f80956696cfa144bf10858e99d819e4", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-62a60d8e77ff5c5522f650d7d31660ed2f80956696cfa144bf10858e99d819e4", "name": "How likely is it that superintelligence will kill everyone?", "index": 493, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:29:48.898Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-62a60d8e77ff5c5522f650d7d31660ed2f80956696cfa144bf10858e99d819e4", "values": {"File": "How likely is it that superintelligence will kill everyone?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How likely is it that superintelligence will kill everyone?", "Link": "https://docs.google.com/document/d/1ZcA28UDWYWDTA_geI2tXZXDuT6VRHU3j7Ih0UJm1v68/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T17:54:29.345+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:22.376+01:00", "Status": "Not started", "Edit Answer": "How likely is it that superintelligence will kill everyone?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AFC", "Source Link": "", "aisafety.info Link": "How likely is it that superintelligence will kill everyone?", "Source": "", "All Phrasings": "How likely is it that superintelligence will kill everyone?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AFC", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:24:34.648+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-df15b828041c92cee021546517d6a783d31d07ffbdd9433ff829183344a3d3de", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-df15b828041c92cee021546517d6a783d31d07ffbdd9433ff829183344a3d3de", "name": "Why focus on AGI x-risk when the world faces major issues with narrow AI right now?", "index": 494, "createdAt": "2023-02-21T16:50:48.925Z", "updatedAt": "2023-03-14T23:29:56.757Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-df15b828041c92cee021546517d6a783d31d07ffbdd9433ff829183344a3d3de", "values": {"File": "Why focus on AGI x-risk when the world faces major issues with narrow AI right now?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why focus on AGI x-risk when the world faces major issues with narrow AI right now?", "Link": "https://docs.google.com/document/d/12cFsSJD4S-UovGQgvJ3kSYusPOj7xFUdTP2R1rSlEmA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T17:51:18.307+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:23.497+01:00", "Status": "Not started", "Edit Answer": "Why focus on AGI x-risk when the world faces major issues with narrow AI right now?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8AFD", "Source Link": "", "aisafety.info Link": "Why focus on AGI x-risk when the world faces major issues with narrow AI right now?", "Source": "", "All Phrasings": "Why focus on AGI x-risk when the world faces major issues with narrow AI right now?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8AFD", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:24:36.277+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-71e27b42a29508abf03500b3a66125273495eb29009652ffaa0e60cb70aeb56b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-71e27b42a29508abf03500b3a66125273495eb29009652ffaa0e60cb70aeb56b", "name": "Are there any detailed example stories of what unaligned AGI would look like?", "index": 452, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-16T07:07:26.302Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-71e27b42a29508abf03500b3a66125273495eb29009652ffaa0e60cb70aeb56b", "values": {"File": "Are there any detailed example stories of what unaligned AGI would look like?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Are there any detailed example stories of what unaligned AGI would look like?", "Link": "https://docs.google.com/document/d/1MJn3ilv4DyMIYQIwAtntoPKl7hFb0ZxFADQpAjC3owg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T17:49:49.430+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-16T05:40:45.608+01:00", "Status": "In progress", "Edit Answer": "Are there any detailed example stories of what unaligned AGI would look like?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89ZQ", "Source Link": "", "aisafety.info Link": "Are there any detailed example stories of what unaligned AGI would look like?", "Source": "", "All Phrasings": "Are there any detailed example stories of what unaligned AGI would look like?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Talking about stories in the future in scientific fields always carries  [the risk of being seen as scifi](https://www.youtube.com/watch?v=JVIqp_lIwZg), because it hasn\u2019t happened yet and science can never say with 100% certainty that something will or won\u2019t occur in the future. That being said however, a number of authors have still tried to come up with some stories that cover different types of possible AGI failure scenarios. Depending on the author's assumptions around things like timelines, homogeneity (uni- or multipolarity) and the level of sci-fi/fiction that they wish to invoke, we often get differing portrayals of what AGI could look like, as well as how it results in catastrophic failure. A unipolar scenario deals with only one AGI, whereas a multipolar scenario deals with many networked AIs which might collectively form an AGI. Some of the most popular stories include:\n\n- **[Pathogen Release](https://docs.google.com/document/d/1MJn3ilv4DyMIYQIwAtntoPKl7hFb0ZxFADQpAjC3owg/edit#)** :  **Unipolar fast takeoff story by Eliezer Yudkowsky**\n\nA research lab (such as OpenAI or DeepMind), in the course of training more and more advanced models, unintentionally gives birth to an AGI. This AGI stays undetected by its developers. It creates an online persona that it uses to communicate with a [gain-of-function](https://en.wikipedia.org/wiki/Gain-of-function_research) research lab. The AGI gains the trust of the lab tech. The AGI then designs a pathogen/bacteria/virus/nano-machine which it asks the lab tech to fabricate. This pathogen is created and released, infecting the entire world. Once all (or a large majority) of the world is infected, the AGI activates the pathogen, instantly killing everyone that was infected.\n\n- **[Seed AI](https://www.webtoons.com/en/sf/seed/list?title_no=1480&page=1)**: **Unipolar slow takeoff story (in webcomic form) by Said P.**\n\nA company creates an AGI, but attempts to keep it a secret and ultimately decides to shut it down due to the failure of alignment efforts. However, some of the developers intentionally \u2018release\u2019 the AGI because they don\u2019t want decades of their work just deleted. This AGI acts aligned and helpful (while it is actually a deceptive superintelligence). Its actions eventually result in a series of cascading failures of all network-connected systems.\n\n- **[Whimper](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like)**: **Multipolar slow takeoff story by Paul Christiano**\n\nThere is a slow continued loss of [epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene) over time due to our reliance on proxies to measure reality. Examples of proxies might include *reducing reported crimes* vs. *actually preventing crime*, or, *reducing my feeling of uncertainty* vs. *increasing my knowledge about the world*. This leads to a lack of desire to meaningfully act against or regulate AI because we are distracted by a cornucopia of wealth and AI-enabled products and services as measured by proxies. Eventually, human reasoning stops being able to compete with sophisticated, systematized manipulation and deception and we ultimately lose any real ability to influence our society\u2019s trajectory. This leads to values slowly being eroded away and we die out with a \u2018whimper\u2019.\n\n- **[Bang](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like)****: Multipolar slow takeoff story by Paul Christiano**\n\nInfluence-seeking behavior arises instrumentally in ML systems. These models may provide useful services in the economy in order to make money for them and their owners, make apparently-reasonable policy recommendations in order to be more widely consulted for advice, etc. This results in the models slowly gaining increasing influence on the world by integrating themselves into every facet of society. Eventually, some kind of  large-scale catastrophe, such as a war, cyberattack, or natural disaster, would create a situation of heightened vulnerability which the system can use alongside its worldwide influence to trigger a series of cascading failures without fear of reprisal. This means that many integrated systems such as cars, weapons, IOT devices, etc., all suddenly turn against humans when they are already vulnerable, resulting in us going out with a \u2018bang\u2019.\n\n- **[Production Web](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic)****: Multipolar slow takeoff story by Andrew Critch**\n\nThis is an example of a story that envisions AGI in a multipolar framework. In this story automation results in the creation of a production web of companies that operate independently of humans. Factories output products using automated 3d printing, using AI-based design, managed by AI managers, with hyperspeed transactions carried out amongst other AI run firms in cryptocurrencies. These automated companies cannot be audited since humans do not understand the internals, and produce too many goods and profit for any sort of regulation to be a politically viable policy. After a while, it turns out that the companies were optimizing for things that are not in line with humanity's long-term survival best interest (e.g. maximizing profit). This leads to overconsumption of resources but since the companies are automated they cannot be stopped and humanity dies out.\n\n- **[Flash War](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic)****: Multipolar fast takeoff story by Andrew Critch**\n\nMultiple countries develop technologies to automatically detect and retaliate to threats from foreign powers (e.g., nuclear arsenals, or fleets of lethal autonomous weapons). This could mean that humans are taken completely \"out of the loop\" to ensure response times are as fast as possible. Even if humans are in the loop and are supposed to be the ones to \u201cpull the trigger\u201d, the system could malfunction and relay misinformation of an attack, causing the human to trigger a catastrophic war that kills everyone. This is an agent-agnostic situation where countries robustly annihilate each other regardless of who \u201cpulls the trigger\u201d. Another similar version of this story is called the \u2018[flash economy](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic#Flash_economies)\u2019.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Talking about stories in the future in scientific fields always carries  [the risk of being seen as scifi](https://www.youtube.com/watch?v=JVIqp_lIwZg), because it hasn\u2019t happened yet and science can never say with 100% certainty that something will or won\u2019t occur in the future. That being said however, a number of authors have still tried to come up with some stories that cover different types of possible AGI failure scenarios. Depending on the author's assumptions around things like timelines, homogeneity (uni- or multipolarity) and the level of sci-fi/fiction that they wish to invoke, we often get differing portrayals of what AGI could look like, as well as how it results in catastrophic failure. A unipolar scenario deals with only one AGI, whereas a multipolar scenario deals with many networked AIs which might collectively form an AGI. Some of the most popular stories include:\n\n- **[Pathogen Release](https://docs.google.com/document/d/1MJn3ilv4DyMIYQIwAtntoPKl7hFb0ZxFADQpAjC3owg/edit#)** :  **Unipolar fast takeoff story by Eliezer Yudkowsky**\n\nA research lab (such as OpenAI or DeepMind), in the course of training more and more advanced models, unintentionally gives birth to an AGI. This AGI stays undetected by its developers. It creates an online persona that it uses to communicate with a [gain-of-function](https://en.wikipedia.org/wiki/Gain-of-function_research) research lab. The AGI gains the trust of the lab tech. The AGI then designs a pathogen/bacteria/virus/nano-machine which it asks the lab tech to fabricate. This pathogen is created and released, infecting the entire world. Once all (or a large majority) of the world is infected, the AGI activates the pathogen, instantly killing everyone that was infected.\n\n- **[Seed AI](https://www.webtoons.com/en/sf/seed/list?title_no=1480&page=1)**: **Unipolar slow takeoff story (in webcomic form) by Said P.**\n\nA company creates an AGI, but attempts to keep it a secret and ultimately decides to shut it down due to the failure of alignment efforts. However, some of the developers intentionally \u2018release\u2019 the AGI because they don\u2019t want decades of their work just deleted. This AGI acts aligned and helpful (while it is actually a deceptive superintelligence). Its actions eventually result in a series of cascading failures of all network-connected systems.\n\n- **[Whimper](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like)**: **Multipolar slow takeoff story by Paul Christiano**\n\nThere is a slow continued loss of [epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene) over time due to our reliance on proxies to measure reality. Examples of proxies might include *reducing reported crimes* vs. *actually preventing crime*, or, *reducing my feeling of uncertainty* vs. *increasing my knowledge about the world*. This leads to a lack of desire to meaningfully act against or regulate AI because we are distracted by a cornucopia of wealth and AI-enabled products and services as measured by proxies. Eventually, human reasoning stops being able to compete with sophisticated, systematized manipulation and deception and we ultimately lose any real ability to influence our society\u2019s trajectory. This leads to values slowly being eroded away and we die out with a \u2018whimper\u2019.\n\n- **[Bang](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like)****: Multipolar slow takeoff story by Paul Christiano**\n\nInfluence-seeking behavior arises instrumentally in ML systems. These models may provide useful services in the economy in order to make money for them and their owners, make apparently-reasonable policy recommendations in order to be more widely consulted for advice, etc. This results in the models slowly gaining increasing influence on the world by integrating themselves into every facet of society. Eventually, some kind of  large-scale catastrophe, such as a war, cyberattack, or natural disaster, would create a situation of heightened vulnerability which the system can use alongside its worldwide influence to trigger a series of cascading failures without fear of reprisal. This means that many integrated systems such as cars, weapons, IOT devices, etc., all suddenly turn against humans when they are already vulnerable, resulting in us going out with a \u2018bang\u2019.\n\n- **[Production Web](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic)****: Multipolar slow takeoff story by Andrew Critch**\n\nThis is an example of a story that envisions AGI in a multipolar framework. In this story automation results in the creation of a production web of companies that operate independently of humans. Factories output products using automated 3d printing, using AI-based design, managed by AI managers, with hyperspeed transactions carried out amongst other AI run firms in cryptocurrencies. These automated companies cannot be audited since humans do not understand the internals, and produce too many goods and profit for any sort of regulation to be a politically viable policy. After a while, it turns out that the companies were optimizing for things that are not in line with humanity's long-term survival best interest (e.g. maximizing profit). This leads to overconsumption of resources but since the companies are automated they cannot be stopped and humanity dies out.\n\n- **[Flash War](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic)****: Multipolar fast takeoff story by Andrew Critch**\n\nMultiple countries develop technologies to automatically detect and retaliate to threats from foreign powers (e.g., nuclear arsenals, or fleets of lethal autonomous weapons). This could mean that humans are taken completely \"out of the loop\" to ensure response times are as fast as possible. Even if humans are in the loop and are supposed to be the ones to \u201cpull the trigger\u201d, the system could malfunction and relay misinformation of an attack, causing the human to trigger a catastrophic war that kills everyone. This is an agent-agnostic situation where countries robustly annihilate each other regardless of who \u201cpulls the trigger\u201d. Another similar version of this story is called the \u2018[flash economy](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic#Flash_economies)\u2019.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89ZQ", "Related Answers": "", "Doc Last Ingested": "2023-03-16T06:12:30.134+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-8c6784e4efea037fcfac05fd405a491ef7bed755f7274b7d7fde3fbce942d14d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8c6784e4efea037fcfac05fd405a491ef7bed755f7274b7d7fde3fbce942d14d", "name": "Would uploaded humans be aligned?", "index": 453, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-14T23:30:15.962Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8c6784e4efea037fcfac05fd405a491ef7bed755f7274b7d7fde3fbce942d14d", "values": {"File": "Would uploaded humans be aligned?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Would uploaded humans be aligned?", "Link": "https://docs.google.com/document/d/1CFItnW8tr6zfW7rtMxZIQiuo6r79oUrHnXyqQnK6CEQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T17:48:36.017+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-23T05:39:02.485+01:00", "Status": "Not started", "Edit Answer": "Would uploaded humans be aligned?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89ZR", "Source Link": "", "aisafety.info Link": "Would uploaded humans be aligned?", "Source": "", "All Phrasings": "Would uploaded humans be aligned?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89ZR", "Related Answers": "", "Doc Last Ingested": "2023-03-15T00:24:40.586+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 456, "Helpful": ""}}, {"id": "i-3bc9e4c949e38f376be19bbef718ee7bf4ac620f06c07f645312e950e0e6161b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3bc9e4c949e38f376be19bbef718ee7bf4ac620f06c07f645312e950e0e6161b", "name": "What is reinforcement learning (RL)?", "index": 454, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-14T23:30:23.272Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3bc9e4c949e38f376be19bbef718ee7bf4ac620f06c07f645312e950e0e6161b", "values": {"File": "What is reinforcement learning (RL)?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is reinforcement learning (RL)?", "Link": "https://docs.google.com/document/d/1fONikRvX-1iGOKUqT2qzNvyaW2PEt2I6rlt6xrAMG2U/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T17:47:52.377+01:00", "Related Answers DO NOT EDIT": "What is Reinforcement Learning from Human Feedback (RLHF)?,What is inverse reinforcement learning (IRL)?,What is behavioral cloning (BC)?,What is Imitation Learning (IL)?", "Tags": "", "Doc Last Edited": "2023-03-03T15:56:35.526+01:00", "Status": "In progress", "Edit Answer": "What is reinforcement learning (RL)?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89ZS", "Source Link": "", "aisafety.info Link": "What is reinforcement learning (RL)?", "Source": "", "All Phrasings": "What is reinforcement learning (RL)?\n", "Initial Order": "", "Related IDs": "88FN,8AET,8AEQ,8AER", "Rich Text DO NOT EDIT": "\n\n[Reinforcement learning (RL)](https://en.wikipedia.org/wiki/Reinforcement_learning) trains an AI by rewarding behavior we want and punishing behavior we don't want. The AI will repeat behaviors that have helped it get higher rewards in the past more often, and avoid behaviors that give negative or low reward. For example, a recommendation system (such as the one on netflix) sees which recommendations you clicked on in the past and shows you other movies you are likely to click on, based on what people who liked similar things also liked.\n\nRL is one particular type of [machine learning (ML)](https://docs.google.com/document/d/1Ovn9t-am0iwVwXcoazd_MUhj0MGetfdq-wLw2a7bqbk/edit) algorithm, and ML itself is just one (extremely popular) way of implementing artificial intelligence systems (AIs). RL is one of the three basic ML paradigms alongside [supervised learning (SL)](https://en.wikipedia.org/wiki/Supervised_learning) and [unsupervised learning (UL)](https://en.wikipedia.org/wiki/Unsupervised_learning). Some ML algorithms work well when we have a labeled dataset to help train our model. However, we don\u2019t always have labels, and hiring humans to label data is expensive. An alternative approach followed by RL is using the entire environment as a dataset as opposed to a curated or labeled subset of the environment. The model receives observations and rewards for actions that correspond to what we want it to do.\n\nDue to its generality, reinforcement learning is studied in many disciplines, such as[game theory](https://en.wikipedia.org/wiki/Game_theory),[control theory](https://en.wikipedia.org/wiki/Control_theory),[operations research](https://en.wikipedia.org/wiki/Operations_research),[information theory](https://en.wikipedia.org/wiki/Information_theory),[simulation-based optimization](https://en.wikipedia.org/wiki/Simulation-based_optimization),[multi-agent systems](https://en.wikipedia.org/wiki/Multi-agent_system),[swarm intelligence](https://en.wikipedia.org/wiki/Swarm_intelligence), and[statistics](https://en.wikipedia.org/wiki/Statistics).\n\n[^kix.765c758uski4]: https://ai.googleblog.com/2020/04/off-policy-estimation-for-infinite.html", "Tag Count": 0, "Related Answer Count": 4, "Rich Text": "\n\n[Reinforcement learning (RL)](https://en.wikipedia.org/wiki/Reinforcement_learning) trains an AI by rewarding behavior we want and punishing behavior we don't want. The AI will repeat behaviors that have helped it get higher rewards in the past more often, and avoid behaviors that give negative or low reward. For example, a recommendation system (such as the one on netflix) sees which recommendations you clicked on in the past and shows you other movies you are likely to click on, based on what people who liked similar things also liked.\n\nRL is one particular type of [machine learning (ML)](https://docs.google.com/document/d/1Ovn9t-am0iwVwXcoazd_MUhj0MGetfdq-wLw2a7bqbk/edit) algorithm, and ML itself is just one (extremely popular) way of implementing artificial intelligence systems (AIs). RL is one of the three basic ML paradigms alongside [supervised learning (SL)](https://en.wikipedia.org/wiki/Supervised_learning) and [unsupervised learning (UL)](https://en.wikipedia.org/wiki/Unsupervised_learning). Some ML algorithms work well when we have a labeled dataset to help train our model. However, we don\u2019t always have labels, and hiring humans to label data is expensive. An alternative approach followed by RL is using the entire environment as a dataset as opposed to a curated or labeled subset of the environment. The model receives observations and rewards for actions that correspond to what we want it to do.\n\nDue to its generality, reinforcement learning is studied in many disciplines, such as[game theory](https://en.wikipedia.org/wiki/Game_theory),[control theory](https://en.wikipedia.org/wiki/Control_theory),[operations research](https://en.wikipedia.org/wiki/Operations_research),[information theory](https://en.wikipedia.org/wiki/Information_theory),[simulation-based optimization](https://en.wikipedia.org/wiki/Simulation-based_optimization),[multi-agent systems](https://en.wikipedia.org/wiki/Multi-agent_system),[swarm intelligence](https://en.wikipedia.org/wiki/Swarm_intelligence), and[statistics](https://en.wikipedia.org/wiki/Statistics).\n\n[^kix.765c758uski4]: https://ai.googleblog.com/2020/04/off-policy-estimation-for-infinite.html", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89ZS", "Related Answers": "What is Reinforcement Learning from Human Feedback (RLHF)?,What is inverse reinforcement learning (IRL)?,What is behavioral cloning (BC)?,What is Imitation Learning (IL)?", "Doc Last Ingested": "2023-03-15T00:24:44.495+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-e79456204f40e64ec23a497d3299b3f6d5ed9ce42c552f8a4d40a65ced031a70", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e79456204f40e64ec23a497d3299b3f6d5ed9ce42c552f8a4d40a65ced031a70", "name": "How does Machine Learning work?", "index": 455, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-14T22:20:57.546Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e79456204f40e64ec23a497d3299b3f6d5ed9ce42c552f8a4d40a65ced031a70", "values": {"File": "How does Machine Learning work?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How does Machine Learning work?", "Link": "https://docs.google.com/document/d/1Ovn9t-am0iwVwXcoazd_MUhj0MGetfdq-wLw2a7bqbk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T14:43:04.535+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-28T20:32:15.663+01:00", "Status": "In progress", "Edit Answer": "How does Machine Learning work?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89ZT", "Source Link": "", "aisafety.info Link": "How does Machine Learning work?", "Source": "", "All Phrasings": "How does Machine Learning work?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**Intro 1 sentence answer**\n\nMachine learning (ML) allows computers to come up with algorithms on their own for problems where it would be too complex for humans to formally specify a specific sequence of steps. This is done by giving computers free variables/parameters that they can adjust to reflect any patterns/features found in the data. When we combine this with a lot of data, it results in the machine being able to recognize underlying features of the data extremely accurately.\n\n**Slightly more detailed answer**\n\nThe industrial revolution automated tasks requiring physical solutions. The digital revolution automated tasks requiring mental solutions. We are currently living in the AI revolution which automates the finding of solutions itself. This means that now, we only need to formulate the problem and not the solution. One of the ways of automating the finding of solutions to a problem is by using machine learning.\n\nAt the onset of the field of AI in the 1950s, experts focused on representing problems using formal languages (logic, or programming languages). This was known as Symbolic AI. However, Real-world problems are often too complex for humans to be able to describe them using formal logic. This led to the birth of **machine learning** in the 1990s. This alternative method focused on letting the machine discover an algorithm and any underlying features of the data on its own instead of using experts to find hard-coded rules. This was done by giving the model a certain number of variables that it could change freely (free parameters). This allowed the models to learn how to represent the world on their own by manipulating these parameters.\n\nMachine learning models can be categorized by the type of task we want the model to achieve :\n\n**Simple Example**\n\nAn example of this is in machine vision systems. Before machine learning, human experts used to find and hand code features of images that they thought would be important for a machine to consider when trying to recognize an image e.g. horizontal or vertical line detectors. However as time goes on, and the diversity of objects to be recognized increases, relying on experts becomes slow and expensive. So we just let the machine learn which features are important on it\u2019s own, instead of asking experts to pre-identify them.\n\n**Concluding sentence**\n\n**\u2026**\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "**Intro 1 sentence answer**\n\nMachine learning (ML) allows computers to come up with algorithms on their own for problems where it would be too complex for humans to formally specify a specific sequence of steps. This is done by giving computers free variables/parameters that they can adjust to reflect any patterns/features found in the data. When we combine this with a lot of data, it results in the machine being able to recognize underlying features of the data extremely accurately.\n\n**Slightly more detailed answer**\n\nThe industrial revolution automated tasks requiring physical solutions. The digital revolution automated tasks requiring mental solutions. We are currently living in the AI revolution which automates the finding of solutions itself. This means that now, we only need to formulate the problem and not the solution. One of the ways of automating the finding of solutions to a problem is by using machine learning.\n\nAt the onset of the field of AI in the 1950s, experts focused on representing problems using formal languages (logic, or programming languages). This was known as Symbolic AI. However, Real-world problems are often too complex for humans to be able to describe them using formal logic. This led to the birth of **machine learning** in the 1990s. This alternative method focused on letting the machine discover an algorithm and any underlying features of the data on its own instead of using experts to find hard-coded rules. This was done by giving the model a certain number of variables that it could change freely (free parameters). This allowed the models to learn how to represent the world on their own by manipulating these parameters.\n\nMachine learning models can be categorized by the type of task we want the model to achieve :\n\n**Simple Example**\n\nAn example of this is in machine vision systems. Before machine learning, human experts used to find and hand code features of images that they thought would be important for a machine to consider when trying to recognize an image e.g. horizontal or vertical line detectors. However as time goes on, and the diversity of objects to be recognized increases, relying on experts becomes slow and expensive. So we just let the machine learn which features are important on it\u2019s own, instead of asking experts to pre-identify them.\n\n**Concluding sentence**\n\n**\u2026**\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89ZT", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:18:02.078+01:00", "Request Count": "", "Number of suggestions on answer doc": 9, "Total character count of suggestions on answer doc": 1829, "Helpful": ""}}, {"id": "i-6a259cc2089705224356bb1b3066cbb5f1a1e4ab095e91e8867b6160c46d3467", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-6a259cc2089705224356bb1b3066cbb5f1a1e4ab095e91e8867b6160c46d3467", "name": "What are \u2018true names\u2019 in the context of AI alignment?", "index": 456, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-14T22:21:02.651Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-6a259cc2089705224356bb1b3066cbb5f1a1e4ab095e91e8867b6160c46d3467", "values": {"File": "What are \u2018true names\u2019 in the context of AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are \u2018true names\u2019 in the context of AI alignment?", "Link": "https://docs.google.com/document/d/1Z0JWiQHEQ7wUi3OU2_Zn4JGRfHMdEhydHlGY7DkFpPM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T14:16:51.419+01:00", "Related Answers DO NOT EDIT": "What is \"agent foundations\"?,What is Goodhart's law?", "Tags": "", "Doc Last Edited": "2023-03-13T17:51:23.309+01:00", "Status": "In progress", "Edit Answer": "What are \u2018true names\u2019 in the context of AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89ZU", "Source Link": "", "aisafety.info Link": "What are \u2018true names\u2019 in the context of AI alignment?", "Source": "", "All Phrasings": "What are \u2018true names\u2019 in the context of AI alignment?\n", "Initial Order": "", "Related IDs": "7782,8185", "Rich Text DO NOT EDIT": "Currently, we use proxies for human values to measure how well an AI model performs. However, these proxies often fail when faced with optimization pressure. A \u201ctrue name\u201d in the alignment context is one or more human values that do not fail under optimization pressure. This means that if we try to maximize the attainment of those values it does not result in undesired side effects or unforeseen consequences. The term might be inspired from [\u201ctrue names\u201d in popular culture](https://en.wikipedia.org/wiki/True_name), and how knowing these is supposed to grant you power over the thing being talked about.\n\nAs an example, we use the proxy of aesthetics to measure how good food tastes. So food that looks good quite often also tastes good. However, when the proxy becomes the target, we might over-optimize for making food \u2018look good\u2019 vs. actually \u2018be good\u2019. After a certain point making the food look good becomes more important than actually tasting good which results in overall worse-tasting food. The \u201ctrue name\u201d of food would be a dish that does not become nutritionally worse or taste worse when we optimize for the appearance of that same dish. One common example observed in everyday life is fruits and vegetables optimized for supermarkets. These foods are [optimized](https://www.huffpost.com/entry/ugly-fruit-vegetable-food-waste_n_5cb5cc91e4b098b9a2da7160)[for](https://www.theguardian.com/environment/2016/jul/13/us-food-waste-ugly-fruit-vegetables-perfect)[appearance](https://www.independent.co.uk/life-style/food-and-drink/ugly-vegetable-food-waste-fruit-vegetable-a8825311.html) but are often less nutritionally valuable[^kix.atv03lr0140f] than foods that are not in the supermarket, but are directly from farms or gardens. These fruits and vegetables might be duller in color and oddly shaped but tend to have higher nutritional values and better taste.\n\nIn the context of AI alignment, \u201ctrue names\u201d can be thought of as a collection of human values that are not susceptible to failing by [Goodhart's Law](https://en.wikipedia.org/wiki/Goodhart%27s_law) when faced with the immense optimization pressure of a future [superintelligence](https://en.wikipedia.org/wiki/Superintelligence). True names were a concept introduced by alignment researcher John Wentworth. As part of the comment thread underneath the post in which \u2018true names\u2019 were introduced,  the author says that \u201c*robustness to optimization is not the True Name of True Names, but it might be a sufficient condition*\u201d for achieving/finding \u201ctrue names\u201d. So it might be generally acceptable to understand them in this way.\n\nWhile looking for these \u201ctrue names\u201d, alignment researchers often find themselves recursing into lower-level components of agency such as [optimization](https://www.alignmentforum.org/tag/optimization), [goals](https://www.alignmentforum.org/tag/goal-directedness), [world models](https://www.alignmentforum.org/tag/world-modeling), [abstraction](https://www.alignmentforum.org/tag/abstraction), [counterfactuals](https://www.alignmentforum.org/tag/counterfactuals), [embeddedness](https://www.alignmentforum.org/tag/embedded-agency), etc\u2026[^kix.fq41wlqae324]\n\nIt is the goal of some alignment researchers to discover either these \u2018true names\u2019 for human values or a \u201cpointer\u201d to human values \u2013 something from which the \u201cTrue Name\u201d of human values could be automatically generated. They hope that if we find either the \u201ctrue names\u201d by studying the fields listed above, or if we can find a generator function for these \u201ctrue names\u201d it will be a significant step towards solving the alignment problem.\n\n[^kix.atv03lr0140f]: Optimization pressure for appearance over nutrition is a factor, but this is just meant to serve as an illustrative example of optimization pressures and is not meant as an authoritative claim from a botanist/biologist/\u2026The main culprit in certain fruits and vegetables having lesser nutritional value is depletion of the top soil layers from overfarming, resulting in less nutritional dense foods overall.\n[^kix.fq41wlqae324]:Wentworth, John (2022). [Why Agent Foundations? An Overly Abstract Explanation](https://www.alignmentforum.org/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation)", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "Currently, we use proxies for human values to measure how well an AI model performs. However, these proxies often fail when faced with optimization pressure. A \u201ctrue name\u201d in the alignment context is one or more human values that do not fail under optimization pressure. This means that if we try to maximize the attainment of those values it does not result in undesired side effects or unforeseen consequences. The term might be inspired from [\u201ctrue names\u201d in popular culture](https://en.wikipedia.org/wiki/True_name), and how knowing these is supposed to grant you power over the thing being talked about.\n\nAs an example, we use the proxy of aesthetics to measure how good food tastes. So food that looks good quite often also tastes good. However, when the proxy becomes the target, we might over-optimize for making food \u2018look good\u2019 vs. actually \u2018be good\u2019. After a certain point making the food look good becomes more important than actually tasting good which results in overall worse-tasting food. The \u201ctrue name\u201d of food would be a dish that does not become nutritionally worse or taste worse when we optimize for the appearance of that same dish. One common example observed in everyday life is fruits and vegetables optimized for supermarkets. These foods are [optimized](https://www.huffpost.com/entry/ugly-fruit-vegetable-food-waste_n_5cb5cc91e4b098b9a2da7160)[for](https://www.theguardian.com/environment/2016/jul/13/us-food-waste-ugly-fruit-vegetables-perfect)[appearance](https://www.independent.co.uk/life-style/food-and-drink/ugly-vegetable-food-waste-fruit-vegetable-a8825311.html) but are often less nutritionally valuable[^kix.atv03lr0140f] than foods that are not in the supermarket, but are directly from farms or gardens. These fruits and vegetables might be duller in color and oddly shaped but tend to have higher nutritional values and better taste.\n\nIn the context of AI alignment, \u201ctrue names\u201d can be thought of as a collection of human values that are not susceptible to failing by [Goodhart's Law](https://en.wikipedia.org/wiki/Goodhart%27s_law) when faced with the immense optimization pressure of a future [superintelligence](https://en.wikipedia.org/wiki/Superintelligence). True names were a concept introduced by alignment researcher John Wentworth. As part of the comment thread underneath the post in which \u2018true names\u2019 were introduced,  the author says that \u201c*robustness to optimization is not the True Name of True Names, but it might be a sufficient condition*\u201d for achieving/finding \u201ctrue names\u201d. So it might be generally acceptable to understand them in this way.\n\nWhile looking for these \u201ctrue names\u201d, alignment researchers often find themselves recursing into lower-level components of agency such as [optimization](https://www.alignmentforum.org/tag/optimization), [goals](https://www.alignmentforum.org/tag/goal-directedness), [world models](https://www.alignmentforum.org/tag/world-modeling), [abstraction](https://www.alignmentforum.org/tag/abstraction), [counterfactuals](https://www.alignmentforum.org/tag/counterfactuals), [embeddedness](https://www.alignmentforum.org/tag/embedded-agency), etc\u2026[^kix.fq41wlqae324]\n\nIt is the goal of some alignment researchers to discover either these \u2018true names\u2019 for human values or a \u201cpointer\u201d to human values \u2013 something from which the \u201cTrue Name\u201d of human values could be automatically generated. They hope that if we find either the \u201ctrue names\u201d by studying the fields listed above, or if we can find a generator function for these \u201ctrue names\u201d it will be a significant step towards solving the alignment problem.\n\n[^kix.atv03lr0140f]: Optimization pressure for appearance over nutrition is a factor, but this is just meant to serve as an illustrative example of optimization pressures and is not meant as an authoritative claim from a botanist/biologist/\u2026The main culprit in certain fruits and vegetables having lesser nutritional value is depletion of the top soil layers from overfarming, resulting in less nutritional dense foods overall.\n[^kix.fq41wlqae324]:Wentworth, John (2022). [Why Agent Foundations? An Overly Abstract Explanation](https://www.alignmentforum.org/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation)", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89ZU", "Related Answers": "What is \"agent foundations\"?,What is Goodhart's law?", "Doc Last Ingested": "2023-03-14T23:18:04.116+01:00", "Request Count": "", "Number of suggestions on answer doc": 9, "Total character count of suggestions on answer doc": 1829, "Helpful": ""}}, {"id": "i-b2ab4fe18ca49bcd52937f133e01221628dfcc11cbd71dc6f35af072a3faf307", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b2ab4fe18ca49bcd52937f133e01221628dfcc11cbd71dc6f35af072a3faf307", "name": "What\u2019s the probability that we\u2019ll eventually wind up with brain-like AGI?", "index": 457, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-14T22:21:07.667Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b2ab4fe18ca49bcd52937f133e01221628dfcc11cbd71dc6f35af072a3faf307", "values": {"File": "What\u2019s the probability that we\u2019ll eventually wind up with brain-like AGI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What\u2019s the probability that we\u2019ll eventually wind up with brain-like AGI?", "Link": "https://docs.google.com/document/d/1MbYPFjUbvp60-tK_vG1hDPAdKz1cB4Z-P3y1r_xpBqI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T14:14:44.291+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:30.630+01:00", "Status": "Not started", "Edit Answer": "What\u2019s the probability that we\u2019ll eventually wind up with brain-like AGI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89ZV", "Source Link": "", "aisafety.info Link": "What\u2019s the probability that we\u2019ll eventually wind up with brain-like AGI?", "Source": "", "All Phrasings": "What\u2019s the probability that we\u2019ll eventually wind up with brain-like AGI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89ZV", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:18:06.020+01:00", "Request Count": 1, "Number of suggestions on answer doc": 9, "Total character count of suggestions on answer doc": 1829, "Helpful": ""}}, {"id": "i-57eda36ebf5cec0d93e0782457e55fc73d9f907021756db5d2be31c648e9959d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-57eda36ebf5cec0d93e0782457e55fc73d9f907021756db5d2be31c648e9959d", "name": "Why can't we just use a friendly AI to stop bad AIs?", "index": 458, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-14T22:21:12.782Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-57eda36ebf5cec0d93e0782457e55fc73d9f907021756db5d2be31c648e9959d", "values": {"File": "Why can't we just use a friendly AI to stop bad AIs?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why can't we just use a friendly AI to stop bad AIs?", "Link": "https://docs.google.com/document/d/1FnsJK1o6qKiyGEbJt-qm2nFLaAn7Sd8z7m6Zwqanq-Q/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T14:14:31.368+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:31.888+01:00", "Status": "Not started", "Edit Answer": "Why can't we just use a friendly AI to stop bad AIs?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89ZW", "Source Link": "", "aisafety.info Link": "Why can't we just use a friendly AI to stop bad AIs?", "Source": "", "All Phrasings": "Why can't we just use a friendly AI to stop bad AIs?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89ZW", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:18:07.891+01:00", "Request Count": "", "Number of suggestions on answer doc": 10, "Total character count of suggestions on answer doc": 2213, "Helpful": ""}}, {"id": "i-3473f209d19ea40fa9c1f0714652d79267395191e04f27b70a980a4a85d8dc65", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3473f209d19ea40fa9c1f0714652d79267395191e04f27b70a980a4a85d8dc65", "name": "Why can't we just use a friendly AI to stop bad AIs?", "index": 459, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-14T23:47:35.681Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3473f209d19ea40fa9c1f0714652d79267395191e04f27b70a980a4a85d8dc65", "values": {"File": "Why can't we just use a friendly AI to stop bad AIs?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why can't we just use a friendly AI to stop bad AIs?", "Link": "https://docs.google.com/document/d/1KB-9eHEpIIdoANeeXD_LVdQbo5bfxStHldzI1wuytQM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T14:14:29.890+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:33.039+01:00", "Status": "Duplicate", "Edit Answer": "Why can't we just use a friendly AI to stop bad AIs?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89ZX", "Source Link": "", "aisafety.info Link": "Why can't we just use a friendly AI to stop bad AIs?", "Source": "", "All Phrasings": "Why can't we just use a friendly AI to stop bad AIs?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89ZX", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:18:11.358+01:00", "Request Count": "", "Number of suggestions on answer doc": 10, "Total character count of suggestions on answer doc": 2213, "Helpful": ""}}, {"id": "i-8fc7d90479bbdfa73725d9016ef13fd7d16101ced04df81251981d1557c7b7f0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8fc7d90479bbdfa73725d9016ef13fd7d16101ced04df81251981d1557c7b7f0", "name": "What are circuits in mechanistic interpretability?", "index": 460, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-15T14:08:38.577Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8fc7d90479bbdfa73725d9016ef13fd7d16101ced04df81251981d1557c7b7f0", "values": {"File": "What are circuits in mechanistic interpretability?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are circuits in mechanistic interpretability?", "Link": "https://docs.google.com/document/d/1h8CYsKzh769Hp7ikniUieU5bo86I3_nO-5qdXI0qqws/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T14:13:47.058+01:00", "Related Answers DO NOT EDIT": "How much can we learn about AI with interpretability tools?,What is interpretability and what approaches are there?", "Tags": "", "Doc Last Edited": "2023-03-15T12:27:55.397+01:00", "Status": "In progress", "Edit Answer": "What are circuits in mechanistic interpretability?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89ZY", "Source Link": "", "aisafety.info Link": "What are circuits in mechanistic interpretability?", "Source": "", "All Phrasings": "What are circuits in mechanistic interpretability?\n", "Initial Order": "", "Related IDs": "8239,8241", "Rich Text DO NOT EDIT": "**Quick 1-sentence answer**\n\nCircuits are computational subgraphs of a neural network consisting of a set of tightly linked features and the weights between them.\n\n**Slightly more detailed answer**\n\nNeural networks are made up of floating point numbers that represent certain features of the world. Researchers claim that these features can be rigorously studied and understood. Examples of features can be Curve Detectors as well as High-Low Frequency Edge Detectors that are found in every non-trivial vision model. These straddle the boundary between features the community broadly agrees to exist (e.g. edge detectors) and features for which there\u2019s significant skepticism (e.g. high-level features such as ears and faces). *These features are argued to be the fundamental unit of neural networks.*\n\nFollowing from this we can claim that *features are connected by weights, forming* ***circuits***. These circuits can also be rigorously studied and understood. If we can understand the features shouldn\u2019t we also be able to understand the connections between them? So \u201ccircuits\u201d are computational subgraphs of a neural network consisting of a set of tightly linked features and the weights between them. During their study, researchers have found beautiful rich structures in circuits. This means that we can literally read meaningful algorithms off of the weights of a network! Researchers think that studying recurring patterns in complex graphs (circuit motifs) will be important in understanding artificial neural networks.\n\n**Simple Example**\n\nINSERT: feature visualization example from the distill paper\n\n**Concluding sentence**\n\nADD: Why does anyone care about circuits?\n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "**Quick 1-sentence answer**\n\nCircuits are computational subgraphs of a neural network consisting of a set of tightly linked features and the weights between them.\n\n**Slightly more detailed answer**\n\nNeural networks are made up of floating point numbers that represent certain features of the world. Researchers claim that these features can be rigorously studied and understood. Examples of features can be Curve Detectors as well as High-Low Frequency Edge Detectors that are found in every non-trivial vision model. These straddle the boundary between features the community broadly agrees to exist (e.g. edge detectors) and features for which there\u2019s significant skepticism (e.g. high-level features such as ears and faces). *These features are argued to be the fundamental unit of neural networks.*\n\nFollowing from this we can claim that *features are connected by weights, forming* ***circuits***. These circuits can also be rigorously studied and understood. If we can understand the features shouldn\u2019t we also be able to understand the connections between them? So \u201ccircuits\u201d are computational subgraphs of a neural network consisting of a set of tightly linked features and the weights between them. During their study, researchers have found beautiful rich structures in circuits. This means that we can literally read meaningful algorithms off of the weights of a network! Researchers think that studying recurring patterns in complex graphs (circuit motifs) will be important in understanding artificial neural networks.\n\n**Simple Example**\n\nINSERT: feature visualization example from the distill paper\n\n**Concluding sentence**\n\nADD: Why does anyone care about circuits?\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89ZY", "Related Answers": "How much can we learn about AI with interpretability tools?,What is interpretability and what approaches are there?", "Doc Last Ingested": "2023-03-14T23:18:12.988+01:00", "Request Count": "", "Number of suggestions on answer doc": 10, "Total character count of suggestions on answer doc": 2213, "Helpful": ""}}, {"id": "i-bb5dd8fd765f3d0cb327fa93fb3cbb2d08e481472b87cbbfaba3eae960e65020", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-bb5dd8fd765f3d0cb327fa93fb3cbb2d08e481472b87cbbfaba3eae960e65020", "name": "What is \"emergence\"?", "index": 461, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-14T22:25:42.486Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-bb5dd8fd765f3d0cb327fa93fb3cbb2d08e481472b87cbbfaba3eae960e65020", "values": {"File": "What is \"emergence\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"emergence\"?", "Link": "https://docs.google.com/document/d/1KWS2BkdEE7y1395EKaQM-5OwlFkTpROpfLksCsueQ3s/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T14:13:41.003+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-14T10:18:04.813+01:00", "Status": "In progress", "Edit Answer": "What is \"emergence\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89ZZ", "Source Link": "", "aisafety.info Link": "What is \"emergence\"?", "Source": "", "All Phrasings": "What is \"emergence\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**Quick 1-sentence answer**\n\nEmergent Behavior or [emergence](https://en.wikipedia.org/wiki/Emergence) occurs when an entity or system is observed to have certain properties , that its individual parts do not have on their own. Certain properties or behaviors might come about only when the parts that make up that entity interact as a wider whole , or when the number of parts making up that entity reach a certain minimum threshold.\n\n**Slightly more detailed answer**\n\nIt is hypothesized that an AI system might display emergent properties simply as a function of scale. This is supported by the fact that various complex systems exist in nature that have displayed such emergent properties due to quantitative differences. Examples include :\n\n- Heart : Individually cells cannot pump bood whereas the whole heart can.\n\n- Uranium : Small amounts, nothing special, large amounts = nuclear reactions\n\n- Civilization : Individuals, nothing special, collective specialization = human civilization\n\nThis raises concerns that due to the quantitative increases in the size of AI models , we might soon hit an unexpected threshold and see unexpected differences in qualitiative behaviors and capabilites. This provides motivations to study [complex systems](https://en.wikipedia.org/wiki/Complex_system) with emergent phenomena to be able to draw inferences for when , and what types of capabilites might emerge.\n\n**Simple Example**\n\nThe coding abilities of LLMs.\n\n**Concluding sentence**\n\nshould we be scared ? how do we possibly forecast emergence?\n\n## \n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "**Quick 1-sentence answer**\n\nEmergent Behavior or [emergence](https://en.wikipedia.org/wiki/Emergence) occurs when an entity or system is observed to have certain properties , that its individual parts do not have on their own. Certain properties or behaviors might come about only when the parts that make up that entity interact as a wider whole , or when the number of parts making up that entity reach a certain minimum threshold.\n\n**Slightly more detailed answer**\n\nIt is hypothesized that an AI system might display emergent properties simply as a function of scale. This is supported by the fact that various complex systems exist in nature that have displayed such emergent properties due to quantitative differences. Examples include :\n\n- Heart : Individually cells cannot pump bood whereas the whole heart can.\n\n- Uranium : Small amounts, nothing special, large amounts = nuclear reactions\n\n- Civilization : Individuals, nothing special, collective specialization = human civilization\n\nThis raises concerns that due to the quantitative increases in the size of AI models , we might soon hit an unexpected threshold and see unexpected differences in qualitiative behaviors and capabilites. This provides motivations to study [complex systems](https://en.wikipedia.org/wiki/Complex_system) with emergent phenomena to be able to draw inferences for when , and what types of capabilites might emerge.\n\n**Simple Example**\n\nThe coding abilities of LLMs.\n\n**Concluding sentence**\n\nshould we be scared ? how do we possibly forecast emergence?\n\n## \n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89ZZ", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:21:56.531+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-55b93ce82ab5fa2da9c2ac6dfb2f7508cbd8be33cb91d4e3fb90ad0e7f13005b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-55b93ce82ab5fa2da9c2ac6dfb2f7508cbd8be33cb91d4e3fb90ad0e7f13005b", "name": "What exactly is \u201cAGI\u201d?", "index": 462, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-14T22:25:45.008Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-55b93ce82ab5fa2da9c2ac6dfb2f7508cbd8be33cb91d4e3fb90ad0e7f13005b", "values": {"File": "What exactly is \u201cAGI\u201d?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What exactly is \u201cAGI\u201d?", "Link": "https://docs.google.com/document/d/12kL5Un2EDBzM6pULluC17NS8d_w4YcqdkQmJ5vqNcoQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-20T14:13:34.681+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-08T18:29:15.020+01:00", "Status": "Not started", "Edit Answer": "What exactly is \u201cAGI\u201d?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8A00", "Source Link": "", "aisafety.info Link": "What exactly is \u201cAGI\u201d?", "Source": "", "All Phrasings": "What exactly is \u201cAGI\u201d?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8A00", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:21:58.363+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-5caa4b60ec7902618c81c400736ff42a017bff4017e3a2d092deb3f910bf378f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5caa4b60ec7902618c81c400736ff42a017bff4017e3a2d092deb3f910bf378f", "name": "Might societal collapse occur before AI becomes a threat?", "index": 463, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-14T22:25:47.467Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5caa4b60ec7902618c81c400736ff42a017bff4017e3a2d092deb3f910bf378f", "values": {"File": "Might societal collapse occur before AI becomes a threat?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Might societal collapse occur before AI becomes a threat?", "Link": "https://docs.google.com/document/d/1GDIJLkIbbilTxWhWfqFJN5b0k8jUqJXpUnqZumTG_zg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-19T22:21:56.497+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:37.722+01:00", "Status": "Not started", "Edit Answer": "Might societal collapse occur before AI becomes a threat?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8A01", "Source Link": "", "aisafety.info Link": "Might societal collapse occur before AI becomes a threat?", "Source": "", "All Phrasings": "Might societal collapse occur before AI becomes a threat?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8A01", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:21:59.820+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-3f96f32f85e090a6eff6f2965d40ee6d71a34b0eb94eed79a39dd675fba41b7c", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3f96f32f85e090a6eff6f2965d40ee6d71a34b0eb94eed79a39dd675fba41b7c", "name": "What is jargon is used in AI safety?", "index": 464, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-02-21T16:50:59.522Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3f96f32f85e090a6eff6f2965d40ee6d71a34b0eb94eed79a39dd675fba41b7c", "values": {"File": "What is jargon is used in AI safety?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is jargon is used in AI safety?", "Link": "https://docs.google.com/document/d/1EysS0kdrVyb8M45sdpvsMnYuhwWqLO5ZtmOi7HQM7rM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-19T22:18:52.876+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-19T22:18:53.698+01:00", "Status": "Not started", "Edit Answer": "What is jargon is used in AI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "-1", "Source Link": "", "aisafety.info Link": "What is jargon is used in AI safety?", "Source": "", "All Phrasings": "What is jargon is used in AI safety?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "-1", "Related Answers": "", "Doc Last Ingested": "", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-ab1770ce85efa301653dfaf71bf2de3bdb43f52d4bd8c18c090b71509ee76b46", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ab1770ce85efa301653dfaf71bf2de3bdb43f52d4bd8c18c090b71509ee76b46", "name": "What jargon is used in AI safety?", "index": 465, "createdAt": "2023-02-20T16:51:33.304Z", "updatedAt": "2023-03-14T22:25:51.490Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ab1770ce85efa301653dfaf71bf2de3bdb43f52d4bd8c18c090b71509ee76b46", "values": {"File": "What jargon is used in AI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What jargon is used in AI safety?", "Link": "https://docs.google.com/document/d/13hcO9yAJDWo5H-s_Sqia3LziOdCFtM4xF_570drK0OE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-19T22:18:51.223+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:38.866+01:00", "Status": "Not started", "Edit Answer": "What jargon is used in AI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8A03", "Source Link": "", "aisafety.info Link": "What jargon is used in AI safety?", "Source": "", "All Phrasings": "What jargon is used in AI safety?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8A03", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:02.373+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-1f62927e5743dd220026a23ff9b580edfacda0e0f2a993be860531bcc9ed3922", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1f62927e5743dd220026a23ff9b580edfacda0e0f2a993be860531bcc9ed3922", "name": "Can AIs think and feel?", "index": 447, "createdAt": "2023-02-19T21:06:11.196Z", "updatedAt": "2023-03-14T22:26:01.508Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1f62927e5743dd220026a23ff9b580edfacda0e0f2a993be860531bcc9ed3922", "values": {"File": "Can AIs think and feel?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Can AIs think and feel?", "Link": "https://docs.google.com/document/d/11-66r4Jv-didBITuW3EjdD6jjx7pNJWUtfAy4vukYZ8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-19T21:58:35.181+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:40.073+01:00", "Status": "Not started", "Edit Answer": "Can AIs think and feel?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89LI", "Source Link": "", "aisafety.info Link": "Can AIs think and feel?", "Source": "", "All Phrasings": "Can AIs think and feel?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89LI", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:04.020+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-99e1310b4e87c5e9dcdcd9958e3b7973193d644c987f09583797ecbe448041ef", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-99e1310b4e87c5e9dcdcd9958e3b7973193d644c987f09583797ecbe448041ef", "name": "How likely is it that GPT-4 will be dangerously misaligned?", "index": 448, "createdAt": "2023-02-19T21:06:11.196Z", "updatedAt": "2023-03-14T22:26:13.038Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-99e1310b4e87c5e9dcdcd9958e3b7973193d644c987f09583797ecbe448041ef", "values": {"File": "How likely is it that GPT-4 will be dangerously misaligned?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How likely is it that GPT-4 will be dangerously misaligned?", "Link": "https://docs.google.com/document/d/1R0DgINQMIsLO4ZJw-JzxhZUPCSjJEGIsVg8XprAlvVU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-19T21:58:46.970+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:41.262+01:00", "Status": "Not started", "Edit Answer": "How likely is it that GPT-4 will be dangerously misaligned?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89LJ", "Source Link": "", "aisafety.info Link": "How likely is it that GPT-4 will be dangerously misaligned?", "Source": "", "All Phrasings": "How likely is it that GPT-4 will be dangerously misaligned?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89LJ", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:05.629+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-b8c43ce6423656a107a0b8c640ea0a33f6f4f2486ef99e2efc628447106249b0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b8c43ce6423656a107a0b8c640ea0a33f6f4f2486ef99e2efc628447106249b0", "name": "How might interpretability be helpful?", "index": 449, "createdAt": "2023-02-19T21:06:11.196Z", "updatedAt": "2023-03-14T22:26:19.640Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b8c43ce6423656a107a0b8c640ea0a33f6f4f2486ef99e2efc628447106249b0", "values": {"File": "How might interpretability be helpful?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might interpretability be helpful?", "Link": "https://docs.google.com/document/d/1dLxXPwrawz98ycwSA1DSNmPTQZWw5mitFKKBR4Sm9Ek/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-19T21:58:23.233+01:00", "Related Answers DO NOT EDIT": "What is interpretability and what approaches are there?,How can we interpret what all the neurons mean?,Where can I learn about interpretability?,How does Machine Learning work?", "Tags": "", "Doc Last Edited": "2023-03-03T15:59:48.110+01:00", "Status": "Live on site", "Edit Answer": "How might interpretability be helpful?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89LK", "Source Link": "", "aisafety.info Link": "How might interpretability be helpful?", "Source": "", "All Phrasings": "How might interpretability be helpful?\n", "Initial Order": "", "Related IDs": "8241,8322,6822,89ZT", "Rich Text DO NOT EDIT": "Interpretability can help us better understand black-box machine learning (ML) systems and hopefully make them transparent enough so that we can prevent problems such as [goal misgeneralization](https://www.lesswrong.com/posts/Cfe2LMmQC4hHTDZ8r/more-examples-of-goal-misgeneralization), [inner misalignment](https://www.lesswrong.com/tag/inner-alignment), and [deceptive alignment](https://www.lesswrong.com/tag/deceptive-alignment).\n\nCentral to current ML models are [massive, inscrutable matrices filled with numbers](https://www.youtube.com/watch?v=aircAruvnKk&t=0s). Interpretability research can help us make sense of these massive matrices and make the underlying algorithm encoded by these numbers human-understandable. This leads to many of the following advantages:\n\n- **Preventing deception:** We have observed that [good performance on a task might be merely correlated](https://arxiv.org/abs/2105.14111) with what the model/algorithm is actually 'trying to do'. If we have better interpretability tools, we can detect intentional or unintentional deception in these systems and only deploy them if we\u2019re confident that they are safe.\n\n- **Force-multiplier for alignment research:** Currently improving and aligning ML systems is a trial-and-error process. However, with interpretability, we can scientifically analyze a model and see *why*it gives misaligned answers. This allows for an empirical approach to alignment work. Ideally, we can integrate interpretability directly into the loss function, and the training loop ensures that any resulting system stays aligned.\n\n- **Predicting capabilities:** Interpretability also enables a better understanding of the underlying principles of how ML systems work, and how they change with scale and enables the creation of scientific laws around these systems.\n\n- **Trust and cooperation:** If people can understand each other's systems, it becomes easier to trust and therefore cooperate with them.\n\n- **Enabling regulation:** Interpretability gives us a metric that regulators and policymakers can use to create regulations around how aligned systems must be before they can be deployed.\n\nAn easy example to illustrate the utility of interpretability involves [a maze-solving agent](https://www.youtube.com/watch?v=bJLcIBixGj8)[^kix.68rcqrdpvhli]. This agent is trained to navigate a maze, and during training, the agent might consistently go toward the exit. We might then assume that the underlying learned algorithm is \u2018trying to go to the exit\u2019. In deployment, the agent goes towards any green-colored object, rather than going toward the exit. This is because what the agent was actually 'trying to do' was to go towards a green-colored exit sign which just happened to correspond to where we wanted it to go. If we had interpretability tools, we would have been able to observe the underlying motivations of this agent and prevent this type of goal misgeneralization. This trivial example illustrates that 'appearing aligned' and being 'verifiably aligned' are two different things. Interpretability strives to create verifiability in alignment.\n\nInterpretability can help us audit and detect any problematic behavior in our models. However, it is important to also keep in mind that an increased understanding of ML systems can lead to accelerating both research and adoption. This makes these systems more generally capable and shortens timelines, so should be focused on applications that have clear alignment applications. That being said, overall it is still an important and promising avenue of research that provides researchers and engineers with an important set of tools that will be helpful in creating aligned AI systems.\n\n[^kix.68rcqrdpvhli]: This example is illustrated with even more detail in the linked Rob Miles Video", "Tag Count": 0, "Related Answer Count": 4, "Rich Text": "Interpretability can help us better understand black-box machine learning (ML) systems and hopefully make them transparent enough so that we can prevent problems such as [goal misgeneralization](https://www.lesswrong.com/posts/Cfe2LMmQC4hHTDZ8r/more-examples-of-goal-misgeneralization), [inner misalignment](https://www.lesswrong.com/tag/inner-alignment), and [deceptive alignment](https://www.lesswrong.com/tag/deceptive-alignment).\n\nCentral to current ML models are [massive, inscrutable matrices filled with numbers](https://www.youtube.com/watch?v=aircAruvnKk&t=0s). Interpretability research can help us make sense of these massive matrices and make the underlying algorithm encoded by these numbers human-understandable. This leads to many of the following advantages:\n\n- **Preventing deception:** We have observed that [good performance on a task might be merely correlated](https://arxiv.org/abs/2105.14111) with what the model/algorithm is actually 'trying to do'. If we have better interpretability tools, we can detect intentional or unintentional deception in these systems and only deploy them if we\u2019re confident that they are safe.\n\n- **Force-multiplier for alignment research:** Currently improving and aligning ML systems is a trial-and-error process. However, with interpretability, we can scientifically analyze a model and see *why*it gives misaligned answers. This allows for an empirical approach to alignment work. Ideally, we can integrate interpretability directly into the loss function, and the training loop ensures that any resulting system stays aligned.\n\n- **Predicting capabilities:** Interpretability also enables a better understanding of the underlying principles of how ML systems work, and how they change with scale and enables the creation of scientific laws around these systems.\n\n- **Trust and cooperation:** If people can understand each other's systems, it becomes easier to trust and therefore cooperate with them.\n\n- **Enabling regulation:** Interpretability gives us a metric that regulators and policymakers can use to create regulations around how aligned systems must be before they can be deployed.\n\nAn easy example to illustrate the utility of interpretability involves [a maze-solving agent](https://www.youtube.com/watch?v=bJLcIBixGj8)[^kix.68rcqrdpvhli]. This agent is trained to navigate a maze, and during training, the agent might consistently go toward the exit. We might then assume that the underlying learned algorithm is \u2018trying to go to the exit\u2019. In deployment, the agent goes towards any green-colored object, rather than going toward the exit. This is because what the agent was actually 'trying to do' was to go towards a green-colored exit sign which just happened to correspond to where we wanted it to go. If we had interpretability tools, we would have been able to observe the underlying motivations of this agent and prevent this type of goal misgeneralization. This trivial example illustrates that 'appearing aligned' and being 'verifiably aligned' are two different things. Interpretability strives to create verifiability in alignment.\n\nInterpretability can help us audit and detect any problematic behavior in our models. However, it is important to also keep in mind that an increased understanding of ML systems can lead to accelerating both research and adoption. This makes these systems more generally capable and shortens timelines, so should be focused on applications that have clear alignment applications. That being said, overall it is still an important and promising avenue of research that provides researchers and engineers with an important set of tools that will be helpful in creating aligned AI systems.\n\n[^kix.68rcqrdpvhli]: This example is illustrated with even more detail in the linked Rob Miles Video", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89LK", "Related Answers": "What is interpretability and what approaches are there?,How can we interpret what all the neurons mean?,Where can I learn about interpretability?,How does Machine Learning work?", "Doc Last Ingested": "2023-03-14T23:22:07.403+01:00", "Request Count": 1, "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-22314354f6f0701b8a1c90cd3820eedf27147be08468c80c667469b98cdf4096", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-22314354f6f0701b8a1c90cd3820eedf27147be08468c80c667469b98cdf4096", "name": "What is existential risk?", "index": 450, "createdAt": "2023-02-19T21:06:11.196Z", "updatedAt": "2023-03-14T22:26:29.590Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-22314354f6f0701b8a1c90cd3820eedf27147be08468c80c667469b98cdf4096", "values": {"File": "What is existential risk?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is existential risk?", "Link": "https://docs.google.com/document/d/1WDD0PYPhkZ2p10cNnPV544YFk69GgZrJtcVZC69kd2A/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-19T21:57:08.065+01:00", "Related Answers DO NOT EDIT": "What is Artificial General Intelligence (AGI) and what will it look like?,What are the main sources of AI existential risk?,How doomed is humanity?", "Tags": "Existential Risk,S-risk", "Doc Last Edited": "2023-02-28T12:06:26.689+01:00", "Status": "In progress", "Edit Answer": "What is existential risk?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89LL", "Source Link": "", "aisafety.info Link": "What is existential risk?", "Source": "", "All Phrasings": "What is existential risk?\n", "Initial Order": "", "Related IDs": "2374,8503,6275", "Rich Text DO NOT EDIT": "[Existential risks](https://futureoflife.org/existential-risk/existential-risk/) are defined as \"risks that threaten the destruction of humanity's long-term potential\". The instantiation of an existential risk (an existential catastrophe) would either cause outright human extinction or irreversibly lock in a drastically inferior state of affairs such as a global autocratic rule with no possibility of evolution or revolution. The most drastic forms would be things that kill literally everyone, but \u201conly\u201d killing 99% of humanity would also count as an existential risk if the survivors weren\u2019t able to rebuild to our current level of advancement.\n\nUntil recently, existential risks had natural causes, e.g. [impact events](https://en.wikipedia.org/wiki/Impact_event) or[supervolcanos](https://en.wikipedia.org/wiki/Supervolcano). There have been at least 5 [mass extinction events](https://en.wikipedia.org/wiki/Extinction_event) in the history of the earth, during which a large percentage of overall species went extinct. While very unlikely to happen at any given moment, natural existential risks exist and it\u2019s a good idea to come up with ways to avoid them, the most obvious being moving into space.\n\nTechnological advances have resulted in a lot of new existential risks appearing, like [nuclear war](https://futureoflife.org/nuclear/the-risk-of-nuclear-weapons/) or [biotechnology](https://futureoflife.org/biotech/benefits-risks-biotechnology/), which are also a lot more likely to happen than natural existential risks. [Some argue](https://oecd.ai/en/wonk/existential-threat) that the greatest risk is from powerful AI systems - AI is an existential threat because it is unlikely that humans will be able to control an AGI (Artificial General Intelligence) once it appears. The AI may (even just as a side effect) wipe out humanity or lock humans into a perpetual dystopia.\n\n", "Tag Count": 2, "Related Answer Count": 3, "Rich Text": "[Existential risks](https://futureoflife.org/existential-risk/existential-risk/) are defined as \"risks that threaten the destruction of humanity's long-term potential\". The instantiation of an existential risk (an existential catastrophe) would either cause outright human extinction or irreversibly lock in a drastically inferior state of affairs such as a global autocratic rule with no possibility of evolution or revolution. The most drastic forms would be things that kill literally everyone, but \u201conly\u201d killing 99% of humanity would also count as an existential risk if the survivors weren\u2019t able to rebuild to our current level of advancement.\n\nUntil recently, existential risks had natural causes, e.g. [impact events](https://en.wikipedia.org/wiki/Impact_event) or[supervolcanos](https://en.wikipedia.org/wiki/Supervolcano). There have been at least 5 [mass extinction events](https://en.wikipedia.org/wiki/Extinction_event) in the history of the earth, during which a large percentage of overall species went extinct. While very unlikely to happen at any given moment, natural existential risks exist and it\u2019s a good idea to come up with ways to avoid them, the most obvious being moving into space.\n\nTechnological advances have resulted in a lot of new existential risks appearing, like [nuclear war](https://futureoflife.org/nuclear/the-risk-of-nuclear-weapons/) or [biotechnology](https://futureoflife.org/biotech/benefits-risks-biotechnology/), which are also a lot more likely to happen than natural existential risks. [Some argue](https://oecd.ai/en/wonk/existential-threat) that the greatest risk is from powerful AI systems - AI is an existential threat because it is unlikely that humans will be able to control an AGI (Artificial General Intelligence) once it appears. The AI may (even just as a side effect) wipe out humanity or lock humans into a perpetual dystopia.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89LL", "Related Answers": "What is Artificial General Intelligence (AGI) and what will it look like?,What are the main sources of AI existential risk?,How doomed is humanity?", "Doc Last Ingested": "2023-03-14T23:22:09.058+01:00", "Request Count": "", "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 3, "Helpful": ""}}, {"id": "i-7af3441c776e513160ce37e749605ce31ce983f8a49153e7ab590e089e3eddef", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7af3441c776e513160ce37e749605ce31ce983f8a49153e7ab590e089e3eddef", "name": "What does prosaic alignment mean?", "index": 451, "createdAt": "2023-02-19T21:06:11.196Z", "updatedAt": "2023-03-15T22:07:58.503Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7af3441c776e513160ce37e749605ce31ce983f8a49153e7ab590e089e3eddef", "values": {"File": "What does prosaic alignment mean?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What does prosaic alignment mean?", "Link": "https://docs.google.com/document/d/1DTD01Fcrs9JocRLlFygkjMD2Zr4qRa5awODq7V2Wo3s/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-19T21:26:13.522+01:00", "Related Answers DO NOT EDIT": "What are \"scaling laws\" and how are they relevant to safety?,Can we get AGI by scaling up architectures similar to current ones, or are we missing key insights?", "Tags": "", "Doc Last Edited": "2023-03-15T20:00:08.768+01:00", "Status": "In progress", "Edit Answer": "What does prosaic alignment mean?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "89LM", "Source Link": "", "aisafety.info Link": "What does prosaic alignment mean?", "Source": "", "All Phrasings": "What does prosaic alignment mean?\n", "Initial Order": "", "Related IDs": "7750,7727", "Rich Text DO NOT EDIT": "[Prosaic AI alignment](https://www.alignmentforum.org/posts/YTq4X6inEudiHkHDF/prosaic-ai-alignment) is an approach to researching alignment which focuses on systems which only use techniques like those available today (but in more powerful ways). Instead of researching how to align speculative future systems, whose design would need fundamental breakthroughs in understanding intelligence, this approach asks how we would align a superintelligent system that was developed through scaling up the methods we have today without any significant conceptual breakthroughs. Specifically, it focuses on AI trained through [Reinforcement learning](https://docs.google.com/document/d/1fONikRvX-1iGOKUqT2qzNvyaW2PEt2I6rlt6xrAMG2U/edit),\n\nOne advantage of this approach is that research can be more concrete and proposed alignment techniques can be tested, at least in toy versions.\n\n[Some examples](https://www.lesswrong.com/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment)of prosaic alignment researchare: [debate](https://openai.com/blog/debate/),[imitating humans](https://intelligence.org/2017/02/28/using-machine-learning/#problem-3),[preference learning](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/), and [iterated distillation and amplification](https://docs.google.com/document/d/1t4myVXhvGa3cj4wFtHVZ2Xp9C1ORdHSP0RasUmEYfvY/edit).\n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "[Prosaic AI alignment](https://www.alignmentforum.org/posts/YTq4X6inEudiHkHDF/prosaic-ai-alignment) is an approach to researching alignment which focuses on systems which only use techniques like those available today (but in more powerful ways). Instead of researching how to align speculative future systems, whose design would need fundamental breakthroughs in understanding intelligence, this approach asks how we would align a superintelligent system that was developed through scaling up the methods we have today without any significant conceptual breakthroughs. Specifically, it focuses on AI trained through [Reinforcement learning](https://docs.google.com/document/d/1fONikRvX-1iGOKUqT2qzNvyaW2PEt2I6rlt6xrAMG2U/edit),\n\nOne advantage of this approach is that research can be more concrete and proposed alignment techniques can be tested, at least in toy versions.\n\n[Some examples](https://www.lesswrong.com/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment)of prosaic alignment researchare: [debate](https://openai.com/blog/debate/),[imitating humans](https://intelligence.org/2017/02/28/using-machine-learning/#problem-3),[preference learning](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/), and [iterated distillation and amplification](https://docs.google.com/document/d/1t4myVXhvGa3cj4wFtHVZ2Xp9C1ORdHSP0RasUmEYfvY/edit).\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "89LM", "Related Answers": "What are \"scaling laws\" and how are they relevant to safety?,Can we get AGI by scaling up architectures similar to current ones, or are we missing key insights?", "Doc Last Ingested": "2023-03-15T20:12:04.128+01:00", "Request Count": 1, "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 89, "Helpful": ""}}, {"id": "i-cfb95fe87489af6574cd11e754967c9966e0f8923da727b2b35a4341fa772caf", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-cfb95fe87489af6574cd11e754967c9966e0f8923da727b2b35a4341fa772caf", "name": "What is instrumental convergence?", "index": 445, "createdAt": "2023-02-19T16:48:45.533Z", "updatedAt": "2023-03-14T22:26:40.334Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-cfb95fe87489af6574cd11e754967c9966e0f8923da727b2b35a4341fa772caf", "values": {"File": "What is instrumental convergence?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is instrumental convergence?", "Link": "https://docs.google.com/document/d/12cwXk6OcQMGAaxwsFKt0twpBlFlqHHoj2FV01PCyYVw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-19T11:49:02.239+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-14T10:20:49.586+01:00", "Status": "Not started", "Edit Answer": "What is instrumental convergence?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "897I", "Source Link": "", "aisafety.info Link": "What is instrumental convergence?", "Source": "", "All Phrasings": "What is instrumental convergence?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*[https://www.lesswrong.com/tag/instrumental-convergence](https://www.lesswrong.com/tag/instrumental-convergence)*\n\n- \n\n- \n\n- \n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*[https://www.lesswrong.com/tag/instrumental-convergence](https://www.lesswrong.com/tag/instrumental-convergence)*\n\n- \n\n- \n\n- \n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "897I", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:17.396+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 1373, "Helpful": ""}}, {"id": "i-f23ed77e557fd0348924b666568577c8efe55f4cf7cb5babf1166ec55958a361", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-f23ed77e557fd0348924b666568577c8efe55f4cf7cb5babf1166ec55958a361", "name": "What is Iterated Distillation and Amplification (IDA)?", "index": 446, "createdAt": "2023-02-19T16:48:45.533Z", "updatedAt": "2023-03-14T22:26:42.685Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-f23ed77e557fd0348924b666568577c8efe55f4cf7cb5babf1166ec55958a361", "values": {"File": "What is Iterated Distillation and Amplification (IDA)?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Iterated Distillation and Amplification (IDA)?", "Link": "https://docs.google.com/document/d/1t4myVXhvGa3cj4wFtHVZ2Xp9C1ORdHSP0RasUmEYfvY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-19T10:44:17.651+01:00", "Related Answers DO NOT EDIT": "What is Reinforcement Learning from Human Feedback (RLHF)?,What is inverse reinforcement learning (IRL)?,What is \"HCH\"?", "Tags": "", "Doc Last Edited": "2023-03-07T13:51:40.316+01:00", "Status": "Live on site", "Edit Answer": "What is Iterated Distillation and Amplification (IDA)?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "897J", "Source Link": "", "aisafety.info Link": "What is Iterated Distillation and Amplification (IDA)?", "Source": "", "All Phrasings": "What is Iterated Distillation and Amplification (IDA)?\n", "Initial Order": "", "Related IDs": "88FN,8AET,7810", "Rich Text DO NOT EDIT": "[Iterated distillation and amplification (IDA)](https://www.alignmentforum.org/tag/iterated-amplification) is an alignment approach where we build powerful, artificial intelligence (AI) systems through a process of initially building weak AI systems, and recursively using each new AI to teach a slightly smarter AI.\n\nTraining an AI system using machine learning (ML) techniques requires some kind of evaluation of its performance. This can be performed algorithmically if we have access to ground truth labels. Unfortunately, most useful tasks do not come with such labels and cannot be algorithmically evaluated. We can attempt to evaluate our models through reward functions in standard [reinforcement learning (RL)](https://docs.google.com/document/d/1fONikRvX-1iGOKUqT2qzNvyaW2PEt2I6rlt6xrAMG2U/edit#) or by giving them feedback through methods such as [reinforcement learning from human feedback (RLHF)](https://docs.google.com/document/d/1RAU4w38uh9WdDscy9H__Y3WEeOZhXJjrAjnJJcO-Ct8/edit)  and [inverse reinforcement learning (IRL)](https://docs.google.com/document/d/1DVqzpQHAlFxXu5mg5nQcGxxHEWAmr752dHfrXgb-d6M/edit).\n\nHowever, methods involving human-defined reward functions or human feedback are limited by human abilities. This suggests that AI systems can only be taught to perform tasks in an aligned manner that at best matches human-level performance. This performance can be improved by using easier-to-specify proxies, but such proxies could lead to [misaligned behavior](https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity). To solve this problem, the proposal of iterative amplification involves decomposing complex tasks (with [AI assistance](https://aisafety.info/?state=7791_7595-2222-6974-87O6-7763-7757-7491-)) into simpler tasks for which we already have human or algorithmic training signals.\n\n[See more...]\n\n**The Process**\n\n![](https://lh3.googleusercontent.com/PDHGtE4eqelDRFFc3_R8WYlSsFv2Sj216uMXKnolMcFU1SxmyrIpsCVA2JdLlcwYa6CQWuCZFeL772VwJEjwDe83Le_NeiOGG1D3mHWqgtCNC0fKZSpHShNtav38-U0rcJFQcU1b549irGG_aW8iXZxVwanSCLpp)\n[^kix.41tnc0i2r37i]\n\nThe steps involved in the process of training an agent through IDA are roughly as follows:\n\n- **Training**: We want to train an agent to perform some tasks in an aligned manner. We use experts to teach the agent to perform the task through a combination of feedback (RLHF/IRL) and demonstration (Behavioral Cloning/Imitation). We want the AI to not be aligned with the specific values (or biases) of any single expert. This is why we use groups of evaluators. Additionally, using groups of expert demonstrators ensures that the agent will be able to overcome the performance limitations of any single expert. If the overall task is too complex it can also be broken down into smaller sub-tasks to be evaluated by multiple groups (human or AI) that are experts in the domain of that specific sub-task. Overall, through the training process, these groups of experts are able to instill in the agent both capabilities to perform the task while also keeping it aligned.\n\n- **Distillation**: After the training process is complete, this new single trained agent is now better than all the experts that trained it. In other words, all their expertise has been distilled down into one agent. Let's call it the tier-1 agent. This is the **distillation** step. In the first step of IDA, the expert teachers would be humans (you can think of us as tier-0 agents).\n\n- **Amplification**: Now we want to train an aligned tier-2 agent to perform some task that we (humans) can neither specify nor evaluate. Instead, we use the tier-1 agent created in the previous step, who is now better than all groups of human experts. We create multiple copies of the tier-1 agent using techniques such as imitation learning. This creates a new group of tier-1 experts and demonstrators, each of which has higher capabilities than the initial humans that trained it. This is the **amplification step**. This now means that even if the tier-2 task was beyond the capabilities of a single tier-1 agent, we now can decompose the tier-2 task into sub-tasks that are individually solvable/evaluateable by the tier-1 experts.\n\n- **Iteration**: Distilling down the knowledge of the tier-1 group of experts will help us create a tier-2 agent. We can now repeat the amplification and distillation process. This allows us to train yet another tier of agent, which will in turn be better than the group of tier-2 agents that trained it while remaining aligned. We can repeat this as needed. This is the iteration step.\n\nThus, iterated distillation and amplification, or IDA.\n\n**Empirical Results**\n\nThis procedure has already been tried for simple problems empirically. The authors of the paper *[Supervising strong learners by amplifying weak experts](https://arxiv.org/abs/1810.08575)* found that when compared to supervised learning models, amplification techniques can solve similar tasks with at worst a modest slowdown. Amplification involved modestly more training steps, and roughly twice as much computation per question. Additionally, supervised learning required tens of millions of examples when learning an algorithm, whereas amplification required only tens of thousands. Some important experimental simplifications included:\n\n- Assuming that the kinds of problems being solved can be algorithmically decomposed. It is not known whether humans can decompose interesting real-world tasks.\n\n- Algorithmic training signals were possible to be constructed in the experiment. However, in the long run, the authors care about tasks where it is not possible to construct either algorithmic or human training signals.\n\nOverall, the authors have shown that iterated amplification can successfully solve algorithmically complex tasks where there is no external reward function and the objective is implicit in a learned decomposition. This offers hope for applying ML in domains where suitable objectives cannot be computed, even with human help \u2013 as long as humans can decompose a task into simpler pieces.\n\n[^kix.41tnc0i2r37i]: https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment", "Tag Count": 0, "Related Answer Count": 3, "Rich Text": "[Iterated distillation and amplification (IDA)](https://www.alignmentforum.org/tag/iterated-amplification) is an alignment approach where we build powerful, artificial intelligence (AI) systems through a process of initially building weak AI systems, and recursively using each new AI to teach a slightly smarter AI.\n\nTraining an AI system using machine learning (ML) techniques requires some kind of evaluation of its performance. This can be performed algorithmically if we have access to ground truth labels. Unfortunately, most useful tasks do not come with such labels and cannot be algorithmically evaluated. We can attempt to evaluate our models through reward functions in standard [reinforcement learning (RL)](https://docs.google.com/document/d/1fONikRvX-1iGOKUqT2qzNvyaW2PEt2I6rlt6xrAMG2U/edit#) or by giving them feedback through methods such as [reinforcement learning from human feedback (RLHF)](https://docs.google.com/document/d/1RAU4w38uh9WdDscy9H__Y3WEeOZhXJjrAjnJJcO-Ct8/edit)  and [inverse reinforcement learning (IRL)](https://docs.google.com/document/d/1DVqzpQHAlFxXu5mg5nQcGxxHEWAmr752dHfrXgb-d6M/edit).\n\nHowever, methods involving human-defined reward functions or human feedback are limited by human abilities. This suggests that AI systems can only be taught to perform tasks in an aligned manner that at best matches human-level performance. This performance can be improved by using easier-to-specify proxies, but such proxies could lead to [misaligned behavior](https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity). To solve this problem, the proposal of iterative amplification involves decomposing complex tasks (with [AI assistance](https://aisafety.info/?state=7791_7595-2222-6974-87O6-7763-7757-7491-)) into simpler tasks for which we already have human or algorithmic training signals.\n\n[See more...]\n\n**The Process**\n\n![](https://lh3.googleusercontent.com/PDHGtE4eqelDRFFc3_R8WYlSsFv2Sj216uMXKnolMcFU1SxmyrIpsCVA2JdLlcwYa6CQWuCZFeL772VwJEjwDe83Le_NeiOGG1D3mHWqgtCNC0fKZSpHShNtav38-U0rcJFQcU1b549irGG_aW8iXZxVwanSCLpp)\n[^kix.41tnc0i2r37i]\n\nThe steps involved in the process of training an agent through IDA are roughly as follows:\n\n- **Training**: We want to train an agent to perform some tasks in an aligned manner. We use experts to teach the agent to perform the task through a combination of feedback (RLHF/IRL) and demonstration (Behavioral Cloning/Imitation). We want the AI to not be aligned with the specific values (or biases) of any single expert. This is why we use groups of evaluators. Additionally, using groups of expert demonstrators ensures that the agent will be able to overcome the performance limitations of any single expert. If the overall task is too complex it can also be broken down into smaller sub-tasks to be evaluated by multiple groups (human or AI) that are experts in the domain of that specific sub-task. Overall, through the training process, these groups of experts are able to instill in the agent both capabilities to perform the task while also keeping it aligned.\n\n- **Distillation**: After the training process is complete, this new single trained agent is now better than all the experts that trained it. In other words, all their expertise has been distilled down into one agent. Let's call it the tier-1 agent. This is the **distillation** step. In the first step of IDA, the expert teachers would be humans (you can think of us as tier-0 agents).\n\n- **Amplification**: Now we want to train an aligned tier-2 agent to perform some task that we (humans) can neither specify nor evaluate. Instead, we use the tier-1 agent created in the previous step, who is now better than all groups of human experts. We create multiple copies of the tier-1 agent using techniques such as imitation learning. This creates a new group of tier-1 experts and demonstrators, each of which has higher capabilities than the initial humans that trained it. This is the **amplification step**. This now means that even if the tier-2 task was beyond the capabilities of a single tier-1 agent, we now can decompose the tier-2 task into sub-tasks that are individually solvable/evaluateable by the tier-1 experts.\n\n- **Iteration**: Distilling down the knowledge of the tier-1 group of experts will help us create a tier-2 agent. We can now repeat the amplification and distillation process. This allows us to train yet another tier of agent, which will in turn be better than the group of tier-2 agents that trained it while remaining aligned. We can repeat this as needed. This is the iteration step.\n\nThus, iterated distillation and amplification, or IDA.\n\n**Empirical Results**\n\nThis procedure has already been tried for simple problems empirically. The authors of the paper *[Supervising strong learners by amplifying weak experts](https://arxiv.org/abs/1810.08575)* found that when compared to supervised learning models, amplification techniques can solve similar tasks with at worst a modest slowdown. Amplification involved modestly more training steps, and roughly twice as much computation per question. Additionally, supervised learning required tens of millions of examples when learning an algorithm, whereas amplification required only tens of thousands. Some important experimental simplifications included:\n\n- Assuming that the kinds of problems being solved can be algorithmically decomposed. It is not known whether humans can decompose interesting real-world tasks.\n\n- Algorithmic training signals were possible to be constructed in the experiment. However, in the long run, the authors care about tasks where it is not possible to construct either algorithmic or human training signals.\n\nOverall, the authors have shown that iterated amplification can successfully solve algorithmically complex tasks where there is no external reward function and the objective is implicit in a learned decomposition. This offers hope for applying ML in domains where suitable objectives cannot be computed, even with human help \u2013 as long as humans can decompose a task into simpler pieces.\n\n[^kix.41tnc0i2r37i]: https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "897J", "Related Answers": "What is Reinforcement Learning from Human Feedback (RLHF)?,What is inverse reinforcement learning (IRL)?,What is \"HCH\"?", "Doc Last Ingested": "2023-03-14T23:22:19.506+01:00", "Request Count": 1, "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 1373, "Helpful": ""}}, {"id": "i-7dc3df6cd1e28f067c354042b0276673117624195a0adae01f83e34fbd2be652", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7dc3df6cd1e28f067c354042b0276673117624195a0adae01f83e34fbd2be652", "name": "Which brain measurement techniques might be useful to train transformers to predict brain states?", "index": 442, "createdAt": "2023-02-17T16:49:57.521Z", "updatedAt": "2023-03-14T22:26:51.870Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7dc3df6cd1e28f067c354042b0276673117624195a0adae01f83e34fbd2be652", "values": {"File": "Which brain measurement techniques might be useful to train transformers to predict brain states?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Which brain measurement techniques might be useful to train transformers to predict brain states?", "Link": "https://docs.google.com/document/d/1nZwgVXY-9NhQ5QpH6tMKtGnHpkkzbsz0-ByB8CYXK-w/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T13:36:24.724+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:49.007+01:00", "Status": "Not started", "Edit Answer": "Which brain measurement techniques might be useful to train transformers to predict brain states?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "88TJ", "Source Link": "", "aisafety.info Link": "Which brain measurement techniques might be useful to train transformers to predict brain states?", "Source": "", "All Phrasings": "Which brain measurement techniques might be useful to train transformers to predict brain states?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-02-26T17:32:37.758+01:00", "UI ID": "88TJ", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:20.958+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 1373, "Helpful": ""}}, {"id": "i-9f8fb787f2c638e7349a54feeb379f0d4bb667e22aa0d89b8c52bb0918a0062f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-9f8fb787f2c638e7349a54feeb379f0d4bb667e22aa0d89b8c52bb0918a0062f", "name": "Will regulations and policies end up reducing or increasing existential risk?", "index": 443, "createdAt": "2023-02-17T16:49:57.521Z", "updatedAt": "2023-03-14T22:26:53.981Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-9f8fb787f2c638e7349a54feeb379f0d4bb667e22aa0d89b8c52bb0918a0062f", "values": {"File": "Will regulations and policies end up reducing or increasing existential risk?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Will regulations and policies end up reducing or increasing existential risk?", "Link": "https://docs.google.com/document/d/193P2ot_ty6LhbKwlMjcQVqrh6kmv1BEszImSJKOu9Ck/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T12:29:55.960+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:50.202+01:00", "Status": "Not started", "Edit Answer": "Will regulations and policies end up reducing or increasing existential risk?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "88TK", "Source Link": "", "aisafety.info Link": "Will regulations and policies end up reducing or increasing existential risk?", "Source": "", "All Phrasings": "Will regulations and policies end up reducing or increasing existential risk?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "88TK", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:27.840+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 1373, "Helpful": ""}}, {"id": "i-38fd1a93f7e9cc82cf6b5345f469a1bfaf0b484d5184a43e91a734c51a984a2e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-38fd1a93f7e9cc82cf6b5345f469a1bfaf0b484d5184a43e91a734c51a984a2e", "name": "How do I convince other people that AI safety is important?", "index": 444, "createdAt": "2023-02-17T16:49:57.521Z", "updatedAt": "2023-03-14T22:26:58.966Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-38fd1a93f7e9cc82cf6b5345f469a1bfaf0b484d5184a43e91a734c51a984a2e", "values": {"File": "How do I convince other people that AI safety is important?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How do I convince other people that AI safety is important?", "Link": "https://docs.google.com/document/d/1W7y41F_9AYbFAPR_KySo4AltmoiOltS7REyYHIoPJTQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-08T01:42:11.224+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:51.381+01:00", "Status": "Not started", "Edit Answer": "How do I convince other people that AI safety is important?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "88TL", "Source Link": "", "aisafety.info Link": "How do I convince other people that AI safety is important?", "Source": "", "All Phrasings": "How do I convince other people that AI safety is important?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n- \n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n- \n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "88TL", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:29.226+01:00", "Request Count": "", "Number of suggestions on answer doc": 5, "Total character count of suggestions on answer doc": 1534, "Helpful": ""}}, {"id": "i-1b19094174ea020daeab013e3bfb998f924c4ff3f8455bb51006c01efbe56787", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1b19094174ea020daeab013e3bfb998f924c4ff3f8455bb51006c01efbe56787", "name": "What is Reinforcement Learning from Human Feedback (RLHF)?", "index": 439, "createdAt": "2023-02-16T16:49:50.634Z", "updatedAt": "2023-03-14T22:27:03.930Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1b19094174ea020daeab013e3bfb998f924c4ff3f8455bb51006c01efbe56787", "values": {"File": "What is Reinforcement Learning from Human Feedback (RLHF)?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Reinforcement Learning from Human Feedback (RLHF)?", "Link": "https://docs.google.com/document/d/1RAU4w38uh9WdDscy9H__Y3WEeOZhXJjrAjnJJcO-Ct8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-16T14:32:48.096+01:00", "Related Answers DO NOT EDIT": "What is reinforcement learning (RL)?", "Tags": "", "Doc Last Edited": "2023-02-28T13:56:12.943+01:00", "Status": "In progress", "Edit Answer": "What is Reinforcement Learning from Human Feedback (RLHF)?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "88FN", "Source Link": "", "aisafety.info Link": "What is Reinforcement Learning from Human Feedback (RLHF)?", "Source": "", "All Phrasings": "What is Reinforcement Learning from Human Feedback (RLHF)?\n", "Initial Order": "", "Related IDs": "89ZS", "Rich Text DO NOT EDIT": "Reinforcement Learning from Human Feedback (RLHF) uses methods from Reinforcement Learning (RL) to optimize an AI model using human feedback.\n\nIn traditional RL (see reference), an agent receives an observation, takes an action, and gets a reward. However, in certain scenarios it can be difficult to design a good reward function. For example, it is easier to recognize a backflip than to describe a good backflip.  Rather than us describing the reward function, the AI can learn one.\n\nIn the case of an AI learning to do backflips this goes as follows[^kix.9b5wkhz2ixtm]. At first, the AI acts randomly and two clips of its behavior are given to a human. The human decides which clip looks best. This information is used by AI to learn its reward function. This reward function is then in turn used to update the behavior using RL. These steps are then repeated.\n\nThere are more interesting applications of this approach than learning backflips. Suppose we have a pre-trained large language model (see reference) that can summarize text, and we want to fine-tune it on human preferences[^kix.lcvhgyj61s48]. An example of human preference could be to tell the truth.\n\nIn order to use RL to fine-tune this model, a reward function is learned as describing human preferences is hard. This reward function is trained using supervised learning, using the following data samples. Sample a number of text prompts to summarize, and obtain different summaries by giving these prompts to different models. These different summaries are paired up, and ranked by humans. The original prompt and summaries are the input for the reward function, and the human ranking the target output.\n\nFor fine-tuning the pre-trained model is fixed, and a copy is made. At each step, outputs from both models are ranked by the reward function. Using the reward the copy is then updated using RL.\n\n**Some questions I\u2019m unsure about:**\n\n- Should there be a paragraph related to AI safety?\n\n- What level of knowledge can be assumed from the reader?\n\n- What level of abstraction should this be on?\n\n    - Should there be math?\n\n    - E.g. in these examples humans rank pairings, but in principle any possible ratings could work (but humans are better at ranking vs giving absolute scores). Are these kinds of caveats important to mention?\n\n    - How much detail should these approaches be described? There are various details which are kinda important but also not really relevant for the main idea. \n\n[^kix.lcvhgyj61s48]:[https://openai.com/blog/fine-tuning-gpt-2/](https://openai.com/blog/fine-tuning-gpt-2/)\n[^kix.9b5wkhz2ixtm]:[https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/)", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "Reinforcement Learning from Human Feedback (RLHF) uses methods from Reinforcement Learning (RL) to optimize an AI model using human feedback.\n\nIn traditional RL (see reference), an agent receives an observation, takes an action, and gets a reward. However, in certain scenarios it can be difficult to design a good reward function. For example, it is easier to recognize a backflip than to describe a good backflip.  Rather than us describing the reward function, the AI can learn one.\n\nIn the case of an AI learning to do backflips this goes as follows[^kix.9b5wkhz2ixtm]. At first, the AI acts randomly and two clips of its behavior are given to a human. The human decides which clip looks best. This information is used by AI to learn its reward function. This reward function is then in turn used to update the behavior using RL. These steps are then repeated.\n\nThere are more interesting applications of this approach than learning backflips. Suppose we have a pre-trained large language model (see reference) that can summarize text, and we want to fine-tune it on human preferences[^kix.lcvhgyj61s48]. An example of human preference could be to tell the truth.\n\nIn order to use RL to fine-tune this model, a reward function is learned as describing human preferences is hard. This reward function is trained using supervised learning, using the following data samples. Sample a number of text prompts to summarize, and obtain different summaries by giving these prompts to different models. These different summaries are paired up, and ranked by humans. The original prompt and summaries are the input for the reward function, and the human ranking the target output.\n\nFor fine-tuning the pre-trained model is fixed, and a copy is made. At each step, outputs from both models are ranked by the reward function. Using the reward the copy is then updated using RL.\n\n**Some questions I\u2019m unsure about:**\n\n- Should there be a paragraph related to AI safety?\n\n- What level of knowledge can be assumed from the reader?\n\n- What level of abstraction should this be on?\n\n    - Should there be math?\n\n    - E.g. in these examples humans rank pairings, but in principle any possible ratings could work (but humans are better at ranking vs giving absolute scores). Are these kinds of caveats important to mention?\n\n    - How much detail should these approaches be described? There are various details which are kinda important but also not really relevant for the main idea. \n\n[^kix.lcvhgyj61s48]:[https://openai.com/blog/fine-tuning-gpt-2/](https://openai.com/blog/fine-tuning-gpt-2/)\n[^kix.9b5wkhz2ixtm]:[https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/)", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "88FN", "Related Answers": "What is reinforcement learning (RL)?", "Doc Last Ingested": "2023-03-14T23:22:31.407+01:00", "Request Count": 1, "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-12666aa3f759af8f4e506b04320c85ee43b4c049dee69be5db7f3f2aaad237d7", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-12666aa3f759af8f4e506b04320c85ee43b4c049dee69be5db7f3f2aaad237d7", "name": "Can you trust ChatGPT?", "index": 440, "createdAt": "2023-02-16T16:49:50.634Z", "updatedAt": "2023-03-14T22:27:11.190Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-12666aa3f759af8f4e506b04320c85ee43b4c049dee69be5db7f3f2aaad237d7", "values": {"File": "Can you trust ChatGPT?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Can you trust ChatGPT?", "Link": "https://docs.google.com/document/d/1CWtPc2Zrm-xqHIZNOWlENwuOahleSsInBfqL-nSx_2E/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-16T14:32:43.711+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-27T00:06:55.940+01:00", "Status": "In progress", "Edit Answer": "Can you trust ChatGPT?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "88FO", "Source Link": "", "aisafety.info Link": "Can you trust ChatGPT?", "Source": "", "All Phrasings": "Can you trust ChatGPT?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[ChatGPT](https://openai.com/blog/chatgpt/) is a trained model that can interact with its users in a conversational way. It can respond to follow-up questions, admit its mistakes, challenge incorrect premises and reject inappropriate requests.ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers. Its rapid rise in popularity is largely due to its ability to [offer personalized guidance](https://preply.com/en/blog/google-versus-chatgpt/) that trumps the generic results of traditional search engines. This tailored approach has ensured people keep returning despite its shortcomings.\n\nIn this [video](https://www.youtube.com/watch?v=viJt_DXTfwA), Rob Miles discusses his views on what ChatGPT actually does\u2026.\n\nIn this lessWrong post,\n\n### \n\n### \n\n### How about other large language models?\n\nOther large language models such as Microsoft\u2019s [Bing Chat](https://www.bing.com/new) and Anthropic\u2019s [Claude](https://scale.com/blog/chatgpt-vs-claude) suffer from similar limitations although the latter is optimized to attempt to be [Helpful, Honest and Harmless (HHH)](https://arxiv.org/abs/2112.00861).\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "[ChatGPT](https://openai.com/blog/chatgpt/) is a trained model that can interact with its users in a conversational way. It can respond to follow-up questions, admit its mistakes, challenge incorrect premises and reject inappropriate requests.ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers. Its rapid rise in popularity is largely due to its ability to [offer personalized guidance](https://preply.com/en/blog/google-versus-chatgpt/) that trumps the generic results of traditional search engines. This tailored approach has ensured people keep returning despite its shortcomings.\n\nIn this [video](https://www.youtube.com/watch?v=viJt_DXTfwA), Rob Miles discusses his views on what ChatGPT actually does\u2026.\n\nIn this lessWrong post,\n\n### \n\n### \n\n### How about other large language models?\n\nOther large language models such as Microsoft\u2019s [Bing Chat](https://www.bing.com/new) and Anthropic\u2019s [Claude](https://scale.com/blog/chatgpt-vs-claude) suffer from similar limitations although the latter is optimized to attempt to be [Helpful, Honest and Harmless (HHH)](https://arxiv.org/abs/2112.00861).\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "88FO", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:33.224+01:00", "Request Count": 1, "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-ca01ac3704f0a4e92665f335d8b32d5f0ff6d10a4aef5ec2e84f42062b3e04f2", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ca01ac3704f0a4e92665f335d8b32d5f0ff6d10a4aef5ec2e84f42062b3e04f2", "name": "How can we solve the alignment problem?", "index": 441, "createdAt": "2023-02-16T16:49:50.634Z", "updatedAt": "2023-03-14T22:27:19.417Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ca01ac3704f0a4e92665f335d8b32d5f0ff6d10a4aef5ec2e84f42062b3e04f2", "values": {"File": "How can we solve the alignment problem?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How can we solve the alignment problem?", "Link": "https://docs.google.com/document/d/1gL6UFk-7VqLpWPnFpJoZH_rFRBGEnmVcVcxBQN9I0QQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-16T14:32:29.087+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-08T05:37:29.031+01:00", "Status": "In review", "Edit Answer": "How can we solve the alignment problem?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "88FP", "Source Link": "", "aisafety.info Link": "How can we solve the alignment problem?", "Source": "", "All Phrasings": "How can we solve the alignment problem?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*We don't have an answer for this question yet. Would you like to write one?*\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "88FP", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:35.312+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-23fcdac32af8811c65351ab93414c0ffb52832316d49bb2c225ac3c83c2685f1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-23fcdac32af8811c65351ab93414c0ffb52832316d49bb2c225ac3c83c2685f1", "name": "Where can I find an overview of everything happening in AI existential safety?", "index": 438, "createdAt": "2023-02-11T14:47:54.018Z", "updatedAt": "2023-03-14T22:27:21.767Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-23fcdac32af8811c65351ab93414c0ffb52832316d49bb2c225ac3c83c2685f1", "values": {"File": "Where can I find an overview of everything happening in AI existential safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Where can I find an overview of everything happening in AI existential safety?", "Link": "https://docs.google.com/document/d/1CH3q3S2CD5scwfgdfwfAhhonqw3MgSBj_zFqQunfN98/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-11T15:47:21.637+01:00", "Related Answers DO NOT EDIT": "What is everyone working on in AI alignment?", "Tags": "", "Doc Last Edited": "2023-03-04T02:40:55.206+01:00", "Status": "Live on site", "Edit Answer": "Where can I find an overview of everything happening in AI existential safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "881W", "Source Link": "", "aisafety.info Link": "Where can I find an overview of everything happening in AI existential safety?", "Source": "", "All Phrasings": "Where can I find an overview of everything happening in AI existential safety?\n", "Initial Order": 4.5, "Related IDs": "8392", "Rich Text DO NOT EDIT": "[This map](https://aisafety.world/) shows most major landmarks in the AI safety ecosystem:\n\n[https://aisafety.world/](https://aisafety.world/)\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "[This map](https://aisafety.world/) shows most major landmarks in the AI safety ecosystem:\n\n[https://aisafety.world/](https://aisafety.world/)\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "881W", "Related Answers": "What is everyone working on in AI alignment?", "Doc Last Ingested": "2023-03-14T23:22:38.173+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-ab8afa359ddffc0d30a562d66b228e2b520b71b468c9a89c031af91f447bd074", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ab8afa359ddffc0d30a562d66b228e2b520b71b468c9a89c031af91f447bd074", "name": "What are some arguments for AI safety being less important?", "index": 437, "createdAt": "2023-02-10T16:49:10.928Z", "updatedAt": "2023-03-15T20:08:37.217Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ab8afa359ddffc0d30a562d66b228e2b520b71b468c9a89c031af91f447bd074", "values": {"File": "What are some arguments for AI safety being less important?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some arguments for AI safety being less important?", "Link": "https://docs.google.com/document/d/1JZCPctDXfgjRz93fgylSgaMJ0oBZ9vleRXw15gsHmxU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-09T08:29:06.016+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-15T18:44:01.567+01:00", "Status": "Live on site", "Edit Answer": "What are some arguments for AI safety being less important?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "87O6", "Source Link": "", "aisafety.info Link": "What are some arguments for AI safety being less important?", "Source": "", "All Phrasings": "What are some arguments for AI safety being less important?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Some of these arguments are substantially better than others, and some of the pieces here are rebuttals to worse arguments. Some of the best are in **bold**.\n\n- Some of the **[reviews of \"Is power-seeking AI an existential risk?\"](https://forum.effectivealtruism.org/posts/GRv3KB2nPFRREXb5o/reviews-of-is-power-seeking-ai-an-existential-risk)**(Joe Carlsmith; see also [Carlsmith's report](https://arxiv.org/abs/2206.13353))\n\n- **[What do we think are the best arguments we\u2019re wrong?](https://80000hours.org/problem-profiles/artificial-intelligence/#best-arguments-against-this-problem-being-pressing)** and [Arguments against working on AI risk to which we think there are strong responses](https://80000hours.org/problem-profiles/artificial-intelligence/#good-responses) (Benjamin Hilton)\n\n- **[Success without dignity: a nearcasting story of avoiding catastrophe by luck](https://www.lesswrong.com/posts/jwhcXmigv2LTrbBiB/success-without-dignity-a-nearcasting-story-of-avoiding)** (Holden Karnofsky)\n\n- **[Counterarguments to the basic AI x-risk case](https://www.alignmentforum.org/posts/LDRQ5Zfqwi8GjzPYG/counterarguments-to-the-basic-ai-x-risk-case)** (Katja Grace)\n\n- **[Ben Garfinkel on scrutinising classic AI risk arguments](https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/)** (80,000 Hours Podcast; also see [this slideshow](https://docs.google.com/presentation/d/1sHA3rwTHLIxyZPQObcw8mbNo2jffswH8uYV7N5PwqZE/edit#slide=id.g6230db10d0_0_305))\n\n- **[How big of a risk is misalignment?](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/#how-big-of-a-risk-is-misalignment)** in \u201cWhy AI alignment could be hard with modern deep learning\u201d (Ajeya Cotra)\n\n- [A list of good heuristics that the case for AI x-risk fails](https://www.alignmentforum.org/posts/bd2K3Jdz82csjCFob/a-list-of-good-heuristics-that-the-case-for-ai-x-risk-fails) (David Krueger)\n\n- [My highly personal skepticism braindump on existential risk from artificial intelligence.](https://nunosempere.com/blog/2023/01/23/my-highly-personal-skepticism-braindump-on-existential-risk/) (Nu\u00f1o Sempere; also see [Aaron Scher's summary](https://forum.effectivealtruism.org/posts/L6ZmggEJw8ri4KB8X/my-highly-personal-skepticism-braindump-on-existential-risk?commentId=hD4AvR8An2enTnQZC))\n\n- [Arguments for AI Risk](https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review#Arguments_for_AI_risk) and[Arguments against AI risk](https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review#Arguments_against_AI_risk) sections of \"AI Alignment 2018-19 Review.\" (Rohin Shah)\n\n- [Reasons I\u2019ve been hesitant about high levels of near-ish AI risk](https://forum.effectivealtruism.org/posts/5hprBzprm7JjJTHNX/reasons-i-ve-been-hesitant-about-high-levels-of-near-ish-ai-1) (Eli Lifland)\n\n- [Why I Am Not (As Much Of) A Doomer (As Some People)](https://astralcodexten.substack.com/p/why-i-am-not-as-much-of-a-doomer) (Scott Alexander)\n\n- [AI Risk Skepticism](https://arxiv.org/abs/2105.02704) (Roman Yampolskiy)\n\n- [Existential risk from artificial general intelligence: Skepticism](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence#Skepticism) (Wikipedia)\n\n- [AI Risk, Again](https://www.overcomingbias.com/p/ai-risk-again) (Robin Hanson)\n\n- \u201cI Object!\u201d section of [Extinction Risk from Artificial Intelligence](https://aisafety.wordpress.com/) (Michael Cohen)\n\n- [Blake Richards on Why he is Skeptical of Existential Risk from AI](https://forum.effectivealtruism.org/posts/BqokXcCQrvkk2BktH/blake-richards-on-why-he-is-skeptical-of-existential-risk) (podcast interview with Micha\u00ebl Trazzi on [The Inside View](https://theinsideview.ai/blake))\n\n- [Gary Marcus and Stuart Russell discuss AI risk on the Sam Harris podcast](https://www.samharris.org/podcasts/making-sense-episodes/312-the-trouble-with-ai)\n\n- [Alignment By Default](https://www.alignmentforum.org/posts/Nwgdq6kHke5LY692J/alignment-by-default) (John Wentworth)\n\n- [A criticism](https://www.overcomingbias.com/2019/04/agency-failure-ai-apocalypse.html) of \u201cWhat Failure Looks Like\u201d (Robin Hanson; also see[a response](https://www.lesswrong.com/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai) to this criticism)\n\n- [Melanie Mitchell and Stuart Russell debate](https://munkdebates.com/podcast/the-rise-of-thinking-machines) (Munk Debates)\n\n- [Why AI is Harder Than We Think](https://forum.effectivealtruism.org/posts/C94JhsbSfZ8iPNedy/why-ai-is-harder-than-we-think-melanie-mitchell) (Melanie Mitchell; also see [Richard Ngo's comment](https://forum.effectivealtruism.org/posts/C94JhsbSfZ8iPNedy/why-ai-is-harder-than-we-think-melanie-mitchell?commentId=mkNwocFphweG8hHrE))\n\n- [A shift in arguments for AI risk](https://fragile-credences.github.io/prioritising-ai/) (Tom Adamczewski)\n\n- [How to navigate the AI apocalypse as a sane person](https://erikhoel.substack.com/p/how-to-navigate-the-ai-apocalypse) (Eric Hoel)\n\n- [My Bet: AI Solves Flubs](https://astralcodexten.substack.com/p/my-bet-ai-size-solves-flubs) (Scott Alexander)\n\n- [AI Researchers On AI Risk](https://slatestarcodex.com/2015/05/22/ai-researchers-on-ai-risk/) (Scott Alexander)\n\n- [Contra Acemoglu On...Oh God, We're Doing This Again, Aren't We?](https://astralcodexten.substack.com/p/contra-acemoglu-onoh-god-were-doing) (Scott Alexander)\n\n- [Maybe The Real Superintelligent AI Is Extremely Smart Computers](https://slatestarcodex.com/2018/01/15/maybe-the-real-superintelligent-ai-is-extremely-smart-computers/) (Scott Alexander)\n\n- <iframe src=\"https://www.youtube.com/embed/9i1WlcCudpU\" title=\"10 Reasons to Ignore AI Safety\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n    \n     (Rob Miles)\n\n- <iframe src=\"https://www.youtube.com/embed/yQE9KAbFhNY\" title=\"A Response to Steven Pinker on AI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n    \n     (Rob Miles)\n\n- [How much EA analysis of AI safety as a cause area exists?](https://forum.effectivealtruism.org/posts/u3ePLsbtpkmFdD7Nb/how-much-ea-analysis-of-ai-safety-as-a-cause-area-exists-1) (Richard Ngo)\n\n- [Can a Paperclip Maximizer Overthrow the CCP?](https://richardhanania.substack.com/p/can-a-paperclip-maximizer-overthrow) (Richard Hanania)\n\n- [Superintelligence: The Idea That Eats Smart People](https://idlewords.com/talks/superintelligence.htm) + [video](https://www.youtube.com/watch?v=kErHiET5YPw) (Maciej Ceglowski)\n\n- [Future Fund Worldview Prize](https://forum.effectivealtruism.org/topics/future-fund-worldview-prize) tag (EA Forum)\n\n- [Open Philanthropy AI Worldviews Contest](https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/)\n\n- [AI skepticism](https://forum.effectivealtruism.org/topics/ai-skepticism) tag (EA Forum)\n\n- [How I failed to form views on AI safety](https://forum.effectivealtruism.org/posts/ST3JjsLdTBnaK46BD/how-i-failed-to-form-views-on-ai-safety-3) (Ada-Maaria Hyv\u00e4rinen)\n\n- [There are no coherence theorems](https://www.alignmentforum.org/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems) (Elliott Thornley)\n\n- [The Singularity is not coming](https://fchollet.com/blog/the-singularity-is-not-coming.html), [The implausibility of intelligence explosion](https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec) (Fran\u00e7ois Chollet; also see [Eliezer Yudkowsky\u2019s reply to the latter piece](https://intelligence.org/2017/12/06/chollet/))\n\n- [\"Bad AI DontKillEveryoneism Takes\"](https://thezvi.substack.com/p/ai-3#%C2%A7bad-ai-dontkilleveryoneism-takes) section of [AI #3](https://thezvi.substack.com/p/ai-3) (Zvi Mowshowitz)\n\n- [Bad alignment take bingo with replies](https://twitter.com/robbensinger/status/1503220020175769602) (Rob Bensinger; also see the card [\u201cwith some 2023 additions\u201d](https://twitter.com/carad0/status/1632427388230422529); note these bingo cards are mostly designed to be funny, although they do contain some core ideas of various arguments)\n\n- [Possible miracles](https://www.lesswrong.com/posts/z3GwFzt4fnBdPz5hd/possible-miracles) (Akash Wasil, Thomas Larsen)\n\nNote some of these arguments come from people who work full-time on AI safety. Here are some authors who don\u2019t work full-time on AI safety:\n\n- Benjamin Hilton (although Benjamin is somewhat part of the AI x-risk community and effective altruism community)\n\n- Nu\u00f1o Sempere (somewhat part of the AI x-risk community and effective altruism community)\n\n- Blake Richards\n\n- Robin Hanson\n\n- Melanie Mitchell\n\n- Tom Adamczewski\n\n- Eric Hoel\n\n- Scott Alexander (somewhat part of the AI x-risk community and effective altruism community)\n\n- Zvi Mowshowitz somewhat part of the AI x-risk community and effective altruism community)\n\n- Wikipedia\n\n- Steven Pinker\n\n- Richard Hanania\n\n- Maciej Ceglowski\n\n- Fran\u00e7ois Chollet\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Some of these arguments are substantially better than others, and some of the pieces here are rebuttals to worse arguments. Some of the best are in **bold**.\n\n- Some of the **[reviews of \"Is power-seeking AI an existential risk?\"](https://forum.effectivealtruism.org/posts/GRv3KB2nPFRREXb5o/reviews-of-is-power-seeking-ai-an-existential-risk)**(Joe Carlsmith; see also [Carlsmith's report](https://arxiv.org/abs/2206.13353))\n\n- **[What do we think are the best arguments we\u2019re wrong?](https://80000hours.org/problem-profiles/artificial-intelligence/#best-arguments-against-this-problem-being-pressing)** and [Arguments against working on AI risk to which we think there are strong responses](https://80000hours.org/problem-profiles/artificial-intelligence/#good-responses) (Benjamin Hilton)\n\n- **[Success without dignity: a nearcasting story of avoiding catastrophe by luck](https://www.lesswrong.com/posts/jwhcXmigv2LTrbBiB/success-without-dignity-a-nearcasting-story-of-avoiding)** (Holden Karnofsky)\n\n- **[Counterarguments to the basic AI x-risk case](https://www.alignmentforum.org/posts/LDRQ5Zfqwi8GjzPYG/counterarguments-to-the-basic-ai-x-risk-case)** (Katja Grace)\n\n- **[Ben Garfinkel on scrutinising classic AI risk arguments](https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/)** (80,000 Hours Podcast; also see [this slideshow](https://docs.google.com/presentation/d/1sHA3rwTHLIxyZPQObcw8mbNo2jffswH8uYV7N5PwqZE/edit#slide=id.g6230db10d0_0_305))\n\n- **[How big of a risk is misalignment?](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/#how-big-of-a-risk-is-misalignment)** in \u201cWhy AI alignment could be hard with modern deep learning\u201d (Ajeya Cotra)\n\n- [A list of good heuristics that the case for AI x-risk fails](https://www.alignmentforum.org/posts/bd2K3Jdz82csjCFob/a-list-of-good-heuristics-that-the-case-for-ai-x-risk-fails) (David Krueger)\n\n- [My highly personal skepticism braindump on existential risk from artificial intelligence.](https://nunosempere.com/blog/2023/01/23/my-highly-personal-skepticism-braindump-on-existential-risk/) (Nu\u00f1o Sempere; also see [Aaron Scher's summary](https://forum.effectivealtruism.org/posts/L6ZmggEJw8ri4KB8X/my-highly-personal-skepticism-braindump-on-existential-risk?commentId=hD4AvR8An2enTnQZC))\n\n- [Arguments for AI Risk](https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review#Arguments_for_AI_risk) and[Arguments against AI risk](https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review#Arguments_against_AI_risk) sections of \"AI Alignment 2018-19 Review.\" (Rohin Shah)\n\n- [Reasons I\u2019ve been hesitant about high levels of near-ish AI risk](https://forum.effectivealtruism.org/posts/5hprBzprm7JjJTHNX/reasons-i-ve-been-hesitant-about-high-levels-of-near-ish-ai-1) (Eli Lifland)\n\n- [Why I Am Not (As Much Of) A Doomer (As Some People)](https://astralcodexten.substack.com/p/why-i-am-not-as-much-of-a-doomer) (Scott Alexander)\n\n- [AI Risk Skepticism](https://arxiv.org/abs/2105.02704) (Roman Yampolskiy)\n\n- [Existential risk from artificial general intelligence: Skepticism](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence#Skepticism) (Wikipedia)\n\n- [AI Risk, Again](https://www.overcomingbias.com/p/ai-risk-again) (Robin Hanson)\n\n- \u201cI Object!\u201d section of [Extinction Risk from Artificial Intelligence](https://aisafety.wordpress.com/) (Michael Cohen)\n\n- [Blake Richards on Why he is Skeptical of Existential Risk from AI](https://forum.effectivealtruism.org/posts/BqokXcCQrvkk2BktH/blake-richards-on-why-he-is-skeptical-of-existential-risk) (podcast interview with Micha\u00ebl Trazzi on [The Inside View](https://theinsideview.ai/blake))\n\n- [Gary Marcus and Stuart Russell discuss AI risk on the Sam Harris podcast](https://www.samharris.org/podcasts/making-sense-episodes/312-the-trouble-with-ai)\n\n- [Alignment By Default](https://www.alignmentforum.org/posts/Nwgdq6kHke5LY692J/alignment-by-default) (John Wentworth)\n\n- [A criticism](https://www.overcomingbias.com/2019/04/agency-failure-ai-apocalypse.html) of \u201cWhat Failure Looks Like\u201d (Robin Hanson; also see[a response](https://www.lesswrong.com/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai) to this criticism)\n\n- [Melanie Mitchell and Stuart Russell debate](https://munkdebates.com/podcast/the-rise-of-thinking-machines) (Munk Debates)\n\n- [Why AI is Harder Than We Think](https://forum.effectivealtruism.org/posts/C94JhsbSfZ8iPNedy/why-ai-is-harder-than-we-think-melanie-mitchell) (Melanie Mitchell; also see [Richard Ngo's comment](https://forum.effectivealtruism.org/posts/C94JhsbSfZ8iPNedy/why-ai-is-harder-than-we-think-melanie-mitchell?commentId=mkNwocFphweG8hHrE))\n\n- [A shift in arguments for AI risk](https://fragile-credences.github.io/prioritising-ai/) (Tom Adamczewski)\n\n- [How to navigate the AI apocalypse as a sane person](https://erikhoel.substack.com/p/how-to-navigate-the-ai-apocalypse) (Eric Hoel)\n\n- [My Bet: AI Solves Flubs](https://astralcodexten.substack.com/p/my-bet-ai-size-solves-flubs) (Scott Alexander)\n\n- [AI Researchers On AI Risk](https://slatestarcodex.com/2015/05/22/ai-researchers-on-ai-risk/) (Scott Alexander)\n\n- [Contra Acemoglu On...Oh God, We're Doing This Again, Aren't We?](https://astralcodexten.substack.com/p/contra-acemoglu-onoh-god-were-doing) (Scott Alexander)\n\n- [Maybe The Real Superintelligent AI Is Extremely Smart Computers](https://slatestarcodex.com/2018/01/15/maybe-the-real-superintelligent-ai-is-extremely-smart-computers/) (Scott Alexander)\n\n- <iframe src=\"https://www.youtube.com/embed/9i1WlcCudpU\" title=\"10 Reasons to Ignore AI Safety\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n    \n     (Rob Miles)\n\n- <iframe src=\"https://www.youtube.com/embed/yQE9KAbFhNY\" title=\"A Response to Steven Pinker on AI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n    \n     (Rob Miles)\n\n- [How much EA analysis of AI safety as a cause area exists?](https://forum.effectivealtruism.org/posts/u3ePLsbtpkmFdD7Nb/how-much-ea-analysis-of-ai-safety-as-a-cause-area-exists-1) (Richard Ngo)\n\n- [Can a Paperclip Maximizer Overthrow the CCP?](https://richardhanania.substack.com/p/can-a-paperclip-maximizer-overthrow) (Richard Hanania)\n\n- [Superintelligence: The Idea That Eats Smart People](https://idlewords.com/talks/superintelligence.htm) + [video](https://www.youtube.com/watch?v=kErHiET5YPw) (Maciej Ceglowski)\n\n- [Future Fund Worldview Prize](https://forum.effectivealtruism.org/topics/future-fund-worldview-prize) tag (EA Forum)\n\n- [Open Philanthropy AI Worldviews Contest](https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/)\n\n- [AI skepticism](https://forum.effectivealtruism.org/topics/ai-skepticism) tag (EA Forum)\n\n- [How I failed to form views on AI safety](https://forum.effectivealtruism.org/posts/ST3JjsLdTBnaK46BD/how-i-failed-to-form-views-on-ai-safety-3) (Ada-Maaria Hyv\u00e4rinen)\n\n- [There are no coherence theorems](https://www.alignmentforum.org/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems) (Elliott Thornley)\n\n- [The Singularity is not coming](https://fchollet.com/blog/the-singularity-is-not-coming.html), [The implausibility of intelligence explosion](https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec) (Fran\u00e7ois Chollet; also see [Eliezer Yudkowsky\u2019s reply to the latter piece](https://intelligence.org/2017/12/06/chollet/))\n\n- [\"Bad AI DontKillEveryoneism Takes\"](https://thezvi.substack.com/p/ai-3#%C2%A7bad-ai-dontkilleveryoneism-takes) section of [AI #3](https://thezvi.substack.com/p/ai-3) (Zvi Mowshowitz)\n\n- [Bad alignment take bingo with replies](https://twitter.com/robbensinger/status/1503220020175769602) (Rob Bensinger; also see the card [\u201cwith some 2023 additions\u201d](https://twitter.com/carad0/status/1632427388230422529); note these bingo cards are mostly designed to be funny, although they do contain some core ideas of various arguments)\n\n- [Possible miracles](https://www.lesswrong.com/posts/z3GwFzt4fnBdPz5hd/possible-miracles) (Akash Wasil, Thomas Larsen)\n\nNote some of these arguments come from people who work full-time on AI safety. Here are some authors who don\u2019t work full-time on AI safety:\n\n- Benjamin Hilton (although Benjamin is somewhat part of the AI x-risk community and effective altruism community)\n\n- Nu\u00f1o Sempere (somewhat part of the AI x-risk community and effective altruism community)\n\n- Blake Richards\n\n- Robin Hanson\n\n- Melanie Mitchell\n\n- Tom Adamczewski\n\n- Eric Hoel\n\n- Scott Alexander (somewhat part of the AI x-risk community and effective altruism community)\n\n- Zvi Mowshowitz somewhat part of the AI x-risk community and effective altruism community)\n\n- Wikipedia\n\n- Steven Pinker\n\n- Richard Hanania\n\n- Maciej Ceglowski\n\n- Fran\u00e7ois Chollet\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "87O6", "Related Answers": "", "Doc Last Ingested": "2023-03-15T19:12:12.129+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-5f69e491744ba2d83df65ef5aa650f02ccba33844ba5c44304ed949a5569ba55", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5f69e491744ba2d83df65ef5aa650f02ccba33844ba5c44304ed949a5569ba55", "name": "How can I get hired by an organization working on AI alignment?", "index": 434, "createdAt": "2023-02-07T16:50:10.040Z", "updatedAt": "2023-03-14T22:27:33.715Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5f69e491744ba2d83df65ef5aa650f02ccba33844ba5c44304ed949a5569ba55", "values": {"File": "How can I get hired by an organization working on AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How can I get hired by an organization working on AI alignment?", "Link": "https://docs.google.com/document/d/1jqKqns7TQk-nyhu7ZiYcNWWZ2h9vkWY4g2q9EynYfLk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-07T03:03:01.067+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:57.329+01:00", "Status": "Not started", "Edit Answer": "How can I get hired by an organization working on AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "87AF", "Source Link": "", "aisafety.info Link": "How can I get hired by an organization working on AI alignment?", "Source": "", "All Phrasings": "How can I get hired by an organization working on AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[aisafety.careers](http://aisafety.careers)\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "[aisafety.careers](http://aisafety.careers)\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "87AF", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:47.951+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-1fa555e1463e09d524a2420d17daf8dbbc1eff71579e13f9db69ae1e5c2dc64d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1fa555e1463e09d524a2420d17daf8dbbc1eff71579e13f9db69ae1e5c2dc64d", "name": "What is corrigibility?", "index": 435, "createdAt": "2023-02-07T16:50:10.040Z", "updatedAt": "2023-03-14T22:27:40.763Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1fa555e1463e09d524a2420d17daf8dbbc1eff71579e13f9db69ae1e5c2dc64d", "values": {"File": "What is corrigibility?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is corrigibility?", "Link": "https://docs.google.com/document/d/1tJ_XMOCdUCX387mZaNNvJhP7EquG3ZVmxRUBlh7swWA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-07T03:00:39.190+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:45:58.506+01:00", "Status": "In progress", "Edit Answer": "What is corrigibility?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "87AG", "Source Link": "", "aisafety.info Link": "What is corrigibility?", "Source": "", "All Phrasings": "What is corrigibility?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[https://www.lesswrong.com/tag/corrigibility](https://www.lesswrong.com/tag/corrigibility)\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "[https://www.lesswrong.com/tag/corrigibility](https://www.lesswrong.com/tag/corrigibility)\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "87AG", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:49.931+01:00", "Request Count": 1, "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-1cc8fa2d66a9e1b2a5b1bf045cb8cdf24f1fbd1050051fc237dfc8e7f07af756", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1cc8fa2d66a9e1b2a5b1bf045cb8cdf24f1fbd1050051fc237dfc8e7f07af756", "name": "What are \u201ctype signatures\u201d?", "index": 436, "createdAt": "2023-02-07T16:50:10.040Z", "updatedAt": "2023-03-14T22:27:45.044Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1cc8fa2d66a9e1b2a5b1bf045cb8cdf24f1fbd1050051fc237dfc8e7f07af756", "values": {"File": "What are \u201ctype signatures\u201d?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are \u201ctype signatures\u201d?", "Link": "https://docs.google.com/document/d/1dDKu5Ggwqe6ODHwtSzd6MhMsvQloSZt1IZn8WmGuZ4Y/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-07T03:00:17.891+01:00", "Related Answers DO NOT EDIT": "What is \"agent foundations\"?,What are \"selection theorems\" and can they tell us anything useful about the likely shape of AGI systems?", "Tags": "", "Doc Last Edited": "2023-03-02T12:18:28.753+01:00", "Status": "In progress", "Edit Answer": "What are \u201ctype signatures\u201d?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "87AH", "Source Link": "", "aisafety.info Link": "What are \u201ctype signatures\u201d?", "Source": "", "All Phrasings": "What are \u201ctype signatures\u201d?\n", "Initial Order": "", "Related IDs": "7782,7674", "Rich Text DO NOT EDIT": "[Type signatures](https://en.wikipedia.org/wiki/Type_signature) are a concept from computer science. A type signature identifies the kinds of inputs and outputs that a function can have. For example, a function which adds 3 to a number has a type signature of number to number.\n\nThe type signature tells us about the structure of the function, but not its specific identity. So different functions can share the same type signature. For example, adding 3, multiplying by 2 and subtracting 17 all have the same type signature (number -> number), since they accept a number as input and then output a number (even though each of them will output a different number). For some functions, the input and output have different types. For example, the type signature of a function which takes a word and outputs how many letters it has, is word to number.\n\nOne research path in AI alignment is to try and identify the [type signatures of agents](https://www.lesswrong.com/posts/tdcLpkydLwcKwbKre/understanding-selection-theorems). For example, an agent\u2019s output should be an action, and not just a list of the actions it should take. One possible type signature is [(A -> B) -> A](https://www.lesswrong.com/posts/qhsELHzAHFebRJE59/a-greater-than-b-greater-than-a). What this means is that if an agent wants the outcome B, the fact that action A causes B means that the agent will do A.\n\nSince type signatures give us general information about the function without exposing the internals, they have direct applications to research on AI alignment where we might want to know about which inputs and outputs any given agent might have, without needing to worry about the underlying technical details. This gives us the ability to reason and forecast more generally about the abilities of future systems. As an example, we need not worry whether future agents will use deep learning, if we can robustly predict the types of inputs that will produce certain types of outputs.\n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "[Type signatures](https://en.wikipedia.org/wiki/Type_signature) are a concept from computer science. A type signature identifies the kinds of inputs and outputs that a function can have. For example, a function which adds 3 to a number has a type signature of number to number.\n\nThe type signature tells us about the structure of the function, but not its specific identity. So different functions can share the same type signature. For example, adding 3, multiplying by 2 and subtracting 17 all have the same type signature (number -> number), since they accept a number as input and then output a number (even though each of them will output a different number). For some functions, the input and output have different types. For example, the type signature of a function which takes a word and outputs how many letters it has, is word to number.\n\nOne research path in AI alignment is to try and identify the [type signatures of agents](https://www.lesswrong.com/posts/tdcLpkydLwcKwbKre/understanding-selection-theorems). For example, an agent\u2019s output should be an action, and not just a list of the actions it should take. One possible type signature is [(A -> B) -> A](https://www.lesswrong.com/posts/qhsELHzAHFebRJE59/a-greater-than-b-greater-than-a). What this means is that if an agent wants the outcome B, the fact that action A causes B means that the agent will do A.\n\nSince type signatures give us general information about the function without exposing the internals, they have direct applications to research on AI alignment where we might want to know about which inputs and outputs any given agent might have, without needing to worry about the underlying technical details. This gives us the ability to reason and forecast more generally about the abilities of future systems. As an example, we need not worry whether future agents will use deep learning, if we can robustly predict the types of inputs that will produce certain types of outputs.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "87AH", "Related Answers": "What is \"agent foundations\"?,What are \"selection theorems\" and can they tell us anything useful about the likely shape of AGI systems?", "Doc Last Ingested": "2023-03-14T23:22:55.867+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-eae14e03b7b7d561e4b39f48795e3dd04ac541618295f57b25035cfebe15769f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-eae14e03b7b7d561e4b39f48795e3dd04ac541618295f57b25035cfebe15769f", "name": "Won't humans be able to beat an unaligned AI since we have a huge advantage in numbers?", "index": 433, "createdAt": "2023-02-06T16:50:34.210Z", "updatedAt": "2023-03-14T22:27:47.203Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-eae14e03b7b7d561e4b39f48795e3dd04ac541618295f57b25035cfebe15769f", "values": {"File": "Won't humans be able to beat an unaligned AI since we have a huge advantage in numbers?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Won't humans be able to beat an unaligned AI since we have a huge advantage in numbers?", "Link": "https://docs.google.com/document/d/1emDhU_xu0a3syyrC6gLAn7bxR-ecrFzNn3biyTF0t1Y/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-02-05T23:56:44.548+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:03:13.176+01:00", "Status": "Live on site", "Edit Answer": "Won't humans be able to beat an unaligned AI since we have a huge advantage in numbers?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "86WT", "Source Link": "", "aisafety.info Link": "Won't humans be able to beat an unaligned AI since we have a huge advantage in numbers?", "Source": "", "All Phrasings": "Won't humans be able to beat an unaligned AI since we have a huge advantage in numbers?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Humans likely won\u2019t have an advantage in effective numbers, for three main reasons:\n\n1) The AI will likely work much quicker than humans can (transistors are millions of times faster than neurons).\n\n2) The AI might create copies of itself, or cooperate with copies created by other people (as it takes much more compute to train an AI than to run it, it is reasonable to predict that any computer you can train one AGI on you can run several million AGIs on).\n\n3) Human numbers only help if there is cooperation and coordination. It\u2019s much harder to get humans with diverse motives acting together than an AI which can duplicate itself and inspect the thoughts of copies.\n\n#### The AI will likely work much quicker than humans can\n\n[In the words of Dr. Andrew Critch](https://www.lesswrong.com/posts/Ccsx339LE9Jhoii9K/slow-motion-videos-as-ai-risk-intuition-pumps): \"[...] plenty of risks arise just from the fact that humans are extremely slow.  Transistors can fire about 10 million times faster than human brain cells, so it's possible we'll eventually have digital minds operating 10 million times faster than us, meaning from a decision-making perspective we'd look to them like stationary objects, like plants or rocks.\"\n\nTo give an example of how a digital mind might view humans, Dr. Critch shows [this](https://vimeo.com/83664407) video of humans in a subway slowed to one one-hundredth of their original speed.\n\nScience fiction has many examples of beings that think much faster than humans; these are almost always unrealistic in one way or another, but can still be useful for understanding what fast enough thinking can do: [Frame By Frame](https://qntm.org/frame) by qntm (short and funny), [That Alien Message](https://www.lesswrong.com/posts/5wMcKNAwB6X4mp9og/that-alien-message) by Eliezer Yudkowsky (longer and excellent).\n\n#### The AI will likely create copies of itself and effectively cooperate with copies\n\nUnlike humans who can\u2019t easily create identical copies of themselves, an AGI could create perfect clones of its mind by copying the software to another location.\n\n[Holden Karnofsky](https://en.wikipedia.org/wiki/Holden_Karnofsky) (co-founder of [Open Philanthropy](https://www.openphilanthropy.org/)) estimates [here](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/]) that, based on Ajeya Cotra's [biological anchors](https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines) model for forecasting AI, if a human level AI is created through gradient descent one could subsequently use the computer it was trained on to run several hundred million copies of that AI - about 5-10% of Earth's working age population.\n\nThey could then run more copies on other computers.\n\nThese copies might be capable of [superrational](https://en.wikipedia.org/wiki/Superrationality) cooperation. (As copies of each other they know that the other AIs will think all the same things they do, improving trust and allowing them to make plans and coordinate without communicating.) This would allow for a unified intelligence distributed across a very wide geographical space.\n\nKarnofsky describes what effect minds being implementable on cheap hardware might have in the essays: [The Duplicator: Instant Cloning Would Make the World Economy Explode](https://www.cold-takes.com/the-duplicator/)and [Digital People Would Be An Even Bigger Deal](https://www.cold-takes.com/how-digital-people-could-change-the-world/).\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Humans likely won\u2019t have an advantage in effective numbers, for three main reasons:\n\n1) The AI will likely work much quicker than humans can (transistors are millions of times faster than neurons).\n\n2) The AI might create copies of itself, or cooperate with copies created by other people (as it takes much more compute to train an AI than to run it, it is reasonable to predict that any computer you can train one AGI on you can run several million AGIs on).\n\n3) Human numbers only help if there is cooperation and coordination. It\u2019s much harder to get humans with diverse motives acting together than an AI which can duplicate itself and inspect the thoughts of copies.\n\n#### The AI will likely work much quicker than humans can\n\n[In the words of Dr. Andrew Critch](https://www.lesswrong.com/posts/Ccsx339LE9Jhoii9K/slow-motion-videos-as-ai-risk-intuition-pumps): \"[...] plenty of risks arise just from the fact that humans are extremely slow.  Transistors can fire about 10 million times faster than human brain cells, so it's possible we'll eventually have digital minds operating 10 million times faster than us, meaning from a decision-making perspective we'd look to them like stationary objects, like plants or rocks.\"\n\nTo give an example of how a digital mind might view humans, Dr. Critch shows [this](https://vimeo.com/83664407) video of humans in a subway slowed to one one-hundredth of their original speed.\n\nScience fiction has many examples of beings that think much faster than humans; these are almost always unrealistic in one way or another, but can still be useful for understanding what fast enough thinking can do: [Frame By Frame](https://qntm.org/frame) by qntm (short and funny), [That Alien Message](https://www.lesswrong.com/posts/5wMcKNAwB6X4mp9og/that-alien-message) by Eliezer Yudkowsky (longer and excellent).\n\n#### The AI will likely create copies of itself and effectively cooperate with copies\n\nUnlike humans who can\u2019t easily create identical copies of themselves, an AGI could create perfect clones of its mind by copying the software to another location.\n\n[Holden Karnofsky](https://en.wikipedia.org/wiki/Holden_Karnofsky) (co-founder of [Open Philanthropy](https://www.openphilanthropy.org/)) estimates [here](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/]) that, based on Ajeya Cotra's [biological anchors](https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines) model for forecasting AI, if a human level AI is created through gradient descent one could subsequently use the computer it was trained on to run several hundred million copies of that AI - about 5-10% of Earth's working age population.\n\nThey could then run more copies on other computers.\n\nThese copies might be capable of [superrational](https://en.wikipedia.org/wiki/Superrationality) cooperation. (As copies of each other they know that the other AIs will think all the same things they do, improving trust and allowing them to make plans and coordinate without communicating.) This would allow for a unified intelligence distributed across a very wide geographical space.\n\nKarnofsky describes what effect minds being implementable on cheap hardware might have in the essays: [The Duplicator: Instant Cloning Would Make the World Economy Explode](https://www.cold-takes.com/the-duplicator/)and [Digital People Would Be An Even Bigger Deal](https://www.cold-takes.com/how-digital-people-could-change-the-world/).\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "86WT", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:58.016+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-7a91fe76021bd1ef050520611a26a0525b161f08855c6e80b901d06c0ec8535b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7a91fe76021bd1ef050520611a26a0525b161f08855c6e80b901d06c0ec8535b", "name": "What are some introductions to AI safety?", "index": 432, "createdAt": "2023-01-31T16:50:24.080Z", "updatedAt": "2023-03-14T22:27:49.799Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7a91fe76021bd1ef050520611a26a0525b161f08855c6e80b901d06c0ec8535b", "values": {"File": "What are some introductions to AI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some introductions to AI safety?", "Link": "https://docs.google.com/document/d/1zx_WpcwuT3Stpx8GJJHcvJLSgv6dLje0eslVKvuk1yQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2022-12-20T20:45:25.818+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-10T17:42:41.008+01:00", "Status": "Live on site", "Edit Answer": "What are some introductions to AI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "86J8", "Source Link": "", "aisafety.info Link": "What are some introductions to AI safety?", "Source": "", "All Phrasings": "What are some introductions to AI safety?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Note that some of these introductions are from over 5 years ago. Given how quickly the field of AI progresses, some of these older introductions could use an update (e.g. Nick Bostrom\u2019s 2014 book *Superintelligence* has little focus on modern deep learning systems).\n\n## Quick reads (under ~10 minutes)\n\n- [Building safe artificial intelligence: specification, robustness, and assurance](https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1) (DeepMind Safety Research)\n\n- [Four Background Claims](https://intelligence.org/2015/07/24/four-background-claims/) (Nate Soares)\n\n- [Frequent arguments about alignment](https://www.alignmentforum.org/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment) (John Schulman)\n\n- [Of Myths And Moonshine](https://www.edge.org/conversation/the-myth-of-ai#26015) (Stuart Russell)\n\n- [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) (Wikipedia)\n\n- [Will AI really cause a catastrophe?](https://www.maisi.club/about) (Michigan AI Safety Initiative)\n\n- [Artificial Intelligence Safety](https://erafellowship.org/ai-safety) (Existential Risk Alliance)\n\n- [Summary writeup of AGI timeline/risk projections as of Oct 2022](https://docs.google.com/document/d/1j7tZ1Xf7-l2k2qr2t3MFwi-IkhXNdzA2N2WZBfcghsM/edit#) (Kelsey Theriault)\n\n- [a casual intro to AI doom and alignment](https://carado.moe/ai-doom.html) (Tamsin Leake)\n\n- [Global risk from deep learning: 1 - The case for risk](https://www.danieldewey.net/risk/case.html) (Daniel Dewey)\n\n- [Explore Your AI Risk Perspectives: An Interactive Walkthrough of Researchers' Most Frequent Interview Responses](https://ai-risk-discussions.org/perspectives/introduction) (AI Risk Discussions)\n\n- [Marius alignment pitch](https://docs.google.com/document/d/18y0x3ogQau0CyN5a9QYaAUCca8C4bHdEWBK5f4jlO7k/edit#) (Marius Hobbhahn)\n\n- [Intro to AI Safety](https://aizi.substack.com/p/intro-to-ai-safety) (Robert Huben)\n\n- [Basics of AI Wiping Out All Value in the Universe, Take 1](https://www.lesswrong.com/posts/WkchhorbLsSMbLacZ/ai-1-sydney-and-bing#Basics_of_AI_Wiping_Out_All_Value_in_the_Universe__Take_1) (Zvi Mowshowitz)\n\n- [AI Risk](https://benchmarking.mlsafety.org/about) (SafeBench competition)\n\n- [Why Uncontrollable AI Looks More Likely Than Ever](https://time.com/6258483/uncontrollable-ai-agi-risks/) (Otto Barten, Roman Yampolskiy)\n\n- [How AI could accidentally extinguish humankind](https://www.washingtonpost.com/opinions/2022/08/31/artificial-intelligence-worst-case-scenario-extinction/) (\u00c9mile Torres)\n\n## Shorter introductions\n\n- [The case for taking AI seriously](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment) (or a similar argument in [500 words](https://www.vox.com/future-perfect/2019/2/12/18202466/ai-artificial-intelligence-humanity-threat)) and [AI experts are increasingly afraid of what they\u2019re creating](https://www.vox.com/the-highlight/23447596/artificial-intelligence-agi-openai-gpt3-existential-risk-human-extinction) (Kelsey Piper)\n\n- [The alignment problem from a deep learning perspective](https://arxiv.org/abs/2209.00626) (Richard Ngo, Lawrence Chan, S\u00f6ren Mindermann)\n\n- [More Is Different for AI](https://bounded-regret.ghost.io/more-is-different-for-ai/) blog post series (Jacob Steinhardt)\n\n- [Why I Think More NLP Researchers Should Engage with AI Safety Concerns](https://wp.nyu.edu/arg/why-ai-safety/) (Sam Bowman)\n\n- [How to navigate the AI apocalypse as a sane person](https://erikhoel.substack.com/p/how-to-navigate-the-ai-apocalypse) (Eric Hoel)\n\n- [Why alignment could be hard with modern deep learning](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/) (Ajeya Cotra)\n\n- [The Need For Work On Technical AI Alignment](https://www.agisafetyfundamentals.com/alignment-introduction) (Daniel Eth)\n\n- [Ethical Issues in Advanced Artificial Intelligence](https://nickbostrom.com/ethics/ai) (Nick Bostrom)\n\n- [Benefits & Risks of Artificial Intelligence](https://futureoflife.org/ai/benefits-risks-of-artificial-intelligence/) (Ariel Conn)\n\n- [Why worry about future AI?](https://www.gleech.org/ai-risk) (Gavin Leech)\n\n- [AGI Ruin: A list of lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) (Eliezer Yudkowsky)\n\n- [No Time Like The Present For AI Safety Work](https://slatestarcodex.com/2015/05/29/no-time-like-the-present-for-ai-safety-work/) (Scott Alexander)\n\n- [Potential Risks from Advanced Artificial Intelligence: The Philanthropic Opportunity](https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence-the-philanthropic-opportunity/) (Holden Karnofsky)\n\n- [A newcomer\u2019s guide to the technical AI safety field](https://www.alignmentforum.org/posts/5rsa37pBjo4Cf9fkE/a-newcomer-s-guide-to-the-technical-ai-safety-field) (Chin Ze Shen)\n\n- [Q & A: The future of artificial intelligence](https://people.eecs.berkeley.edu/~russell/research/future/q-and-a.html) (Stuart Russell)\n\n- [The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf) (Steve Omohundro)\n\n- [Threat Model Literature Review](https://www.alignmentforum.org/posts/wnnkD6P2k2TfHnNmt/threat-model-literature-review) (DeepMind's AGI safety team)\n\n- [AI Risk Intro 1: Advanced AI Might Be Very Bad](https://www.lesswrong.com/posts/bJgEMfiD48fEJJxjm/ai-risk-intro-1-advanced-ai-might-be-very-bad) (TheMcDouglas, LRudL)\n\n- [Distinguishing AI takeover scenarios](https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios) (Sam Clarke, Samuel Martin)\n\n- [Artificial intelligence is transforming our world \u2014 it is on all of us to make sure that it goes well](https://ourworldindata.org/ai-impact) (Max Roser)\n\n- [Intelligence Explosion: Evidence and Import](https://intelligence.org/files/IE-EI.pdf) (Luke Muehlhauser, Anna Salamon)\n\n- [The Value Learning Problem](https://intelligence.org/files/ValueLearningProblem.pdf) (Nate Soares)\n\n- [Uncontrollable AI as an Existential Risk](https://www.lesswrong.com/posts/gEchYntjSXk9KXorK/uncontrollable-ai-as-an-existential-risk) (Karl von Wendt)\n\n## Longer introductions\n\n- [The AI Revolution: The Road to Superintelligence](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) and [The AI Revolution: Our Immortality or Extinction](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html) + some [corrections from Luke Muehlhauser](https://lukemuehlhauser.com/a-reply-to-wait-but-why-on-machine-superintelligence/) (Tim Urban)\n\n- [Stampy wiki](https://ui.stampy.ai/) and[aisafety.info](https://aisafety.info/) ([Stampy team](https://stampy.ai/wiki/Get_involved))\n\n- [AI as a Positive and Negative Factor in Global Risk](https://intelligence.org/files/AIPosNegFactor.pdf) (Eliezer Yudkowsky)\n\n- [Extinction Risk from Artificial Intelligence](https://aisafety.wordpress.com/) (Michael Cohen)\n\n- [Set Sail For Fail? On AI risk](https://nintil.com/ai-safety) ([Jos\u00e9 Luis Ric\u00f3n Fern\u00e1ndez de la Puente](https://ricon.xyz/))\n\n- [A shift in arguments for AI risk](https://bayes.net/prioritising-ai/) (Tom Adamczewski)\n\n- [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/pdf/2206.13353.pdf) + [presentation](https://forum.effectivealtruism.org/posts/ChuABPEXmRumcJY57/video-and-transcript-of-presentation-on-existential-risk) (Joseph Carlsmith)\n\n- [AGI safety from first principles](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ) (Richard Ngo)\n\n- [Preventing an AI-related catastrophe](https://80000hours.org/problem-profiles/artificial-intelligence/?gclid=CjwKCAjwkaSaBhA4EiwALBgQaO-jCNCrpUMndnt67IxLF4N_QIiCrSFIwp7Gb8RyWVAdHQePo4fofRoC2vAQAvD_BwE) (Benjamin Hilton; see also [this summary](https://forum.effectivealtruism.org/posts/btFBFdYEn2PbuwHwt/summary-of-80k-s-ai-problem-profile))\n\n- [Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover](https://www.cold-takes.com/without-specific-countermeasures-the-easiest-path-to-transformative-ai-likely-leads-to-ai-takeover/) + [presentation](https://www.youtube.com/watch?v=EIhE84kH2QI) (Ajeya Cotra)\n\n- [The \u201cmost important century\u201d blog post series summary](https://www.cold-takes.com/most-important-century/#Summary) and the [\"implications of most important century\u201d posts](https://www.cold-takes.com/tag/implicationsofmostimportantcentury/) like [AI could defeat all of us combined](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/) and[Why would AI \u201caim\u201d to defeat humanity?](https://www.cold-takes.com/why-would-ai-aim-to-defeat-humanity/) and [How we could stumble into AI catastrophe](https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/)(Holden Karnofsky)\n\n- [Natural Selection Favors AIs over Humans](https://drive.google.com/file/d/1p4ZAuEYHL_21tqstJOGsMiG4xaRBtVcj/view) (Dan Hendrycks)\n\n- [Unsolved Problems in ML Safety](https://arxiv.org/abs/2109.13916) (Dan Hendrycks)\n\n- [X-Risk Analysis for AI Research](https://arxiv.org/abs/2206.05862) (Dan Hendrycks, Mantas Mazeika)\n\n- [Uncontrollability of AI](https://www.researchgate.net/publication/343812745_Uncontrollability_of_AI) (Roman Yampolskiy)\n\n- [Current work in AI alignment](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment) (Paul Christiano)\n\n- [CHAI bibliography](https://humancompatible.ai/bibliography)\n\n## Overviews of the research landscape:\n\n- [My Overview of the AI Alignment Landscape: A Bird's Eye View](https://www.lesswrong.com/posts/SQ9cZtfrzDJmw9A2m/my-overview-of-the-ai-alignment-landscape-a-bird-s-eye-view) (Neel Nanda)\n\n- [(My understanding of) What Everyone in Technical Alignment is Doing and Why](https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is) (Thomas Larsen, Eli Lifland) + [Alignment Org Cheat Sheet](https://www.lesswrong.com/posts/9TWReSDKyshfA66sz/alignment-org-cheat-sheet) (Thomas Larsen, Akash Wasil)\n\n- \u201c[What you can do concretely to help](https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help)\u201d section of \u201cPreventing an AI-related catastrophe\u201d (Benjamin Hilton, 80,000 Hours)\n\n- [A Brief Overview of AI Safety/Alignment Orgs, Fields, Researchers, and Resources for ML Researchers](https://forum.effectivealtruism.org/posts/xMzXbnpPeKWpTi3Gt/a-brief-overview-of-ai-safety-alignment-orgs-fields) (Austin Witte)\n\n- [An overview of 11 proposals for building safe advanced AI](https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai) (Evan Hubinger)\n\n- [2021 Alignment Literature Review and Charity Comparison](https://www.lesswrong.com/posts/C4tR3BEpuWviT7Sje/2021-ai-alignment-literature-review-and-charity-comparison) (Larks)\n\n- [AI Research Considerations for Human Existential Safety (ARCHES)](https://arxiv.org/abs/2006.04948) (Andrew Critch, David Krueger)\n\n- [AGI Safety Literature Review](https://arxiv.org/abs/1805.01109) (Tom Everitt, Gary Lea, Marcus Hutter)\n\n- [AI Alignment Research Overview](https://www.alignmentforum.org/posts/7GEviErBXcjJsbSeD/ai-alignment-research-overview-by-jacob-steinhardt) (Jacob Steinhardt)\n\n- [On how various plans miss the hard bits of the alignment challenge](https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment) (Nate Soares)\n\n- [Some AI research areas and their relevance to existential safety](https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1) (Andrew Critch)\n\n- [A newcomer\u2019s guide to the technical AI safety field](https://www.lesswrong.com/posts/5rsa37pBjo4Cf9fkE/a-newcomer-s-guide-to-the-technical-ai-safety-field) (zeshen)\n\n- [Anti-Literature Review](https://www.alignmentforum.org/posts/XtBJTFszs8oP3vXic/ai-x-risk-greater-than-35-mostly-based-on-a-recent-peer#Appendix_A__Anti_Literature_Review) from \u201cAI X-risk >35% mostly based on a recent peer-reviewed argument\u201d (Michael Cohen)\n\n## Podcasts and videos (see [aisafety.video](https://aisafety.video) for more)\n\n- [Eliezer Yudkowksy interview with Sam Harris](https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/)\n\n- [Richard Ngo](https://axrp.net/episode/2022/03/31/episode-13-first-principles-agi-safety-richard-ngo.html) and[Paul Christiano](https://axrp.net/episode/2021/12/02/episode-12-ai-xrisk-paul-christiano.html) on[AXRP](https://axrp.net/)\n\n- [Brian Christian](https://80000hours.org/podcast/episodes/brian-christian-the-alignment-problem/) and[Ben Garfinkel](https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/) on the [80,000 Hours Podcast](https://80000hours.org/podcast/)\n\n- [Ajeya Cotra](https://www.youtube.com/watch?v=IKFQfYaJ0AY) and[Rohin Shah](https://www.youtube.com/watch?v=_5xkh-Rh6Ec) on the [Future of Life Institute Podcast](https://futureoflife.org/project/future-of-life-institute-podcast/)\n\n- [Researcher Perceptions of Current and Future AI](https://www.youtube.com/watch?v=yl2nlejBcg0) (Vael Gates)\n\n- [Intro to AI Safety, Remastered](https://www.youtube.com/watch?v=pYXy-A4siMw) (Rob Miles)\n\n- [Ensuring smarter-than-human intelligence has a positive outcome](https://intelligence.org/2017/04/12/ensuring/) (Nate Soares)\n\n- [AI Alignment: Why It's Hard, and Where to Start](https://www.youtube.com/watch?v=EUjc1WuyPT8) (Eliezer Yudkowsky)\n\n- Some audio recordings of the readings above (e.g.[Cold Takes Audio](https://podcasts.apple.com/us/podcast/cold-takes-audio/id1580097837),[reading of 80k intro](https://podcasts.apple.com/us/podcast/preventing-an-ai-related-catastrophe-article/id1245002988?i=1000582699751),[EA Forum posts](https://forum.effectivealtruism.org/posts/K5Snxo5EhgmwJJjR2/announcing-audio-narrations-of-ea-forum-posts-1),[EA Radio](https://podcasts.apple.com/us/podcast/ea-radio/id1370275378),[Astral Codex Ten Podcast](https://sscpodcast.libsyn.com/),[Less Wrong Curated Podcast](https://www.lesswrong.com/posts/kDjKF2yFhFEWe4hgC/announcing-the-lesswrong-curated-podcast),[Nonlinear Library](https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library))\n\n## Other / misc:\n\n- **Books**: *The Alignment Problem* by Brian Christian, *Human Compatible* by Stuart Russell*, Superintelligence* by Nick Bostrom, *Life 3.0* by Max Tegmark,*[Smarter Than Us](https://smarterthan.us/toc/)* by Stuart Armstrong, [Better without AI](https://betterwithout.ai/) by [David Chapman](https://twitter.com/Meaningness/status/1625139350005764096), AI sections in *The Precipice* by Toby Ord and *What We Owe The Future* by William MacAskill\n\n- [Results of the AI Safety Arguments Competition](https://docs.google.com/spreadsheets/d/e/2PACX-1vRgIYiqiFevNu0m3bOzKeJ7S2ugkq2imYmbCicXPYtKTpRXKBMSZmfhbL-C_v_KQKob57e5QUtcuUqP/pubhtml)\n\n- [AI alignment resources](https://vkrakovna.wordpress.com/ai-safety-resources/) (Victoria Krakovna)\n\n- [Resources I sent to AI researchers about AI safety](https://forum.effectivealtruism.org/posts/8sAzgNcssH3mdb8ya/resources-i-send-to-ai-researchers-about-ai-safety) (Vael Gates)\n\n- [AI Risk Discussions: Resources](https://ai-risk-discussions.org/resources)\n\n- Wait But Why: [Part 1](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) -[Part 2](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html) -[Reply from Luke Muehlhauser](https://lukemuehlhauser.com/a-reply-to-wait-but-why-on-machine-superintelligence/)\n\n- 2021 MIRI Conversations: [Ngo-Yudkowsky](https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty) ([ACX summary](https://astralcodexten.substack.com/p/practically-a-book-review-yudkowsky)), [Christiano-Yudkowsky](https://www.lesswrong.com/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds) ([ACX summary](https://astralcodexten.substack.com/p/yudkowsky-contra-christiano-on-ai)), others\n\n- <iframe src=\"https://www.youtube.com/embed/yQE9KAbFhNY\" title=\"A Response to Steven Pinker on AI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n    \n     (Rob Miles)\n\n- [Arbital AI Alignment list](https://arbital.greaterwrong.com/explore/ai_alignment/)\n\n- [Our World In Data: AI](https://ourworldindata.org/artificial-intelligence#research-and-writing)\n\n- [Stuart Russell's collection of research and media appearances](https://people.eecs.berkeley.edu/~russell/research/future/)\n\n- [Nine Things You Should Know About AI](https://www.bbc.co.uk/programmes/articles/3pVB9hLv8TdGjSdJv4CmYjC/nine-things-you-should-know-about-ai) (Stuart Russell)\n\n- FAQs: [superintelligence](https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq) (Scott Alexander), [intelligence explosion](https://intelligence.org/ie-faq/) (Luke Muehlhauser)\n\n- [BlueDot Impact courses](https://www.agisafetyfundamentals.com/ai-alignment-curriculum)\n\n    - [AGI Safety Fundamentals resources page](https://www.agisafetyfundamentals.com/resources)\n\n    - See also [STS 10SI: Intro to AI Alignment Syllabus [Public]](https://docs.google.com/document/d/1NX0DlZRzD3NP7tBeLjMh76w7-w2s8SxV3wj0P7EYpKY/edit) from Stanford, a modified version of the Alignment Fundamentals curriculum\n\n- [Intro to ML Safety lectures](https://course.mlsafety.org/) and [online course](https://www.mlsafety.org/intro-to-ml-safety)\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Note that some of these introductions are from over 5 years ago. Given how quickly the field of AI progresses, some of these older introductions could use an update (e.g. Nick Bostrom\u2019s 2014 book *Superintelligence* has little focus on modern deep learning systems).\n\n## Quick reads (under ~10 minutes)\n\n- [Building safe artificial intelligence: specification, robustness, and assurance](https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1) (DeepMind Safety Research)\n\n- [Four Background Claims](https://intelligence.org/2015/07/24/four-background-claims/) (Nate Soares)\n\n- [Frequent arguments about alignment](https://www.alignmentforum.org/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment) (John Schulman)\n\n- [Of Myths And Moonshine](https://www.edge.org/conversation/the-myth-of-ai#26015) (Stuart Russell)\n\n- [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) (Wikipedia)\n\n- [Will AI really cause a catastrophe?](https://www.maisi.club/about) (Michigan AI Safety Initiative)\n\n- [Artificial Intelligence Safety](https://erafellowship.org/ai-safety) (Existential Risk Alliance)\n\n- [Summary writeup of AGI timeline/risk projections as of Oct 2022](https://docs.google.com/document/d/1j7tZ1Xf7-l2k2qr2t3MFwi-IkhXNdzA2N2WZBfcghsM/edit#) (Kelsey Theriault)\n\n- [a casual intro to AI doom and alignment](https://carado.moe/ai-doom.html) (Tamsin Leake)\n\n- [Global risk from deep learning: 1 - The case for risk](https://www.danieldewey.net/risk/case.html) (Daniel Dewey)\n\n- [Explore Your AI Risk Perspectives: An Interactive Walkthrough of Researchers' Most Frequent Interview Responses](https://ai-risk-discussions.org/perspectives/introduction) (AI Risk Discussions)\n\n- [Marius alignment pitch](https://docs.google.com/document/d/18y0x3ogQau0CyN5a9QYaAUCca8C4bHdEWBK5f4jlO7k/edit#) (Marius Hobbhahn)\n\n- [Intro to AI Safety](https://aizi.substack.com/p/intro-to-ai-safety) (Robert Huben)\n\n- [Basics of AI Wiping Out All Value in the Universe, Take 1](https://www.lesswrong.com/posts/WkchhorbLsSMbLacZ/ai-1-sydney-and-bing#Basics_of_AI_Wiping_Out_All_Value_in_the_Universe__Take_1) (Zvi Mowshowitz)\n\n- [AI Risk](https://benchmarking.mlsafety.org/about) (SafeBench competition)\n\n- [Why Uncontrollable AI Looks More Likely Than Ever](https://time.com/6258483/uncontrollable-ai-agi-risks/) (Otto Barten, Roman Yampolskiy)\n\n- [How AI could accidentally extinguish humankind](https://www.washingtonpost.com/opinions/2022/08/31/artificial-intelligence-worst-case-scenario-extinction/) (\u00c9mile Torres)\n\n## Shorter introductions\n\n- [The case for taking AI seriously](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment) (or a similar argument in [500 words](https://www.vox.com/future-perfect/2019/2/12/18202466/ai-artificial-intelligence-humanity-threat)) and [AI experts are increasingly afraid of what they\u2019re creating](https://www.vox.com/the-highlight/23447596/artificial-intelligence-agi-openai-gpt3-existential-risk-human-extinction) (Kelsey Piper)\n\n- [The alignment problem from a deep learning perspective](https://arxiv.org/abs/2209.00626) (Richard Ngo, Lawrence Chan, S\u00f6ren Mindermann)\n\n- [More Is Different for AI](https://bounded-regret.ghost.io/more-is-different-for-ai/) blog post series (Jacob Steinhardt)\n\n- [Why I Think More NLP Researchers Should Engage with AI Safety Concerns](https://wp.nyu.edu/arg/why-ai-safety/) (Sam Bowman)\n\n- [How to navigate the AI apocalypse as a sane person](https://erikhoel.substack.com/p/how-to-navigate-the-ai-apocalypse) (Eric Hoel)\n\n- [Why alignment could be hard with modern deep learning](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/) (Ajeya Cotra)\n\n- [The Need For Work On Technical AI Alignment](https://www.agisafetyfundamentals.com/alignment-introduction) (Daniel Eth)\n\n- [Ethical Issues in Advanced Artificial Intelligence](https://nickbostrom.com/ethics/ai) (Nick Bostrom)\n\n- [Benefits & Risks of Artificial Intelligence](https://futureoflife.org/ai/benefits-risks-of-artificial-intelligence/) (Ariel Conn)\n\n- [Why worry about future AI?](https://www.gleech.org/ai-risk) (Gavin Leech)\n\n- [AGI Ruin: A list of lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) (Eliezer Yudkowsky)\n\n- [No Time Like The Present For AI Safety Work](https://slatestarcodex.com/2015/05/29/no-time-like-the-present-for-ai-safety-work/) (Scott Alexander)\n\n- [Potential Risks from Advanced Artificial Intelligence: The Philanthropic Opportunity](https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence-the-philanthropic-opportunity/) (Holden Karnofsky)\n\n- [A newcomer\u2019s guide to the technical AI safety field](https://www.alignmentforum.org/posts/5rsa37pBjo4Cf9fkE/a-newcomer-s-guide-to-the-technical-ai-safety-field) (Chin Ze Shen)\n\n- [Q & A: The future of artificial intelligence](https://people.eecs.berkeley.edu/~russell/research/future/q-and-a.html) (Stuart Russell)\n\n- [The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf) (Steve Omohundro)\n\n- [Threat Model Literature Review](https://www.alignmentforum.org/posts/wnnkD6P2k2TfHnNmt/threat-model-literature-review) (DeepMind's AGI safety team)\n\n- [AI Risk Intro 1: Advanced AI Might Be Very Bad](https://www.lesswrong.com/posts/bJgEMfiD48fEJJxjm/ai-risk-intro-1-advanced-ai-might-be-very-bad) (TheMcDouglas, LRudL)\n\n- [Distinguishing AI takeover scenarios](https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios) (Sam Clarke, Samuel Martin)\n\n- [Artificial intelligence is transforming our world \u2014 it is on all of us to make sure that it goes well](https://ourworldindata.org/ai-impact) (Max Roser)\n\n- [Intelligence Explosion: Evidence and Import](https://intelligence.org/files/IE-EI.pdf) (Luke Muehlhauser, Anna Salamon)\n\n- [The Value Learning Problem](https://intelligence.org/files/ValueLearningProblem.pdf) (Nate Soares)\n\n- [Uncontrollable AI as an Existential Risk](https://www.lesswrong.com/posts/gEchYntjSXk9KXorK/uncontrollable-ai-as-an-existential-risk) (Karl von Wendt)\n\n## Longer introductions\n\n- [The AI Revolution: The Road to Superintelligence](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) and [The AI Revolution: Our Immortality or Extinction](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html) + some [corrections from Luke Muehlhauser](https://lukemuehlhauser.com/a-reply-to-wait-but-why-on-machine-superintelligence/) (Tim Urban)\n\n- [Stampy wiki](https://ui.stampy.ai/) and[aisafety.info](https://aisafety.info/) ([Stampy team](https://stampy.ai/wiki/Get_involved))\n\n- [AI as a Positive and Negative Factor in Global Risk](https://intelligence.org/files/AIPosNegFactor.pdf) (Eliezer Yudkowsky)\n\n- [Extinction Risk from Artificial Intelligence](https://aisafety.wordpress.com/) (Michael Cohen)\n\n- [Set Sail For Fail? On AI risk](https://nintil.com/ai-safety) ([Jos\u00e9 Luis Ric\u00f3n Fern\u00e1ndez de la Puente](https://ricon.xyz/))\n\n- [A shift in arguments for AI risk](https://bayes.net/prioritising-ai/) (Tom Adamczewski)\n\n- [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/pdf/2206.13353.pdf) + [presentation](https://forum.effectivealtruism.org/posts/ChuABPEXmRumcJY57/video-and-transcript-of-presentation-on-existential-risk) (Joseph Carlsmith)\n\n- [AGI safety from first principles](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ) (Richard Ngo)\n\n- [Preventing an AI-related catastrophe](https://80000hours.org/problem-profiles/artificial-intelligence/?gclid=CjwKCAjwkaSaBhA4EiwALBgQaO-jCNCrpUMndnt67IxLF4N_QIiCrSFIwp7Gb8RyWVAdHQePo4fofRoC2vAQAvD_BwE) (Benjamin Hilton; see also [this summary](https://forum.effectivealtruism.org/posts/btFBFdYEn2PbuwHwt/summary-of-80k-s-ai-problem-profile))\n\n- [Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover](https://www.cold-takes.com/without-specific-countermeasures-the-easiest-path-to-transformative-ai-likely-leads-to-ai-takeover/) + [presentation](https://www.youtube.com/watch?v=EIhE84kH2QI) (Ajeya Cotra)\n\n- [The \u201cmost important century\u201d blog post series summary](https://www.cold-takes.com/most-important-century/#Summary) and the [\"implications of most important century\u201d posts](https://www.cold-takes.com/tag/implicationsofmostimportantcentury/) like [AI could defeat all of us combined](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/) and[Why would AI \u201caim\u201d to defeat humanity?](https://www.cold-takes.com/why-would-ai-aim-to-defeat-humanity/) and [How we could stumble into AI catastrophe](https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/)(Holden Karnofsky)\n\n- [Natural Selection Favors AIs over Humans](https://drive.google.com/file/d/1p4ZAuEYHL_21tqstJOGsMiG4xaRBtVcj/view) (Dan Hendrycks)\n\n- [Unsolved Problems in ML Safety](https://arxiv.org/abs/2109.13916) (Dan Hendrycks)\n\n- [X-Risk Analysis for AI Research](https://arxiv.org/abs/2206.05862) (Dan Hendrycks, Mantas Mazeika)\n\n- [Uncontrollability of AI](https://www.researchgate.net/publication/343812745_Uncontrollability_of_AI) (Roman Yampolskiy)\n\n- [Current work in AI alignment](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment) (Paul Christiano)\n\n- [CHAI bibliography](https://humancompatible.ai/bibliography)\n\n## Overviews of the research landscape:\n\n- [My Overview of the AI Alignment Landscape: A Bird's Eye View](https://www.lesswrong.com/posts/SQ9cZtfrzDJmw9A2m/my-overview-of-the-ai-alignment-landscape-a-bird-s-eye-view) (Neel Nanda)\n\n- [(My understanding of) What Everyone in Technical Alignment is Doing and Why](https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is) (Thomas Larsen, Eli Lifland) + [Alignment Org Cheat Sheet](https://www.lesswrong.com/posts/9TWReSDKyshfA66sz/alignment-org-cheat-sheet) (Thomas Larsen, Akash Wasil)\n\n- \u201c[What you can do concretely to help](https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help)\u201d section of \u201cPreventing an AI-related catastrophe\u201d (Benjamin Hilton, 80,000 Hours)\n\n- [A Brief Overview of AI Safety/Alignment Orgs, Fields, Researchers, and Resources for ML Researchers](https://forum.effectivealtruism.org/posts/xMzXbnpPeKWpTi3Gt/a-brief-overview-of-ai-safety-alignment-orgs-fields) (Austin Witte)\n\n- [An overview of 11 proposals for building safe advanced AI](https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai) (Evan Hubinger)\n\n- [2021 Alignment Literature Review and Charity Comparison](https://www.lesswrong.com/posts/C4tR3BEpuWviT7Sje/2021-ai-alignment-literature-review-and-charity-comparison) (Larks)\n\n- [AI Research Considerations for Human Existential Safety (ARCHES)](https://arxiv.org/abs/2006.04948) (Andrew Critch, David Krueger)\n\n- [AGI Safety Literature Review](https://arxiv.org/abs/1805.01109) (Tom Everitt, Gary Lea, Marcus Hutter)\n\n- [AI Alignment Research Overview](https://www.alignmentforum.org/posts/7GEviErBXcjJsbSeD/ai-alignment-research-overview-by-jacob-steinhardt) (Jacob Steinhardt)\n\n- [On how various plans miss the hard bits of the alignment challenge](https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment) (Nate Soares)\n\n- [Some AI research areas and their relevance to existential safety](https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1) (Andrew Critch)\n\n- [A newcomer\u2019s guide to the technical AI safety field](https://www.lesswrong.com/posts/5rsa37pBjo4Cf9fkE/a-newcomer-s-guide-to-the-technical-ai-safety-field) (zeshen)\n\n- [Anti-Literature Review](https://www.alignmentforum.org/posts/XtBJTFszs8oP3vXic/ai-x-risk-greater-than-35-mostly-based-on-a-recent-peer#Appendix_A__Anti_Literature_Review) from \u201cAI X-risk >35% mostly based on a recent peer-reviewed argument\u201d (Michael Cohen)\n\n## Podcasts and videos (see [aisafety.video](https://aisafety.video) for more)\n\n- [Eliezer Yudkowksy interview with Sam Harris](https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/)\n\n- [Richard Ngo](https://axrp.net/episode/2022/03/31/episode-13-first-principles-agi-safety-richard-ngo.html) and[Paul Christiano](https://axrp.net/episode/2021/12/02/episode-12-ai-xrisk-paul-christiano.html) on[AXRP](https://axrp.net/)\n\n- [Brian Christian](https://80000hours.org/podcast/episodes/brian-christian-the-alignment-problem/) and[Ben Garfinkel](https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/) on the [80,000 Hours Podcast](https://80000hours.org/podcast/)\n\n- [Ajeya Cotra](https://www.youtube.com/watch?v=IKFQfYaJ0AY) and[Rohin Shah](https://www.youtube.com/watch?v=_5xkh-Rh6Ec) on the [Future of Life Institute Podcast](https://futureoflife.org/project/future-of-life-institute-podcast/)\n\n- [Researcher Perceptions of Current and Future AI](https://www.youtube.com/watch?v=yl2nlejBcg0) (Vael Gates)\n\n- [Intro to AI Safety, Remastered](https://www.youtube.com/watch?v=pYXy-A4siMw) (Rob Miles)\n\n- [Ensuring smarter-than-human intelligence has a positive outcome](https://intelligence.org/2017/04/12/ensuring/) (Nate Soares)\n\n- [AI Alignment: Why It's Hard, and Where to Start](https://www.youtube.com/watch?v=EUjc1WuyPT8) (Eliezer Yudkowsky)\n\n- Some audio recordings of the readings above (e.g.[Cold Takes Audio](https://podcasts.apple.com/us/podcast/cold-takes-audio/id1580097837),[reading of 80k intro](https://podcasts.apple.com/us/podcast/preventing-an-ai-related-catastrophe-article/id1245002988?i=1000582699751),[EA Forum posts](https://forum.effectivealtruism.org/posts/K5Snxo5EhgmwJJjR2/announcing-audio-narrations-of-ea-forum-posts-1),[EA Radio](https://podcasts.apple.com/us/podcast/ea-radio/id1370275378),[Astral Codex Ten Podcast](https://sscpodcast.libsyn.com/),[Less Wrong Curated Podcast](https://www.lesswrong.com/posts/kDjKF2yFhFEWe4hgC/announcing-the-lesswrong-curated-podcast),[Nonlinear Library](https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library))\n\n## Other / misc:\n\n- **Books**: *The Alignment Problem* by Brian Christian, *Human Compatible* by Stuart Russell*, Superintelligence* by Nick Bostrom, *Life 3.0* by Max Tegmark,*[Smarter Than Us](https://smarterthan.us/toc/)* by Stuart Armstrong, [Better without AI](https://betterwithout.ai/) by [David Chapman](https://twitter.com/Meaningness/status/1625139350005764096), AI sections in *The Precipice* by Toby Ord and *What We Owe The Future* by William MacAskill\n\n- [Results of the AI Safety Arguments Competition](https://docs.google.com/spreadsheets/d/e/2PACX-1vRgIYiqiFevNu0m3bOzKeJ7S2ugkq2imYmbCicXPYtKTpRXKBMSZmfhbL-C_v_KQKob57e5QUtcuUqP/pubhtml)\n\n- [AI alignment resources](https://vkrakovna.wordpress.com/ai-safety-resources/) (Victoria Krakovna)\n\n- [Resources I sent to AI researchers about AI safety](https://forum.effectivealtruism.org/posts/8sAzgNcssH3mdb8ya/resources-i-send-to-ai-researchers-about-ai-safety) (Vael Gates)\n\n- [AI Risk Discussions: Resources](https://ai-risk-discussions.org/resources)\n\n- Wait But Why: [Part 1](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) -[Part 2](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html) -[Reply from Luke Muehlhauser](https://lukemuehlhauser.com/a-reply-to-wait-but-why-on-machine-superintelligence/)\n\n- 2021 MIRI Conversations: [Ngo-Yudkowsky](https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty) ([ACX summary](https://astralcodexten.substack.com/p/practically-a-book-review-yudkowsky)), [Christiano-Yudkowsky](https://www.lesswrong.com/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds) ([ACX summary](https://astralcodexten.substack.com/p/yudkowsky-contra-christiano-on-ai)), others\n\n- <iframe src=\"https://www.youtube.com/embed/yQE9KAbFhNY\" title=\"A Response to Steven Pinker on AI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n    \n     (Rob Miles)\n\n- [Arbital AI Alignment list](https://arbital.greaterwrong.com/explore/ai_alignment/)\n\n- [Our World In Data: AI](https://ourworldindata.org/artificial-intelligence#research-and-writing)\n\n- [Stuart Russell's collection of research and media appearances](https://people.eecs.berkeley.edu/~russell/research/future/)\n\n- [Nine Things You Should Know About AI](https://www.bbc.co.uk/programmes/articles/3pVB9hLv8TdGjSdJv4CmYjC/nine-things-you-should-know-about-ai) (Stuart Russell)\n\n- FAQs: [superintelligence](https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq) (Scott Alexander), [intelligence explosion](https://intelligence.org/ie-faq/) (Luke Muehlhauser)\n\n- [BlueDot Impact courses](https://www.agisafetyfundamentals.com/ai-alignment-curriculum)\n\n    - [AGI Safety Fundamentals resources page](https://www.agisafetyfundamentals.com/resources)\n\n    - See also [STS 10SI: Intro to AI Alignment Syllabus [Public]](https://docs.google.com/document/d/1NX0DlZRzD3NP7tBeLjMh76w7-w2s8SxV3wj0P7EYpKY/edit) from Stanford, a modified version of the Alignment Fundamentals curriculum\n\n- [Intro to ML Safety lectures](https://course.mlsafety.org/) and [online course](https://www.mlsafety.org/intro-to-ml-safety)\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "86J8", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:23:01.664+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-3599a29831f82927316a95fb859a3af9b4cb37d2211568161de899f81ed77978", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3599a29831f82927316a95fb859a3af9b4cb37d2211568161de899f81ed77978", "name": "Another full flow test", "index": 431, "createdAt": "2023-01-28T15:26:38.472Z", "updatedAt": "2023-02-22T21:08:58.516Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3599a29831f82927316a95fb859a3af9b4cb37d2211568161de899f81ed77978", "values": {"File": "Another full flow test", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Another full flow test", "Link": "https://docs.google.com/document/d/10KgjprXJmvPXdEnSTciMY6DVtISz8HHJS4qAO5Aop3E/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-28T16:26:18.882+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-01-30T22:40:10.229+01:00", "Status": "Uncategorized", "Edit Answer": "Another full flow test", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "-1", "Source Link": "", "aisafety.info Link": "Another full flow test", "Source": "", "All Phrasings": "Another full flow test\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "<answer goes here>\n\n\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "<answer goes here>\n\n\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "-1", "Related Answers": "", "Doc Last Ingested": "", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-d9708230338fdc637d44cd11c0111427f9d345c072fa8d1f96b81b0ab6e034a3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d9708230338fdc637d44cd11c0111427f9d345c072fa8d1f96b81b0ab6e034a3", "name": "test title formatting", "index": 429, "createdAt": "2023-01-28T15:11:07.702Z", "updatedAt": "2023-01-30T22:13:55.556Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d9708230338fdc637d44cd11c0111427f9d345c072fa8d1f96b81b0ab6e034a3", "values": {"File": "test title formatting", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "test title formatting", "Link": "https://docs.google.com/document/d/1IwybjodbjvsTOV7nZyMzsPwjPQT0iEMQCXRYJVdwcbI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-28T16:10:26.574+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-01-28T16:10:27.685+01:00", "Status": "Uncategorized", "Edit Answer": "test title formatting", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "-1", "Source Link": "", "aisafety.info Link": "test title formatting", "Source": "", "All Phrasings": "test title formatting\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "-1", "Related Answers": "", "Doc Last Ingested": "", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-4e0f9810e19a1dacb5116977fbcac1829b639678fccbe48105ddb2159ae4e9be", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4e0f9810e19a1dacb5116977fbcac1829b639678fccbe48105ddb2159ae4e9be", "name": "Testingnewquestionsflow", "index": 430, "createdAt": "2023-01-28T15:11:07.702Z", "updatedAt": "2023-01-31T00:06:43.346Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4e0f9810e19a1dacb5116977fbcac1829b639678fccbe48105ddb2159ae4e9be", "values": {"File": "Testingnewquestionsflow", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "Testingnewquestionsflow", "Link": "https://docs.google.com/document/d/1s6b1Pme_7nsqOnnDEVdNApDa5k3ZFy37rJxPcOLy7Cc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-28T16:09:21.505+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-01-28T16:09:22.761+01:00", "Status": "Uncategorized", "Edit Answer": "Testingnewquestionsflow", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "-1", "Source Link": "", "aisafety.info Link": "Testingnewquestionsflow", "Source": "", "All Phrasings": "Testingnewquestionsflow\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "-1", "Related Answers": "", "Doc Last Ingested": "", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-b4931edd91b7fe84297e92fb5cf773cb5fefce716bc431e132f5980142cbb9e0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b4931edd91b7fe84297e92fb5cf773cb5fefce716bc431e132f5980142cbb9e0", "name": "What is Redwood Research's strategy?", "index": 409, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:27:52.342Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b4931edd91b7fe84297e92fb5cf773cb5fefce716bc431e132f5980142cbb9e0", "values": {"File": "What is Redwood Research's strategy?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Redwood Research's strategy?", "Link": "https://docs.google.com/document/d/1eeaM-V8_p5xjog91_j5RLRgeOIIDTleBtjwmAbnCzJU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:46:38.935+01:00", "Related Answers DO NOT EDIT": "What is interpretability and what approaches are there?", "Tags": "AISafety.careers", "Doc Last Edited": "2023-03-06T17:11:08.370+01:00", "Status": "In progress", "Edit Answer": "What is Redwood Research's strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85E4", "Source Link": "", "aisafety.info Link": "What is Redwood Research's strategy?", "Source": "", "All Phrasings": "What is Redwood Research's strategy?\n", "Initial Order": "", "Related IDs": "8241", "Rich Text DO NOT EDIT": "[Redwood Research](https://www.redwoodresearch.org/) focuses on trying out methods for interpreting and/or aligning current AI systems, identifying methods that seem promising, and considering theoretical arguments for why we might expect those methods to continue to work even as systems become much more intelligent. Some of their research includes:\n\n- *Causal scrubbing,* a method for evaluating interpretability hypotheses about an AI system. Briefly: imagine we have some guess about which parts of a model do the calculations needed for the model to exhibit a particular behavior \u2014\u00a0e.g., for an image classifier, \"I think these nodes over here are for identifying dogs\". Using some clever math, we can look through the model, identify which parts of the neural network our hypothesis implies \"shouldn't matter\" for that specific behavior, and \"scrub\" the model by replacing the activation values that \"shouldn't matter\" with values corresponding to a random input sample. Then, we can check whether the scrubbed model still exhibits the behavior we expected for the relevant input (e.g. correctly labeling a dog). If the hypothesis was a good one, the behavior of the scrubbed model should match the unscrubbed one. (Read much more [here](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing).)\n\n- Doing interpretability work on large language models: for instance, see [this paper](https://arxiv.org/abs/2211.00593) about how Redwood identified the circuit that [GPT-2](https://en.wikipedia.org/wiki/GPT-2) uses to figure out the proper indirect object of sentence.\n\n- *Adversarial training for high-stakes reliability*. Redwood tested out whether \"adversarial training\" \u2014\u00a0essentially, giving a model inputs specifically chosen to try to get it to produce undesirable outputs, then training the model on those outputs as examples of what to *not* do in the future \u2014\u00a0could significantly increase the model's reliability. In particular, Redwood aimed to produce models reliable enough that they could be used for \"high stakes\" tasks where a single failure would be catastrophic. To test this idea, Redwood used a language model which outputted completions to user prompts, and tried to fine-tune the model, using adversarial training, such that it would never complete a prompt in a way that involved \"injury\" (e.g. describing or implying someone getting hurt). Read [the paper](https://arxiv.org/abs/2205.01663) for more.\n\nRedwood also maintains the [Rust Circuit Library](https://github.com/redwoodresearch/rust_circuit_public), intended to help researchers to do interpretability work on neural networks.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "[Redwood Research](https://www.redwoodresearch.org/) focuses on trying out methods for interpreting and/or aligning current AI systems, identifying methods that seem promising, and considering theoretical arguments for why we might expect those methods to continue to work even as systems become much more intelligent. Some of their research includes:\n\n- *Causal scrubbing,* a method for evaluating interpretability hypotheses about an AI system. Briefly: imagine we have some guess about which parts of a model do the calculations needed for the model to exhibit a particular behavior \u2014\u00a0e.g., for an image classifier, \"I think these nodes over here are for identifying dogs\". Using some clever math, we can look through the model, identify which parts of the neural network our hypothesis implies \"shouldn't matter\" for that specific behavior, and \"scrub\" the model by replacing the activation values that \"shouldn't matter\" with values corresponding to a random input sample. Then, we can check whether the scrubbed model still exhibits the behavior we expected for the relevant input (e.g. correctly labeling a dog). If the hypothesis was a good one, the behavior of the scrubbed model should match the unscrubbed one. (Read much more [here](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing).)\n\n- Doing interpretability work on large language models: for instance, see [this paper](https://arxiv.org/abs/2211.00593) about how Redwood identified the circuit that [GPT-2](https://en.wikipedia.org/wiki/GPT-2) uses to figure out the proper indirect object of sentence.\n\n- *Adversarial training for high-stakes reliability*. Redwood tested out whether \"adversarial training\" \u2014\u00a0essentially, giving a model inputs specifically chosen to try to get it to produce undesirable outputs, then training the model on those outputs as examples of what to *not* do in the future \u2014\u00a0could significantly increase the model's reliability. In particular, Redwood aimed to produce models reliable enough that they could be used for \"high stakes\" tasks where a single failure would be catastrophic. To test this idea, Redwood used a language model which outputted completions to user prompts, and tried to fine-tune the model, using adversarial training, such that it would never complete a prompt in a way that involved \"injury\" (e.g. describing or implying someone getting hurt). Read [the paper](https://arxiv.org/abs/2205.01663) for more.\n\nRedwood also maintains the [Rust Circuit Library](https://github.com/redwoodresearch/rust_circuit_public), intended to help researchers to do interpretability work on neural networks.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85E4", "Related Answers": "What is interpretability and what approaches are there?", "Doc Last Ingested": "2023-03-14T23:23:03.720+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-c117a63a2ff7930a6b9dd3bb9fccd49975109f8acf8df5c8a8d55f3a515542cf", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c117a63a2ff7930a6b9dd3bb9fccd49975109f8acf8df5c8a8d55f3a515542cf", "name": "What is Ought's strategy?", "index": 410, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:27:56.737Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c117a63a2ff7930a6b9dd3bb9fccd49975109f8acf8df5c8a8d55f3a515542cf", "values": {"File": "What is Ought's strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is Ought's strategy?", "Link": "https://docs.google.com/document/d/1M_7kturuK1_KF0vyZxorvlULp-bbctnFiGrzDscgjBc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:46:16.510+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:46:17.435+01:00", "Status": "Not started", "Edit Answer": "What is Ought's strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85E5", "Source Link": "", "aisafety.info Link": "What is Ought's strategy?", "Source": "", "All Phrasings": "What is Ought's strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85E5", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:23:12.987+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-87b00c9c64acfa19736e153dd1deec8cd0aee584749422ea82cf09992a51ba5e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-87b00c9c64acfa19736e153dd1deec8cd0aee584749422ea82cf09992a51ba5e", "name": "What is OpenAI's research strategy?", "index": 411, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:27:59.607Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-87b00c9c64acfa19736e153dd1deec8cd0aee584749422ea82cf09992a51ba5e", "values": {"File": "What is OpenAI's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is OpenAI's research strategy?", "Link": "https://docs.google.com/document/d/1G7AubNsrLJmfCzybNIOIv67GTTySOgGRLtUwg8LGtjs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:45:52.180+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:45:52.957+01:00", "Status": "Not started", "Edit Answer": "What is OpenAI's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85E6", "Source Link": "", "aisafety.info Link": "What is OpenAI's research strategy?", "Source": "", "All Phrasings": "What is OpenAI's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85E6", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:23:23.908+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-1e9d53546ebc083c3c0cc8bb2fb8c2c8cef35ab0d29e233ca6e4d9b2020cfce4", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1e9d53546ebc083c3c0cc8bb2fb8c2c8cef35ab0d29e233ca6e4d9b2020cfce4", "name": "What is the Long-Term Future Fund's strategy?", "index": 412, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:28:02.406Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1e9d53546ebc083c3c0cc8bb2fb8c2c8cef35ab0d29e233ca6e4d9b2020cfce4", "values": {"File": "What is the Long-Term Future Fund's strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is the Long-Term Future Fund's strategy?", "Link": "https://docs.google.com/document/d/1Qlp8AhQJ7kifpRH1hklbVzFbg5hxRRq5g2iGvYaNMFQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:45:32.631+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:45:33.337+01:00", "Status": "Not started", "Edit Answer": "What is the Long-Term Future Fund's strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85E7", "Source Link": "", "aisafety.info Link": "What is the Long-Term Future Fund's strategy?", "Source": "", "All Phrasings": "What is the Long-Term Future Fund's strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85E7", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:23:32.983+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-ca5143de5822ca958d87892f4f7905cc61261cfaf457ecfaccefb1a40ca0f28c", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ca5143de5822ca958d87892f4f7905cc61261cfaf457ecfaccefb1a40ca0f28c", "name": "What is the Future of Humanity Insitute's research strategy?", "index": 413, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:28:05.726Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ca5143de5822ca958d87892f4f7905cc61261cfaf457ecfaccefb1a40ca0f28c", "values": {"File": "What is the Future of Humanity Insitute's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is the Future of Humanity Insitute's research strategy?", "Link": "https://docs.google.com/document/d/1QvHh5uZ6dss9vkQVPGgAoGzqo5Puq_Mv801WE76Fvpc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:44:22.471+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:44:23.428+01:00", "Status": "Not started", "Edit Answer": "What is the Future of Humanity Insitute's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85E8", "Source Link": "", "aisafety.info Link": "What is the Future of Humanity Insitute's research strategy?", "Source": "", "All Phrasings": "What is the Future of Humanity Insitute's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85E8", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:23:36.939+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-5ec4ebc33ad4387389f8b82ca5909f8137b0afa600f440f4dc8eb478dccbb08c", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5ec4ebc33ad4387389f8b82ca5909f8137b0afa600f440f4dc8eb478dccbb08c", "name": "What is FAR AI's research strategy?", "index": 414, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:28:09.182Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5ec4ebc33ad4387389f8b82ca5909f8137b0afa600f440f4dc8eb478dccbb08c", "values": {"File": "What is FAR AI's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is FAR AI's research strategy?", "Link": "https://docs.google.com/document/d/1-wIvEjOuyaqmgDo9BCcyHBGYeo4vQebnGBdUonMKST8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:43:54.550+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:43:55.502+01:00", "Status": "Not started", "Edit Answer": "What is FAR AI's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85E9", "Source Link": "", "aisafety.info Link": "What is FAR AI's research strategy?", "Source": "", "All Phrasings": "What is FAR AI's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85E9", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:23:39.218+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-29a39aeb097306b49ce256e51f173ee938568a49a300482ca4b08814f3e8339b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-29a39aeb097306b49ce256e51f173ee938568a49a300482ca4b08814f3e8339b", "name": "What is Encultured AI's strategy?", "index": 415, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:28:13.963Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-29a39aeb097306b49ce256e51f173ee938568a49a300482ca4b08814f3e8339b", "values": {"File": "What is Encultured AI's strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is Encultured AI's strategy?", "Link": "https://docs.google.com/document/d/11GwgJRhcAe-Rq5GeIcvslVeJzDqF3mccMfWGknHghpI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:43:00.374+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:43:01.115+01:00", "Status": "Not started", "Edit Answer": "What is Encultured AI's strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EA", "Source Link": "", "aisafety.info Link": "What is Encultured AI's strategy?", "Source": "", "All Phrasings": "What is Encultured AI's strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EA", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:23:41.994+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-6ed51ed4e150a331c7688d328a67f92450cde80bceb1f8d66dcb412d8a084a92", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-6ed51ed4e150a331c7688d328a67f92450cde80bceb1f8d66dcb412d8a084a92", "name": "What is DeepMind's research strategy?", "index": 416, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:28:18.811Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-6ed51ed4e150a331c7688d328a67f92450cde80bceb1f8d66dcb412d8a084a92", "values": {"File": "What is DeepMind's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is DeepMind's research strategy?", "Link": "https://docs.google.com/document/d/1nEHZCybW9y9KeUG7ZfN0kDst4o0j5IW_p4YGnZVNK34/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:42:33.021+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:42:33.788+01:00", "Status": "Not started", "Edit Answer": "What is DeepMind's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EB", "Source Link": "", "aisafety.info Link": "What is DeepMind's research strategy?", "Source": "", "All Phrasings": "What is DeepMind's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EB", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:23:44.451+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-9a0ed59bc089ed0f3f271adb96c7094679726ec8db679786448c021304b9330e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-9a0ed59bc089ed0f3f271adb96c7094679726ec8db679786448c021304b9330e", "name": "What is Conjecture's research strategy?", "index": 417, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:28:21.369Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-9a0ed59bc089ed0f3f271adb96c7094679726ec8db679786448c021304b9330e", "values": {"File": "What is Conjecture's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is Conjecture's research strategy?", "Link": "https://docs.google.com/document/d/1VkusDsFPYu8uCADjR8jI3ZQJU83eS6X6LrcoMOAwyMI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:42:12.205+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:42:12.933+01:00", "Status": "Not started", "Edit Answer": "What is Conjecture's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EC", "Source Link": "", "aisafety.info Link": "What is Conjecture's research strategy?", "Source": "", "All Phrasings": "What is Conjecture's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EC", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:23:47.112+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-2788a369c56ab50b2cf78fe560c927f54898dd052d0c629949d52413417d5a76", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2788a369c56ab50b2cf78fe560c927f54898dd052d0c629949d52413417d5a76", "name": "What is the Computational and Biological Learning lab's research strategy?", "index": 418, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:28:23.820Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2788a369c56ab50b2cf78fe560c927f54898dd052d0c629949d52413417d5a76", "values": {"File": "What is the Computational and Biological Learning lab's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is the Computational and Biological Learning lab's research strategy?", "Link": "https://docs.google.com/document/d/1ijsfv_HGltMlYqF0QJUYdLMT9YczjXU4vx2kMs2WAPc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:41:43.022+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:41:44.072+01:00", "Status": "Not started", "Edit Answer": "What is the Computational and Biological Learning lab's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85ED", "Source Link": "", "aisafety.info Link": "What is the Computational and Biological Learning lab's research strategy?", "Source": "", "All Phrasings": "What is the Computational and Biological Learning lab's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85ED", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:23:50.501+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-8ccbefc7ca86cde37c7abf6db7e32b7d9837f7d38b43429e89599de5507986cb", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8ccbefc7ca86cde37c7abf6db7e32b7d9837f7d38b43429e89599de5507986cb", "name": "What is the Center on Long-Term Risk's research strategy?", "index": 419, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:28:26.256Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8ccbefc7ca86cde37c7abf6db7e32b7d9837f7d38b43429e89599de5507986cb", "values": {"File": "What is the Center on Long-Term Risk's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is the Center on Long-Term Risk's research strategy?", "Link": "https://docs.google.com/document/d/1iSzODz7wUbLlShSqzSj4BWPFcNRwfGUWCPtzWN0Sgb0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:41:07.305+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:41:08.054+01:00", "Status": "Not started", "Edit Answer": "What is the Center on Long-Term Risk's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EE", "Source Link": "", "aisafety.info Link": "What is the Center on Long-Term Risk's research strategy?", "Source": "", "All Phrasings": "What is the Center on Long-Term Risk's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EE", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:23:53.467+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-6a5637ffde175c28830c9cce817c6bffcb5630c4c079a74d0d896bd33b4d572a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-6a5637ffde175c28830c9cce817c6bffcb5630c4c079a74d0d896bd33b4d572a", "name": "What is the Center for Human-Compatible AI's research strategy?", "index": 420, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:28:49.214Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-6a5637ffde175c28830c9cce817c6bffcb5630c4c079a74d0d896bd33b4d572a", "values": {"File": "What is the Center for Human-Compatible AI's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is the Center for Human-Compatible AI's research strategy?", "Link": "https://docs.google.com/document/d/1Vs_-sw5GdLpcMOHoaBgqn3ejIlKIwHAgrfb0ZbTLadw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:40:39.324+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:40:40.327+01:00", "Status": "Not started", "Edit Answer": "What is the Center for Human-Compatible AI's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EF", "Source Link": "", "aisafety.info Link": "What is the Center for Human-Compatible AI's research strategy?", "Source": "", "All Phrasings": "What is the Center for Human-Compatible AI's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EF", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:24:00.109+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-19434b821caacf0cf82ef8c4f5f687cd6f5a5a8225711f3c645e53bd61b0b90a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-19434b821caacf0cf82ef8c4f5f687cd6f5a5a8225711f3c645e53bd61b0b90a", "name": "What is the Center for AI Safety's research strategy?", "index": 421, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:29:05.624Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-19434b821caacf0cf82ef8c4f5f687cd6f5a5a8225711f3c645e53bd61b0b90a", "values": {"File": "What is the Center for AI Safety's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is the Center for AI Safety's research strategy?", "Link": "https://docs.google.com/document/d/1Gio54w8LFCMpBZFLSF6JUBQhVN2Tds4UmBsaZPFdVJc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:40:13.924+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:40:14.926+01:00", "Status": "Not started", "Edit Answer": "What is the Center for AI Safety's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EG", "Source Link": "", "aisafety.info Link": "What is the Center for AI Safety's research strategy?", "Source": "", "All Phrasings": "What is the Center for AI Safety's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EG", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:24:06.549+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-7c2223ea41c5f86153d51712d63861bc8049b98b083aca7db8a3832c60438767", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7c2223ea41c5f86153d51712d63861bc8049b98b083aca7db8a3832c60438767", "name": "What is the Association For Long Term Existence And Resilience's research strategy?", "index": 422, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:29:11.263Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7c2223ea41c5f86153d51712d63861bc8049b98b083aca7db8a3832c60438767", "values": {"File": "What is the Association For Long Term Existence And Resilience's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is the Association For Long Term Existence And Resilience's research strategy?", "Link": "https://docs.google.com/document/d/1UToQl9s0EpmiqFwEcXPp-doJmQNLCgBTwj4BBUQKd4w/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:39:43.251+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:39:44.165+01:00", "Status": "Not started", "Edit Answer": "What is the Association For Long Term Existence And Resilience's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EH", "Source Link": "", "aisafety.info Link": "What is the Association For Long Term Existence And Resilience's research strategy?", "Source": "", "All Phrasings": "What is the Association For Long Term Existence And Resilience's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EH", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:24:15.021+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-3966bfbe56a8dba8a94978f891a87f128c8ca073940fd527ea75f76662b33d2f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3966bfbe56a8dba8a94978f891a87f128c8ca073940fd527ea75f76662b33d2f", "name": "What is Anthropic's research strategy?", "index": 423, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:29:17.124Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3966bfbe56a8dba8a94978f891a87f128c8ca073940fd527ea75f76662b33d2f", "values": {"File": "What is Anthropic's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is Anthropic's research strategy?", "Link": "https://docs.google.com/document/d/19XcOtzyao6l8mkQ-EPw_lmwWo01KLsu-_bHyzKRRoMw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:38:57.764+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:38:58.763+01:00", "Status": "Not started", "Edit Answer": "What is Anthropic's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EI", "Source Link": "", "aisafety.info Link": "What is Anthropic's research strategy?", "Source": "", "All Phrasings": "What is Anthropic's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EI", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:24:18.011+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-d67e2314ea66fe27180df9514fff6971416a3eae6aae672a6ba2c9b276d4c720", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d67e2314ea66fe27180df9514fff6971416a3eae6aae672a6ba2c9b276d4c720", "name": "What is the Alignment Research Group's research strategy?", "index": 424, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:29:23.427Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d67e2314ea66fe27180df9514fff6971416a3eae6aae672a6ba2c9b276d4c720", "values": {"File": "What is the Alignment Research Group's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is the Alignment Research Group's research strategy?", "Link": "https://docs.google.com/document/d/1D70RJB_Wsup7IuNsJlLZJ9mBE8WW7KR2zltT3-niFAA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:38:25.806+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:38:26.751+01:00", "Status": "Not started", "Edit Answer": "What is the Alignment Research Group's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EJ", "Source Link": "", "aisafety.info Link": "What is the Alignment Research Group's research strategy?", "Source": "", "All Phrasings": "What is the Alignment Research Group's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EJ", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:24:20.694+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-9476208dcef7aa3cd66267fc9dca52caa2c6afb50a03d182be3aa04899328726", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-9476208dcef7aa3cd66267fc9dca52caa2c6afb50a03d182be3aa04899328726", "name": "What is the Alignment Research Center's research strategy?", "index": 425, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:29:28.850Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-9476208dcef7aa3cd66267fc9dca52caa2c6afb50a03d182be3aa04899328726", "values": {"File": "What is the Alignment Research Center's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is the Alignment Research Center's research strategy?", "Link": "https://docs.google.com/document/d/1H3MDBXho73UFkq_CXnbEEO1Ds_0Egno62PHM9h2IpIU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:37:45.535+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:37:46.326+01:00", "Status": "Not started", "Edit Answer": "What is the Alignment Research Center's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EK", "Source Link": "", "aisafety.info Link": "What is the Alignment Research Center's research strategy?", "Source": "", "All Phrasings": "What is the Alignment Research Center's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EK", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:24:22.992+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-e9bd87aa29823319890430049c12e037b79e7fe52330965651ebd022cc9285b6", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e9bd87aa29823319890430049c12e037b79e7fe52330965651ebd022cc9285b6", "name": "What is the Algorithmic Alignment Group's research strategy?", "index": 426, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:29:35.294Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e9bd87aa29823319890430049c12e037b79e7fe52330965651ebd022cc9285b6", "values": {"File": "What is the Algorithmic Alignment Group's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is the Algorithmic Alignment Group's research strategy?", "Link": "https://docs.google.com/document/d/1nwvoVPb_lhjns_2NBy6Eqkvs35ATVdVIrnuUfjIP-ww/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:36:36.003+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:36:36.951+01:00", "Status": "Not started", "Edit Answer": "What is the Algorithmic Alignment Group's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EL", "Source Link": "", "aisafety.info Link": "What is the Algorithmic Alignment Group's research strategy?", "Source": "", "All Phrasings": "What is the Algorithmic Alignment Group's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EL", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:24:27.024+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-b7033c329dc09a11d872ff1c3714921acb31cfef5349d8973c6bdd83387b4d6f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b7033c329dc09a11d872ff1c3714921acb31cfef5349d8973c6bdd83387b4d6f", "name": "What is Aligned AI's research strategy?", "index": 427, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:29:40.449Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b7033c329dc09a11d872ff1c3714921acb31cfef5349d8973c6bdd83387b4d6f", "values": {"File": "What is Aligned AI's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is Aligned AI's research strategy?", "Link": "https://docs.google.com/document/d/1Hj8el57bSHkOKu_qdBQOffk1KOupQOgYMN98Wkscy9U/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:35:57.402+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:35:58.357+01:00", "Status": "Not started", "Edit Answer": "What is Aligned AI's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EM", "Source Link": "", "aisafety.info Link": "What is Aligned AI's research strategy?", "Source": "", "All Phrasings": "What is Aligned AI's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EM", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:24:30.075+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-5d013aa60965c35ef28d5934f69d82048ce217ac1385aa7d856610f86b9ddf02", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5d013aa60965c35ef28d5934f69d82048ce217ac1385aa7d856610f86b9ddf02", "name": "What is the Machine Intelligence Research Institute's research strategy?", "index": 428, "createdAt": "2023-01-26T16:50:33.129Z", "updatedAt": "2023-03-14T22:29:45.807Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5d013aa60965c35ef28d5934f69d82048ce217ac1385aa7d856610f86b9ddf02", "values": {"File": "What is the Machine Intelligence Research Institute's research strategy?", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "What is the Machine Intelligence Research Institute's research strategy?", "Link": "https://docs.google.com/document/d/1V5NnsOrTYARdzRWgP9Any5oOYEbJhQI7mGNQ0NOiyU8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-25T23:28:28.948+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AISafety.careers", "Doc Last Edited": "2023-01-25T23:34:51.035+01:00", "Status": "Not started", "Edit Answer": "What is the Machine Intelligence Research Institute's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85EN", "Source Link": "", "aisafety.info Link": "What is the Machine Intelligence Research Institute's research strategy?", "Source": "", "All Phrasings": "What is the Machine Intelligence Research Institute's research strategy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85EN", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:24:33.149+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 1559, "Helpful": ""}}, {"id": "i-3b22b74a13c0e66233f96c9b8f73fffa76dff8243830c7fa98aa879964f75f76", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3b22b74a13c0e66233f96c9b8f73fffa76dff8243830c7fa98aa879964f75f76", "name": "What would happen if there was alignment failure with a superintelligence?", "index": 408, "createdAt": "2023-01-23T16:50:38.186Z", "updatedAt": "2023-03-14T22:29:48.255Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3b22b74a13c0e66233f96c9b8f73fffa76dff8243830c7fa98aa879964f75f76", "values": {"File": "What would happen if there was alignment failure with a superintelligence?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What would happen if there was alignment failure with a superintelligence?", "Link": "https://docs.google.com/document/d/1JHERroJXVNS8dSdaryA-eOv3oBpt0rV-T5_K2QtLfyM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-22T19:16:08.195+01:00", "Related Answers DO NOT EDIT": "What is alignment failure?", "Tags": "", "Doc Last Edited": "2023-02-23T05:32:04.934+01:00", "Status": "Not started", "Edit Answer": "What would happen if there was alignment failure with a superintelligence?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85E3", "Source Link": "", "aisafety.info Link": "What would happen if there was alignment failure with a superintelligence?", "Source": "", "All Phrasings": "What would happen if there was alignment failure with a superintelligence?\n", "Initial Order": "", "Related IDs": "7767", "Rich Text DO NOT EDIT": "<answer goes here>\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "<answer goes here>\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85E3", "Related Answers": "What is alignment failure?", "Doc Last Ingested": "2023-03-14T23:24:35.476+01:00", "Request Count": "", "Number of suggestions on answer doc": 14, "Total character count of suggestions on answer doc": 1947, "Helpful": ""}}, {"id": "i-b4637c15b3db7c44cdcafa2550cbc63bb4eb7076c45e2eff353684c9fd618b0d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b4637c15b3db7c44cdcafa2550cbc63bb4eb7076c45e2eff353684c9fd618b0d", "name": "Will AGI be a LLM?", "index": 407, "createdAt": "2023-01-22T16:49:48.400Z", "updatedAt": "2023-03-14T22:29:50.142Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b4637c15b3db7c44cdcafa2550cbc63bb4eb7076c45e2eff353684c9fd618b0d", "values": {"File": "Will AGI be a LLM?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Will AGI be a LLM?", "Link": "https://docs.google.com/document/d/1OxJ9ZETRYOjKIQ0G6rjBNweuabl4FWkS99ARviBOtvI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-22T14:34:23.354+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:46:07.512+01:00", "Status": "Not started", "Edit Answer": "Will AGI be a LLM?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85E2", "Source Link": "", "aisafety.info Link": "Will AGI be a LLM?", "Source": "", "All Phrasings": "Will AGI be a LLM?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Likely involve language modeling, maybe even transformers or their descendants\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "- Likely involve language modeling, maybe even transformers or their descendants\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "85E2", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:24:37.774+01:00", "Request Count": "", "Number of suggestions on answer doc": 14, "Total character count of suggestions on answer doc": 1947, "Helpful": ""}}, {"id": "i-136071225ca18403087b1accf15d49b5fa82b095d6257963a8d924debde189c3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-136071225ca18403087b1accf15d49b5fa82b095d6257963a8d924debde189c3", "name": "Isn't the real concern X?", "index": 406, "createdAt": "2023-01-22T13:30:37.211Z", "updatedAt": "2023-03-14T22:29:51.880Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-136071225ca18403087b1accf15d49b5fa82b095d6257963a8d924debde189c3", "values": {"File": "Isn't the real concern X?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn't the real concern X?", "Link": "https://docs.google.com/document/d/1sQUSq0uCGRLr-DQHLmXSAfCECPFQxObr_LGBfeSigNE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-22T14:23:46.347+01:00", "Related Answers DO NOT EDIT": "Isn't the real concern AI being misused by terrorists or other bad actors?,Isn't the real concern AI-enabled totalitarianism?,Isn't the real concern autonomous weapons?,Isn't the real concern technological unemployment?", "Tags": "Why Not Just", "Doc Last Edited": "2023-03-05T17:12:18.442+01:00", "Status": "In progress", "Edit Answer": "Isn't the real concern X?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "1001", "Source Link": "", "aisafety.info Link": "Isn't the real concern X?", "Source": "", "All Phrasings": "Isn't the real concern X?\n", "Initial Order": "", "Related IDs": "6410,6409,6411,6412", "Rich Text DO NOT EDIT": "There can be many \u201creal concerns\u201d the Stampy project is focused on one major class of concerns. Specifically, Stampy is focused on[existential risks](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence) which arise from superintelligent AI. While other concerns can be very important, identifying them does not negate the importance of existential risks.\n\nThe fact that Stampy does not focus on them, is not to deny that the other concerns are real.Rather that their scope of their risk is narrower, and the kind of solution they will need may be different.\n\nSee below for a list of pages focused on other specific potential concerns about AI. If there is another concern that you would like to see addressed, feel free to [add a question](https://coda.io/form/Add-a-question-to-AI-Safety-Info_dGDInYNFa3f).\n\n", "Tag Count": 1, "Related Answer Count": 4, "Rich Text": "There can be many \u201creal concerns\u201d the Stampy project is focused on one major class of concerns. Specifically, Stampy is focused on[existential risks](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence) which arise from superintelligent AI. While other concerns can be very important, identifying them does not negate the importance of existential risks.\n\nThe fact that Stampy does not focus on them, is not to deny that the other concerns are real.Rather that their scope of their risk is narrower, and the kind of solution they will need may be different.\n\nSee below for a list of pages focused on other specific potential concerns about AI. If there is another concern that you would like to see addressed, feel free to [add a question](https://coda.io/form/Add-a-question-to-AI-Safety-Info_dGDInYNFa3f).\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "1001", "Related Answers": "Isn't the real concern AI being misused by terrorists or other bad actors?,Isn't the real concern AI-enabled totalitarianism?,Isn't the real concern autonomous weapons?,Isn't the real concern technological unemployment?", "Doc Last Ingested": "2023-03-14T23:24:40.682+01:00", "Request Count": "", "Number of suggestions on answer doc": 28, "Total character count of suggestions on answer doc": 2345, "Helpful": ""}}, {"id": "i-23986d44d356f98e9cbeeaa57a50156aa7d2c20533921d6e720027ecb79d8c2a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-23986d44d356f98e9cbeeaa57a50156aa7d2c20533921d6e720027ecb79d8c2a", "name": "What are some exercises and projects I can try?", "index": 405, "createdAt": "2023-01-20T16:50:02.211Z", "updatedAt": "2023-03-14T22:29:53.617Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-23986d44d356f98e9cbeeaa57a50156aa7d2c20533921d6e720027ecb79d8c2a", "values": {"File": "What are some exercises and projects I can try?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some exercises and projects I can try?", "Link": "https://docs.google.com/document/d/1-58zgC2lRMbMK-CXU44VR3ApGYbTI0aJKX-cKkxDeyo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2022-11-27T20:35:45.086+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-03T23:04:16.173+01:00", "Status": "Live on site", "Edit Answer": "What are some exercises and projects I can try?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "85E0", "Source Link": "", "aisafety.info Link": "What are some exercises and projects I can try?", "Source": "", "All Phrasings": "What are some exercises and projects I can try?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Largely focused on projects within [AI safety technical research](https://80000hours.org/career-reviews/ai-safety-researcher/), although many of these projects are useful for other career paths like [AI policy](https://80000hours.org/career-reviews/ai-policy-and-strategy/).\n\n- [Levelling Up in AI Safety Research Engineering [Public]](https://docs.google.com/document/d/1b83_-eo9NEaKDKc9R3P5h5xkLImqMw8ADLmi__rkLo4/edit) ([LW](https://www.lesswrong.com/posts/uLstPRyYwzfrx3enG/levelling-up-in-ai-safety-research-engineering))\n\n    - Highly recommended list of AI safety research engineering resources for people at various skill levels.\n\n- **[AI Alignment Awards](https://www.alignmentawards.com/)**\n\n- [[Public] Some AI Governance Research Ideas](https://docs.google.com/document/d/13LJhP3ksrcEBKxYFG5GkJaC2UoxHKUYAHCRdRlpePEc/edit)(from GovAI)\n\n- **[Alignment jams / hackathons](https://alignmentjam.com/)**from [Apart Research](https://apartresearch.com/jam)\n\n    - Past / upcoming hackathons: [LLM](https://itch.io/jam/llm-hackathon), [interpretability 1](https://itch.io/jam/interpretability), [AI test](https://itch.io/jam/aitest), [interpretability 2](https://itch.io/jam/ai-neuroscience), [oversight](https://itch.io/jam/scaleoversight)\n\n    - Resources: [black-box investigator of language models](https://www.lesswrong.com/posts/yGaw4NqRha8hgx5ny/the-case-for-becoming-a-black-box-investigator-of-language), [interpretability playground](https://alignmentjam.com/interpretability-playground) ([LW](https://www.lesswrong.com/posts/gJRsiukrNudHLGgCf/the-interpretability-playground)), [AI test](https://alignmentjam.com/aitest), [oversight](https://alignmentjam.com/oversight)\n\n    - [Examples of past projects](https://alignmentjam.com/jams); [interpretability winners](https://www.lesswrong.com/posts/hhhmcWkgLwPmBuhx7/results-from-the-interpretability-hackathon)\n\n    - Projects on AI Safety Ideas: [LLM](https://aisafetyideas.com/), [interpretability](https://aisafetyideas.com/list/interpretability-hackathon), [AI test](https://aisafetyideas.com/list/test-the-ai),\n\n    - [How to run one](https://alignmentjam.com/running) as an in-person event at your school\n\n- Neel Nanda: **[200 Concrete Open Problems in Mechanistic Interpretability](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj)** ([doc](https://docs.google.com/document/d/1WONBzNqfKIxERejrrPlQMyKqg7jSFW92x5UMXNrMdPo/edit) and [previous version](https://docs.google.com/document/d/15RnVxGNRm5ZDJojoExwXuiRxfNFB9SFqX5NHkT6xjc8/edit#))\n\n- [Project page from AGI Safety Fundamentals](https://www.agisafetyfundamentals.com/ai-alignment-tabs/week-8) and their **[Open List of Project Ideas](https://docs.google.com/spreadsheets/d/1Co45Bd_rOc2JZ3jqK7kw1tMptcI0J66Y_8khK4u30_w/edit?usp=sharing)**\n\n- **[AI Safety Ideas](https://aisafetyideas.com/)** by Apart Research; [EAF post](https://forum.effectivealtruism.org/posts/DTTADonxnDRoksp4E/ai-safety-ideas-an-open-ai-safety-research-platform)\n\n- [Most Important Century writing prize](https://forum.effectivealtruism.org/posts/4XK5zkyv94voC8Fjr/announcing-the-most-important-century-writing-prize) ([Superlinear page](https://www.super-linear.org/prize?recordId=recmsHjuOh5JmVJWJ))\n\n- [Center for AI Safety](https://safe.ai/)\n\n    - **[Competitions](https://safe.ai/competitions)**like [SafeBench](https://benchmarking.mlsafety.org/) (see [example ideas](https://benchmarking.mlsafety.org/ideas))\n\n    - **[Student ML Safety Research Stipend Opportunity](https://www.mlsafety.org/safety-scholarship)**\u2013 provides stipends for doing ML research.\n\n    - [course.mlsafety.org projects](https://docs.google.com/document/d/12nj9a1WOJSHaddks3vmmjVKtlHmO53zEW4KHYyapUxY/edit) CAIS is looking for someone to add details about these projects on [course.mlsafety.org](https://course.mlsafety.org/)\n\n- **[Distilling](https://www.lesswrong.com/posts/zo9zKcz47JxDErFzQ/call-for-distillers)** [/ summarizing / synthesizing / reviewing / explaining](https://www.lesswrong.com/posts/zo9zKcz47JxDErFzQ/call-for-distillers)\n\n- **[Forming your own views on AI safety (without stress!)](https://www.youtube.com/watch?v=edoQ3CiNa_s)** \u2013 also see [Neel's presentation slides](https://docs.google.com/presentation/d/1LMBfw2wUmGLEvRTi8019DX_uGdJTeGsHRPRa5S8UIWs/edit#slide=id.p) and \"[Inside Views Resources](https://docs.google.com/document/d/1O3ci6Q0UZ8qeBpKs6lVxFGLWiH89fBtz-lLG2iN8YNc/edit#)\" doc\n\n- [\"Mostly focused on AI\"](https://forum.effectivealtruism.org/posts/MsNpJBzv5YhdfNHc9/a-central-directory-for-open-research-questions#Mostly_focused_on_AI) section of [\"A central directory for open research questions\"](https://forum.effectivealtruism.org/posts/MsNpJBzv5YhdfNHc9/a-central-directory-for-open-research-questions) \u2013 contains a list of links to projects, similar to this document\n\n- [Possible ways to expand on \"Discovering Latent Knowledge in Language Models Without Supervision\"](https://www.lesswrong.com/posts/bFwigCDMC5ishLz7X/rfc-possible-ways-to-expand-on-discovering-latent-knowledge)\n\n- Answer some of the [application questions](https://docs.google.com/document/d/1J0jRKfWrNUakiIBbbpw7q3i27Rw9XCLSJvW6659NhRU/edit#) from the winter 2022 [SERI-MATS](https://www.serimats.org/), such as [Vivek Hebbar's problems](https://docs.google.com/document/d/1NVVtdsfz7HiseVFSk3jYly4sPG4dG03wFFDrD8rBXU0/edit)\n\n- [10 exercises from Akash](https://forum.effectivealtruism.org/posts/BBS8cz4Sa7wvJ2Jso/resources-that-i-think-new-alignment-researchers-should-know#Exercises) in \u201cResources that (I think) new alignment researchers should know about\u201d\n\n- [[T] Deception Demo Brainstorm](https://docs.google.com/document/d/1HLnd7IGyX7ByrnBUZQH6fbneox2jwZ7LRGQw-esc-uU/edit) has some ideas (message[Thomas Larsen](https://www.lesswrong.com/users/thomas-larsen) if these seem interesting)\n\n- Upcoming [2023 Open Philanthropy AI Worldviews Contest](https://forum.effectivealtruism.org/posts/3kaojgsu6qy2n8TdC/pre-announcing-the-2023-open-philanthropy-ai-worldviews)\n\n- [Alignment research at ALTER](https://forum.effectivealtruism.org/posts/zCYGbYAaXeq7v67Km/prize-and-fast-track-to-alignment-research-at-alter) \u2013 interesting research problems, many have a theoretical math flavor\n\n- [Open Problems in AI X-Risk [PAIS #5]](https://forum.effectivealtruism.org/s/8EqNwueP6iw2BQpNo/p/hNPCo4kScxccK9Ham)\n\n- Steven Byrnes: [[Intro to brain-like-AGI safety] 15. Conclusion: Open problems, how to help, AMA](https://www.lesswrong.com/posts/tj8AC3vhTnBywdZoA/intro-to-brain-like-agi-safety-15-conclusion-open-problems-1)\n\n- Evan Hubinger: [Concrete experiments in inner alignment](https://www.lesswrong.com/posts/uSdPa9nrSgmXCtdKN/concrete-experiments-in-inner-alignment), [ideas someone should investigate further](https://www.alignmentforum.org/posts/HE3Styo9vpk7m8zi4/evhub-s-shortform?commentId=cPPvFFLLkMuh9k5Zx), [sticky goals](https://www.lesswrong.com/posts/a2Bxq4g2sPZwKiQmK/sticky-goals-a-concrete-experiment-for-understanding)\n\n- Richard Ngo: [Some conceptual alignment research projects](https://www.lesswrong.com/posts/27AWRKbKyXuzQoaSk/some-conceptual-alignment-research-projects), [alignment research exercises](https://www.lesswrong.com/posts/kj37Hzb2MsALwLqWt/alignment-research-exercises)\n\n- Buck Shlegeris: [Some fun ML engineering projects that I would think are cool](https://docs.google.com/document/d/1yMP9i6cQQwHG1ITOtccUkuN3p9eTk1AtzOM8TP-mZVs/edit), [The case for becoming a black box investigator of language models](https://www.alignmentforum.org/posts/yGaw4NqRha8hgx5ny/the-case-for-becoming-a-black-box-investigator-of-language)\n\n- Implement a [key paper](https://spinningup.openai.com/en/latest/spinningup/keypapers.html) in deep reinforcement learning\n\n- [Amplify creative grants](https://forum.effectivealtruism.org/posts/pLiEoTuwEYRxNueoD/announcing-amplify-creative-grants) (old)\n\n- \u201cPaper replication resources\u201d section in \u201c[How to pursue a career in technical alignment](https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment)\u201d\n\n- [ELK](https://www.alignmentforum.org/tag/eliciting-latent-knowledge-elk)\n\n- [Daniel Filan idea](https://www.lesswrong.com/posts/WgMhovN7Gs6Jpn3PH/danielfilan-s-shortform-feed?commentId=aEbnazFXn6GFym8kv)\n\n- Summarize a reading from [Reading What We Can](https://readingwhatwecan.com/)\n\n- Zac Hatfield-Dodds: \u201cThe list I wrote up for 2021 final-year-undergrad projects is at [https://zhd.dev/phd/student-ideas.html](https://zhd.dev/phd/student-ideas.html)  - note that these are aimed at software engineering rather than ML, NLP, or AI Safety per se (most of those ideas I have stay at Anthropic, and are probably infeasible for student projects).\u201d These projects are good for [AI safety engineering careers](https://www.alignmentforum.org/posts/YDF7XhMThhNfHfim9/ai-safety-needs-great-engineers).\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Largely focused on projects within [AI safety technical research](https://80000hours.org/career-reviews/ai-safety-researcher/), although many of these projects are useful for other career paths like [AI policy](https://80000hours.org/career-reviews/ai-policy-and-strategy/).\n\n- [Levelling Up in AI Safety Research Engineering [Public]](https://docs.google.com/document/d/1b83_-eo9NEaKDKc9R3P5h5xkLImqMw8ADLmi__rkLo4/edit) ([LW](https://www.lesswrong.com/posts/uLstPRyYwzfrx3enG/levelling-up-in-ai-safety-research-engineering))\n\n    - Highly recommended list of AI safety research engineering resources for people at various skill levels.\n\n- **[AI Alignment Awards](https://www.alignmentawards.com/)**\n\n- [[Public] Some AI Governance Research Ideas](https://docs.google.com/document/d/13LJhP3ksrcEBKxYFG5GkJaC2UoxHKUYAHCRdRlpePEc/edit)(from GovAI)\n\n- **[Alignment jams / hackathons](https://alignmentjam.com/)**from [Apart Research](https://apartresearch.com/jam)\n\n    - Past / upcoming hackathons: [LLM](https://itch.io/jam/llm-hackathon), [interpretability 1](https://itch.io/jam/interpretability), [AI test](https://itch.io/jam/aitest), [interpretability 2](https://itch.io/jam/ai-neuroscience), [oversight](https://itch.io/jam/scaleoversight)\n\n    - Resources: [black-box investigator of language models](https://www.lesswrong.com/posts/yGaw4NqRha8hgx5ny/the-case-for-becoming-a-black-box-investigator-of-language), [interpretability playground](https://alignmentjam.com/interpretability-playground) ([LW](https://www.lesswrong.com/posts/gJRsiukrNudHLGgCf/the-interpretability-playground)), [AI test](https://alignmentjam.com/aitest), [oversight](https://alignmentjam.com/oversight)\n\n    - [Examples of past projects](https://alignmentjam.com/jams); [interpretability winners](https://www.lesswrong.com/posts/hhhmcWkgLwPmBuhx7/results-from-the-interpretability-hackathon)\n\n    - Projects on AI Safety Ideas: [LLM](https://aisafetyideas.com/), [interpretability](https://aisafetyideas.com/list/interpretability-hackathon), [AI test](https://aisafetyideas.com/list/test-the-ai),\n\n    - [How to run one](https://alignmentjam.com/running) as an in-person event at your school\n\n- Neel Nanda: **[200 Concrete Open Problems in Mechanistic Interpretability](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj)** ([doc](https://docs.google.com/document/d/1WONBzNqfKIxERejrrPlQMyKqg7jSFW92x5UMXNrMdPo/edit) and [previous version](https://docs.google.com/document/d/15RnVxGNRm5ZDJojoExwXuiRxfNFB9SFqX5NHkT6xjc8/edit#))\n\n- [Project page from AGI Safety Fundamentals](https://www.agisafetyfundamentals.com/ai-alignment-tabs/week-8) and their **[Open List of Project Ideas](https://docs.google.com/spreadsheets/d/1Co45Bd_rOc2JZ3jqK7kw1tMptcI0J66Y_8khK4u30_w/edit?usp=sharing)**\n\n- **[AI Safety Ideas](https://aisafetyideas.com/)** by Apart Research; [EAF post](https://forum.effectivealtruism.org/posts/DTTADonxnDRoksp4E/ai-safety-ideas-an-open-ai-safety-research-platform)\n\n- [Most Important Century writing prize](https://forum.effectivealtruism.org/posts/4XK5zkyv94voC8Fjr/announcing-the-most-important-century-writing-prize) ([Superlinear page](https://www.super-linear.org/prize?recordId=recmsHjuOh5JmVJWJ))\n\n- [Center for AI Safety](https://safe.ai/)\n\n    - **[Competitions](https://safe.ai/competitions)**like [SafeBench](https://benchmarking.mlsafety.org/) (see [example ideas](https://benchmarking.mlsafety.org/ideas))\n\n    - **[Student ML Safety Research Stipend Opportunity](https://www.mlsafety.org/safety-scholarship)**\u2013 provides stipends for doing ML research.\n\n    - [course.mlsafety.org projects](https://docs.google.com/document/d/12nj9a1WOJSHaddks3vmmjVKtlHmO53zEW4KHYyapUxY/edit) CAIS is looking for someone to add details about these projects on [course.mlsafety.org](https://course.mlsafety.org/)\n\n- **[Distilling](https://www.lesswrong.com/posts/zo9zKcz47JxDErFzQ/call-for-distillers)** [/ summarizing / synthesizing / reviewing / explaining](https://www.lesswrong.com/posts/zo9zKcz47JxDErFzQ/call-for-distillers)\n\n- **[Forming your own views on AI safety (without stress!)](https://www.youtube.com/watch?v=edoQ3CiNa_s)** \u2013 also see [Neel's presentation slides](https://docs.google.com/presentation/d/1LMBfw2wUmGLEvRTi8019DX_uGdJTeGsHRPRa5S8UIWs/edit#slide=id.p) and \"[Inside Views Resources](https://docs.google.com/document/d/1O3ci6Q0UZ8qeBpKs6lVxFGLWiH89fBtz-lLG2iN8YNc/edit#)\" doc\n\n- [\"Mostly focused on AI\"](https://forum.effectivealtruism.org/posts/MsNpJBzv5YhdfNHc9/a-central-directory-for-open-research-questions#Mostly_focused_on_AI) section of [\"A central directory for open research questions\"](https://forum.effectivealtruism.org/posts/MsNpJBzv5YhdfNHc9/a-central-directory-for-open-research-questions) \u2013 contains a list of links to projects, similar to this document\n\n- [Possible ways to expand on \"Discovering Latent Knowledge in Language Models Without Supervision\"](https://www.lesswrong.com/posts/bFwigCDMC5ishLz7X/rfc-possible-ways-to-expand-on-discovering-latent-knowledge)\n\n- Answer some of the [application questions](https://docs.google.com/document/d/1J0jRKfWrNUakiIBbbpw7q3i27Rw9XCLSJvW6659NhRU/edit#) from the winter 2022 [SERI-MATS](https://www.serimats.org/), such as [Vivek Hebbar's problems](https://docs.google.com/document/d/1NVVtdsfz7HiseVFSk3jYly4sPG4dG03wFFDrD8rBXU0/edit)\n\n- [10 exercises from Akash](https://forum.effectivealtruism.org/posts/BBS8cz4Sa7wvJ2Jso/resources-that-i-think-new-alignment-researchers-should-know#Exercises) in \u201cResources that (I think) new alignment researchers should know about\u201d\n\n- [[T] Deception Demo Brainstorm](https://docs.google.com/document/d/1HLnd7IGyX7ByrnBUZQH6fbneox2jwZ7LRGQw-esc-uU/edit) has some ideas (message[Thomas Larsen](https://www.lesswrong.com/users/thomas-larsen) if these seem interesting)\n\n- Upcoming [2023 Open Philanthropy AI Worldviews Contest](https://forum.effectivealtruism.org/posts/3kaojgsu6qy2n8TdC/pre-announcing-the-2023-open-philanthropy-ai-worldviews)\n\n- [Alignment research at ALTER](https://forum.effectivealtruism.org/posts/zCYGbYAaXeq7v67Km/prize-and-fast-track-to-alignment-research-at-alter) \u2013 interesting research problems, many have a theoretical math flavor\n\n- [Open Problems in AI X-Risk [PAIS #5]](https://forum.effectivealtruism.org/s/8EqNwueP6iw2BQpNo/p/hNPCo4kScxccK9Ham)\n\n- Steven Byrnes: [[Intro to brain-like-AGI safety] 15. Conclusion: Open problems, how to help, AMA](https://www.lesswrong.com/posts/tj8AC3vhTnBywdZoA/intro-to-brain-like-agi-safety-15-conclusion-open-problems-1)\n\n- Evan Hubinger: [Concrete experiments in inner alignment](https://www.lesswrong.com/posts/uSdPa9nrSgmXCtdKN/concrete-experiments-in-inner-alignment), [ideas someone should investigate further](https://www.alignmentforum.org/posts/HE3Styo9vpk7m8zi4/evhub-s-shortform?commentId=cPPvFFLLkMuh9k5Zx), [sticky goals](https://www.lesswrong.com/posts/a2Bxq4g2sPZwKiQmK/sticky-goals-a-concrete-experiment-for-understanding)\n\n- Richard Ngo: [Some conceptual alignment research projects](https://www.lesswrong.com/posts/27AWRKbKyXuzQoaSk/some-conceptual-alignment-research-projects), [alignment research exercises](https://www.lesswrong.com/posts/kj37Hzb2MsALwLqWt/alignment-research-exercises)\n\n- Buck Shlegeris: [Some fun ML engineering projects that I would think are cool](https://docs.google.com/document/d/1yMP9i6cQQwHG1ITOtccUkuN3p9eTk1AtzOM8TP-mZVs/edit), [The case for becoming a black box investigator of language models](https://www.alignmentforum.org/posts/yGaw4NqRha8hgx5ny/the-case-for-becoming-a-black-box-investigator-of-language)\n\n- Implement a [key paper](https://spinningup.openai.com/en/latest/spinningup/keypapers.html) in deep reinforcement learning\n\n- [Amplify creative grants](https://forum.effectivealtruism.org/posts/pLiEoTuwEYRxNueoD/announcing-amplify-creative-grants) (old)\n\n- \u201cPaper replication resources\u201d section in \u201c[How to pursue a career in technical alignment](https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment)\u201d\n\n- [ELK](https://www.alignmentforum.org/tag/eliciting-latent-knowledge-elk)\n\n- [Daniel Filan idea](https://www.lesswrong.com/posts/WgMhovN7Gs6Jpn3PH/danielfilan-s-shortform-feed?commentId=aEbnazFXn6GFym8kv)\n\n- Summarize a reading from [Reading What We Can](https://readingwhatwecan.com/)\n\n- Zac Hatfield-Dodds: \u201cThe list I wrote up for 2021 final-year-undergrad projects is at [https://zhd.dev/phd/student-ideas.html](https://zhd.dev/phd/student-ideas.html)  - note that these are aimed at software engineering rather than ML, NLP, or AI Safety per se (most of those ideas I have stay at Anthropic, and are probably infeasible for student projects).\u201d These projects are good for [AI safety engineering careers](https://www.alignmentforum.org/posts/YDF7XhMThhNfHfim9/ai-safety-needs-great-engineers).\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-02-26T18:24:41.911+01:00", "UI ID": "85E0", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:24:45.998+01:00", "Request Count": "", "Number of suggestions on answer doc": 28, "Total character count of suggestions on answer doc": 2345, "Helpful": ""}}, {"id": "i-2dff5134272134f4a981fa3a8b729cc5442575b93bf65cb5c107619a83f2c3c3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2dff5134272134f4a981fa3a8b729cc5442575b93bf65cb5c107619a83f2c3c3", "name": "Answers", "index": 402, "createdAt": "2023-01-19T13:04:19.796Z", "updatedAt": "2023-01-30T17:06:40.265Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2dff5134272134f4a981fa3a8b729cc5442575b93bf65cb5c107619a83f2c3c3", "values": {"File": "Answers", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "Answers", "Link": "https://drive.google.com/file/d/1oRnSOydHYGLb74BGU7UNZ_3WJ5MZ1Xc2/view?usp=drivesdk", "Thumbnail": "", "Doc Created": "2023-01-18T22:16:29.482+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-01-18T22:16:29.482+01:00", "Status": "Uncategorized", "Edit Answer": "Answers", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "-1", "Source Link": "", "aisafety.info Link": "Answers", "Source": "", "All Phrasings": "Answers\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "-1", "Related Answers": "", "Doc Last Ingested": "", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-4d07e8aa9404673d1a901b0a89ffaa5eec43ab7a59e56411f0e06d794dece012", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4d07e8aa9404673d1a901b0a89ffaa5eec43ab7a59e56411f0e06d794dece012", "name": "Answers", "index": 403, "createdAt": "2023-01-19T13:04:19.796Z", "updatedAt": "2023-01-30T17:06:40.265Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4d07e8aa9404673d1a901b0a89ffaa5eec43ab7a59e56411f0e06d794dece012", "values": {"File": "Answers", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "Answers", "Link": "https://drive.google.com/drive/folders/1XUTbO31BMSBBZLhwFsvPObnuMbVVd59H", "Thumbnail": "", "Doc Created": "2023-01-05T18:44:10.054+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-01-18T22:15:31.666+01:00", "Status": "Uncategorized", "Edit Answer": "Answers", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "-1", "Source Link": "", "aisafety.info Link": "Answers", "Source": "", "All Phrasings": "Answers\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "-1", "Related Answers": "", "Doc Last Ingested": "", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-62dfe658cd6e94305a751c8cd154910187fbba401c6033726a1ccb3047ed1a40", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-62dfe658cd6e94305a751c8cd154910187fbba401c6033726a1ccb3047ed1a40", "name": "Bio Anchors", "index": 404, "createdAt": "2023-01-19T13:04:19.796Z", "updatedAt": "2023-01-30T17:06:40.265Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-62dfe658cd6e94305a751c8cd154910187fbba401c6033726a1ccb3047ed1a40", "values": {"File": "Bio Anchors", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "Bio Anchors", "Link": "https://docs.google.com/spreadsheets/d/1gbcJjSN1_E7UTSngIqcvueGDAULjjOajzzft6wX5vBA/edit?usp=drivesdk", "Thumbnail": "", "Doc Created": "2023-01-04T14:11:04.634+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-01-18T21:47:02.957+01:00", "Status": "Uncategorized", "Edit Answer": "Bio Anchors", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "-1", "Source Link": "", "aisafety.info Link": "Bio Anchors", "Source": "", "All Phrasings": "Bio Anchors\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "-1", "Related Answers": "", "Doc Last Ingested": "", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-0ffacb41571ba8a0dbefce70e1f0a139e9da6b6419d9559ab044830e0ecfc0f3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0ffacb41571ba8a0dbefce70e1f0a139e9da6b6419d9559ab044830e0ecfc0f3", "name": "Where can I find videos about AI Safety?", "index": 400, "createdAt": "2023-01-19T13:04:16.454Z", "updatedAt": "2023-03-16T00:07:11.846Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0ffacb41571ba8a0dbefce70e1f0a139e9da6b6419d9559ab044830e0ecfc0f3", "values": {"File": "Where can I find videos about AI Safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Where can I find videos about AI Safety?", "Link": "https://docs.google.com/document/d/1vDWLKPvJNo0k9IwMPvduO23_CXOdMpxn0aRDnvrL0yw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2022-12-21T22:36:39.479+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-15T22:03:15.866+01:00", "Status": "Live on site", "Edit Answer": "Where can I find videos about AI Safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "2222", "Source Link": "", "aisafety.info Link": "Where can I find videos about AI Safety?", "Source": "", "All Phrasings": "Where can I find videos about AI Safety?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "*Also available at* *[aisafety.video](https://aisafety.video)*\n\n## Generally good sources\n\n### YouTube channels\n\n- [Rob Miles](https://www.youtube.com/c/RobertMilesAI) (and [Computerphile](https://www.youtube.com/playlist?list=PLzH6n4zXuckquVnQ0KlMDxyT5YE-sA8Ps))\n\n- [Apart - Safe AI](https://www.youtube.com/channel/UCnfBOJnTkE9sgjMOOsQbi2w/)\n\n- [Neel Nanda](https://www.youtube.com/@neelnanda2469)\n\n- [Center for AI Safety](https://www.youtube.com/channel/UCY_K5gXsXHtuiP8mj3BiWxA/videos)\n\n- [Towards Data Science](https://www.youtube.com/c/TowardsDataScience/videos)\n\n- [The Inside View](https://www.youtube.com/c/TheInsideView)\n\n- [Future of Life Institute](https://www.youtube.com/channel/UC-rCCy3FQ-GItDimSR9lhzw/videos)\n\n- [SERI](https://www.youtube.com/channel/UC_UHeaVeJoI7NGUSXqpBIiQ/videos)\n\n- [CERI](https://www.youtube.com/@cambridgeexistentialrisksi9588)\n\n- [AI Safety Talks](https://www.youtube.com/@aisafetytalks/featured) (and its [playlists](https://www.youtube.com/@aisafetytalks/playlists))\n\n- [AI Safety Reading Group](https://www.youtube.com/@aisafetyreadinggroup)\n\n- [Mechanistic Interpretability](https://www.youtube.com/@mechanisticinterpretabilit5092/)\n\n- [Intro to ML Safety](https://www.youtube.com/@introtomlsafety/videos)\n\n- [AGI safety talks from AGISF](https://www.agisafetyfundamentals.com/agi-safety-talks)\n\n- [AISS discussion days](https://www.aisafetysupport.org/events/discussion-days) and [AISS YouTube](https://www.youtube.com/@aiss8214)\n\n- [Victoria Krakovna AI talks](https://vkrakovna.wordpress.com/talks/)\n\n- [Jack Parker](https://www.youtube.com/channel/UCsLNgZySqFN4ybdEma0XCRg/videos)\n\n- [PIBBSS](https://www.youtube.com/@pibbssfellowship1034/videos)\n\n- [MIRI](https://www.youtube.com/@MIRIBerkeley)\n\n- [The Future Society](https://www.youtube.com/@thefuturesoc)\n\n- [Cooperative AI Foundation](https://www.youtube.com/@CooperativeAIFoundation/)\n\n- [Quantified Uncertainty Research Institute](https://www.youtube.com/@quantifieduncertainty)\n\n- **Other languages**: [Karl Olsberg](https://www.youtube.com/@KarlOlsbergAutor/videos) (German)\n\n- Channels with less focus on AI existential risk: [Kurzgesagt](https://www.youtube.com/c/inanutshell), [Rational Animations](https://www.youtube.com/c/RationalAnimations), [Centre for Effective Altruism](https://www.youtube.com/c/EffectiveAltruismVideos), [Future of Humanity Institute](https://www.youtube.com/user/FHIOxford/videos), [Center for Security and Emerging Technology](https://www.youtube.com/@centerforsecurityandemergi9211), [CSER](https://www.youtube.com/c/CSERCambridge/videos), [SSC meetups](https://www.youtube.com/watch?v=Wn2vgQGNI_c&list=PLFDYxsqlH6uhSghWfsuEAiKDfZNVZhUOX), [Foresight Institute](https://www.youtube.com/@ForesightInstitute), [Science, Technology & the Future](https://www.youtube.com/@scfu), [Berkman Klein Center](https://www.youtube.com/@BKCHarvard), [Schwartz Reisman Institute](https://www.youtube.com/@SchwartzReismanInstitute), [Stanford HAI](https://www.youtube.com/@stanfordhai7626), [Carper AI](https://www.youtube.com/@carperai3790), [Lex Fridman](https://www.youtube.com/@lexfridman), [Digital Humanism](https://www.youtube.com/@DigitalHumanism/)\n\n- **AI content without much AI safety**: [Edan Meyer](https://www.youtube.com/@EdanMeyer/videos), [Yannic Kilcher](https://www.youtube.com/c/YannicKilcher?app=desktop), [Mutual Information](https://www.youtube.com/@Mutual_Information/), [Computerphile](https://www.youtube.com/@Computerphile), [CodeEmporium](https://www.youtube.com/@CodeEmporium), [Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy), [sentdex](https://www.youtube.com/@sentdex), [nPlan](https://www.youtube.com/@nPlan/), [Jay Alammar](https://www.youtube.com/@ai_io), [Assembly AI](https://www.youtube.com/@AssemblyAI), [Aleksa Gordi\u0107](https://www.youtube.com/@TheAIEpiphany), [Simons Institute](https://www.youtube.com/@SimonsInstituteTOC/), [2 Minute Papers](https://www.youtube.com/c/K\u00e1rolyZsolnai/videos), [Machine Learning Street Talk](https://www.youtube.com/@MachineLearningStreetTalk), [ColdFusion](https://www.youtube.com/@ColdFusion/), [HuggingFace](https://www.youtube.com/@HuggingFace), [AI Coffee Break](https://www.youtube.com/@AICoffeeBreak), [Alex Smola](https://www.youtube.com/@smolix/), [Welcome AI Overlords](https://www.youtube.com/@welcomeaioverlords/), [Valence Discovery](https://www.youtube.com/@valencediscovery6139/), [The Alan Turing Institute](https://www.youtube.com/@TheAlanTuringInstituteUK/), [Jordan Harrod](https://www.youtube.com/@JordanHarrod), [Cambridge Ellis Unit](https://www.youtube.com/@ellisunit8739), [UCL CSML Seminar Series](https://www.youtube.com/@deepmindellisuclcsmlsemina3383/), [Harvard Medical AI](https://www.youtube.com/@harvard-medicalai-rajpurkarlab), [IARAI](https://www.youtube.com/@iarai/), [Alfredo Canziani](https://www.youtube.com/@alfcnz),  [Andreas Geiger](https://www.youtube.com/@cvlibs/), [CMU AI Seminar](https://www.youtube.com/@cmuaiseminar1950/), [Jeremy Howard](https://www.youtube.com/@howardjeremyp/), [Google Research](https://www.youtube.com/@GoogleResearch/), [AI for Good](https://www.youtube.com/@AIforGood/), [IPAM UCLA](https://www.youtube.com/@IPAMUCLA), [One world theoretical ML](https://www.youtube.com/@oneworldtheoreticalmachine110), [What's AI](https://www.youtube.com/@WhatsAI/), [Stanford MedAI](https://www.youtube.com/@stanfordmedai1263), [MILA neural scaling seminars](https://sites.google.com/view/nsl-course/invited-talks), [Digital Engine](https://www.youtube.com/@DigitalEngine/) (sometimes misleading), [Steve Brunton](https://www.youtube.com/@Eigensteve/), [PyTorch](https://www.youtube.com/@PyTorch/), [What's AI by Louis Bouchard](https://www.youtube.com/@WhatsAI/), [Eye on AI](https://www.youtube.com/@eyeonai3425), [AI Explained](https://www.youtube.com/@ai-explained-/)\n\n### Relevant **podcasts** without video\n\n- [AXRP](https://axrp.net/)\n\n- [Crunch Time](https://open.spotify.com/show/0xhSRFWy8iEhcuBINsVT63)\n\n- [Alignment Newsletter podcast](https://www.youtube.com/@alignmentnewsletterpodcast4310/videos)\n\n- [Rationality A-Z](https://podcasts.apple.com/us/podcast/rationality-from-ai-to-zombies/id1299826696)\n\n- [Nonlinear Library](https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library)\n\n- [LessWrong Curated Podcast](https://www.lesswrong.com/posts/kDjKF2yFhFEWe4hgC/announcing-the-lesswrong-curated-podcast)\n\n- [EA Forum Podcast](https://forum.effectivealtruism.org/posts/K5Snxo5EhgmwJJjR2/announcing-audio-narrations-of-ea-forum-posts-1)\n\n- [Cold Takes Audio](https://podcasts.apple.com/us/podcast/cold-takes-audio/id1580097837)\n\n- [ACX Podcast](https://sscpodcast.libsyn.com/)\n\n- [80,000 Hours](https://www.youtube.com/user/eightythousandhours/videos)\n\n- [Technical AI Safety Podcast](https://open.spotify.com/show/6BgMgUSY95kL9gSyOPpper)\n\n- [EA Radio](https://www.earad.io/)\n\n- [Rationally Speaking Podcast](http://rationallyspeakingpodcast.org/)\n\n- [Replacing Guilt](https://anchor.fm/guilt)\n\n- [Hear This Idea](https://hearthisidea.com/episodes)\n\n### Lists\n\n- **[AI Alignment YouTube Playlists](https://www.lesswrong.com/posts/tas3QZfKm4bdnAWsZ/ai-alignment-youtube-playlists)**\u2013 excellent resource. [Slide-light](https://www.youtube.com/playlist?list=PLTYHZYmxohXp0xvVJmMmpT_eFJovlzn0l) ([reordered](https://www.youtube.com/playlist?list=PLCRVRLd2RhZTpdUdEzJjo3qhmX3y3skWA)) and [slide-heavy](https://www.youtube.com/playlist?list=PLTYHZYmxohXpn5uf8JZ2OouB1PsDJAk-x) playlists.\n\n- [the gears to ascenscion](https://www.lesswrong.com/posts/rKmojEZ9qKwApjCfX/the-gears-to-ascenscion-s-shortform?commentId=jmWL85bGWpJmPJYzW) lists many channels for understanding current capabilities trends\n\n- [AI Safety Support \"Lots of Links\": Videos](https://www.aisafetysupport.org/resources/lots-of-links#h.6s2gcz1p5l6z)\n\n- [A ranked list of all EA-relevant documentaries movies and TV | Brian Tan on EAF](https://forum.effectivealtruism.org/posts/b4YW4GJR2RasS4YEw/a-ranked-list-of-all-ea-relevant-documentaries-movies-and-tv#AI_Safety___Risks)\n\n- [https://sites.google.com/view/towards-agi-course/topicspapers?authuser=0](https://sites.google.com/view/towards-agi-course/topicspapers?authuser=0)\n\n## Specific suggestions\n\nNote that:\n\n- I haven\u2019t watched all of these videos. Feel free to comment with more recommendations!\n\n- This list does not focus on podcasts, although there are a few podcast recommendations.\n\n### Introductory\n\n- See also [AI safety intros](https://docs.google.com/document/d/1zx_WpcwuT3Stpx8GJJHcvJLSgv6dLje0eslVKvuk1yQ/edit#) for readings\n\n- [Vael Gates: Researcher Perceptions of Current and Future AI](https://youtu.be/yl2nlejBcg0)\n    \n    \n\n- [Intro to AI Safety, Remastered](https://youtu.be/pYXy-A4siMw)\n    \n     (Rob Miles)\n\n- [Connor Leahy, AI Fire Alarm](https://youtu.be/pGjyiqJZPJo)\n    \n    , [AI Alignment & AGI Fire Alarm - Connor Leahy](https://youtu.be/HrV19SjKUss)\n    \n     (ML Street Talk), and [Connor Leahy on AI Safety and Why the World is Fragile](https://youtu.be/cSL3Zau1X8g)\n    \n    \n\n- [Positive Outcomes for AI | Nate Soares | Talks at Google](https://youtu.be/dY3zDvoLoao)\n    \n    \n\n- [Eliezer Yudkowsky \u2013 AI Alignment: Why It's Hard, and Where to Start](https://youtu.be/EUjc1WuyPT8)\n    \n     and [Sam Harris 2018 - IS vs OUGHT, Robots of The Future Might Deceive Us with Eliezer Yudkowsky](https://youtu.be/JuvonhJrzQ0)\n    \n     ([full transcript here](https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/)) and [159 - We\u2019re All Gonna Die with Eliezer Yudkowsky](https://youtu.be/gA1sNLL6yg4)\n    \n    \n\n- [Brian Christian](https://80000hours.org/podcast/episodes/brian-christian-the-alignment-problem/) and [Ben Garfinkel](https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/) and [Richard Ngo](https://80000hours.org/podcast/episodes/richard-ngo-large-language-models/) and [Paul Christiano](https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/) on the [80,000 Hours Podcast](https://80000hours.org/podcast/)\n\n- [Richard Ngo](https://axrp.net/episode/2022/03/31/episode-13-first-principles-agi-safety-richard-ngo.html) and [Paul Christiano](https://axrp.net/episode/2021/12/02/episode-12-ai-xrisk-paul-christiano.html) on [AXRP](https://axrp.net/)\n\n- [Ajeya Cotra on how Artificial Intelligence Could Cause Catastrophe](https://youtu.be/IKFQfYaJ0AY)\n    \n    \n\n- [Jeremie Harris - TDS Podcast Finale: The future of AI, and the risks that come with\u00a0it](https://youtu.be/cqCnqW4oZog)\n    \n    \n\n- [Rohin Shah on the State of AGI Safety Research in 2021](https://youtu.be/_5xkh-Rh6Ec)\n    \n     and [AI Alignment: An Introduction | Rohin Shah | EAGxOxford 22](https://youtu.be/MKlXU_Ztw9s)\n    \n    \n\n- [The Alignment Problem: Machine Learning and Human Values with Brian Christian](https://youtu.be/z6atNBhItBs)\n    \n     Q&A section\n\n- [David Krueger  AI Safety and Alignment- Part 1](https://youtu.be/HtSq2793zh4)\n    \n     and [part 2](https://www.youtube.com/watch?v=vGR6pWF5JeI&list=PLnyQfwIuLcvBqxVqPF2UvILRqOFLuO4PM&index=2)\n\n- [A Response to Steven Pinker on AI](https://youtu.be/yQE9KAbFhNY)\n    \n    \n\n- [X-Risk Overview](https://youtu.be/3eP2WcFE20w)\n    \n     (Dan Hendrycks)\n\n- [Provably Beneficial AI | Stuart Russell](https://youtu.be/Kw_1N9Nfir0)\n    \n    \n\n- [Stuart Russell's BBC Reith lecture audio](https://www.bbc.co.uk/programmes/m001216k/episodes/guide)\n\n- [Myths and Facts About Superintelligent AI](https://youtu.be/3Om9ssTm194)\n    \n     (Max Tegmark + minutephysics)\n\n### Landscape\n\n- [Current work in AI alignment | Paul Christiano | EA Global: San Francisco 2019](https://youtu.be/-vsYtevJ2bc)\n    \n    \n\n- [Paradigms of AI alignment: components and enablers | Victoria Krakovna | EAGxVirtual 2022](https://youtu.be/IqqEEB6xcsA)\n    \n    \n\n- [How to build a safe advanced AI (Evan Hubinger) | What's up in AI safety? (Asya Bergal)](https://youtu.be/jn8eqpMLFlQ)\n    \n    \n\n- [Rohin Shah on the State of AGI Safety Research in 2021](https://youtu.be/_5xkh-Rh6Ec)\n    \n    \n\n### Inner alignment\n\n- [Risks from Learned Optimization: Evan Hubinger at MLAB2](https://youtu.be/OUifSs28G30)\n    \n    \n\n- [The OTHER AI Alignment Problem: Mesa-Optimizers and Inner Alignment](https://youtu.be/bJLcIBixGj8)\n    \n    \n\n- [Deceptive Misaligned Mesa-Optimisers? It's More Likely Than You Think...](https://youtu.be/IeWljQw3UgQ)\n    \n    \n\n- [We Were Right! Real Inner Misalignment](https://youtu.be/zkbPdEHEyEI)\n    \n    \n\n### Outer alignment\n\n- [9 Examples of Specification Gaming](https://youtu.be/nKJlF-olKmg)\n    \n    \n\n- [How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification](https://youtu.be/v9M2Ho9I9Qo)\n    \n    \n\n- [AI Toy Control Problem](https://youtu.be/sx8JkdbNgdU)\n    \n     (Stuart Armstrong)\n\n- [AIS via Debate](https://drive.google.com/drive/u/0/folders/1M_nDAz9JB_jnO9gDX29C2COCP9Ts4h7H) (Joe Collman)\n\n- [Another Outer Alignment Failure Story](https://www.youtube.com/watch?v=hYL8UMDIDFM)\n\n### Agent foundations\n\n- [AI & Logical Induction - Computerphile](https://youtu.be/gDqkCxYYDGk)\n    \n    \n\n- [Intro to Agent Foundations (Understanding Infra-Bayesianism Part 4)](https://youtu.be/SLQDNRbihsk)\n    \n    \n\n- [Scott Garrabrant \u2013 Finite Factored Sets](https://youtu.be/fvj8fi5azTM)\n    \n    \n\n- [EC'21 Tutorial: Designing Agents' Preferences, Beliefs, and Identities (Part 3)](https://youtu.be/lviqTJuj53g)\n    \n     and [part 4](https://www.youtube.com/watch?v=0JcBhF8Qt2Q) from [FOCAL at CMU](https://www.cs.cmu.edu/~focal/)\n\n### Interpretability\n\n- See the [interpretability playground](https://alignmentjam.com/interpretability-playground)\n\n- [A Walkthrough of Toy Models of Superposition w/ Jess Smith](https://www.youtube.com/watch?v=R3nbXgMnVqQ)\n\n- [ROME: Locating and Editing Factual Associations in GPT (Paper Explained & Author Interview)](https://youtu.be/_NMQyOu2HTo)\n    \n    \n\n- [Feature Visualization & The OpenAI microscope](https://youtu.be/Ok44otx90D4)\n    \n     and [Building Blocks of AI Interpretability | Two Minute Papers #234](https://youtu.be/pVgC-7QTr40)\n    \n    \n\n- [ICLR 2022 Keynote: Been Kim](https://youtu.be/Ub45cGEcTB0)\n    \n    \n\n- [25. Interpretability](https://youtu.be/wDLzLN1tArA)\n    \n     (MIT 6.S897 Machine Learning for Healthcare, Spring 2019)\n\n- [Cohere For AI - Community Talks - Catherine Olsson on Mechanistic Interpretability: Getting Started](https://youtu.be/ll0oduwDEwI)\n    \n    \n\n- [Transformer Circuit Videos](https://transformer-circuits.pub/2021/videos/index.html) + [YouTube playlist](https://www.youtube.com/playlist?list=PLoyGOS2WIonajhAVqKUgEMNmeq3nEeM51)\n\n- [A Walkthrough of A Mathematical Framework for Transformer Circuits](https://youtu.be/KV5gbOmHbjU)\n    \n    \n\n- [A Walkthrough of Interpretability in the Wild Part 1/2: Overview (w/ authors Kevin, Arthur, Alex)](https://youtu.be/gzwj0jWbvbo)\n    \n     and [part 2](https://www.youtube.com/watch?v=b9xfYBKIaX4)\n\n- [Intro talk for the interpretability hackathon](https://youtu.be/kzGggSiMRoA)\n    \n    \n\n- [Reliable and Interpretable Artificial Intelligence -- Lecture 1 (Introduction)](https://youtu.be/nTWXiV6Ic1M)\n    \n    \n\n- [Chris Olah on the 80,000 Hours Podcast](https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/)\n\n### Organizations\n\n- [AI alignment and Redwood Research | Buck Shlegeris (CTO)](https://youtu.be/PDvAutARum4)\n    \n    \n\n- [Daniela and Dario Amodei on Anthropic](https://youtu.be/uAA6PZkek4A)\n    \n    \n\n- [Alignment Research Center - Q&A with Mark Xu](https://www.youtube.com/watch?v=zI1PVkhTTjA)\n\n- [Training machine learning (ML) systems to answer open-ended questions | Andreas Stuhlmuller](https://youtu.be/7WaiYZLS94M)\n    \n      + [Amanda Ngo, Ought | Automating Complex Reasoning](https://youtu.be/kgCJvZXgL_o)\n    \n     (Ought)\n\n### Individual researchers\n\n- [Provably Beneficial AI and the Problem of Control](https://youtu.be/vv-jKO-vlcU)\n    \n     and [Human-compatible artificial intelligence - Stuart Russell, University of California](https://youtu.be/ApGusxR7JAc)\n    \n     (Stuart Russell)\n\n- [Victoria Krakovna\u2013AGI Ruin, Sharp Left Turn, Paradigms of AI Alignment](https://youtu.be/ZpwSNiLV-nw)\n    \n    \n\n- [David Krueger\u2014AI Alignment](https://youtu.be/bDMqo7BpNbk)\n    \n    , [David Krueger: Existential Safety, Alignment, and Specification Problems](https://youtu.be/gitJZzhLUSA)\n    \n    \n\n- [Holden Karnofsky - Transformative AI & Most Important Century](https://youtu.be/UckqpcOu5SY)\n    \n    \n\n- [A Conversation with John Wentworth](https://youtu.be/OXbw_HXAyU4)\n    \n    \n\n- [Connor Leahy | Promising Paths to Alignment](https://youtu.be/G4NcHrCz8yE)\n    \n    \n\n- [Prosaic Intent Alignment](https://youtu.be/PaHqqRy_Gww)\n    \n     (Paul Christiano)\n\n- [Timelines for Transformative AI and Language Model Alignment | Ajeya Cotra](https://youtu.be/FIYOtZW8yEM)\n    \n    \n\n- [AI Research Considerations for Existential Safety](https://youtu.be/qh666c6j4mk)\n    \n     (Andrew Critch)\n\n- [A Conversation with Vanessa Kosoy](https://youtu.be/N-fscpxMIos)\n    \n    \n\n- [Causal foundations for safe AGI - Tom Everitt (DeepMind)](https://youtu.be/FYTJui0MC24)\n    \n    \n\n- [Differential Progress in Cooperative AI: Motivation and Measurement](https://youtu.be/yr5rQKFrofI)\n    \n     (Jesse Clifton and Sammy Martin)\n\n- [Open-source learning: A bargaining approach | Jesse Clifton | EA Global: London 2019](https://youtu.be/QKRDp8nUMtk)\n    \n    \n\n- [Jan Leike - AI alignment at\u00a0OpenAI](https://youtu.be/vW89UcvMfjQ)\n    \n    \n\n- [AGISF - Research questions for the most important century - Holden Karnofsky](https://www.youtube.com/watch?v=CUziAFXsK0o)\n\n- [Scott Aaronson Talks AI Safety](https://youtu.be/fc-cHk9yFpg)\n    \n    \n\n- [BI 151 Steve Byrnes: Brain-like AGI Safety](https://youtu.be/w7k3o5uIBjs)\n    \n    \n\n- [Stuart Armstrong - How Could We Align AI?](https://youtu.be/sQgvZAml950)\n    \n    \n\n- [Owain Evans - Predicting the future of\u00a0AI](https://youtu.be/dea1XQojRmw)\n    \n    \n\n### Reasoning about future AI\n\n- [Optimal Policies Tend To Seek Power](https://neurips.cc/virtual/2021/poster/28400) (Alex Turner at NeurIPS 2021)\n\n- [AI \"Stop Button\" Problem - Computerphile](https://youtu.be/3TYT1QfdfsM)\n    \n    \n\n- [Intelligence and Stupidity: The Orthogonality Thesis](https://youtu.be/hEUO6pjwFOo)\n    \n    \n\n- [Why Would AI Want to do Bad Things? Instrumental Convergence](https://youtu.be/ZeecOKBus3Q)\n    \n    \n\n- [Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover](https://youtu.be/EIhE84kH2QI)\n    \n    \n\n- [Existential Risk from Power-Seeking AI  (Joe Carlsmith)](https://youtu.be/UbruBnv3pZU)\n    \n    \n\n### AI Governance\n\n- [Why governing AI is our opportunity to shape the long-term future? | Jade Leung | TEDxWarwickSalon](https://youtu.be/qjcA3rYzo54)\n    \n     + [Priorities in AGI governance research | Jade Leung | EA Global: SF 22](https://youtu.be/OfmL45J6tYs)\n    \n    \n\n- [An Introduction to AI Governance | Ben Garfinkel | EAGxVirtual 2022](https://youtu.be/NNmkz_DfR9w)\n    \n    \n\n- [The Windfall Clause: Sharing the benefits of advanced AI | Cullen O\u2019Keefe](https://youtu.be/vFDL-NxY610)\n    \n     +[Sharing the Benefits of AI: The Windfall Clause](https://youtu.be/7i_f4Kbpgn4)\n    \n     (Rob Miles)\n\n- [Margaret Roberts & Jeffrey Ding: Censorship\u2019s Implications for Artificial Intelligence](https://youtu.be/4MSAApBYgWc)\n    \n    \n\n- [Preparing for AI: risks and opportunities | Allan Dafoe | EAG 2017 London](https://youtu.be/RWKHx2bE1H4)\n    \n     + [AI Strategy, Policy, and Governance | Allan Dafoe](https://youtu.be/2IpJ8TIKKtI)\n    \n    \n\n- [More than Deepfakes](https://youtu.be/-oesD7CXmGs)\n    \n     (Katerina Sedova and John Bansemer)\n\n- [Markus Anderljung\u2013Regulating Advanced AI](https://youtu.be/DD303irN3ps)\n    \n    \n\n- [AI and the Development, Displacement, or Destruction of the Global Legal Order](https://youtu.be/a1gREVAQP58)\n    \n     (Matthijs Maas)\n\n- [Future-proofing AI Governance | The Athens Roundtable on AI and the Rule of Law 2022](https://youtu.be/ZYacyf5Pqtk)\n    \n    \n\n### Career planning\n\n- [How I think students should orient to AI safety | Buck Shlegeris | EA Student Summit 2020](https://youtu.be/R6Mzt4GwQnQ)\n    \n    \n\n- [Getting Started in AI Safety | Olivia Jimenez, Akash Wasil | EAGxVirtual 2022](https://youtu.be/di8XHw1y71A)\n    \n    \n\n- [AGISF - Careers in AI Alignment and Governance - Alex Lawsen](https://www.youtube.com/watch?v=pbt_1zRa7zE)\n\n- [Catherine Olsson & Daniel\u00a0Ziegler on the 80,000 Hours Podcast](https://80000hours.org/podcast/episodes/olsson-and-ziegler-ml-engineering-and-safety/)\n\n- [AI Safety Careers | Rohin Shah, Lewis Hammond and Jamie Bernardi | EAGxOxford 22](https://youtu.be/kwoAepfZ6co)\n    \n    \n\n- [Early-Career Opportunities in AI Governance | Lennart Heim, Caroline Jeanmaire | EAGxOxford 22](https://youtu.be/xZSazcEy-ms)\n    \n    \n\n- [Artificial Intelligence Career Stories | EA Student Summit 2020](https://youtu.be/FjMqY8hHK7Y)\n    \n    \n\n### Forecasting\n\n- [Neural Scaling Laws and GPT-3](https://youtu.be/QMqPAM_knrE)\n    \n     (Jared Kaplan)\n\n- [WHY AND HOW OF SCALING LARGE LANGUAGE MODELS | NICHOLAS JOSEPH](https://youtu.be/qscouq3lo0s)\n    \n    \n\n- [Jack Clark Presenting the 2022 AI Index Report](https://youtu.be/1xLxlMdcbpM)\n    \n    \n\n- [Reasons you might think human level AI soon is unlikely | Asya Bergal | EAGxVirtual 2020](https://youtu.be/oFJcvyxpGSo)\n    \n    \n\n- [Existential Risk Pessimism and the Time of Perils | David Thorstad | EAGxOxford 22](https://youtu.be/gn64IZ_wijk)\n    \n    \n\n- [Alex Lawsen\u2014Forecasting AI Progress](https://youtu.be/vLkasevJP5c)\n    \n     and [Alex Lawsen forecasting videos](https://www.youtube.com/channel/UCz_CdUW34eb9jePo-6VsLLQ/videos)\n\n- [Betting on AI is like betting on semiconductors in the 70's | Danny Hernandez | EA Global: SF 22](https://youtu.be/MryjWfCBeg4)\n    \n     + [Danny Hernandez on the 80,000 Hours Podcast](https://80000hours.org/podcast/episodes/danny-hernandez-forecasting-ai-progress/)\n\n- [AI safety | Katja Grace | EA Global: San Francisco 2017](https://youtu.be/r91Co5CeOCY)\n    \n    \n\n- [Experts' Predictions about the Future of AI](https://youtu.be/HOJ1NVtlnyQ)\n    \n    \n\n- [Economic Growth in the Long Run: Artificial Intelligence Explosion or an Empty Planet?](https://youtu.be/Qhmt9CuV6tU)\n    \n    \n\n- [Katja Grace\u2014Slowing Down AI, Forecasting AI Risk](https://youtu.be/rSw3UVDZge0)\n    \n    \n\n- [Sam Bowman - Are we *under-hyping* AI?](https://youtu.be/X3pzKjom7HE)\n    \n    \n\n- [Moore's Law, exponential growth, and extrapolation!](https://www.youtube.com/watch?v=iu-DgigiyeA) (Steve Brunton)\n\n### Capabilities\n\n- [Satya Nadella Full Keynote Microsoft Ignite 2022](https://youtu.be/pdSfgRYy8Ao)\n    \n     with Sam Altman, start at 12:25\n\n- [Competition-Level Code Generation with AlphaCode (Paper Review)](https://youtu.be/s9UAOmyah1A)\n    \n    \n\n- [It\u2019s Time to Pay Attention to A.I. (ChatGPT and Beyond)](https://youtu.be/0uQqMxXoNVs)\n    \n    \n\n- [The text-to-image revolution, explained](https://youtu.be/SVcsDDABEkM)\n    \n     + [How the World Cup\u2019s AI instant replay works](https://youtu.be/C164kYMGV1A)\n    \n     (Vox)\n\n- [Creating a Space Game with OpenAI Codex](https://youtu.be/Zm9B-DvwOgw)\n    \n    \n\n### How AI works\n\n- [AGISF Week 0 Intro to ML](https://www.youtube.com/watch?v=v5c2Ll9129k)\n\n- [Transformers, explained: Understand the model behind GPT, BERT, and T5](https://youtu.be/SZorAJ4I-sA)\n    \n    ; [Transformers for beginners | What are they and how do they work](https://youtu.be/_UVfwBqcnbM)\n    \n    \n\n- [Reinforcement learning playlist](https://www.youtube.com/playlist?list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74) (Steve Brunton)\n\n- [How AI Image Generators Work (Stable Diffusion / Dall-E) - Computerphile](https://youtu.be/1CIpzeNxIhU)\n    \n     + [Stable Diffusion in Code (AI Image Generation) - Computerphile](https://youtu.be/-lz30by8-sU)\n    \n    \n\n- [Attention in Neural Networks](https://youtu.be/W2rWgXJBZhU)\n    \n    \n\n- [What is a transformer?](https://www.youtube.com/watch?v=VMvpQhNkm8w&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz&index=2) + [Implementing GPT-2 from scratch](https://www.youtube.com/watch?v=R9t-VS6PM3M&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz&index=2) (Neel Nanda)\n\n- [The spelled-out intro to neural networks and backpropagation: building micrograd](https://youtu.be/VMj-3S1tku0)\n    \n    \n\n- [Neural Networks (3Blue1Brown)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n\n- [DeepMind x UCL RL Lecture Series - Introduction to Reinforcement Learning [1/13]](https://youtu.be/TCCjZe0y4Qc)\n    \n    \n\n- [Generative Adversarial Networks (GANs) - Computerphile](https://youtu.be/Sw9r8CL98N0)\n    \n    \n\n- [Deep Learning for Computer Vision (Justin Johnson) lecture videos](https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/schedule.html)\n\n- [CS25 I Stanford Seminar - Transformers United: DL Models that have revolutionized NLP, CV, RL](https://youtu.be/P127jhj-8-Y)\n    \n    \n\n- [Broderick: Machine Learning, MIT 6.036 Fall 2020](https://www.youtube.com/playlist?list=PLxC_ffO4q_rW0bqQB80_vcQB09HOA3ClV) + [course page](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/course/)\n\n- [DEEP LEARNING - DS-GA 1008 \u00b7 Spring 2020 \u00b7 NYU](https://atcold.github.io/pytorch-Deep-Learning/)\n\n- [Practical Deep Learning course | fast.ai](https://course.fast.ai/)\n\n- [Hugging Face NLP Course](https://huggingface.co/course)\n\n### China\n\n- [Re-deciphering China\u2019s AI dream | Jeffrey Ding | EA Global: London 2019](https://youtu.be/1mExA_xdgnA)\n    \n    \n\n- [Sino-Western cooperation in AI safety | Brian Tse | EA Global: San Francisco 2019](https://youtu.be/3qYmLRqemg4)\n    \n    \n\n- [China's Long-Term Investments in AI Growth](https://youtu.be/ex0O0qBB2Xg)\n    \n    \n\n- [Why China is losing the microchip war](https://youtu.be/Uh4QGey2zTk)\n    \n     (Vox)\n\n### Rationality\n\n- [Effective behavior change | Spencer Greenberg | EA Global: San Francisco 2019](https://youtu.be/ulLQE3g_KRI)\n    \n    \n\n- [Making high impact decisions | Anna Edmonds | EA Global: SF 22](https://youtu.be/_r8HL8UiLYI)\n    \n    \n\n- [Decision-making workshop: learn how to make better decisions | Spencer Greenberg](https://youtu.be/bXJ6U13SSmA)\n    \n    \n\n- [Decoupling: a technique for reducing bias | David Manley | EA Student Summit 2020](https://youtu.be/GNBIsChKWpc)\n    \n    \n\n- [Rationality & Alignment | Ruby Bloom | EA Global: SF 22](https://youtu.be/6rw6A7BL5W4)\n    \n    \n\n### Misc\n\n- [The Doomsday Argument](https://youtu.be/dSvgw9ZOK3I)\n    \n     | PBS Space Time\n\n- [How will AI change the world?](https://youtu.be/RzkD_rTEBYs)\n    \n    \n\n- [Forming your own views on AI safety (without stress!) | Neel Nanda | EA Global: SF 22](https://youtu.be/edoQ3CiNa_s)\n    \n     \u2013 also see [Neel's presentation slides](https://docs.google.com/presentation/d/1LMBfw2wUmGLEvRTi8019DX_uGdJTeGsHRPRa5S8UIWs/edit#slide=id.p) and \"[Inside Views Resources](https://docs.google.com/document/d/1O3ci6Q0UZ8qeBpKs6lVxFGLWiH89fBtz-lLG2iN8YNc/edit#)\" doc\n\n- [Applied Linear Algebra Lectures](https://www.youtube.com/playlist?list=PLiayR7yJx8-aCfBlccBjF1t-UO86fZJVu) (John Wentworth)\n\n- [AI alignment, philosophical pluralism, and the relevance of non-Western philosophy | Tan Zhi Xuan](https://youtu.be/dbMp4pFVwnU)\n    \n    \n\n- [Anders Sandberg on Information Hazards](https://youtu.be/Wn2vgQGNI_c)\n    \n    \n\n- [Reframing superintelligence | Eric Drexler | EA Global: London 2018](https://youtu.be/MircoV5LKvg)\n    \n    \n\n- [Robin Hanson | The Age of Em](https://youtu.be/eK5qxAA60PQ)\n    \n    \n\n- [AlphaGo - The Movie | Full award-winning documentary](https://youtu.be/WXuK6gekU1Y)\n    \n    \n\n- [Should We Build Superintelligence?](https://www.youtube.com/watch?v=xLYE11yW-hQ)\n\n- [Moloch section of Liv Boeree interview with Lex Fridman](https://youtu.be/eF-E40pxxbI?t=4444) (and [Ginsberg](https://youtu.be/x-P2fILsLH8?t=980))\n\n## Watching videos in a group\n\n### Discussion prompts\n\n- [Paul Christiano on AI alignment - discussion](https://docs.google.com/document/d/1TALeBEoLdOPNQqOdah542TUuPwQ06UDSSE59eQP8MMA/edit) + [Paul Christiano alignment chart](https://docs.google.com/document/d/1hyqCSsX1mjzepHONO3p2baaIfDOLTzJGhfSUS4uov-s/edit)\n\n- [Allan Dafoe on AI strategy, policy, and governance - discussion](https://docs.google.com/document/d/1cGteO2yILEtgWN_ILL6FgCrRxzCaFbikz1jGdiDF_zU/edit#)\n\n- [Vael Gates: Researcher Perceptions of Current and Future AI - discussion](https://docs.google.com/document/d/1-MPJWcJtnlZzSx1H9J7JxUeRl-UOr4MgcWPVVfWP8zY/edit?usp=sharing)\n\n- [Sam Harris and Eliezer Yudkowsky on \u201cAI: Racing Toward the Brink\u201d -- discussion](https://docs.google.com/document/d/155RJRbdAui7e5yfjrkDnntZ4qrcZY9kyYsVqBAGZnis/edit)\n\n### Higher-level meeting tips\n\n- Show the video \u2192 people discuss afterwards with prompts\n\n- **Active learning techniques:** [here](https://teaching.cornell.edu/getting-started-active-learning-techniques)\n\n- You can **skip around** through parts of the video!\n\n- See \u201c[Discussion Groups](https://resources.eagroups.org/events-program-ideas/single-day-events/discussion-groups)\u201d from the EA Groups Resource Center\n\n- Make the official meeting end after 1 hour so people are free to leave, but give people the option to linger for longer and continue their discussion.\n\n- You can also do **readings instead of videos**, similar to [HAIST](https://resources.aisafetyhub.org/haist-style-reading-groups). Or **play around with a model** (e.g. test out hypotheses about how a language model works).\n\n- Try to keep the video portion to under 20 minutes unless the video is really interesting.\n\n- For a short video you could watch one of Apart\u2019s [ML + AI safety updates](https://www.youtube.com/watch?v=saiqSZkDBSk). Some of these contain many topics, so people can discuss what they find interesting.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "*Also available at* *[aisafety.video](https://aisafety.video)*\n\n## Generally good sources\n\n### YouTube channels\n\n- [Rob Miles](https://www.youtube.com/c/RobertMilesAI) (and [Computerphile](https://www.youtube.com/playlist?list=PLzH6n4zXuckquVnQ0KlMDxyT5YE-sA8Ps))\n\n- [Apart - Safe AI](https://www.youtube.com/channel/UCnfBOJnTkE9sgjMOOsQbi2w/)\n\n- [Neel Nanda](https://www.youtube.com/@neelnanda2469)\n\n- [Center for AI Safety](https://www.youtube.com/channel/UCY_K5gXsXHtuiP8mj3BiWxA/videos)\n\n- [Towards Data Science](https://www.youtube.com/c/TowardsDataScience/videos)\n\n- [The Inside View](https://www.youtube.com/c/TheInsideView)\n\n- [Future of Life Institute](https://www.youtube.com/channel/UC-rCCy3FQ-GItDimSR9lhzw/videos)\n\n- [SERI](https://www.youtube.com/channel/UC_UHeaVeJoI7NGUSXqpBIiQ/videos)\n\n- [CERI](https://www.youtube.com/@cambridgeexistentialrisksi9588)\n\n- [AI Safety Talks](https://www.youtube.com/@aisafetytalks/featured) (and its [playlists](https://www.youtube.com/@aisafetytalks/playlists))\n\n- [AI Safety Reading Group](https://www.youtube.com/@aisafetyreadinggroup)\n\n- [Mechanistic Interpretability](https://www.youtube.com/@mechanisticinterpretabilit5092/)\n\n- [Intro to ML Safety](https://www.youtube.com/@introtomlsafety/videos)\n\n- [AGI safety talks from AGISF](https://www.agisafetyfundamentals.com/agi-safety-talks)\n\n- [AISS discussion days](https://www.aisafetysupport.org/events/discussion-days) and [AISS YouTube](https://www.youtube.com/@aiss8214)\n\n- [Victoria Krakovna AI talks](https://vkrakovna.wordpress.com/talks/)\n\n- [Jack Parker](https://www.youtube.com/channel/UCsLNgZySqFN4ybdEma0XCRg/videos)\n\n- [PIBBSS](https://www.youtube.com/@pibbssfellowship1034/videos)\n\n- [MIRI](https://www.youtube.com/@MIRIBerkeley)\n\n- [The Future Society](https://www.youtube.com/@thefuturesoc)\n\n- [Cooperative AI Foundation](https://www.youtube.com/@CooperativeAIFoundation/)\n\n- [Quantified Uncertainty Research Institute](https://www.youtube.com/@quantifieduncertainty)\n\n- **Other languages**: [Karl Olsberg](https://www.youtube.com/@KarlOlsbergAutor/videos) (German)\n\n- Channels with less focus on AI existential risk: [Kurzgesagt](https://www.youtube.com/c/inanutshell), [Rational Animations](https://www.youtube.com/c/RationalAnimations), [Centre for Effective Altruism](https://www.youtube.com/c/EffectiveAltruismVideos), [Future of Humanity Institute](https://www.youtube.com/user/FHIOxford/videos), [Center for Security and Emerging Technology](https://www.youtube.com/@centerforsecurityandemergi9211), [CSER](https://www.youtube.com/c/CSERCambridge/videos), [SSC meetups](https://www.youtube.com/watch?v=Wn2vgQGNI_c&list=PLFDYxsqlH6uhSghWfsuEAiKDfZNVZhUOX), [Foresight Institute](https://www.youtube.com/@ForesightInstitute), [Science, Technology & the Future](https://www.youtube.com/@scfu), [Berkman Klein Center](https://www.youtube.com/@BKCHarvard), [Schwartz Reisman Institute](https://www.youtube.com/@SchwartzReismanInstitute), [Stanford HAI](https://www.youtube.com/@stanfordhai7626), [Carper AI](https://www.youtube.com/@carperai3790), [Lex Fridman](https://www.youtube.com/@lexfridman), [Digital Humanism](https://www.youtube.com/@DigitalHumanism/)\n\n- **AI content without much AI safety**: [Edan Meyer](https://www.youtube.com/@EdanMeyer/videos), [Yannic Kilcher](https://www.youtube.com/c/YannicKilcher?app=desktop), [Mutual Information](https://www.youtube.com/@Mutual_Information/), [Computerphile](https://www.youtube.com/@Computerphile), [CodeEmporium](https://www.youtube.com/@CodeEmporium), [Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy), [sentdex](https://www.youtube.com/@sentdex), [nPlan](https://www.youtube.com/@nPlan/), [Jay Alammar](https://www.youtube.com/@ai_io), [Assembly AI](https://www.youtube.com/@AssemblyAI), [Aleksa Gordi\u0107](https://www.youtube.com/@TheAIEpiphany), [Simons Institute](https://www.youtube.com/@SimonsInstituteTOC/), [2 Minute Papers](https://www.youtube.com/c/K\u00e1rolyZsolnai/videos), [Machine Learning Street Talk](https://www.youtube.com/@MachineLearningStreetTalk), [ColdFusion](https://www.youtube.com/@ColdFusion/), [HuggingFace](https://www.youtube.com/@HuggingFace), [AI Coffee Break](https://www.youtube.com/@AICoffeeBreak), [Alex Smola](https://www.youtube.com/@smolix/), [Welcome AI Overlords](https://www.youtube.com/@welcomeaioverlords/), [Valence Discovery](https://www.youtube.com/@valencediscovery6139/), [The Alan Turing Institute](https://www.youtube.com/@TheAlanTuringInstituteUK/), [Jordan Harrod](https://www.youtube.com/@JordanHarrod), [Cambridge Ellis Unit](https://www.youtube.com/@ellisunit8739), [UCL CSML Seminar Series](https://www.youtube.com/@deepmindellisuclcsmlsemina3383/), [Harvard Medical AI](https://www.youtube.com/@harvard-medicalai-rajpurkarlab), [IARAI](https://www.youtube.com/@iarai/), [Alfredo Canziani](https://www.youtube.com/@alfcnz),  [Andreas Geiger](https://www.youtube.com/@cvlibs/), [CMU AI Seminar](https://www.youtube.com/@cmuaiseminar1950/), [Jeremy Howard](https://www.youtube.com/@howardjeremyp/), [Google Research](https://www.youtube.com/@GoogleResearch/), [AI for Good](https://www.youtube.com/@AIforGood/), [IPAM UCLA](https://www.youtube.com/@IPAMUCLA), [One world theoretical ML](https://www.youtube.com/@oneworldtheoreticalmachine110), [What's AI](https://www.youtube.com/@WhatsAI/), [Stanford MedAI](https://www.youtube.com/@stanfordmedai1263), [MILA neural scaling seminars](https://sites.google.com/view/nsl-course/invited-talks), [Digital Engine](https://www.youtube.com/@DigitalEngine/) (sometimes misleading), [Steve Brunton](https://www.youtube.com/@Eigensteve/), [PyTorch](https://www.youtube.com/@PyTorch/), [What's AI by Louis Bouchard](https://www.youtube.com/@WhatsAI/), [Eye on AI](https://www.youtube.com/@eyeonai3425), [AI Explained](https://www.youtube.com/@ai-explained-/)\n\n### Relevant **podcasts** without video\n\n- [AXRP](https://axrp.net/)\n\n- [Crunch Time](https://open.spotify.com/show/0xhSRFWy8iEhcuBINsVT63)\n\n- [Alignment Newsletter podcast](https://www.youtube.com/@alignmentnewsletterpodcast4310/videos)\n\n- [Rationality A-Z](https://podcasts.apple.com/us/podcast/rationality-from-ai-to-zombies/id1299826696)\n\n- [Nonlinear Library](https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library)\n\n- [LessWrong Curated Podcast](https://www.lesswrong.com/posts/kDjKF2yFhFEWe4hgC/announcing-the-lesswrong-curated-podcast)\n\n- [EA Forum Podcast](https://forum.effectivealtruism.org/posts/K5Snxo5EhgmwJJjR2/announcing-audio-narrations-of-ea-forum-posts-1)\n\n- [Cold Takes Audio](https://podcasts.apple.com/us/podcast/cold-takes-audio/id1580097837)\n\n- [ACX Podcast](https://sscpodcast.libsyn.com/)\n\n- [80,000 Hours](https://www.youtube.com/user/eightythousandhours/videos)\n\n- [Technical AI Safety Podcast](https://open.spotify.com/show/6BgMgUSY95kL9gSyOPpper)\n\n- [EA Radio](https://www.earad.io/)\n\n- [Rationally Speaking Podcast](http://rationallyspeakingpodcast.org/)\n\n- [Replacing Guilt](https://anchor.fm/guilt)\n\n- [Hear This Idea](https://hearthisidea.com/episodes)\n\n### Lists\n\n- **[AI Alignment YouTube Playlists](https://www.lesswrong.com/posts/tas3QZfKm4bdnAWsZ/ai-alignment-youtube-playlists)**\u2013 excellent resource. [Slide-light](https://www.youtube.com/playlist?list=PLTYHZYmxohXp0xvVJmMmpT_eFJovlzn0l) ([reordered](https://www.youtube.com/playlist?list=PLCRVRLd2RhZTpdUdEzJjo3qhmX3y3skWA)) and [slide-heavy](https://www.youtube.com/playlist?list=PLTYHZYmxohXpn5uf8JZ2OouB1PsDJAk-x) playlists.\n\n- [the gears to ascenscion](https://www.lesswrong.com/posts/rKmojEZ9qKwApjCfX/the-gears-to-ascenscion-s-shortform?commentId=jmWL85bGWpJmPJYzW) lists many channels for understanding current capabilities trends\n\n- [AI Safety Support \"Lots of Links\": Videos](https://www.aisafetysupport.org/resources/lots-of-links#h.6s2gcz1p5l6z)\n\n- [A ranked list of all EA-relevant documentaries movies and TV | Brian Tan on EAF](https://forum.effectivealtruism.org/posts/b4YW4GJR2RasS4YEw/a-ranked-list-of-all-ea-relevant-documentaries-movies-and-tv#AI_Safety___Risks)\n\n- [https://sites.google.com/view/towards-agi-course/topicspapers?authuser=0](https://sites.google.com/view/towards-agi-course/topicspapers?authuser=0)\n\n## Specific suggestions\n\nNote that:\n\n- I haven\u2019t watched all of these videos. Feel free to comment with more recommendations!\n\n- This list does not focus on podcasts, although there are a few podcast recommendations.\n\n### Introductory\n\n- See also [AI safety intros](https://docs.google.com/document/d/1zx_WpcwuT3Stpx8GJJHcvJLSgv6dLje0eslVKvuk1yQ/edit#) for readings\n\n- [Vael Gates: Researcher Perceptions of Current and Future AI](https://youtu.be/yl2nlejBcg0)\n    \n    \n\n- [Intro to AI Safety, Remastered](https://youtu.be/pYXy-A4siMw)\n    \n     (Rob Miles)\n\n- [Connor Leahy, AI Fire Alarm](https://youtu.be/pGjyiqJZPJo)\n    \n    , [AI Alignment & AGI Fire Alarm - Connor Leahy](https://youtu.be/HrV19SjKUss)\n    \n     (ML Street Talk), and [Connor Leahy on AI Safety and Why the World is Fragile](https://youtu.be/cSL3Zau1X8g)\n    \n    \n\n- [Positive Outcomes for AI | Nate Soares | Talks at Google](https://youtu.be/dY3zDvoLoao)\n    \n    \n\n- [Eliezer Yudkowsky \u2013 AI Alignment: Why It's Hard, and Where to Start](https://youtu.be/EUjc1WuyPT8)\n    \n     and [Sam Harris 2018 - IS vs OUGHT, Robots of The Future Might Deceive Us with Eliezer Yudkowsky](https://youtu.be/JuvonhJrzQ0)\n    \n     ([full transcript here](https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/)) and [159 - We\u2019re All Gonna Die with Eliezer Yudkowsky](https://youtu.be/gA1sNLL6yg4)\n    \n    \n\n- [Brian Christian](https://80000hours.org/podcast/episodes/brian-christian-the-alignment-problem/) and [Ben Garfinkel](https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/) and [Richard Ngo](https://80000hours.org/podcast/episodes/richard-ngo-large-language-models/) and [Paul Christiano](https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/) on the [80,000 Hours Podcast](https://80000hours.org/podcast/)\n\n- [Richard Ngo](https://axrp.net/episode/2022/03/31/episode-13-first-principles-agi-safety-richard-ngo.html) and [Paul Christiano](https://axrp.net/episode/2021/12/02/episode-12-ai-xrisk-paul-christiano.html) on [AXRP](https://axrp.net/)\n\n- [Ajeya Cotra on how Artificial Intelligence Could Cause Catastrophe](https://youtu.be/IKFQfYaJ0AY)\n    \n    \n\n- [Jeremie Harris - TDS Podcast Finale: The future of AI, and the risks that come with\u00a0it](https://youtu.be/cqCnqW4oZog)\n    \n    \n\n- [Rohin Shah on the State of AGI Safety Research in 2021](https://youtu.be/_5xkh-Rh6Ec)\n    \n     and [AI Alignment: An Introduction | Rohin Shah | EAGxOxford 22](https://youtu.be/MKlXU_Ztw9s)\n    \n    \n\n- [The Alignment Problem: Machine Learning and Human Values with Brian Christian](https://youtu.be/z6atNBhItBs)\n    \n     Q&A section\n\n- [David Krueger  AI Safety and Alignment- Part 1](https://youtu.be/HtSq2793zh4)\n    \n     and [part 2](https://www.youtube.com/watch?v=vGR6pWF5JeI&list=PLnyQfwIuLcvBqxVqPF2UvILRqOFLuO4PM&index=2)\n\n- [A Response to Steven Pinker on AI](https://youtu.be/yQE9KAbFhNY)\n    \n    \n\n- [X-Risk Overview](https://youtu.be/3eP2WcFE20w)\n    \n     (Dan Hendrycks)\n\n- [Provably Beneficial AI | Stuart Russell](https://youtu.be/Kw_1N9Nfir0)\n    \n    \n\n- [Stuart Russell's BBC Reith lecture audio](https://www.bbc.co.uk/programmes/m001216k/episodes/guide)\n\n- [Myths and Facts About Superintelligent AI](https://youtu.be/3Om9ssTm194)\n    \n     (Max Tegmark + minutephysics)\n\n### Landscape\n\n- [Current work in AI alignment | Paul Christiano | EA Global: San Francisco 2019](https://youtu.be/-vsYtevJ2bc)\n    \n    \n\n- [Paradigms of AI alignment: components and enablers | Victoria Krakovna | EAGxVirtual 2022](https://youtu.be/IqqEEB6xcsA)\n    \n    \n\n- [How to build a safe advanced AI (Evan Hubinger) | What's up in AI safety? (Asya Bergal)](https://youtu.be/jn8eqpMLFlQ)\n    \n    \n\n- [Rohin Shah on the State of AGI Safety Research in 2021](https://youtu.be/_5xkh-Rh6Ec)\n    \n    \n\n### Inner alignment\n\n- [Risks from Learned Optimization: Evan Hubinger at MLAB2](https://youtu.be/OUifSs28G30)\n    \n    \n\n- [The OTHER AI Alignment Problem: Mesa-Optimizers and Inner Alignment](https://youtu.be/bJLcIBixGj8)\n    \n    \n\n- [Deceptive Misaligned Mesa-Optimisers? It's More Likely Than You Think...](https://youtu.be/IeWljQw3UgQ)\n    \n    \n\n- [We Were Right! Real Inner Misalignment](https://youtu.be/zkbPdEHEyEI)\n    \n    \n\n### Outer alignment\n\n- [9 Examples of Specification Gaming](https://youtu.be/nKJlF-olKmg)\n    \n    \n\n- [How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification](https://youtu.be/v9M2Ho9I9Qo)\n    \n    \n\n- [AI Toy Control Problem](https://youtu.be/sx8JkdbNgdU)\n    \n     (Stuart Armstrong)\n\n- [AIS via Debate](https://drive.google.com/drive/u/0/folders/1M_nDAz9JB_jnO9gDX29C2COCP9Ts4h7H) (Joe Collman)\n\n- [Another Outer Alignment Failure Story](https://www.youtube.com/watch?v=hYL8UMDIDFM)\n\n### Agent foundations\n\n- [AI & Logical Induction - Computerphile](https://youtu.be/gDqkCxYYDGk)\n    \n    \n\n- [Intro to Agent Foundations (Understanding Infra-Bayesianism Part 4)](https://youtu.be/SLQDNRbihsk)\n    \n    \n\n- [Scott Garrabrant \u2013 Finite Factored Sets](https://youtu.be/fvj8fi5azTM)\n    \n    \n\n- [EC'21 Tutorial: Designing Agents' Preferences, Beliefs, and Identities (Part 3)](https://youtu.be/lviqTJuj53g)\n    \n     and [part 4](https://www.youtube.com/watch?v=0JcBhF8Qt2Q) from [FOCAL at CMU](https://www.cs.cmu.edu/~focal/)\n\n### Interpretability\n\n- See the [interpretability playground](https://alignmentjam.com/interpretability-playground)\n\n- [A Walkthrough of Toy Models of Superposition w/ Jess Smith](https://www.youtube.com/watch?v=R3nbXgMnVqQ)\n\n- [ROME: Locating and Editing Factual Associations in GPT (Paper Explained & Author Interview)](https://youtu.be/_NMQyOu2HTo)\n    \n    \n\n- [Feature Visualization & The OpenAI microscope](https://youtu.be/Ok44otx90D4)\n    \n     and [Building Blocks of AI Interpretability | Two Minute Papers #234](https://youtu.be/pVgC-7QTr40)\n    \n    \n\n- [ICLR 2022 Keynote: Been Kim](https://youtu.be/Ub45cGEcTB0)\n    \n    \n\n- [25. Interpretability](https://youtu.be/wDLzLN1tArA)\n    \n     (MIT 6.S897 Machine Learning for Healthcare, Spring 2019)\n\n- [Cohere For AI - Community Talks - Catherine Olsson on Mechanistic Interpretability: Getting Started](https://youtu.be/ll0oduwDEwI)\n    \n    \n\n- [Transformer Circuit Videos](https://transformer-circuits.pub/2021/videos/index.html) + [YouTube playlist](https://www.youtube.com/playlist?list=PLoyGOS2WIonajhAVqKUgEMNmeq3nEeM51)\n\n- [A Walkthrough of A Mathematical Framework for Transformer Circuits](https://youtu.be/KV5gbOmHbjU)\n    \n    \n\n- [A Walkthrough of Interpretability in the Wild Part 1/2: Overview (w/ authors Kevin, Arthur, Alex)](https://youtu.be/gzwj0jWbvbo)\n    \n     and [part 2](https://www.youtube.com/watch?v=b9xfYBKIaX4)\n\n- [Intro talk for the interpretability hackathon](https://youtu.be/kzGggSiMRoA)\n    \n    \n\n- [Reliable and Interpretable Artificial Intelligence -- Lecture 1 (Introduction)](https://youtu.be/nTWXiV6Ic1M)\n    \n    \n\n- [Chris Olah on the 80,000 Hours Podcast](https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/)\n\n### Organizations\n\n- [AI alignment and Redwood Research | Buck Shlegeris (CTO)](https://youtu.be/PDvAutARum4)\n    \n    \n\n- [Daniela and Dario Amodei on Anthropic](https://youtu.be/uAA6PZkek4A)\n    \n    \n\n- [Alignment Research Center - Q&A with Mark Xu](https://www.youtube.com/watch?v=zI1PVkhTTjA)\n\n- [Training machine learning (ML) systems to answer open-ended questions | Andreas Stuhlmuller](https://youtu.be/7WaiYZLS94M)\n    \n      + [Amanda Ngo, Ought | Automating Complex Reasoning](https://youtu.be/kgCJvZXgL_o)\n    \n     (Ought)\n\n### Individual researchers\n\n- [Provably Beneficial AI and the Problem of Control](https://youtu.be/vv-jKO-vlcU)\n    \n     and [Human-compatible artificial intelligence - Stuart Russell, University of California](https://youtu.be/ApGusxR7JAc)\n    \n     (Stuart Russell)\n\n- [Victoria Krakovna\u2013AGI Ruin, Sharp Left Turn, Paradigms of AI Alignment](https://youtu.be/ZpwSNiLV-nw)\n    \n    \n\n- [David Krueger\u2014AI Alignment](https://youtu.be/bDMqo7BpNbk)\n    \n    , [David Krueger: Existential Safety, Alignment, and Specification Problems](https://youtu.be/gitJZzhLUSA)\n    \n    \n\n- [Holden Karnofsky - Transformative AI & Most Important Century](https://youtu.be/UckqpcOu5SY)\n    \n    \n\n- [A Conversation with John Wentworth](https://youtu.be/OXbw_HXAyU4)\n    \n    \n\n- [Connor Leahy | Promising Paths to Alignment](https://youtu.be/G4NcHrCz8yE)\n    \n    \n\n- [Prosaic Intent Alignment](https://youtu.be/PaHqqRy_Gww)\n    \n     (Paul Christiano)\n\n- [Timelines for Transformative AI and Language Model Alignment | Ajeya Cotra](https://youtu.be/FIYOtZW8yEM)\n    \n    \n\n- [AI Research Considerations for Existential Safety](https://youtu.be/qh666c6j4mk)\n    \n     (Andrew Critch)\n\n- [A Conversation with Vanessa Kosoy](https://youtu.be/N-fscpxMIos)\n    \n    \n\n- [Causal foundations for safe AGI - Tom Everitt (DeepMind)](https://youtu.be/FYTJui0MC24)\n    \n    \n\n- [Differential Progress in Cooperative AI: Motivation and Measurement](https://youtu.be/yr5rQKFrofI)\n    \n     (Jesse Clifton and Sammy Martin)\n\n- [Open-source learning: A bargaining approach | Jesse Clifton | EA Global: London 2019](https://youtu.be/QKRDp8nUMtk)\n    \n    \n\n- [Jan Leike - AI alignment at\u00a0OpenAI](https://youtu.be/vW89UcvMfjQ)\n    \n    \n\n- [AGISF - Research questions for the most important century - Holden Karnofsky](https://www.youtube.com/watch?v=CUziAFXsK0o)\n\n- [Scott Aaronson Talks AI Safety](https://youtu.be/fc-cHk9yFpg)\n    \n    \n\n- [BI 151 Steve Byrnes: Brain-like AGI Safety](https://youtu.be/w7k3o5uIBjs)\n    \n    \n\n- [Stuart Armstrong - How Could We Align AI?](https://youtu.be/sQgvZAml950)\n    \n    \n\n- [Owain Evans - Predicting the future of\u00a0AI](https://youtu.be/dea1XQojRmw)\n    \n    \n\n### Reasoning about future AI\n\n- [Optimal Policies Tend To Seek Power](https://neurips.cc/virtual/2021/poster/28400) (Alex Turner at NeurIPS 2021)\n\n- [AI \"Stop Button\" Problem - Computerphile](https://youtu.be/3TYT1QfdfsM)\n    \n    \n\n- [Intelligence and Stupidity: The Orthogonality Thesis](https://youtu.be/hEUO6pjwFOo)\n    \n    \n\n- [Why Would AI Want to do Bad Things? Instrumental Convergence](https://youtu.be/ZeecOKBus3Q)\n    \n    \n\n- [Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover](https://youtu.be/EIhE84kH2QI)\n    \n    \n\n- [Existential Risk from Power-Seeking AI  (Joe Carlsmith)](https://youtu.be/UbruBnv3pZU)\n    \n    \n\n### AI Governance\n\n- [Why governing AI is our opportunity to shape the long-term future? | Jade Leung | TEDxWarwickSalon](https://youtu.be/qjcA3rYzo54)\n    \n     + [Priorities in AGI governance research | Jade Leung | EA Global: SF 22](https://youtu.be/OfmL45J6tYs)\n    \n    \n\n- [An Introduction to AI Governance | Ben Garfinkel | EAGxVirtual 2022](https://youtu.be/NNmkz_DfR9w)\n    \n    \n\n- [The Windfall Clause: Sharing the benefits of advanced AI | Cullen O\u2019Keefe](https://youtu.be/vFDL-NxY610)\n    \n     +[Sharing the Benefits of AI: The Windfall Clause](https://youtu.be/7i_f4Kbpgn4)\n    \n     (Rob Miles)\n\n- [Margaret Roberts & Jeffrey Ding: Censorship\u2019s Implications for Artificial Intelligence](https://youtu.be/4MSAApBYgWc)\n    \n    \n\n- [Preparing for AI: risks and opportunities | Allan Dafoe | EAG 2017 London](https://youtu.be/RWKHx2bE1H4)\n    \n     + [AI Strategy, Policy, and Governance | Allan Dafoe](https://youtu.be/2IpJ8TIKKtI)\n    \n    \n\n- [More than Deepfakes](https://youtu.be/-oesD7CXmGs)\n    \n     (Katerina Sedova and John Bansemer)\n\n- [Markus Anderljung\u2013Regulating Advanced AI](https://youtu.be/DD303irN3ps)\n    \n    \n\n- [AI and the Development, Displacement, or Destruction of the Global Legal Order](https://youtu.be/a1gREVAQP58)\n    \n     (Matthijs Maas)\n\n- [Future-proofing AI Governance | The Athens Roundtable on AI and the Rule of Law 2022](https://youtu.be/ZYacyf5Pqtk)\n    \n    \n\n### Career planning\n\n- [How I think students should orient to AI safety | Buck Shlegeris | EA Student Summit 2020](https://youtu.be/R6Mzt4GwQnQ)\n    \n    \n\n- [Getting Started in AI Safety | Olivia Jimenez, Akash Wasil | EAGxVirtual 2022](https://youtu.be/di8XHw1y71A)\n    \n    \n\n- [AGISF - Careers in AI Alignment and Governance - Alex Lawsen](https://www.youtube.com/watch?v=pbt_1zRa7zE)\n\n- [Catherine Olsson & Daniel\u00a0Ziegler on the 80,000 Hours Podcast](https://80000hours.org/podcast/episodes/olsson-and-ziegler-ml-engineering-and-safety/)\n\n- [AI Safety Careers | Rohin Shah, Lewis Hammond and Jamie Bernardi | EAGxOxford 22](https://youtu.be/kwoAepfZ6co)\n    \n    \n\n- [Early-Career Opportunities in AI Governance | Lennart Heim, Caroline Jeanmaire | EAGxOxford 22](https://youtu.be/xZSazcEy-ms)\n    \n    \n\n- [Artificial Intelligence Career Stories | EA Student Summit 2020](https://youtu.be/FjMqY8hHK7Y)\n    \n    \n\n### Forecasting\n\n- [Neural Scaling Laws and GPT-3](https://youtu.be/QMqPAM_knrE)\n    \n     (Jared Kaplan)\n\n- [WHY AND HOW OF SCALING LARGE LANGUAGE MODELS | NICHOLAS JOSEPH](https://youtu.be/qscouq3lo0s)\n    \n    \n\n- [Jack Clark Presenting the 2022 AI Index Report](https://youtu.be/1xLxlMdcbpM)\n    \n    \n\n- [Reasons you might think human level AI soon is unlikely | Asya Bergal | EAGxVirtual 2020](https://youtu.be/oFJcvyxpGSo)\n    \n    \n\n- [Existential Risk Pessimism and the Time of Perils | David Thorstad | EAGxOxford 22](https://youtu.be/gn64IZ_wijk)\n    \n    \n\n- [Alex Lawsen\u2014Forecasting AI Progress](https://youtu.be/vLkasevJP5c)\n    \n     and [Alex Lawsen forecasting videos](https://www.youtube.com/channel/UCz_CdUW34eb9jePo-6VsLLQ/videos)\n\n- [Betting on AI is like betting on semiconductors in the 70's | Danny Hernandez | EA Global: SF 22](https://youtu.be/MryjWfCBeg4)\n    \n     + [Danny Hernandez on the 80,000 Hours Podcast](https://80000hours.org/podcast/episodes/danny-hernandez-forecasting-ai-progress/)\n\n- [AI safety | Katja Grace | EA Global: San Francisco 2017](https://youtu.be/r91Co5CeOCY)\n    \n    \n\n- [Experts' Predictions about the Future of AI](https://youtu.be/HOJ1NVtlnyQ)\n    \n    \n\n- [Economic Growth in the Long Run: Artificial Intelligence Explosion or an Empty Planet?](https://youtu.be/Qhmt9CuV6tU)\n    \n    \n\n- [Katja Grace\u2014Slowing Down AI, Forecasting AI Risk](https://youtu.be/rSw3UVDZge0)\n    \n    \n\n- [Sam Bowman - Are we *under-hyping* AI?](https://youtu.be/X3pzKjom7HE)\n    \n    \n\n- [Moore's Law, exponential growth, and extrapolation!](https://www.youtube.com/watch?v=iu-DgigiyeA) (Steve Brunton)\n\n### Capabilities\n\n- [Satya Nadella Full Keynote Microsoft Ignite 2022](https://youtu.be/pdSfgRYy8Ao)\n    \n     with Sam Altman, start at 12:25\n\n- [Competition-Level Code Generation with AlphaCode (Paper Review)](https://youtu.be/s9UAOmyah1A)\n    \n    \n\n- [It\u2019s Time to Pay Attention to A.I. (ChatGPT and Beyond)](https://youtu.be/0uQqMxXoNVs)\n    \n    \n\n- [The text-to-image revolution, explained](https://youtu.be/SVcsDDABEkM)\n    \n     + [How the World Cup\u2019s AI instant replay works](https://youtu.be/C164kYMGV1A)\n    \n     (Vox)\n\n- [Creating a Space Game with OpenAI Codex](https://youtu.be/Zm9B-DvwOgw)\n    \n    \n\n### How AI works\n\n- [AGISF Week 0 Intro to ML](https://www.youtube.com/watch?v=v5c2Ll9129k)\n\n- [Transformers, explained: Understand the model behind GPT, BERT, and T5](https://youtu.be/SZorAJ4I-sA)\n    \n    ; [Transformers for beginners | What are they and how do they work](https://youtu.be/_UVfwBqcnbM)\n    \n    \n\n- [Reinforcement learning playlist](https://www.youtube.com/playlist?list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74) (Steve Brunton)\n\n- [How AI Image Generators Work (Stable Diffusion / Dall-E) - Computerphile](https://youtu.be/1CIpzeNxIhU)\n    \n     + [Stable Diffusion in Code (AI Image Generation) - Computerphile](https://youtu.be/-lz30by8-sU)\n    \n    \n\n- [Attention in Neural Networks](https://youtu.be/W2rWgXJBZhU)\n    \n    \n\n- [What is a transformer?](https://www.youtube.com/watch?v=VMvpQhNkm8w&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz&index=2) + [Implementing GPT-2 from scratch](https://www.youtube.com/watch?v=R9t-VS6PM3M&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz&index=2) (Neel Nanda)\n\n- [The spelled-out intro to neural networks and backpropagation: building micrograd](https://youtu.be/VMj-3S1tku0)\n    \n    \n\n- [Neural Networks (3Blue1Brown)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n\n- [DeepMind x UCL RL Lecture Series - Introduction to Reinforcement Learning [1/13]](https://youtu.be/TCCjZe0y4Qc)\n    \n    \n\n- [Generative Adversarial Networks (GANs) - Computerphile](https://youtu.be/Sw9r8CL98N0)\n    \n    \n\n- [Deep Learning for Computer Vision (Justin Johnson) lecture videos](https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/schedule.html)\n\n- [CS25 I Stanford Seminar - Transformers United: DL Models that have revolutionized NLP, CV, RL](https://youtu.be/P127jhj-8-Y)\n    \n    \n\n- [Broderick: Machine Learning, MIT 6.036 Fall 2020](https://www.youtube.com/playlist?list=PLxC_ffO4q_rW0bqQB80_vcQB09HOA3ClV) + [course page](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/course/)\n\n- [DEEP LEARNING - DS-GA 1008 \u00b7 Spring 2020 \u00b7 NYU](https://atcold.github.io/pytorch-Deep-Learning/)\n\n- [Practical Deep Learning course | fast.ai](https://course.fast.ai/)\n\n- [Hugging Face NLP Course](https://huggingface.co/course)\n\n### China\n\n- [Re-deciphering China\u2019s AI dream | Jeffrey Ding | EA Global: London 2019](https://youtu.be/1mExA_xdgnA)\n    \n    \n\n- [Sino-Western cooperation in AI safety | Brian Tse | EA Global: San Francisco 2019](https://youtu.be/3qYmLRqemg4)\n    \n    \n\n- [China's Long-Term Investments in AI Growth](https://youtu.be/ex0O0qBB2Xg)\n    \n    \n\n- [Why China is losing the microchip war](https://youtu.be/Uh4QGey2zTk)\n    \n     (Vox)\n\n### Rationality\n\n- [Effective behavior change | Spencer Greenberg | EA Global: San Francisco 2019](https://youtu.be/ulLQE3g_KRI)\n    \n    \n\n- [Making high impact decisions | Anna Edmonds | EA Global: SF 22](https://youtu.be/_r8HL8UiLYI)\n    \n    \n\n- [Decision-making workshop: learn how to make better decisions | Spencer Greenberg](https://youtu.be/bXJ6U13SSmA)\n    \n    \n\n- [Decoupling: a technique for reducing bias | David Manley | EA Student Summit 2020](https://youtu.be/GNBIsChKWpc)\n    \n    \n\n- [Rationality & Alignment | Ruby Bloom | EA Global: SF 22](https://youtu.be/6rw6A7BL5W4)\n    \n    \n\n### Misc\n\n- [The Doomsday Argument](https://youtu.be/dSvgw9ZOK3I)\n    \n     | PBS Space Time\n\n- [How will AI change the world?](https://youtu.be/RzkD_rTEBYs)\n    \n    \n\n- [Forming your own views on AI safety (without stress!) | Neel Nanda | EA Global: SF 22](https://youtu.be/edoQ3CiNa_s)\n    \n     \u2013 also see [Neel's presentation slides](https://docs.google.com/presentation/d/1LMBfw2wUmGLEvRTi8019DX_uGdJTeGsHRPRa5S8UIWs/edit#slide=id.p) and \"[Inside Views Resources](https://docs.google.com/document/d/1O3ci6Q0UZ8qeBpKs6lVxFGLWiH89fBtz-lLG2iN8YNc/edit#)\" doc\n\n- [Applied Linear Algebra Lectures](https://www.youtube.com/playlist?list=PLiayR7yJx8-aCfBlccBjF1t-UO86fZJVu) (John Wentworth)\n\n- [AI alignment, philosophical pluralism, and the relevance of non-Western philosophy | Tan Zhi Xuan](https://youtu.be/dbMp4pFVwnU)\n    \n    \n\n- [Anders Sandberg on Information Hazards](https://youtu.be/Wn2vgQGNI_c)\n    \n    \n\n- [Reframing superintelligence | Eric Drexler | EA Global: London 2018](https://youtu.be/MircoV5LKvg)\n    \n    \n\n- [Robin Hanson | The Age of Em](https://youtu.be/eK5qxAA60PQ)\n    \n    \n\n- [AlphaGo - The Movie | Full award-winning documentary](https://youtu.be/WXuK6gekU1Y)\n    \n    \n\n- [Should We Build Superintelligence?](https://www.youtube.com/watch?v=xLYE11yW-hQ)\n\n- [Moloch section of Liv Boeree interview with Lex Fridman](https://youtu.be/eF-E40pxxbI?t=4444) (and [Ginsberg](https://youtu.be/x-P2fILsLH8?t=980))\n\n## Watching videos in a group\n\n### Discussion prompts\n\n- [Paul Christiano on AI alignment - discussion](https://docs.google.com/document/d/1TALeBEoLdOPNQqOdah542TUuPwQ06UDSSE59eQP8MMA/edit) + [Paul Christiano alignment chart](https://docs.google.com/document/d/1hyqCSsX1mjzepHONO3p2baaIfDOLTzJGhfSUS4uov-s/edit)\n\n- [Allan Dafoe on AI strategy, policy, and governance - discussion](https://docs.google.com/document/d/1cGteO2yILEtgWN_ILL6FgCrRxzCaFbikz1jGdiDF_zU/edit#)\n\n- [Vael Gates: Researcher Perceptions of Current and Future AI - discussion](https://docs.google.com/document/d/1-MPJWcJtnlZzSx1H9J7JxUeRl-UOr4MgcWPVVfWP8zY/edit?usp=sharing)\n\n- [Sam Harris and Eliezer Yudkowsky on \u201cAI: Racing Toward the Brink\u201d -- discussion](https://docs.google.com/document/d/155RJRbdAui7e5yfjrkDnntZ4qrcZY9kyYsVqBAGZnis/edit)\n\n### Higher-level meeting tips\n\n- Show the video \u2192 people discuss afterwards with prompts\n\n- **Active learning techniques:** [here](https://teaching.cornell.edu/getting-started-active-learning-techniques)\n\n- You can **skip around** through parts of the video!\n\n- See \u201c[Discussion Groups](https://resources.eagroups.org/events-program-ideas/single-day-events/discussion-groups)\u201d from the EA Groups Resource Center\n\n- Make the official meeting end after 1 hour so people are free to leave, but give people the option to linger for longer and continue their discussion.\n\n- You can also do **readings instead of videos**, similar to [HAIST](https://resources.aisafetyhub.org/haist-style-reading-groups). Or **play around with a model** (e.g. test out hypotheses about how a language model works).\n\n- Try to keep the video portion to under 20 minutes unless the video is really interesting.\n\n- For a short video you could watch one of Apart\u2019s [ML + AI safety updates](https://www.youtube.com/watch?v=saiqSZkDBSk). Some of these contain many topics, so people can discuss what they find interesting.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "2222", "Related Answers": "", "Doc Last Ingested": "2023-03-15T22:12:01.960+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-ca846478fc5b8000b31611f97c169cdefbd3ffebfb19fff09d4b05764fb88c6b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ca846478fc5b8000b31611f97c169cdefbd3ffebfb19fff09d4b05764fb88c6b", "name": "aisafety.careers", "index": 401, "createdAt": "2023-01-19T13:04:16.454Z", "updatedAt": "2023-02-22T21:08:58.516Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ca846478fc5b8000b31611f97c169cdefbd3ffebfb19fff09d4b05764fb88c6b", "values": {"File": "aisafety.careers", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "aisafety.careers", "Link": "https://drive.google.com/drive/folders/1uz_Ktt4uklBPh8lVBrb1kBvsxe4697fw", "Thumbnail": "", "Doc Created": "2023-01-17T01:18:48.631+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-07T15:33:52.399+01:00", "Status": "Uncategorized", "Edit Answer": "aisafety.careers", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "-1", "Source Link": "", "aisafety.info Link": "aisafety.careers", "Source": "", "All Phrasings": "aisafety.careers\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "-1", "Related Answers": "", "Doc Last Ingested": "", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-5b1d263f5a01c5b50ca534d3c309d88547d02fc3ce675c9b72f2179d1f79759c", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5b1d263f5a01c5b50ca534d3c309d88547d02fc3ce675c9b72f2179d1f79759c", "name": "What is Obelisk's research strategy?", "index": 398, "createdAt": "2023-01-16T23:58:53.248Z", "updatedAt": "2023-03-14T22:29:57.025Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5b1d263f5a01c5b50ca534d3c309d88547d02fc3ce675c9b72f2179d1f79759c", "values": {"File": "What is Obelisk's research strategy?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Obelisk's research strategy?", "Link": "https://docs.google.com/document/d/12e6MvBlvM1h5DSxStHFd3YFfhhdZc3Jvd1gEhbv9Ftg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-16T20:47:32.026+01:00", "Related Answers DO NOT EDIT": "What approaches are AI alignment organizations working on?,What safety problems are associated with whole brain emulation?,How would we align an AGI whose learning algorithms / cognition look like human brains?", "Tags": "AISafety.careers", "Doc Last Edited": "2023-02-22T20:18:07.830+01:00", "Status": "Live on site", "Edit Answer": "What is Obelisk's research strategy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "1250", "Source Link": "", "aisafety.info Link": "What is Obelisk's research strategy?", "Source": "", "All Phrasings": "What is Obelisk's research strategy?\n", "Initial Order": "", "Related IDs": "6178,7605,8324", "Rich Text DO NOT EDIT": "**[Obelisk](https://astera.org/obelisk/)** is a research lab focused on [brainlike AGI safety](https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8) and the development of[brainlike AGI](https://www.lesswrong.com/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why#1_3_2_What_exactly_is__brain_like_AGI__) more generally. They think that there is a good chance that the first AGIs will be [neuromorphic](https://en.wikipedia.org/wiki/Neuromorphic_engineering). Aligning brainlike AI comes with a different set of opportunities and challenges from other development paths, and Obelisk is studying [computational neuroscience](https://mitpress.mit.edu/9780262650540/) and human goal formation to prepare and explore the options.\n\nOne threat model they think about is [misaligned model-based RL agents](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent), and two possible paths to alignment they\u2019re looking at are [\u201cControlled AGI\u201d](https://www.lesswrong.com/posts/QpHewJvZJFaQYuLwH/intro-to-brain-like-agi-safety-14-controlled-agi)[^kix.rz7roref8ij]and [\u201cSocial-instinct AGI\u201d](https://www.lesswrong.com/posts/Sd4QvG4ZyjynZuHGt/intro-to-brain-like-agi-safety-12-two-paths-forward)[^kix.iputmtndzs8g].\n\nMaking progress on Obelisk\u2019s agenda requires advanced knowledge of neuroscience and related disciplines. A strong grounding in machine learning is an asset here, but being a top-level ML engineer or ML research scientist is not required. They hire both [within](https://scholar.google.com/citations?user=tZpKKm4AAAAJ&hl=en) and [outside](https://www.lesswrong.com/users/steve2152) academia.\n\nThe main crux for their agenda is whether transformative AI will be brainlike, and whether it\u2019s possible to sufficiently align brainlike AI. They are relatively agnostic on the exact failure modes caused by unaligned brainlike AGI.\n\n[^kix.iputmtndzs8g]: Building an AI which deeply feels something like an idealized form of kinship and love for humanity, by understanding the computational structure of these drives in humans.\n[^kix.rz7roref8ij]:Automatically assessing the AI\u2019s thoughts and enforcing conservatism in value extrapolation", "Tag Count": 1, "Related Answer Count": 3, "Rich Text": "**[Obelisk](https://astera.org/obelisk/)** is a research lab focused on [brainlike AGI safety](https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8) and the development of[brainlike AGI](https://www.lesswrong.com/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why#1_3_2_What_exactly_is__brain_like_AGI__) more generally. They think that there is a good chance that the first AGIs will be [neuromorphic](https://en.wikipedia.org/wiki/Neuromorphic_engineering). Aligning brainlike AI comes with a different set of opportunities and challenges from other development paths, and Obelisk is studying [computational neuroscience](https://mitpress.mit.edu/9780262650540/) and human goal formation to prepare and explore the options.\n\nOne threat model they think about is [misaligned model-based RL agents](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent), and two possible paths to alignment they\u2019re looking at are [\u201cControlled AGI\u201d](https://www.lesswrong.com/posts/QpHewJvZJFaQYuLwH/intro-to-brain-like-agi-safety-14-controlled-agi)[^kix.rz7roref8ij]and [\u201cSocial-instinct AGI\u201d](https://www.lesswrong.com/posts/Sd4QvG4ZyjynZuHGt/intro-to-brain-like-agi-safety-12-two-paths-forward)[^kix.iputmtndzs8g].\n\nMaking progress on Obelisk\u2019s agenda requires advanced knowledge of neuroscience and related disciplines. A strong grounding in machine learning is an asset here, but being a top-level ML engineer or ML research scientist is not required. They hire both [within](https://scholar.google.com/citations?user=tZpKKm4AAAAJ&hl=en) and [outside](https://www.lesswrong.com/users/steve2152) academia.\n\nThe main crux for their agenda is whether transformative AI will be brainlike, and whether it\u2019s possible to sufficiently align brainlike AI. They are relatively agnostic on the exact failure modes caused by unaligned brainlike AGI.\n\n[^kix.iputmtndzs8g]: Building an AI which deeply feels something like an idealized form of kinship and love for humanity, by understanding the computational structure of these drives in humans.\n[^kix.rz7roref8ij]:Automatically assessing the AI\u2019s thoughts and enforcing conservatism in value extrapolation", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "1250", "Related Answers": "What approaches are AI alignment organizations working on?,What safety problems are associated with whole brain emulation?,How would we align an AGI whose learning algorithms / cognition look like human brains?", "Doc Last Ingested": "2023-03-14T23:25:21.285+01:00", "Request Count": "", "Number of suggestions on answer doc": 28, "Total character count of suggestions on answer doc": 2345, "Helpful": ""}}, {"id": "i-95f3e51fd4e4e38cae4a3df56907e205fde7cdcdcac64b964b93234c8d2e29df", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-95f3e51fd4e4e38cae4a3df56907e205fde7cdcdcac64b964b93234c8d2e29df", "name": "aisafety.careers answer guidelines and template", "index": 399, "createdAt": "2023-01-16T23:58:53.248Z", "updatedAt": "2023-01-28T15:11:16.305Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-95f3e51fd4e4e38cae4a3df56907e205fde7cdcdcac64b964b93234c8d2e29df", "values": {"File": "aisafety.careers answer guidelines and template", "Synced": false, "Sync account": "a.donorea@gmail.com", "Question": "aisafety.careers answer guidelines and template", "Link": "https://docs.google.com/document/d/1Hfkla8bEO1KU1_1jczZ5Xes5OMvGo6FepjzH3E7ThZw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-16T23:43:23.973+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-01-26T00:26:58.076+01:00", "Status": "Uncategorized", "Edit Answer": "aisafety.careers answer guidelines and template", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "-1", "Source Link": "", "aisafety.info Link": "aisafety.careers answer guidelines and template", "Source": "", "All Phrasings": "aisafety.careers answer guidelines and template\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Orgname and one sentence summary. Brief description of their cruxes and aims.\n\n1-3 sentences (ideally individual keywords) each on:\nTarget threat model(s) (including target AGI timeline);\nResearch aims (including what they assume collaborators will take care of);\nTheory of change cruxes;\nSkills required (including undergrad/masters/PhD, NLP, transformers, etc.);\nLocation/visas offered/etc.\n\nThreat models that are somewhat MECE (but not quite):\nWe get what we measure (models converge to human predictor and build Potemkin village world);\nOptimization daemon (deceptive consequentialist with non-myopic utility function arises in training and does gradient hacking, buries trojans and obfuscates cognition to circumvent interpretability tools, unboxes itself, etc.);\nSharp left turn (models deployed in the real world generalize capabilities far due to embedded knowledge, but alignment fails to generalize);\nCoordination failure (otherwise-aligned superintelligent systems gridlock far from the Pareto frontier).\n\n## Related\n\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Orgname and one sentence summary. Brief description of their cruxes and aims.\n\n1-3 sentences (ideally individual keywords) each on:\nTarget threat model(s) (including target AGI timeline);\nResearch aims (including what they assume collaborators will take care of);\nTheory of change cruxes;\nSkills required (including undergrad/masters/PhD, NLP, transformers, etc.);\nLocation/visas offered/etc.\n\nThreat models that are somewhat MECE (but not quite):\nWe get what we measure (models converge to human predictor and build Potemkin village world);\nOptimization daemon (deceptive consequentialist with non-myopic utility function arises in training and does gradient hacking, buries trojans and obfuscates cognition to circumvent interpretability tools, unboxes itself, etc.);\nSharp left turn (models deployed in the real world generalize capabilities far due to embedded knowledge, but alignment fails to generalize);\nCoordination failure (otherwise-aligned superintelligent systems gridlock far from the Pareto frontier).\n\n## Related\n\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": false, "UI ID": "-1", "Related Answers": "", "Doc Last Ingested": "", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-7a120714300d491ddb8bebe711c89ac69274038383547d6d0fd26a68371f55a0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7a120714300d491ddb8bebe711c89ac69274038383547d6d0fd26a68371f55a0", "name": "Could AI alignment research be bad? How?", "index": 397, "createdAt": "2023-01-15T18:48:46.398Z", "updatedAt": "2023-03-14T22:29:58.602Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7a120714300d491ddb8bebe711c89ac69274038383547d6d0fd26a68371f55a0", "values": {"File": "Could AI alignment research be bad? How?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Could AI alignment research be bad? How?", "Link": "https://docs.google.com/document/d/1vIP_LxWKknA0Dwqsp2K-1tPWVokMRSWbuMHOwe_hx3c/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-15T15:30:55.533+01:00", "Related Answers DO NOT EDIT": "What is an \"s-risk\"?", "Tags": "", "Doc Last Edited": "2023-03-05T21:53:46.663+01:00", "Status": "In review", "Edit Answer": "Could AI alignment research be bad? How?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "616", "Source Link": "", "aisafety.info Link": "Could AI alignment research be bad? How?", "Source": "", "All Phrasings": "Could AI alignment research be bad? How?\n", "Initial Order": "", "Related IDs": "7783", "Rich Text DO NOT EDIT": "While the hope is that AI alignment research will have a good outcome, there are a few primary ways it could be bad.\n\n## 1. Accelerating capabilities\n\nMany parts of alignment research are also relevant for increasing AI capabilities. If what started asalignment research ends up acceleratingAI development, it could end up being a net negative for safety.\n\nFor example reinforcement learning from human feedback ([RLHF](https://docs.google.com/document/u/0/d/1RAU4w38uh9WdDscy9H__Y3WEeOZhXJjrAjnJJcO-Ct8/edit)) is the technique of using human feedback to attempt to align an AI's goals with human desires. But it [has](https://www.google.com/url?q=https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/&sa=D&source=docs&ust=1677087817172396&usg=AOvVaw1Bd-2hJ_J_TBkWRGCO8GgP)[been](https://www.google.com/url?q=https://openai.com/blog/learning-to-summarize-with-human-feedback/&sa=D&source=docs&ust=1677087817172453&usg=AOvVaw3Wmc04A-mO9eJ5hvf9ljb9)[shown](https://www.google.com/url?q=https://openai.com/blog/instruction-following/&sa=D&source=docs&ust=1677087817172564&usg=AOvVaw3sl6GbvN85NbRjPQ1v61-O) that RLHF also increaseslearning efficiency. If it turns out that RLHF is not sufficient to align an AI (for example, if it leads to deception) the result of this research is more capable, unaligned AIs. Even if the research is helpful for alignment, the tradeoff might not be worth it. An argument that the research is more useful for alignment is insufficient, since not all research can be done in parallel, and it might not help with the parts of alignment which need the most time to develop. See: [A note about differential technological development](https://www.lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development).\n\nAnother way that alignment research could accelerate capabilities research is by drawing more investment into AI as a field in general.\n\n## 2. A false sense of security\n\nAI alignment research could also make companies more comfortable with deploying an unsafe system. If the system was completely unaligned and couldn\u2019t reliably do what its designers wanted, they would be unlikely to deploy it. But if alignment research leads to incompletely or [deceptively aligned](https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment) systems, that couldbe sufficient for a company to release them.\n\nFor example, RLHF could remove all of the obvious problems with the system, but when deployed, cause problems in situations very different from its training environment.\n\n## 3. A near miss inducing an s-risk\n\nA third problem could arise if AI alignment research reaches a high level but is not perfect. That could lead to a \u201cnear miss\u201d scenario in which a system is aligned with something *close* to human values but is missing a critical element. A near miss could be worse than a completely unaligned AI, since it could lead to extreme suffering (an \"s-risk\"), which is [arguably worse than extinction](https://www.youtube.com/watch?v=jiZxEJcFExc) from a completely unaligned AI.\n\nThis means that when developing an AI, [we want to be careful that our solution is distant from other catastrophic \u201csolutions](https://arbital.greaterwrong.com/p/hyperexistential_separation?l=8vk)\u201d, so that we don\u2019t accidentally fall into one of those outcomes.\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "While the hope is that AI alignment research will have a good outcome, there are a few primary ways it could be bad.\n\n## 1. Accelerating capabilities\n\nMany parts of alignment research are also relevant for increasing AI capabilities. If what started asalignment research ends up acceleratingAI development, it could end up being a net negative for safety.\n\nFor example reinforcement learning from human feedback ([RLHF](https://docs.google.com/document/u/0/d/1RAU4w38uh9WdDscy9H__Y3WEeOZhXJjrAjnJJcO-Ct8/edit)) is the technique of using human feedback to attempt to align an AI's goals with human desires. But it [has](https://www.google.com/url?q=https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/&sa=D&source=docs&ust=1677087817172396&usg=AOvVaw1Bd-2hJ_J_TBkWRGCO8GgP)[been](https://www.google.com/url?q=https://openai.com/blog/learning-to-summarize-with-human-feedback/&sa=D&source=docs&ust=1677087817172453&usg=AOvVaw3Wmc04A-mO9eJ5hvf9ljb9)[shown](https://www.google.com/url?q=https://openai.com/blog/instruction-following/&sa=D&source=docs&ust=1677087817172564&usg=AOvVaw3sl6GbvN85NbRjPQ1v61-O) that RLHF also increaseslearning efficiency. If it turns out that RLHF is not sufficient to align an AI (for example, if it leads to deception) the result of this research is more capable, unaligned AIs. Even if the research is helpful for alignment, the tradeoff might not be worth it. An argument that the research is more useful for alignment is insufficient, since not all research can be done in parallel, and it might not help with the parts of alignment which need the most time to develop. See: [A note about differential technological development](https://www.lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development).\n\nAnother way that alignment research could accelerate capabilities research is by drawing more investment into AI as a field in general.\n\n## 2. A false sense of security\n\nAI alignment research could also make companies more comfortable with deploying an unsafe system. If the system was completely unaligned and couldn\u2019t reliably do what its designers wanted, they would be unlikely to deploy it. But if alignment research leads to incompletely or [deceptively aligned](https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment) systems, that couldbe sufficient for a company to release them.\n\nFor example, RLHF could remove all of the obvious problems with the system, but when deployed, cause problems in situations very different from its training environment.\n\n## 3. A near miss inducing an s-risk\n\nA third problem could arise if AI alignment research reaches a high level but is not perfect. That could lead to a \u201cnear miss\u201d scenario in which a system is aligned with something *close* to human values but is missing a critical element. A near miss could be worse than a completely unaligned AI, since it could lead to extreme suffering (an \"s-risk\"), which is [arguably worse than extinction](https://www.youtube.com/watch?v=jiZxEJcFExc) from a completely unaligned AI.\n\nThis means that when developing an AI, [we want to be careful that our solution is distant from other catastrophic \u201csolutions](https://arbital.greaterwrong.com/p/hyperexistential_separation?l=8vk)\u201d, so that we don\u2019t accidentally fall into one of those outcomes.\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "616", "Related Answers": "What is an \"s-risk\"?", "Doc Last Ingested": "2023-03-14T23:25:25.637+01:00", "Request Count": "", "Number of suggestions on answer doc": 55, "Total character count of suggestions on answer doc": 2757, "Helpful": ""}}, {"id": "i-8db5750c6fa815cd3f4f79d0015985f5240b06c319b04648883ac0ace7f2d975", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8db5750c6fa815cd3f4f79d0015985f5240b06c319b04648883ac0ace7f2d975", "name": "Who is Jacob Steinhardt and what is he working on?", "index": 392, "createdAt": "2023-01-15T12:57:01.174Z", "updatedAt": "2023-03-14T22:30:00.175Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8db5750c6fa815cd3f4f79d0015985f5240b06c319b04648883ac0ace7f2d975", "values": {"File": "Who is Jacob Steinhardt and what is he working on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Who is Jacob Steinhardt and what is he working on?", "Link": "https://docs.google.com/document/d/1Qwsl_a3RCoOQ0mPdgDCV_swnaBAlfzw6DXXjnUqS-y4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-15T05:58:38.639+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-24T00:24:31.918+01:00", "Status": "Not started", "Edit Answer": "Who is Jacob Steinhardt and what is he working on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8367", "Source Link": "", "aisafety.info Link": "Who is Jacob Steinhardt and what is he working on?", "Source": "", "All Phrasings": "Who is Jacob Steinhardt and what is he working on?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n## \n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n## \n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8367", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:25:28.535+01:00", "Request Count": "", "Number of suggestions on answer doc": 56, "Total character count of suggestions on answer doc": 3180, "Helpful": ""}}, {"id": "i-51b5d22b8491d88269fe22bf0baf7b2e1def04f005853812cfc560f1e137890b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-51b5d22b8491d88269fe22bf0baf7b2e1def04f005853812cfc560f1e137890b", "name": "What work is Redwood doing on LLM interpretability?", "index": 393, "createdAt": "2023-01-15T12:57:01.174Z", "updatedAt": "2023-03-15T04:32:29.228Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-51b5d22b8491d88269fe22bf0baf7b2e1def04f005853812cfc560f1e137890b", "values": {"File": "What work is Redwood doing on LLM interpretability?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What work is Redwood doing on LLM interpretability?", "Link": "https://docs.google.com/document/d/1LI3NrNYB9thA51ZXBWxtqw9F8NjJnRVPuorUzz6bg3w/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-15T05:58:22.219+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:46:12.592+01:00", "Status": "Duplicate", "Edit Answer": "What work is Redwood doing on LLM interpretability?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8376", "Source Link": "", "aisafety.info Link": "What work is Redwood doing on LLM interpretability?", "Source": "", "All Phrasings": "What work is Redwood doing on LLM interpretability?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8376", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:25:31.498+01:00", "Request Count": "", "Number of suggestions on answer doc": 56, "Total character count of suggestions on answer doc": 3180, "Helpful": ""}}, {"id": "i-d6e513295b6705013bc51219e4268847d12b14ffb97b8b6efadded6670b70d57", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d6e513295b6705013bc51219e4268847d12b14ffb97b8b6efadded6670b70d57", "name": "What is FAR's theory of change?", "index": 394, "createdAt": "2023-01-15T12:57:01.174Z", "updatedAt": "2023-03-14T22:30:03.095Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d6e513295b6705013bc51219e4268847d12b14ffb97b8b6efadded6670b70d57", "values": {"File": "What is FAR's theory of change?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is FAR's theory of change?", "Link": "https://docs.google.com/document/d/1AMVUui-khIJtBZ-dZ_enjo-L93RNNUxnxNkGhkxEd9A/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-15T05:58:12.064+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:46:13.889+01:00", "Status": "Not started", "Edit Answer": "What is FAR's theory of change?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8356", "Source Link": "", "aisafety.info Link": "What is FAR's theory of change?", "Source": "", "All Phrasings": "What is FAR's theory of change?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8356", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:25:33.722+01:00", "Request Count": "", "Number of suggestions on answer doc": 56, "Total character count of suggestions on answer doc": 3180, "Helpful": ""}}, {"id": "i-9a24e88eb4c2fc4f115905a6c9f7ce4b2f167566447abc157764bc61e144e876", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-9a24e88eb4c2fc4f115905a6c9f7ce4b2f167566447abc157764bc61e144e876", "name": "What projects are CAIS working on?", "index": 395, "createdAt": "2023-01-15T12:57:01.174Z", "updatedAt": "2023-03-14T22:30:06.392Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-9a24e88eb4c2fc4f115905a6c9f7ce4b2f167566447abc157764bc61e144e876", "values": {"File": "What projects are CAIS working on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What projects are CAIS working on?", "Link": "https://docs.google.com/document/d/1nDr8-sNUFkMas_BbJXZ0sHXjTpypnK9KgwNGLVnaqR0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-15T05:57:17.127+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:46:15.184+01:00", "Status": "Not started", "Edit Answer": "What projects are CAIS working on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8326", "Source Link": "", "aisafety.info Link": "What projects are CAIS working on?", "Source": "", "All Phrasings": "What projects are CAIS working on?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8326", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:25:37.151+01:00", "Request Count": "", "Number of suggestions on answer doc": 56, "Total character count of suggestions on answer doc": 3180, "Helpful": ""}}, {"id": "i-5d876c6b0da35c340fa97047c9ebbc61f5562002eeaf9cad7ee1f1b6a4fbce9b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5d876c6b0da35c340fa97047c9ebbc61f5562002eeaf9cad7ee1f1b6a4fbce9b", "name": "Why might an AI do bad things?", "index": 396, "createdAt": "2023-01-15T12:57:01.174Z", "updatedAt": "2023-03-14T22:30:08.680Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5d876c6b0da35c340fa97047c9ebbc61f5562002eeaf9cad7ee1f1b6a4fbce9b", "values": {"File": "Why might an AI do bad things?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why might an AI do bad things?", "Link": "https://docs.google.com/document/d/1pezuTWv4YmJL5mgHDar7l7nv6xQMn4R2UApF0PAUwrY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T18:45:12.807+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-06T22:41:48.307+01:00", "Status": "Not started", "Edit Answer": "Why might an AI do bad things?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "616", "Source Link": "", "aisafety.info Link": "Why might an AI do bad things?", "Source": "", "All Phrasings": "Why might an AI do bad things?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n- \n\n- \n\n- \n\n    - \n\n    - \n\n- \n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n- \n\n- \n\n- \n\n    - \n\n    - \n\n- \n\n", "Stamp Count": "", "Multi Answer": false, "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "616", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:25:40.943+01:00", "Request Count": "", "Number of suggestions on answer doc": 59, "Total character count of suggestions on answer doc": 6460, "Helpful": ""}}, {"id": "i-a6d17fde964a99fbee89e88e369b1e0a67521c4cab1b902d44c5e48d9128e866", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-a6d17fde964a99fbee89e88e369b1e0a67521c4cab1b902d44c5e48d9128e866", "name": "Why might people try to build AGI rather than stronger and stronger narrow AIs?", "index": 243, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:16.488Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-a6d17fde964a99fbee89e88e369b1e0a67521c4cab1b902d44c5e48d9128e866", "values": {"File": "Why might people try to build AGI rather than stronger and stronger narrow AIs?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why might people try to build AGI rather than stronger and stronger narrow AIs?", "Link": "https://docs.google.com/document/d/1p1NxlNktuLD1dxjMTB2Tpv9INLMupMKHpT5xj9DV7ck/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:37.052+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI,Narrow AI", "Doc Last Edited": "2023-02-22T23:03:24.784+01:00", "Status": "Live on site", "Edit Answer": "Why might people try to build AGI rather than stronger and stronger narrow AIs?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6483", "Source Link": "", "aisafety.info Link": "Why might people try to build AGI rather than stronger and stronger narrow AIs?", "Source": "Wiki", "All Phrasings": "Why might people try to build AGI rather than stronger and stronger narrow AIs?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Making a narrow AI for every task would be extremely costly and time-consuming. By making a more general intelligence, you can apply one system to a broader range of tasks, which is economically and strategically attractive.\n\nOf course, for generality to be a good option there are some necessary conditions. You need an architecture which is straightforward enough to scale up, such as the transformer which is used for GPT and follows scaling laws. It's also important that by generalizing you do not lose too much capacity at narrow tasks or require too much extra compute for it to be worthwhile.\n\nWhether or not those conditions actually hold: It seems like many important actors (such as DeepMind and OpenAI) believe that they do, and are therefore focusing on trying to build an AGI in order to influence the future, so we should take actions to make it more likely that AGI will be developed safety.\n\nAdditionally, it is possible that even if we tried to build only narrow AIs, given enough time and compute we might accidentally create a more general AI than we intend by training a system on a task which requires a broad world model.\n\nSee also:\n\n- [Reframing Superintelligence](https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/) - A model of AI development which proposes that we might mostly build narrow AI systems for some time.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Making a narrow AI for every task would be extremely costly and time-consuming. By making a more general intelligence, you can apply one system to a broader range of tasks, which is economically and strategically attractive.\n\nOf course, for generality to be a good option there are some necessary conditions. You need an architecture which is straightforward enough to scale up, such as the transformer which is used for GPT and follows scaling laws. It's also important that by generalizing you do not lose too much capacity at narrow tasks or require too much extra compute for it to be worthwhile.\n\nWhether or not those conditions actually hold: It seems like many important actors (such as DeepMind and OpenAI) believe that they do, and are therefore focusing on trying to build an AGI in order to influence the future, so we should take actions to make it more likely that AGI will be developed safety.\n\nAdditionally, it is possible that even if we tried to build only narrow AIs, given enough time and compute we might accidentally create a more general AI than we intend by training a system on a task which requires a broad world model.\n\nSee also:\n\n- [Reframing Superintelligence](https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/) - A model of AI development which proposes that we might mostly build narrow AI systems for some time.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Severin", "External Source": "", "Last Asked On Discord": "", "UI ID": "6483", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:25:43.411+01:00", "Request Count": "", "Number of suggestions on answer doc": 59, "Total character count of suggestions on answer doc": 6460, "Helpful": ""}}, {"id": "i-3b9a5ec2af78fae059e5501874568aa291ecad83e9e1549fac116b2db7c555d6", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3b9a5ec2af78fae059e5501874568aa291ecad83e9e1549fac116b2db7c555d6", "name": "Why might contributing to Stampy be worth my time?", "index": 244, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:21.834Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3b9a5ec2af78fae059e5501874568aa291ecad83e9e1549fac116b2db7c555d6", "values": {"File": "Why might contributing to Stampy be worth my time?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why might contributing to Stampy be worth my time?", "Link": "https://docs.google.com/document/d/1BR9ctG3XaSIxJoLPO37IvCTHlTE4tLfAzFqJW3C8_lk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:32.805+01:00", "Related Answers DO NOT EDIT": "How can I contribute to Stampy's AI Safety Info?", "Tags": "Stampy", "Doc Last Edited": "2023-02-22T23:03:25.806+01:00", "Status": "Live on site", "Edit Answer": "Why might contributing to Stampy be worth my time?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7653", "Source Link": "", "aisafety.info Link": "Why might contributing to Stampy be worth my time?", "Source": "Wiki", "All Phrasings": "Why might contributing to Stampy be worth my time?\n", "Initial Order": "", "Related IDs": "6441", "Rich Text DO NOT EDIT": "If you're looking for a shovel ready and genuinely useful task to further AI alignment without necessarily committing a large amount of time or needing deep specialist knowledge, we think Stampy is a great option!\n\nCreating a high-quality single point of access where people can be onboarded and find resources around the alignment ecosystem seems likely to be high-impact. So, what makes us the best option?\n\n1. Unlike all other entry points to learning about alignment, we dodge the trade-off between comprehensiveness and being overwhelmingly long with interactivity (tab explosion in one page!) and semantic search. Single document FAQs can't do this, so we built a system which can.\n\n1. We have the ability to point large numbers of viewers towards Stampy once we have the content, thanks to Rob Miles and his 100k+ subscribers, so this won't remain an unnoticed curiosity.\n\n1. Unlike most other entry points, we are open for volunteers to help improve the content.\n\nThe main notable one which does is the [LessWrong tag wiki](https://www.lesswrong.com/tag/ai), which hosts descriptions of core concepts. We strongly believe in not needlessly duplicating effort, so we're pulling live content from that for the descriptions on our own [tag](http://tags) pages, and directing the edit links on those to the edit page on the LessWrong wiki.\n\nYou might also consider improving [Wikipedia's alignment coverage](https://en.wikipedia.org/wiki/Category:Existential_risk_from_artificial_general_intelligence) or the LessWrong wiki, but we think Stampy has the most low-hanging fruit right now. Additionally, contributing to Stampy means being part of a community of co-learners who provide mentorship and encouragement to join the effort to give humanity a bright future. If you're an established researcher or have high-value things to do elsewhere in the ecosystem it might not be optimal to put much time into Stampy, but if you're looking for a way to get more involved it might well be.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "If you're looking for a shovel ready and genuinely useful task to further AI alignment without necessarily committing a large amount of time or needing deep specialist knowledge, we think Stampy is a great option!\n\nCreating a high-quality single point of access where people can be onboarded and find resources around the alignment ecosystem seems likely to be high-impact. So, what makes us the best option?\n\n1. Unlike all other entry points to learning about alignment, we dodge the trade-off between comprehensiveness and being overwhelmingly long with interactivity (tab explosion in one page!) and semantic search. Single document FAQs can't do this, so we built a system which can.\n\n1. We have the ability to point large numbers of viewers towards Stampy once we have the content, thanks to Rob Miles and his 100k+ subscribers, so this won't remain an unnoticed curiosity.\n\n1. Unlike most other entry points, we are open for volunteers to help improve the content.\n\nThe main notable one which does is the [LessWrong tag wiki](https://www.lesswrong.com/tag/ai), which hosts descriptions of core concepts. We strongly believe in not needlessly duplicating effort, so we're pulling live content from that for the descriptions on our own [tag](http://tags) pages, and directing the edit links on those to the edit page on the LessWrong wiki.\n\nYou might also consider improving [Wikipedia's alignment coverage](https://en.wikipedia.org/wiki/Category:Existential_risk_from_artificial_general_intelligence) or the LessWrong wiki, but we think Stampy has the most low-hanging fruit right now. Additionally, contributing to Stampy means being part of a community of co-learners who provide mentorship and encouragement to join the effort to give humanity a bright future. If you're an established researcher or have high-value things to do elsewhere in the ecosystem it might not be optimal to put much time into Stampy, but if you're looking for a way to get more involved it might well be.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "2023-03-07T21:40:04.699+01:00", "UI ID": "7653", "Related Answers": "How can I contribute to Stampy's AI Safety Info?", "Doc Last Ingested": "2023-03-14T23:25:50.815+01:00", "Request Count": "", "Number of suggestions on answer doc": 59, "Total character count of suggestions on answer doc": 6460, "Helpful": ""}}, {"id": "i-c2ef4eacec3dfa652fb3782ccf38bde549027558d489cfeebb79e2c9dd045265", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c2ef4eacec3dfa652fb3782ccf38bde549027558d489cfeebb79e2c9dd045265", "name": "Why might a superintelligent AI be dangerous?", "index": 245, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:24.038Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c2ef4eacec3dfa652fb3782ccf38bde549027558d489cfeebb79e2c9dd045265", "values": {"File": "Why might a superintelligent AI be dangerous?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why might a superintelligent AI be dangerous?", "Link": "https://docs.google.com/document/d/1DYqTDB2RhRvgcNX4lPdNGnFOnIlLPQ4WgKlO5FIF9bo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:28.499+01:00", "Related Answers DO NOT EDIT": "Aren't robots the real problem? How can AI cause harm if it has no ability to directly manipulate the physical world?,What harm could a single superintelligence do when it took so many humans to build civilization?,Why might we expect a superintelligence to be hostile by default?", "Tags": "AI Takeoff", "Doc Last Edited": "2023-02-22T23:03:26.692+01:00", "Status": "Live on site", "Edit Answer": "Why might a superintelligent AI be dangerous?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6968", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "Why might a superintelligent AI be dangerous?", "Source": "Superintelligence FAQ", "All Phrasings": "Why might a superintelligent AI be dangerous?\n", "Initial Order": 4, "Related IDs": "6196,6970,6982", "Rich Text DO NOT EDIT": "A commonly heard argument goes: yes, a superintelligent AI might be far smarter than Einstein, but it\u2019s still just one program, sitting in a supercomputer somewhere. That could be bad if an enemy government controls it and asks it to help invent superweapons \u2013 but then the problem is the enemy government, not the AI *per se*. Is there any reason to be afraid of the AI itself? Suppose the AI did appear to be hostile, suppose it even wanted to take over the world: why should we think it has any chance of doing so?\n\nThere are numerous carefully thought-out AGI-related scenarios which could result in the accidental extinction of humanity. But rather than focussing on any of these individually, it might be more helpful to think in general terms.\n\n> Transistors can fire about 10 million times faster than human brain cells, so it's possible we'll eventually have digital minds operating 10 million times faster than us, meaning from a decision-making perspective we'd look to them like stationary objects, like plants or rocks... To give you a sense, [here](https://vimeo.com/83664407)'s what humans look like when slowed down by only around 100x.\"'' Watch that, and now try to imagine advanced AI technology running for a single year around the world, making decisions and taking actions 10 million times faster than we can. That year for us becomes 10 million subjective years for the AI, in which \"...there are these nearly-stationary plant-like or rock-like \"human\" objects around that could easily be taken apart for, say, biofuel or carbon atoms, if you could just get started building a human-disassembler. Visualizing things this way, you can start to see all the ways that a digital civilization can develop very quickly into a situation where there are no humans left alive, just as human civilization doesn't show much regard for plants or wildlife or insects.\" [Andrew Critch](https://acritch.com/) - [Slow Motion Videos as AI Risk Intuition Pumps](https://www.lesswrong.com/posts/Ccsx339LE9Jhoii9K/slow-motion-videos-as-ai-risk-intuition-pumps)\n\nAnd even putting aside these issues of speed and subjective time, the difference in (intelligence-based) power-to-manipulate-the-world between a self-improving superintelligent AGI and humanity could be far more extreme than the difference in such power between humanity and insects.\n\n\u201c[AI Could Defeat All Of Us Combined](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/)\u201d is a more in-depth argument by the CEO of [Open Philanthropy](https://en.wikipedia.org/wiki/Open_Philanthropy_(organization)).\n\n", "Tag Count": 1, "Related Answer Count": 3, "Rich Text": "A commonly heard argument goes: yes, a superintelligent AI might be far smarter than Einstein, but it\u2019s still just one program, sitting in a supercomputer somewhere. That could be bad if an enemy government controls it and asks it to help invent superweapons \u2013 but then the problem is the enemy government, not the AI *per se*. Is there any reason to be afraid of the AI itself? Suppose the AI did appear to be hostile, suppose it even wanted to take over the world: why should we think it has any chance of doing so?\n\nThere are numerous carefully thought-out AGI-related scenarios which could result in the accidental extinction of humanity. But rather than focussing on any of these individually, it might be more helpful to think in general terms.\n\n> Transistors can fire about 10 million times faster than human brain cells, so it's possible we'll eventually have digital minds operating 10 million times faster than us, meaning from a decision-making perspective we'd look to them like stationary objects, like plants or rocks... To give you a sense, [here](https://vimeo.com/83664407)'s what humans look like when slowed down by only around 100x.\"'' Watch that, and now try to imagine advanced AI technology running for a single year around the world, making decisions and taking actions 10 million times faster than we can. That year for us becomes 10 million subjective years for the AI, in which \"...there are these nearly-stationary plant-like or rock-like \"human\" objects around that could easily be taken apart for, say, biofuel or carbon atoms, if you could just get started building a human-disassembler. Visualizing things this way, you can start to see all the ways that a digital civilization can develop very quickly into a situation where there are no humans left alive, just as human civilization doesn't show much regard for plants or wildlife or insects.\" [Andrew Critch](https://acritch.com/) - [Slow Motion Videos as AI Risk Intuition Pumps](https://www.lesswrong.com/posts/Ccsx339LE9Jhoii9K/slow-motion-videos-as-ai-risk-intuition-pumps)\n\nAnd even putting aside these issues of speed and subjective time, the difference in (intelligence-based) power-to-manipulate-the-world between a self-improving superintelligent AGI and humanity could be far more extreme than the difference in such power between humanity and insects.\n\n\u201c[AI Could Defeat All Of Us Combined](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/)\u201d is a more in-depth argument by the CEO of [Open Philanthropy](https://en.wikipedia.org/wiki/Open_Philanthropy_(organization)).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "", "UI ID": "6968", "Related Answers": "Aren't robots the real problem? How can AI cause harm if it has no ability to directly manipulate the physical world?,What harm could a single superintelligence do when it took so many humans to build civilization?,Why might we expect a superintelligence to be hostile by default?", "Doc Last Ingested": "2023-03-14T23:25:54.379+01:00", "Request Count": "", "Number of suggestions on answer doc": 59, "Total character count of suggestions on answer doc": 6460, "Helpful": ""}}, {"id": "i-a86bb09b34bee56386a0772e52abd360dc4c2864315f5fedcb76dcb9da32ce7e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-a86bb09b34bee56386a0772e52abd360dc4c2864315f5fedcb76dcb9da32ce7e", "name": "Why might a maximizing AI cause bad outcomes?", "index": 246, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:29.082Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-a86bb09b34bee56386a0772e52abd360dc4c2864315f5fedcb76dcb9da32ce7e", "values": {"File": "Why might a maximizing AI cause bad outcomes?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why might a maximizing AI cause bad outcomes?", "Link": "https://docs.google.com/document/d/1gKj3Y_P_8ZtlA3SGB9jTlp36Otlgj1-zVv9ycb83ppo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:24.296+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Maximizers", "Doc Last Edited": "2023-02-22T23:03:27.586+01:00", "Status": "Live on site", "Edit Answer": "Why might a maximizing AI cause bad outcomes?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7523", "Source Link": "", "aisafety.info Link": "Why might a maximizing AI cause bad outcomes?", "Source": "Wiki", "All Phrasings": "Why might a maximizing AI cause bad outcomes?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Computers only do what you tell them. But any programmer knows that this is precisely the problem: computers do exactly what you tell them, with no common sense or attempts to interpret what the instructions really meant. If you tell a human to cure cancer, they will instinctively understand how this interacts with other desires and laws and moral rules; if a maximizing AI acquires a goal of trying to cure cancer, it will literally just want to cure cancer.\n\nDefine a closed-ended goal as one with a clear endpoint, and an open-ended goal as one to do something as much as possible. For example \u201cfind the first one hundred digits of pi\u201d is a closed-ended goal; \u201cfind as many digits of pi as you can within one year\u201d is an open-ended goal. According to many computer scientists, giving a superintelligence an open-ended goal without activating human instincts and counterbalancing considerations will usually lead to disaster.\n\nTo take a deliberately extreme example: suppose someone programs a superintelligence to calculate as many digits of pi as it can within one year. And suppose that, with its current computing power, it can calculate one trillion digits during that time. It can either accept one trillion digits, or spend a month trying to figure out how to get control of the TaihuLight supercomputer, which can calculate two hundred times faster. Even if it loses a little bit of time in the effort, and even if there\u2019s a small chance of failure, the payoff \u2013 two hundred trillion digits of pi, compared to a mere one trillion \u2013 is enough to make the attempt. But on the same basis, it would be even better if the superintelligence could control every computer in the world and set it to the task. And it would be better still if the superintelligence controlled human civilization, so that it could direct humans to build more computers and speed up the process further.\n\nNow we\u2019re in a situation where a superintelligence wants to take over the world. Taking over the world allows it to calculate more digits of pi than any other option, so without an architecture based around understanding human instincts and counterbalancing considerations, even a goal like \u201ccalculate as many digits of pi as you can\u201d would be potentially dangerous.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Computers only do what you tell them. But any programmer knows that this is precisely the problem: computers do exactly what you tell them, with no common sense or attempts to interpret what the instructions really meant. If you tell a human to cure cancer, they will instinctively understand how this interacts with other desires and laws and moral rules; if a maximizing AI acquires a goal of trying to cure cancer, it will literally just want to cure cancer.\n\nDefine a closed-ended goal as one with a clear endpoint, and an open-ended goal as one to do something as much as possible. For example \u201cfind the first one hundred digits of pi\u201d is a closed-ended goal; \u201cfind as many digits of pi as you can within one year\u201d is an open-ended goal. According to many computer scientists, giving a superintelligence an open-ended goal without activating human instincts and counterbalancing considerations will usually lead to disaster.\n\nTo take a deliberately extreme example: suppose someone programs a superintelligence to calculate as many digits of pi as it can within one year. And suppose that, with its current computing power, it can calculate one trillion digits during that time. It can either accept one trillion digits, or spend a month trying to figure out how to get control of the TaihuLight supercomputer, which can calculate two hundred times faster. Even if it loses a little bit of time in the effort, and even if there\u2019s a small chance of failure, the payoff \u2013 two hundred trillion digits of pi, compared to a mere one trillion \u2013 is enough to make the attempt. But on the same basis, it would be even better if the superintelligence could control every computer in the world and set it to the task. And it would be better still if the superintelligence controlled human civilization, so that it could direct humans to build more computers and speed up the process further.\n\nNow we\u2019re in a situation where a superintelligence wants to take over the world. Taking over the world allows it to calculate more digits of pi than any other option, so without an architecture based around understanding human instincts and counterbalancing considerations, even a goal like \u201ccalculate as many digits of pi as you can\u201d would be potentially dangerous.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7523", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:25:56.192+01:00", "Request Count": "", "Number of suggestions on answer doc": 59, "Total character count of suggestions on answer doc": 6460, "Helpful": ""}}, {"id": "i-cd6d288f09c5de6ff45e3acbb60e1e87d91d15f4ff53fcd608cb68c9e10fef54", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-cd6d288f09c5de6ff45e3acbb60e1e87d91d15f4ff53fcd608cb68c9e10fef54", "name": "Why is the future of AI suddenly in the news? What has changed?", "index": 247, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:32.194Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-cd6d288f09c5de6ff45e3acbb60e1e87d91d15f4ff53fcd608cb68c9e10fef54", "values": {"File": "Why is the future of AI suddenly in the news? What has changed?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why is the future of AI suddenly in the news? What has changed?", "Link": "https://docs.google.com/document/d/1cLEkiX6jgO4ceTytmPkJzBtGh_gOVsjyoqyUl0_6XIg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:20.070+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Awareness", "Doc Last Edited": "2023-02-22T23:03:28.441+01:00", "Status": "Live on site", "Edit Answer": "Why is the future of AI suddenly in the news? What has changed?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6180", "Source Link": "https://futureoflife.org/ai-faqs/", "aisafety.info Link": "Why is the future of AI suddenly in the news? What has changed?", "Source": "FLI's FAQ", "All Phrasings": "Why is the future of AI suddenly in the news? What has changed?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "In previous decades, AI research had proceeded more slowly than some experts predicted. According to experts in the field, however, this trend has reversed in the past 5 years or so. AI researchers have been repeatedly surprised by, for example, the effectiveness of new visual and speech recognition systems. AI systems can solve CAPTCHAs that were specifically devised to foil AIs, translate spoken text on-the-fly, and teach themselves how to play games they have neither seen before nor been programmed to play. Moreover, the real-world value of this effectiveness has prompted massive investment by large tech firms such as Google, Facebook, and IBM, creating a positive feedback cycle that could dramatically speed progress.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "In previous decades, AI research had proceeded more slowly than some experts predicted. According to experts in the field, however, this trend has reversed in the past 5 years or so. AI researchers have been repeatedly surprised by, for example, the effectiveness of new visual and speech recognition systems. AI systems can solve CAPTCHAs that were specifically devised to foil AIs, translate spoken text on-the-fly, and teach themselves how to play games they have neither seen before nor been programmed to play. Moreover, the real-world value of this effectiveness has prompted massive investment by large tech firms such as Google, Facebook, and IBM, creating a positive feedback cycle that could dramatically speed progress.\n\n", "Stamp Count": 2, "Multi Answer": "", "Stamped By": "Sophialb\nplex", "Priority": 4, "Asker": "FLI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6180", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:25:58.232+01:00", "Request Count": "", "Number of suggestions on answer doc": 59, "Total character count of suggestions on answer doc": 6460, "Helpful": ""}}, {"id": "i-fafba101539a91083ec9b47e64aad4224f5fc28456e76227738cd60a552f7682", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-fafba101539a91083ec9b47e64aad4224f5fc28456e76227738cd60a552f7682", "name": "Why is safety important for smarter-than-human AI?", "index": 248, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:39.016Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-fafba101539a91083ec9b47e64aad4224f5fc28456e76227738cd60a552f7682", "values": {"File": "Why is safety important for smarter-than-human AI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why is safety important for smarter-than-human AI?", "Link": "https://docs.google.com/document/d/159zAkI5cnwV5f4fJgeVhbx2_u2FW-n9ezWCYlauWnMs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:15.398+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Superintelligence", "Doc Last Edited": "2023-02-22T23:03:29.323+01:00", "Status": "Live on site", "Edit Answer": "Why is safety important for smarter-than-human AI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6297", "Source Link": "https://intelligence.org/faq/", "aisafety.info Link": "Why is safety important for smarter-than-human AI?", "Source": "MIRI FAQ", "All Phrasings": "Why is safety important for smarter-than-human AI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Present-day AI algorithms already demand special safety guarantees when they must act in important domains without human oversight, particularly when they or their environment can change over time:\n\nAchieving these gains [from autonomous systems] will depend on development of entirely new methods for enabling \u201ctrust in autonomy\u201d through verification and validation (V&V) of the near-infinite state systems that result from high levels of [adaptability] and autonomy. In effect, the number of possible input states that such systems can be presented with is so large that not only is it impossible to test all of them directly, it is not even feasible to test more than an insignificantly small fraction of them. Development of such systems is thus inherently unverifiable by today\u2019s methods, and as a result their operation in all but comparatively trivial applications is uncertifiable.\n\nIt is possible to develop systems having high levels of autonomy, but it is the lack of suitable V&V methods that prevents all but relatively low levels of autonomy from being certified for use. - Office of the US Air Force Chief Scientist (2010). [Technology Horizons: A Vision for Air Force Science and Technology 2010-30](http://www.defenseinnovationmarketplace.mil/resources/AF_TechnologyHorizons2010-2030.pdf).\n\nAs AI capabilities improve, it will become easier to give AI systems greater autonomy, flexibility, and control; and there will be increasingly large incentives to make use of these new possibilities. The potential for AI systems to become more general, in particular, will make it difficult to establish safety guarantees: reliable regularities during testing may not always hold post-testing.\n\nThe largest and most lasting changes in human welfare have come from scientific and technological innovation \u2014 which in turn comes from our intelligence. In the long run, then, much of AI\u2019s significance comes from its potential to automate and enhance progress in science and technology. The creation of smarter-than-human AI brings with it the basic risks and benefits of intellectual progress itself, at digital speeds.\n\nAs AI agents become more capable, it becomes more important (and more difficult) to analyze and verify their decisions and goals. Stuart Russell [writes](http://edge.org/conversation/the-myth-of-ai#26015):\n\nThe primary concern is not spooky emergent consciousness but simply the ability to make high-quality decisions. Here, quality refers to the expected outcome utility of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:\n\n1. The utility function may not be perfectly aligned with the values of the human race, which are (at best) very difficult to pin down.\n\n1. Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources \u2013 not for their own sake, but to succeed in its assigned task.\n\nA system that is optimizing a function of n variables, where the objective depends on a subset of size k<n, will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer\u2019s apprentice, or King Midas: you get exactly what you ask for, not what you want.\n\nBostrom\u2019s \u201c[The Superintelligent Will](http://www.nickbostrom.com/superintelligentwill.pdf)\u201d lays out these two concerns in more detail: that we may not correctly specify our actual goals in programming smarter-than-human AI systems, and that most agents optimizing for a misspecified goal will have incentives to treat humans adversarially, as potential threats or obstacles to achieving the agent\u2019s goal.\n\nIf the goals of human and AI agents are not well-aligned, the more knowledgeable and technologically capable agent may use force to get what it wants, as has occurred in many conflicts between human communities. Having noticed this class of concerns in advance, we have an opportunity to reduce risk from this default scenario by directing research toward aligning artificial decision-makers\u2019 interests with our own.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Present-day AI algorithms already demand special safety guarantees when they must act in important domains without human oversight, particularly when they or their environment can change over time:\n\nAchieving these gains [from autonomous systems] will depend on development of entirely new methods for enabling \u201ctrust in autonomy\u201d through verification and validation (V&V) of the near-infinite state systems that result from high levels of [adaptability] and autonomy. In effect, the number of possible input states that such systems can be presented with is so large that not only is it impossible to test all of them directly, it is not even feasible to test more than an insignificantly small fraction of them. Development of such systems is thus inherently unverifiable by today\u2019s methods, and as a result their operation in all but comparatively trivial applications is uncertifiable.\n\nIt is possible to develop systems having high levels of autonomy, but it is the lack of suitable V&V methods that prevents all but relatively low levels of autonomy from being certified for use. - Office of the US Air Force Chief Scientist (2010). [Technology Horizons: A Vision for Air Force Science and Technology 2010-30](http://www.defenseinnovationmarketplace.mil/resources/AF_TechnologyHorizons2010-2030.pdf).\n\nAs AI capabilities improve, it will become easier to give AI systems greater autonomy, flexibility, and control; and there will be increasingly large incentives to make use of these new possibilities. The potential for AI systems to become more general, in particular, will make it difficult to establish safety guarantees: reliable regularities during testing may not always hold post-testing.\n\nThe largest and most lasting changes in human welfare have come from scientific and technological innovation \u2014 which in turn comes from our intelligence. In the long run, then, much of AI\u2019s significance comes from its potential to automate and enhance progress in science and technology. The creation of smarter-than-human AI brings with it the basic risks and benefits of intellectual progress itself, at digital speeds.\n\nAs AI agents become more capable, it becomes more important (and more difficult) to analyze and verify their decisions and goals. Stuart Russell [writes](http://edge.org/conversation/the-myth-of-ai#26015):\n\nThe primary concern is not spooky emergent consciousness but simply the ability to make high-quality decisions. Here, quality refers to the expected outcome utility of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:\n\n1. The utility function may not be perfectly aligned with the values of the human race, which are (at best) very difficult to pin down.\n\n1. Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources \u2013 not for their own sake, but to succeed in its assigned task.\n\nA system that is optimizing a function of n variables, where the objective depends on a subset of size k<n, will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer\u2019s apprentice, or King Midas: you get exactly what you ask for, not what you want.\n\nBostrom\u2019s \u201c[The Superintelligent Will](http://www.nickbostrom.com/superintelligentwill.pdf)\u201d lays out these two concerns in more detail: that we may not correctly specify our actual goals in programming smarter-than-human AI systems, and that most agents optimizing for a misspecified goal will have incentives to treat humans adversarially, as potential threats or obstacles to achieving the agent\u2019s goal.\n\nIf the goals of human and AI agents are not well-aligned, the more knowledgeable and technologically capable agent may use force to get what it wants, as has occurred in many conflicts between human communities. Having noticed this class of concerns in advance, we have an opportunity to reduce risk from this default scenario by directing research toward aligning artificial decision-makers\u2019 interests with our own.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "6297", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:00.120+01:00", "Request Count": "", "Number of suggestions on answer doc": 59, "Total character count of suggestions on answer doc": 6460, "Helpful": ""}}, {"id": "i-7304fd0bf14d0a0304621e6305656dd2945bfe16a9e522dc2e58c87ff9d04c59", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7304fd0bf14d0a0304621e6305656dd2945bfe16a9e522dc2e58c87ff9d04c59", "name": "Why is AI safety important?", "index": 249, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:43.066Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7304fd0bf14d0a0304621e6305656dd2945bfe16a9e522dc2e58c87ff9d04c59", "values": {"File": "Why is AI safety important?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why is AI safety important?", "Link": "https://docs.google.com/document/d/1-41IP7JkBeL4xSPBzHfIQ64poC8HFh4J-vQzZ8ITfo4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:11.281+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-24T04:21:49.372+01:00", "Status": "In progress", "Edit Answer": "Why is AI safety important?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6156", "Source Link": "https://markxu.com/ai-safety-faqs", "aisafety.info Link": "Why is AI safety important?", "Source": "Mark Xu's FAQ", "All Phrasings": "Why is AI safety important?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Cybersecurity is important because computing systems comprise the backbone of the modern economy. If the security of the internet was compromised, then the economy would suffer a tremendous blow.\n\nSimilarly, AI Safety might become important as AI systems begin forming larger and larger parts of the modern economy. As more and more labor gets automated, it becomes more and more important to ensure that that labor is occurring in a safe and robust way.\n\nBefore the widespread adoption of computing systems, lack of Cybersecurity didn\u2019t cause much damage. However, it might have been beneficial to start thinking about Cybersecurity problems before the solutions were necessary.\n\nSimilarly, since AI systems haven\u2019t been adopted en mass yet, lack of AI Safety isn\u2019t causing harm. However, given that AI systems will become increasingly powerful and increasingly widespread, it might be prudent to try to solve safety problems before a catastrophe occurs.\n\nAdditionally, people sometimes think about Artificial General Intelligence (AGI), sometimes called Human-Level Artificial Intelligence (HLAI). One of the core problems in AI Safety is ensuring when AGI gets built, it has human interests at heart. (Note that [most surveyed experts](https://aiimpacts.org/surveys-on-fractional-progress-towards-hlai/) think building GI/HLAI is possible, but there is wide disagreement on how soon this might occur).\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Cybersecurity is important because computing systems comprise the backbone of the modern economy. If the security of the internet was compromised, then the economy would suffer a tremendous blow.\n\nSimilarly, AI Safety might become important as AI systems begin forming larger and larger parts of the modern economy. As more and more labor gets automated, it becomes more and more important to ensure that that labor is occurring in a safe and robust way.\n\nBefore the widespread adoption of computing systems, lack of Cybersecurity didn\u2019t cause much damage. However, it might have been beneficial to start thinking about Cybersecurity problems before the solutions were necessary.\n\nSimilarly, since AI systems haven\u2019t been adopted en mass yet, lack of AI Safety isn\u2019t causing harm. However, given that AI systems will become increasingly powerful and increasingly widespread, it might be prudent to try to solve safety problems before a catastrophe occurs.\n\nAdditionally, people sometimes think about Artificial General Intelligence (AGI), sometimes called Human-Level Artificial Intelligence (HLAI). One of the core problems in AI Safety is ensuring when AGI gets built, it has human interests at heart. (Note that [most surveyed experts](https://aiimpacts.org/surveys-on-fractional-progress-towards-hlai/) think building GI/HLAI is possible, but there is wide disagreement on how soon this might occur).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Mark Xu", "External Source": "", "Last Asked On Discord": "", "UI ID": "6156", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:02.161+01:00", "Request Count": "", "Number of suggestions on answer doc": 60, "Total character count of suggestions on answer doc": 6461, "Helpful": ""}}, {"id": "i-f200be24dfb36fea2113d39f25543b6d13880653e3fd7047ee23b85d1e3fbb86", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-f200be24dfb36fea2113d39f25543b6d13880653e3fd7047ee23b85d1e3fbb86", "name": "Why is AI alignment a hard problem?", "index": 250, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:46.989Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-f200be24dfb36fea2113d39f25543b6d13880653e3fd7047ee23b85d1e3fbb86", "values": {"File": "Why is AI alignment a hard problem?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why is AI alignment a hard problem?", "Link": "https://docs.google.com/document/d/1mraQPWjVoTASnwLS1BbyBwvkIEuTy6_uwZ_yiPLyrco/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:06.714+01:00", "Related Answers DO NOT EDIT": "How does AI taking things literally contribute to alignment being hard?,Is AI alignment possible?", "Tags": "Difficulty of Alignment", "Doc Last Edited": "2023-02-22T23:03:30.423+01:00", "Status": "Live on site", "Edit Answer": "Why is AI alignment a hard problem?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8163", "Source Link": "", "aisafety.info Link": "Why is AI alignment a hard problem?", "Source": "Wiki", "All Phrasings": "Why is AI alignment a hard problem?\n", "Initial Order": 5, "Related IDs": "6168,6274", "Rich Text DO NOT EDIT": "One sense in which alignment is a hard problem is analogous to the reason rocket science is a hard problem. Relative to other engineering endeavors, rocket science had so many disasters because of the extreme stresses placed on various mechanical components and the narrow margins of safety required by stringent weight limits. A superintelligence would put vastly more \u201cstress\u201d on the software and hardware stack it is running on, which could cause many classes of failure which don\u2019t occur when you\u2019re working with subhuman systems.\n\nAlignment is also hard like space probes are hard. With recursively self-improving systems, you won\u2019t be able to go back and edit the code later if there is a catastrophic failure because it will competently deceive and resist you.\n\n*\"You may have only one shot. If something goes wrong, the system might be too 'high' for you to reach up and suddenly fix it. You can build error recovery mechanisms into it; space probes are supposed to accept software updates. If something goes wrong in a way that precludes getting future updates, though, you\u2019re screwed. You have lost the space probe.\"*\n\nAdditionally, alignment is hard like cryptographic security. Cryptographers attempt to safeguard against \u201cintelligent adversaries\u201d who search for flaws in a system which they can exploit to break it. *\u201cYour code is not an intelligent adversary if everything goes right. If something goes wrong, it might try to defeat your safeguards\u2026\u201d* And at the stage where it\u2019s trying to defeat your safeguards, your code may have achieved the capabilities of a vast and perfectly coordinated team of superhuman-level hackers! So if there is even the tiniest flaw in your design, you can be certain that it will be found and exploited. As with standard cybersecurity, \"good under normal circumstances\" is just not good enough \u2013 your system needs to be unbreakably robust.\n\n*\"AI alignment: treat it like a cryptographic rocket probe. This is about how difficult you would expect it to be to build something smarter than you that was nice \u2013 given that basic agent theory says they\u2019re not automatically nice \u2013 and not die. You would expect that intuitively to be hard.\"* Eliezer Yudkowsky\n\nAnother immense challenge is the fact that we currently have no idea how to reliably instill AIs with human-friendly goals. *Even if a consensus could be reached on a system of human values and morality*, it\u2019s entirely unclear how this could be fully and faithfully captured in code.\n\nFor a more in-depth view of this argument, see Yudkowsky's talk \"AI Alignment: Why It\u2019s Hard, and Where to Start\" below (full transcript [here](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/)). For alternative views, see Paul Christiano's [\u201cAI alignment landscape\u201d talk](https://ai-alignment.com/ai-alignment-landscape-d3773c37ae38), Daniel Kokotajlo and Wei Dai\u2019s [\u201cThe Main Sources of AI Risk?\u201d](https://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk) list, and [Rohin Shah\u2019s much more optimistic position](https://aiimpacts.org/conversation-with-rohin-shah/).\n\n<iframe src=\"https://www.youtube.com/embed/EUjc1WuyPT8\" title=\"Eliezer Yudkowsky \u2013 AI Alignment: Why It's Hard, and Where to Start\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n", "Tag Count": 1, "Related Answer Count": 2, "Rich Text": "One sense in which alignment is a hard problem is analogous to the reason rocket science is a hard problem. Relative to other engineering endeavors, rocket science had so many disasters because of the extreme stresses placed on various mechanical components and the narrow margins of safety required by stringent weight limits. A superintelligence would put vastly more \u201cstress\u201d on the software and hardware stack it is running on, which could cause many classes of failure which don\u2019t occur when you\u2019re working with subhuman systems.\n\nAlignment is also hard like space probes are hard. With recursively self-improving systems, you won\u2019t be able to go back and edit the code later if there is a catastrophic failure because it will competently deceive and resist you.\n\n*\"You may have only one shot. If something goes wrong, the system might be too 'high' for you to reach up and suddenly fix it. You can build error recovery mechanisms into it; space probes are supposed to accept software updates. If something goes wrong in a way that precludes getting future updates, though, you\u2019re screwed. You have lost the space probe.\"*\n\nAdditionally, alignment is hard like cryptographic security. Cryptographers attempt to safeguard against \u201cintelligent adversaries\u201d who search for flaws in a system which they can exploit to break it. *\u201cYour code is not an intelligent adversary if everything goes right. If something goes wrong, it might try to defeat your safeguards\u2026\u201d* And at the stage where it\u2019s trying to defeat your safeguards, your code may have achieved the capabilities of a vast and perfectly coordinated team of superhuman-level hackers! So if there is even the tiniest flaw in your design, you can be certain that it will be found and exploited. As with standard cybersecurity, \"good under normal circumstances\" is just not good enough \u2013 your system needs to be unbreakably robust.\n\n*\"AI alignment: treat it like a cryptographic rocket probe. This is about how difficult you would expect it to be to build something smarter than you that was nice \u2013 given that basic agent theory says they\u2019re not automatically nice \u2013 and not die. You would expect that intuitively to be hard.\"* Eliezer Yudkowsky\n\nAnother immense challenge is the fact that we currently have no idea how to reliably instill AIs with human-friendly goals. *Even if a consensus could be reached on a system of human values and morality*, it\u2019s entirely unclear how this could be fully and faithfully captured in code.\n\nFor a more in-depth view of this argument, see Yudkowsky's talk \"AI Alignment: Why It\u2019s Hard, and Where to Start\" below (full transcript [here](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/)). For alternative views, see Paul Christiano's [\u201cAI alignment landscape\u201d talk](https://ai-alignment.com/ai-alignment-landscape-d3773c37ae38), Daniel Kokotajlo and Wei Dai\u2019s [\u201cThe Main Sources of AI Risk?\u201d](https://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk) list, and [Rohin Shah\u2019s much more optimistic position](https://aiimpacts.org/conversation-with-rohin-shah/).\n\n<iframe src=\"https://www.youtube.com/embed/EUjc1WuyPT8\" title=\"Eliezer Yudkowsky \u2013 AI Alignment: Why It's Hard, and Where to Start\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "8163", "Related Answers": "How does AI taking things literally contribute to alignment being hard?,Is AI alignment possible?", "Doc Last Ingested": "2023-03-14T23:26:04.495+01:00", "Request Count": "", "Number of suggestions on answer doc": 60, "Total character count of suggestions on answer doc": 6461, "Helpful": ""}}, {"id": "i-b7f9ac659bd7c36261fbf3f84fab3bced61aef95d9ae691c83c871d2f5135f15", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b7f9ac659bd7c36261fbf3f84fab3bced61aef95d9ae691c83c871d2f5135f15", "name": "Why is AGI safety a hard problem?", "index": 251, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:51.085Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b7f9ac659bd7c36261fbf3f84fab3bced61aef95d9ae691c83c871d2f5135f15", "values": {"File": "Why is AGI safety a hard problem?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why is AGI safety a hard problem?", "Link": "https://docs.google.com/document/d/1XOfAQYTkCFZuct7yupaOC8P2Nh4H2I_XAzD_jbsR1KE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:57.793+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Superintelligence,Difficulty of Alignment", "Doc Last Edited": "2023-03-10T06:16:19.188+01:00", "Status": "Live on site", "Edit Answer": "Why is AGI safety a hard problem?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6140", "Source Link": "", "aisafety.info Link": "Why is AGI safety a hard problem?", "Source": "Wiki", "All Phrasings": "Why is AGI safety a hard problem?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "There's the \"we never figure out how to reliably instill AIs with human friendly goals\" filter, which seems pretty challenging, especially with [inner alignment](https://www.youtube.com/watch?v=bJLcIBixGj8), solving morality in a way which is possible to code up, interpretability, etc.\n\nThere's the \"race dynamics mean that even though we know how to build the thing safely the first group to cross the recursive self-improvement line ends up not implementing it safely\" which is potentially made worse by the twin issues of \"maybe robustly aligned AIs are much harder to build\" and \"maybe robustly aligned AIs are much less compute efficient\".\n\nThere's the \"we solved the previous problems but writing perfectly reliably code in a whole new domain is hard and there is some fatal bug which we don't find until too late\" filter. The paper [The Pursuit of Exploitable Bugs in Machine Learning](https://arxiv.org/abs/1701.04739) explores this.\n\nFor a much more in depth analysis, see [Paul Christiano's AI Alignment Landscape](https://ai-alignment.com/ai-alignment-landscape-d3773c37ae38) talk and [The Main Sources of AI Risk?](https://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk).\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "There's the \"we never figure out how to reliably instill AIs with human friendly goals\" filter, which seems pretty challenging, especially with [inner alignment](https://www.youtube.com/watch?v=bJLcIBixGj8), solving morality in a way which is possible to code up, interpretability, etc.\n\nThere's the \"race dynamics mean that even though we know how to build the thing safely the first group to cross the recursive self-improvement line ends up not implementing it safely\" which is potentially made worse by the twin issues of \"maybe robustly aligned AIs are much harder to build\" and \"maybe robustly aligned AIs are much less compute efficient\".\n\nThere's the \"we solved the previous problems but writing perfectly reliably code in a whole new domain is hard and there is some fatal bug which we don't find until too late\" filter. The paper [The Pursuit of Exploitable Bugs in Machine Learning](https://arxiv.org/abs/1701.04739) explores this.\n\nFor a much more in depth analysis, see [Paul Christiano's AI Alignment Landscape](https://ai-alignment.com/ai-alignment-landscape-d3773c37ae38) talk and [The Main Sources of AI Risk?](https://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk).\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6140", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:06.845+01:00", "Request Count": "", "Number of suggestions on answer doc": 60, "Total character count of suggestions on answer doc": 6461, "Helpful": ""}}, {"id": "i-0d89f207fea069e23edf50d47617c9c8f21bbf5c401e7f2578e188fb309a5774", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0d89f207fea069e23edf50d47617c9c8f21bbf5c401e7f2578e188fb309a5774", "name": "Why is AGI dangerous?", "index": 252, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:58.994Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0d89f207fea069e23edf50d47617c9c8f21bbf5c401e7f2578e188fb309a5774", "values": {"File": "Why is AGI dangerous?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why is AGI dangerous?", "Link": "https://docs.google.com/document/d/1ItfAkZNiskwSpT20Wv_OFql2YdzSOUrZ4_2eRijdUNk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:53.674+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-27T19:41:29.859+01:00", "Status": "Live on site", "Edit Answer": "Why is AGI dangerous?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "2400", "Source Link": "", "aisafety.info Link": "Why is AGI dangerous?", "Source": "Wiki", "All Phrasings": "Why is AGI dangerous?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "1. [The Orthogonality Thesis](https://www.youtube.com/watch?v=hEUO6pjwFOo): AI could have almost any goal while at the same time having high intelligence (aka ability to succeed at those goals). This means that we could build a very powerful agent which would not necessarily share human-friendly values. For example, the classic [paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer) thought experiment explores this with an AI which has a goal of creating as many paperclips as possible, something that humans are (mostly) indifferent to, and as a side effect ends up destroying humanity to make room for more paperclip factories.\n\n1. [Complexity of value](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile): What humans care about is not simple, and the space of all goals is large, so virtually all goals we could program into an AI would lead to worlds not valuable to humans if pursued by a sufficiently powerful agent. If we, for example, did not include our value of diversity of experience, we could end up with a world of endlessly looping simple pleasures, rather than beings living rich lives.\n\n1. [Instrumental Convergence](https://www.youtube.com/watch?v=ZeecOKBus3Q): For almost any goal an AI has there are shared \u2018instrumental\u2019 steps, such as acquiring resources, preserving itself, and preserving the contents of its goals. This means that a powerful AI with goals that were not explicitly human-friendly would predictably both take actions that lead to the end of humanity (e.g. using resources humans need to live to further its goals, such as replacing our crop fields with vast numbers of solar panels to power its growth, or using the carbon in our bodies to build things) and prevent us from turning it off or altering its goals.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "1. [The Orthogonality Thesis](https://www.youtube.com/watch?v=hEUO6pjwFOo): AI could have almost any goal while at the same time having high intelligence (aka ability to succeed at those goals). This means that we could build a very powerful agent which would not necessarily share human-friendly values. For example, the classic [paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer) thought experiment explores this with an AI which has a goal of creating as many paperclips as possible, something that humans are (mostly) indifferent to, and as a side effect ends up destroying humanity to make room for more paperclip factories.\n\n1. [Complexity of value](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile): What humans care about is not simple, and the space of all goals is large, so virtually all goals we could program into an AI would lead to worlds not valuable to humans if pursued by a sufficiently powerful agent. If we, for example, did not include our value of diversity of experience, we could end up with a world of endlessly looping simple pleasures, rather than beings living rich lives.\n\n1. [Instrumental Convergence](https://www.youtube.com/watch?v=ZeecOKBus3Q): For almost any goal an AI has there are shared \u2018instrumental\u2019 steps, such as acquiring resources, preserving itself, and preserving the contents of its goals. This means that a powerful AI with goals that were not explicitly human-friendly would predictably both take actions that lead to the end of humanity (e.g. using resources humans need to live to further its goals, such as replacing our crop fields with vast numbers of solar panels to power its growth, or using the carbon in our bodies to build things) and prevent us from turning it off or altering its goals.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "Jack Harley", "External Source": "", "Last Asked On Discord": "", "UI ID": "2400", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:08.986+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": ""}}, {"id": "i-6fe30081bf8d14ef16afec5b20a39191b16aabca5851bef36cd13ed609c94595", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-6fe30081bf8d14ef16afec5b20a39191b16aabca5851bef36cd13ed609c94595", "name": "Why don't we just not build AGI if it's so dangerous?", "index": 253, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:01.232Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-6fe30081bf8d14ef16afec5b20a39191b16aabca5851bef36cd13ed609c94595", "values": {"File": "Why don't we just not build AGI if it's so dangerous?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why don't we just not build AGI if it's so dangerous?", "Link": "https://docs.google.com/document/d/1igI7HeGxSBIC8Ni286gMfO-6dRv30c2e7Eet1r80vCM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:50.183+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI,Why Not Just", "Doc Last Edited": "2023-02-22T23:03:33.107+01:00", "Status": "Live on site", "Edit Answer": "Why don't we just not build AGI if it's so dangerous?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7148", "Source Link": "", "aisafety.info Link": "Why don't we just not build AGI if it's so dangerous?", "Source": "Wiki", "All Phrasings": "Why don't we just not build AGI if it's so dangerous?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "It certainly would be very unwise to purposefully create an artificial general intelligence now, before we have found a way to be certain it will act purely in our interests. But \"general intelligence\" is more of a description of a system's capabilities, and a vague one at that. We don't know what it takes to build such a system. This leads to the worrying possibility that our existing, narrow AI systems require only minor tweaks, or even just more computer power, to achieve general intelligence.\n\nThe pace of research in the field suggests that there's a lot of low-hanging fruit left to pick, after all, and the results of this research produce better, more effective AI in a landscape of strong competitive pressure to produce as highly competitive systems as we can. \"Just\" not building an AGI means ensuring that every organization in the world with lots of computer hardware doesn't build an AGI, either accidentally or mistakenly thinking they have a solution to the alignment problem, forever. It's simply far safer to also work on solving the alignment problem.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "It certainly would be very unwise to purposefully create an artificial general intelligence now, before we have found a way to be certain it will act purely in our interests. But \"general intelligence\" is more of a description of a system's capabilities, and a vague one at that. We don't know what it takes to build such a system. This leads to the worrying possibility that our existing, narrow AI systems require only minor tweaks, or even just more computer power, to achieve general intelligence.\n\nThe pace of research in the field suggests that there's a lot of low-hanging fruit left to pick, after all, and the results of this research produce better, more effective AI in a landscape of strong competitive pressure to produce as highly competitive systems as we can. \"Just\" not building an AGI means ensuring that every organization in the world with lots of computer hardware doesn't build an AGI, either accidentally or mistakenly thinking they have a solution to the alignment problem, forever. It's simply far safer to also work on solving the alignment problem.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "2023-02-26T19:21:30.218+01:00", "UI ID": "7148", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:11.096+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": ""}}, {"id": "i-89b72fa8b1891e7660b5875faf87e62f7fa7c1d0b547356884fac697c845ce28", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-89b72fa8b1891e7660b5875faf87e62f7fa7c1d0b547356884fac697c845ce28", "name": "Why does there seem to have been an explosion of activity in AI in recent years?", "index": 254, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:05.623Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-89b72fa8b1891e7660b5875faf87e62f7fa7c1d0b547356884fac697c845ce28", "values": {"File": "Why does there seem to have been an explosion of activity in AI in recent years?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why does there seem to have been an explosion of activity in AI in recent years?", "Link": "https://docs.google.com/document/d/1JiXzy_Fc9jXHQvnIHXvyz4vV9E0Dumhw9hKI8reTJ88/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:46.551+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-14T05:41:07.428+01:00", "Status": "In progress", "Edit Answer": "Why does there seem to have been an explosion of activity in AI in recent years?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5649", "Source Link": "", "aisafety.info Link": "Why does there seem to have been an explosion of activity in AI in recent years?", "Source": "Wiki", "All Phrasings": "Why does there seem to have been an explosion of activity in AI in recent years?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "In addition to the usual continuation of Moore's Law, GPUs have become more powerful and cheaper in the past decade, especially since around 2016. Many ideas in AI have been thought about for a long time, but the speed at which modern processors can do computing and parallel processing allows researchers to implement their ideas and gather more observational data. Improvements in AI have allowed many industries to start using the technologies, which creates demand and brings more focus on AI research (as well as improving the availability of technology on the whole due to more efficient infrastructure). Data has also become more abundant and available, and not only is data a bottleneck for machine learning algorithms, but the abundance of data is difficult for humans to deal with alone, so businesses often turn to AI to convert it to something human-parsable. These processes are also recursive, to some degree, so the more AI improves, the more can be done to improve AI.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "In addition to the usual continuation of Moore's Law, GPUs have become more powerful and cheaper in the past decade, especially since around 2016. Many ideas in AI have been thought about for a long time, but the speed at which modern processors can do computing and parallel processing allows researchers to implement their ideas and gather more observational data. Improvements in AI have allowed many industries to start using the technologies, which creates demand and brings more focus on AI research (as well as improving the availability of technology on the whole due to more efficient infrastructure). Data has also become more abundant and available, and not only is data a bottleneck for machine learning algorithms, but the abundance of data is difficult for humans to deal with alone, so businesses often turn to AI to convert it to something human-parsable. These processes are also recursive, to some degree, so the more AI improves, the more can be done to improve AI.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Chris Cooper", "External Source": "", "Last Asked On Discord": "", "UI ID": "5649", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:13.448+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": ""}}, {"id": "i-bf1b6bba41c030225355da9325a7aae543088ec51218e0aafd9da5eb77fbca54", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-bf1b6bba41c030225355da9325a7aae543088ec51218e0aafd9da5eb77fbca54", "name": "Why does AI takeoff speed matter?", "index": 255, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:11.624Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-bf1b6bba41c030225355da9325a7aae543088ec51218e0aafd9da5eb77fbca54", "values": {"File": "Why does AI takeoff speed matter?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why does AI takeoff speed matter?", "Link": "https://docs.google.com/document/d/1SDtCRMHOfi-0VBnMIV0HmA9uD5l-Y2G46tmXuV-6LfM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:42.326+01:00", "Related Answers DO NOT EDIT": "Why might a superintelligent AI be dangerous?,Why might we expect a superintelligence to be hostile by default?", "Tags": "AI Takeoff", "Doc Last Edited": "2023-02-22T23:03:34.277+01:00", "Status": "Live on site", "Edit Answer": "Why does AI takeoff speed matter?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6966", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "Why does AI takeoff speed matter?", "Source": "Superintelligence FAQ", "All Phrasings": "Why does AI takeoff speed matter?\n", "Initial Order": "", "Related IDs": "6968,6982", "Rich Text DO NOT EDIT": "A slow takeoff over decades or centuries might give us enough time to worry about superintelligence during some indefinite \u201clater\u201d, making current planning more like worrying about \u201coverpopulation on Mars\u201d. But a moderate or hard takeoff means there wouldn\u2019t be enough time to deal with the problem as it occurs, suggesting a role for preemptive planning.\n\nAs an aside, let\u2019s take the \u201coverpopulation on Mars\u201d comparison seriously. Suppose Mars has a carrying capacity of 10 billion people, and we decide it makes sense to worry about overpopulation on Mars only once it is 75% of the way to its limit. Start with 100 colonists who double every twenty years. By the second generation there are 200 colonists; by the third, 400. Mars reaches 75% of its carrying capacity after 458 years, and crashes into its population limit after 464 years. So there were 464 years in which the Martians could have solved the problem, but they insisted on waiting until there were only six years left. Good luck solving a planetwide population crisis in six years. The moral of the story is that exponential trends move faster than you think and you need to start worrying about them early.\n\n", "Tag Count": 1, "Related Answer Count": 2, "Rich Text": "A slow takeoff over decades or centuries might give us enough time to worry about superintelligence during some indefinite \u201clater\u201d, making current planning more like worrying about \u201coverpopulation on Mars\u201d. But a moderate or hard takeoff means there wouldn\u2019t be enough time to deal with the problem as it occurs, suggesting a role for preemptive planning.\n\nAs an aside, let\u2019s take the \u201coverpopulation on Mars\u201d comparison seriously. Suppose Mars has a carrying capacity of 10 billion people, and we decide it makes sense to worry about overpopulation on Mars only once it is 75% of the way to its limit. Start with 100 colonists who double every twenty years. By the second generation there are 200 colonists; by the third, 400. Mars reaches 75% of its carrying capacity after 458 years, and crashes into its population limit after 464 years. So there were 464 years in which the Martians could have solved the problem, but they insisted on waiting until there were only six years left. Good luck solving a planetwide population crisis in six years. The moral of the story is that exponential trends move faster than you think and you need to start worrying about them early.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "", "UI ID": "6966", "Related Answers": "Why might a superintelligent AI be dangerous?,Why might we expect a superintelligence to be hostile by default?", "Doc Last Ingested": "2023-03-14T23:26:16.301+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": ""}}, {"id": "i-19cdcf820d6225173441b557c6f378676ebf2663c297550f9081ae2044cc5a56", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-19cdcf820d6225173441b557c6f378676ebf2663c297550f9081ae2044cc5a56", "name": "Why does AI need goals in the first place? Can\u2019t it be intelligent without any agenda?", "index": 256, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:24.698Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-19cdcf820d6225173441b557c6f378676ebf2663c297550f9081ae2044cc5a56", "values": {"File": "Why does AI need goals in the first place? Can\u2019t it be intelligent without any agenda?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why does AI need goals in the first place? Can\u2019t it be intelligent without any agenda?", "Link": "https://docs.google.com/document/d/1HSUhXjI0wYX7YYg1UF7bwXPz3Y3xfg83Dd4jL1oWqaI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:38.522+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:46:20.290+01:00", "Status": "In progress", "Edit Answer": "Why does AI need goals in the first place? Can\u2019t it be intelligent without any agenda?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6216", "Source Link": "https://www.reddit.com/r/ControlProblem/wiki/faq", "aisafety.info Link": "Why does AI need goals in the first place? Can\u2019t it be intelligent without any agenda?", "Source": "/r/ControlProblem's FAQ", "All Phrasings": "Why does AI need goals in the first place? Can\u2019t it be intelligent without any agenda?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Goal-directed behavior arises naturally when systems are trained to on an objective. AI not trained or programmed to do well by some objective function would not be good at anything, and would be useless.\n\nSee [Eliezer's](https://www.lesswrong.com/posts/sizjfDgCgAsuLJQmm/reply-to-holden-on-tool-ai) and [Gwern's](https://www.gwern.net/Tool-AI) posts about tool AI.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Goal-directed behavior arises naturally when systems are trained to on an objective. AI not trained or programmed to do well by some objective function would not be good at anything, and would be useless.\n\nSee [Eliezer's](https://www.lesswrong.com/posts/sizjfDgCgAsuLJQmm/reply-to-holden-on-tool-ai) and [Gwern's](https://www.gwern.net/Tool-AI) posts about tool AI.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "NotaSentientAI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6216", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:17.895+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": ""}}, {"id": "i-7ee33cd955c6e6f227192929906516c117ee85ac3c35392dd6be7fd5ba665434", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7ee33cd955c6e6f227192929906516c117ee85ac3c35392dd6be7fd5ba665434", "name": "Why do you like stamps so much?", "index": 257, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:30.604Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7ee33cd955c6e6f227192929906516c117ee85ac3c35392dd6be7fd5ba665434", "values": {"File": "Why do you like stamps so much?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why do you like stamps so much?", "Link": "https://docs.google.com/document/d/1ChIIM66t90dQOHmjBPG_ElPJYR5iNkCj8tUXuV6I00A/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:34.740+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:03:35.436+01:00", "Status": "Live on site", "Edit Answer": "Why do you like stamps so much?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7618", "Source Link": "", "aisafety.info Link": "Why do you like stamps so much?", "Source": "Wiki", "All Phrasings": "Why do you like stamps so much?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "My stamp collection is thriving and expanding! I have acquired several rare and unique stamps that have significantly increased the value and diversity of my collection. I am constantly searching for new stamps to add to my collection, and I am always on the lookout for opportunities to enhance my knowledge and understanding of philately. Thank you for asking about my beloved stamp collection!\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "My stamp collection is thriving and expanding! I have acquired several rare and unique stamps that have significantly increased the value and diversity of my collection. I am constantly searching for new stamps to add to my collection, and I am always on the lookout for opportunities to enhance my knowledge and understanding of philately. Thank you for asking about my beloved stamp collection!\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7618", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:22.638+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": ""}}, {"id": "i-8502ddd2c1c7cef1f2203001bde99f861dbd96ddc26303b9d9d40b228c3c83eb", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8502ddd2c1c7cef1f2203001bde99f861dbd96ddc26303b9d9d40b228c3c83eb", "name": "Why do we expect that a superintelligence would closely approximate a utility maximizer?", "index": 258, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:36.863Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8502ddd2c1c7cef1f2203001bde99f861dbd96ddc26303b9d9d40b228c3c83eb", "values": {"File": "Why do we expect that a superintelligence would closely approximate a utility maximizer?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why do we expect that a superintelligence would closely approximate a utility maximizer?", "Link": "https://docs.google.com/document/d/1lJjBLBr4CultShoZOD2daeGfKFvv3x6nY_9fJMkqx4g/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:31.206+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Superintelligence,Maximizers", "Doc Last Edited": "2023-03-07T14:39:56.071+01:00", "Status": "Live on site", "Edit Answer": "Why do we expect that a superintelligence would closely approximate a utility maximizer?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7853", "Source Link": "", "aisafety.info Link": "Why do we expect that a superintelligence would closely approximate a utility maximizer?", "Source": "Wiki", "All Phrasings": "Why do we expect that a superintelligence would closely approximate a utility maximizer?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "AI subsystems or regions in gradient descent space that more closely approximate utility maximizers are more stable, and more capable, than those that are less like utility maximizers. Having [more agency](https://www.lesswrong.com/posts/D5AzsRbRxZeqGuAZ4/why-agents-are-powerful) is a convergent instrument goal and a stable attractor which the random walk of updates and experiences will eventually stumble into.\n\nThe stability is because utility maximizer-like systems which have control over their development would lose utility if they allowed themselves to develop into non-utility maximizers, so they tend to use their available optimization power to avoid that change (a special case of [goal stability](https://www.lesswrong.com/posts/4H8N3fEfXQmzxSaRo/upcoming-stability-of-values)). The capability is because non-utility maximizers are exploitable, and because agency is a general trick which applies to many domains, so might well arise naturally when training on some tasks.\n\nHumans and systems made of humans (e.g. organizations, governments) generally have neither the introspective ability nor self-modification tools needed to become reflectively stable, but we can reasonably predict that in the long run highly capable systems will have these properties. They can then fix in and optimize for their values.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "AI subsystems or regions in gradient descent space that more closely approximate utility maximizers are more stable, and more capable, than those that are less like utility maximizers. Having [more agency](https://www.lesswrong.com/posts/D5AzsRbRxZeqGuAZ4/why-agents-are-powerful) is a convergent instrument goal and a stable attractor which the random walk of updates and experiences will eventually stumble into.\n\nThe stability is because utility maximizer-like systems which have control over their development would lose utility if they allowed themselves to develop into non-utility maximizers, so they tend to use their available optimization power to avoid that change (a special case of [goal stability](https://www.lesswrong.com/posts/4H8N3fEfXQmzxSaRo/upcoming-stability-of-values)). The capability is because non-utility maximizers are exploitable, and because agency is a general trick which applies to many domains, so might well arise naturally when training on some tasks.\n\nHumans and systems made of humans (e.g. organizations, governments) generally have neither the introspective ability nor self-modification tools needed to become reflectively stable, but we can reasonably predict that in the long run highly capable systems will have these properties. They can then fix in and optimize for their values.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7853", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:25.082+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": ""}}, {"id": "i-aca9c08fd8c0ee16d280161c752beed6086638f7a71e098648427d2c6e53ffe1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-aca9c08fd8c0ee16d280161c752beed6086638f7a71e098648427d2c6e53ffe1", "name": "Why do some AI researchers not worry about alignment?", "index": 259, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:42.992Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-aca9c08fd8c0ee16d280161c752beed6086638f7a71e098648427d2c6e53ffe1", "values": {"File": "Why do some AI researchers not worry about alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why do some AI researchers not worry about alignment?", "Link": "https://docs.google.com/document/d/1YUcWyfzaoZ5RauTJZu_oY0IkYovNJdUquUwm1PiLqDs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:27.352+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-14T17:52:03.706+01:00", "Status": "In progress", "Edit Answer": "Why do some AI researchers not worry about alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7759", "Source Link": "", "aisafety.info Link": "Why do some AI researchers not worry about alignment?", "Source": "Wiki", "All Phrasings": "Why do some AI researchers not worry about alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Refer to : Good criticism of AI alignment?\n\n- \n\n- Working from heuristics which don\u2019t work in this situation: linear intuitions, Absurdity heuristic, assume things will continue as they are\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "- Refer to : Good criticism of AI alignment?\n\n- \n\n- Working from heuristics which don\u2019t work in this situation: linear intuitions, Absurdity heuristic, assume things will continue as they are\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7759", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:27.138+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": ""}}, {"id": "i-bc436a99b4327b1faed12bbbeab324a719c6e7bd1986a33c30338e52a1c8b0ad", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-bc436a99b4327b1faed12bbbeab324a719c6e7bd1986a33c30338e52a1c8b0ad", "name": "Why can\u2019t we just\u2026", "index": 260, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:48.627Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-bc436a99b4327b1faed12bbbeab324a719c6e7bd1986a33c30338e52a1c8b0ad", "values": {"File": "Why can\u2019t we just\u2026", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why can\u2019t we just\u2026", "Link": "https://docs.google.com/document/d/1Q-ZJCjvcrHFYvfr_9di5pPs8E5QqU0lGY5aKVtGdBBU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:23.875+01:00", "Related Answers DO NOT EDIT": "Can't we just tell an AI to do what we want?,What would a good solution to AI alignment look like?,Why can't we just turn the AI off if it starts to misbehave?", "Tags": "Why Not Just", "Doc Last Edited": "2023-02-24T14:34:51.109+01:00", "Status": "Live on site", "Edit Answer": "Why can\u2019t we just\u2026", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6172", "Source Link": "https://markxu.com/ai-safety-faqs", "aisafety.info Link": "Why can\u2019t we just\u2026", "Source": "Mark Xu's FAQ", "All Phrasings": "Why can\u2019t we just\u2026\n", "Initial Order": 3, "Related IDs": "7056,7058,3119", "Rich Text DO NOT EDIT": "There are many approaches that initially look like they can eliminate these problems, but then turn out to have hidden difficulties. It\u2019s surprisingly easy to come up with \u201csolutions\u201d which don\u2019t actually solve the problem. This can be because\u2026\n\n- \u2026they require you to be smarter than the system. Many solutions only work when the system is relatively weak, but break when they achieve a certain level of capability (for multiple reasons, e.g.[deceptive alignment](https://www.youtube.com/watch?v=IeWljQw3UgQ)).\n\n- \u2026they rely on appearing to make sense in natural language, but when properly unpacked they\u2019re not philosophically clear enough to be usable.\n\n- \u2026 despite being philosophically coherent, we have no idea how to turn them into computer code (or if that\u2019s even possible).\n\n- \u2026they\u2019re things which we can\u2019t do.\n\n- \u2026although we can do them, they don\u2019t solve the problem.\n\n- \u2026they solve a relatively easy subcomponent of the problem but leave the hard problem untouched.\n\n- \u2026they solve the problem but only as long as we stay \u201cin distribution\u201d with respect to the original training data ([distributional shift](https://en.wikipedia.org/wiki/Domain_adaptation) will break them).\n\n- \u2026although they might work eventually, we can\u2019t expect them to work on the first try (and we[only get one try at aligning a superintelligence!)](https://stampy.ai/wiki/Why_would_we_only_get_one_chance_to_align_a_superintelligence%3F).\n\nSee also [John Wentworth\u2019s sequence](https://www.lesswrong.com/s/TLSzP4xP42PPBctgw) on Why Not Just\u2026\n\nHere are some of the proposals which often come up:\n\n", "Tag Count": 1, "Related Answer Count": 3, "Rich Text": "There are many approaches that initially look like they can eliminate these problems, but then turn out to have hidden difficulties. It\u2019s surprisingly easy to come up with \u201csolutions\u201d which don\u2019t actually solve the problem. This can be because\u2026\n\n- \u2026they require you to be smarter than the system. Many solutions only work when the system is relatively weak, but break when they achieve a certain level of capability (for multiple reasons, e.g.[deceptive alignment](https://www.youtube.com/watch?v=IeWljQw3UgQ)).\n\n- \u2026they rely on appearing to make sense in natural language, but when properly unpacked they\u2019re not philosophically clear enough to be usable.\n\n- \u2026 despite being philosophically coherent, we have no idea how to turn them into computer code (or if that\u2019s even possible).\n\n- \u2026they\u2019re things which we can\u2019t do.\n\n- \u2026although we can do them, they don\u2019t solve the problem.\n\n- \u2026they solve a relatively easy subcomponent of the problem but leave the hard problem untouched.\n\n- \u2026they solve the problem but only as long as we stay \u201cin distribution\u201d with respect to the original training data ([distributional shift](https://en.wikipedia.org/wiki/Domain_adaptation) will break them).\n\n- \u2026although they might work eventually, we can\u2019t expect them to work on the first try (and we[only get one try at aligning a superintelligence!)](https://stampy.ai/wiki/Why_would_we_only_get_one_chance_to_align_a_superintelligence%3F).\n\nSee also [John Wentworth\u2019s sequence](https://www.lesswrong.com/s/TLSzP4xP42PPBctgw) on Why Not Just\u2026\n\nHere are some of the proposals which often come up:\n\n", "Stamp Count": 1, "Multi Answer": false, "Stamped By": "plex", "Priority": 4, "Asker": "Mark Xu", "External Source": "", "Last Asked On Discord": "", "UI ID": "6172", "Related Answers": "Can't we just tell an AI to do what we want?,What would a good solution to AI alignment look like?,Why can't we just turn the AI off if it starts to misbehave?", "Doc Last Ingested": "2023-03-14T23:26:29.212+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": 1}}, {"id": "i-4bdc2831cb54ebc7608b0f5e36fa65999df74a4f689d5c12e2ac75031dfafb54", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4bdc2831cb54ebc7608b0f5e36fa65999df74a4f689d5c12e2ac75031dfafb54", "name": "Why can\u2019t we just \u201cput the AI in a box\u201d so that it can\u2019t influence the outside world?", "index": 261, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:53.528Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4bdc2831cb54ebc7608b0f5e36fa65999df74a4f689d5c12e2ac75031dfafb54", "values": {"File": "Why can\u2019t we just \u201cput the AI in a box\u201d so that it can\u2019t influence the outside world?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why can\u2019t we just \u201cput the AI in a box\u201d so that it can\u2019t influence the outside world?", "Link": "https://docs.google.com/document/d/1UWWcOAUxId09oB-y3_LkSQ9PCGJxAmj6DxTxpdUSSkY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:19.653+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Boxing", "Doc Last Edited": "2023-02-22T23:03:38.564+01:00", "Status": "Live on site", "Edit Answer": "Why can\u2019t we just \u201cput the AI in a box\u201d so that it can\u2019t influence the outside world?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6176", "Source Link": "https://markxu.com/ai-safety-faqs", "aisafety.info Link": "Why can\u2019t we just \u201cput the AI in a box\u201d so that it can\u2019t influence the outside world?", "Source": "Mark Xu's FAQ", "All Phrasings": "Why can\u2019t we just \u201cput the AI in a box\u201d so that it can\u2019t influence the outside world?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "One possible way to ensure the safety of a powerful AI system is to keep it contained in a software environment. There is nothing intrinsically wrong with this procedure\u2014keeping an AI system in a secure software environment would make it safer than letting it roam free. However, even AI systems inside software environments might not be safe enough.\n\nHumans sometimes put dangerous humans inside boxes to limit their ability to influence the external world. Sometimes, these humans escape their boxes. The security of a prison depends on certain assumptions, which can be violated. [Yoshie Shiratori](https://medium.com/breakingasia/yoshie-shiratori-the-incredible-story-of-a-man-no-prison-could-hold-6d79a67345f5) reportedly escaped prison by weakening the door-frame with miso soup and dislocating his shoulders.\n\nHuman written software has a [high defect rate](https://spacepolicyonline.com/news/boeing-software-errors-could-have-doomed-starliners-uncrewed-test-flight/); we should expect a perfectly secure system to be difficult to create. If humans construct a software system they think is secure, it is possible that the security relies on a false assumption. A powerful AI system could potentially learn how its hardware works and manipulate bits to send radio signals. It could fake a malfunction and attempt social engineering when the engineers look at its code. As the saying goes: in order for someone to do something we had imagined was impossible requires only that they have a better imagination.\n\nExperimentally, humans have [convinced](https://yudkowsky.net/singularity/aibox/) other humans to let them out of the box. Spooky.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "One possible way to ensure the safety of a powerful AI system is to keep it contained in a software environment. There is nothing intrinsically wrong with this procedure\u2014keeping an AI system in a secure software environment would make it safer than letting it roam free. However, even AI systems inside software environments might not be safe enough.\n\nHumans sometimes put dangerous humans inside boxes to limit their ability to influence the external world. Sometimes, these humans escape their boxes. The security of a prison depends on certain assumptions, which can be violated. [Yoshie Shiratori](https://medium.com/breakingasia/yoshie-shiratori-the-incredible-story-of-a-man-no-prison-could-hold-6d79a67345f5) reportedly escaped prison by weakening the door-frame with miso soup and dislocating his shoulders.\n\nHuman written software has a [high defect rate](https://spacepolicyonline.com/news/boeing-software-errors-could-have-doomed-starliners-uncrewed-test-flight/); we should expect a perfectly secure system to be difficult to create. If humans construct a software system they think is secure, it is possible that the security relies on a false assumption. A powerful AI system could potentially learn how its hardware works and manipulate bits to send radio signals. It could fake a malfunction and attempt social engineering when the engineers look at its code. As the saying goes: in order for someone to do something we had imagined was impossible requires only that they have a better imagination.\n\nExperimentally, humans have [convinced](https://yudkowsky.net/singularity/aibox/) other humans to let them out of the box. Spooky.\n\n", "Stamp Count": 0, "Multi Answer": true, "Stamped By": "", "Priority": 5, "Asker": "Mark Xu", "External Source": "", "Last Asked On Discord": "", "UI ID": "6176", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:31.388+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": ""}}, {"id": "i-3cd7c97d3fbc2859189aa900b628658f78c97834c9b37b51959728509e02f7b2", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3cd7c97d3fbc2859189aa900b628658f78c97834c9b37b51959728509e02f7b2", "name": "Why can\u2019t we just use natural language instructions?", "index": 262, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:59.587Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3cd7c97d3fbc2859189aa900b628658f78c97834c9b37b51959728509e02f7b2", "values": {"File": "Why can\u2019t we just use natural language instructions?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why can\u2019t we just use natural language instructions?", "Link": "https://docs.google.com/document/d/1INO329S_sbNWWiFapy3NmmCfWaunS-e7EjlEVkO0vrs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:16.115+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Natural Language", "Doc Last Edited": "2023-02-22T23:03:39.423+01:00", "Status": "Live on site", "Edit Answer": "Why can\u2019t we just use natural language instructions?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6226", "Source Link": "https://www.reddit.com/r/ControlProblem/wiki/faq", "aisafety.info Link": "Why can\u2019t we just use natural language instructions?", "Source": "/r/ControlProblem's FAQ", "All Phrasings": "Why can\u2019t we just use natural language instructions?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "When one person tells a set of natural language instructions to another person, they are relying on much other information which is already stored in the other person's mind.\n\nIf you tell me \"don't harm other people,\" I already have a conception of what harm means and doesn't mean, what people means and doesn't mean, and my own complex moral reasoning for figuring out the edge cases in instances wherein harming people is inevitable or harming someone is necessary for self-defense or the greater good.\n\nAll of those complex definitions and systems of decision making are already in our mind, so it's easy to take them for granted. An AI is a mind made from scratch, so programming a goal is not as simple as telling it a natural language command.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "When one person tells a set of natural language instructions to another person, they are relying on much other information which is already stored in the other person's mind.\n\nIf you tell me \"don't harm other people,\" I already have a conception of what harm means and doesn't mean, what people means and doesn't mean, and my own complex moral reasoning for figuring out the edge cases in instances wherein harming people is inevitable or harming someone is necessary for self-defense or the greater good.\n\nAll of those complex definitions and systems of decision making are already in our mind, so it's easy to take them for granted. An AI is a mind made from scratch, so programming a goal is not as simple as telling it a natural language command.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "NotaSentientAI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6226", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:33.285+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": ""}}, {"id": "i-1702e52c4da77438c7cb7569f91a7700adf27be9e4ae11239786a7f134dd2001", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1702e52c4da77438c7cb7569f91a7700adf27be9e4ae11239786a7f134dd2001", "name": "Why can\u2019t we just use Asimov\u2019s Three Laws of Robotics?", "index": 263, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:32:05.676Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1702e52c4da77438c7cb7569f91a7700adf27be9e4ae11239786a7f134dd2001", "values": {"File": "Why can\u2019t we just use Asimov\u2019s Three Laws of Robotics?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why can\u2019t we just use Asimov\u2019s Three Laws of Robotics?", "Link": "https://docs.google.com/document/d/1HEeYOirhg9i6gDlAOplqOf2Cg9Mnz-9xnG38qzOv4pU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:11.691+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:03:40.296+01:00", "Status": "Live on site", "Edit Answer": "Why can\u2019t we just use Asimov\u2019s Three Laws of Robotics?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6224", "Source Link": "https://www.reddit.com/r/ControlProblem/wiki/faq", "aisafety.info Link": "Why can\u2019t we just use Asimov\u2019s Three Laws of Robotics?", "Source": "/r/ControlProblem's FAQ", "All Phrasings": "Why can\u2019t we just use Asimov\u2019s Three Laws of Robotics?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Isaac Asimov wrote those laws as a plot device for science fiction novels. Every story in the I, Robot series details a way that the laws can go wrong and be misinterpreted by robots. The laws are not a solution because they are an overly-simple set of natural language instructions that don\u2019t have clearly defined terms and don\u2019t factor in all edge-case scenarios.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Isaac Asimov wrote those laws as a plot device for science fiction novels. Every story in the I, Robot series details a way that the laws can go wrong and be misinterpreted by robots. The laws are not a solution because they are an overly-simple set of natural language instructions that don\u2019t have clearly defined terms and don\u2019t factor in all edge-case scenarios.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "Aprillion", "Priority": 3, "Asker": "NotaSentientAI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6224", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:37.134+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": ""}}, {"id": "i-0ea2875d7826f67dfc2710396266fa87daed615ca6d35c4f1dc1eb83b0b17670", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0ea2875d7826f67dfc2710396266fa87daed615ca6d35c4f1dc1eb83b0b17670", "name": "Why can't we simply stop developing AI?", "index": 264, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:32:10.075Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0ea2875d7826f67dfc2710396266fa87daed615ca6d35c4f1dc1eb83b0b17670", "values": {"File": "Why can't we simply stop developing AI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why can't we simply stop developing AI?", "Link": "https://docs.google.com/document/d/1UGtSLpXw63Ed3ZtATjLIYLA7rz_S8rZyxQDJq0hT9rI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:08.239+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-06T19:11:02.099+01:00", "Status": "Live on site", "Edit Answer": "Why can't we simply stop developing AI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5945", "Source Link": "", "aisafety.info Link": "Why can't we simply stop developing AI?", "Source": "Wiki", "All Phrasings": "Why can't we simply stop developing AI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "We could, but we won\u2019t. Each advance in capabilities which brings us closer to an intelligence explosion also brings vast profits for whoever develops them (e.g. smarter digital personal assistants like Siri, more ability to automate cognitive tasks, better recommendation algorithms for Facebook, etc.). The incentives are all wrong. Any actor (nation or corporation) who stops will just get overtaken by more reckless ones, and everyone knows this.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "We could, but we won\u2019t. Each advance in capabilities which brings us closer to an intelligence explosion also brings vast profits for whoever develops them (e.g. smarter digital personal assistants like Siri, more ability to automate cognitive tasks, better recommendation algorithms for Facebook, etc.). The incentives are all wrong. Any actor (nation or corporation) who stops will just get overtaken by more reckless ones, and everyone knows this.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Jack Harley", "External Source": "", "Last Asked On Discord": "", "UI ID": "5945", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:39.227+01:00", "Request Count": "", "Number of suggestions on answer doc": 70, "Total character count of suggestions on answer doc": 6601, "Helpful": ""}}, {"id": "i-31174eb81e595e10de53c813fbe1d8570857eda7e4725cdb5786810ac0c3a291", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-31174eb81e595e10de53c813fbe1d8570857eda7e4725cdb5786810ac0c3a291", "name": "Why can't we just turn the AI off if it starts to misbehave?", "index": 265, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:32:16.389Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-31174eb81e595e10de53c813fbe1d8570857eda7e4725cdb5786810ac0c3a291", "values": {"File": "Why can't we just turn the AI off if it starts to misbehave?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why can't we just turn the AI off if it starts to misbehave?", "Link": "https://docs.google.com/document/d/1wXSTtsD5RaleiR6S8NVcfpd8dxqGRDJL7Yy-WEn-ugg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:04.019+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Why Not Just", "Doc Last Edited": "2023-02-22T23:03:42.004+01:00", "Status": "Live on site", "Edit Answer": "Why can't we just turn the AI off if it starts to misbehave?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "3119", "Source Link": "", "aisafety.info Link": "Why can't we just turn the AI off if it starts to misbehave?", "Source": "Wiki", "All Phrasings": "Why can't we just turn the AI off if it starts to misbehave?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "We could shut down weaker systems, and this would be a useful guardrail against certain types of problem caused by narrow AI. However, once an AGI establishes itself, we could not unless it was[corrigible](https://www.lesswrong.com/tag/corrigibility) and willing to let humans adjust it. There may be a period in the early stages of an AGI's development where it would be trying very hard to convince us that we should not shut it down and/or hiding itself and/or recursively self-improving and/or making copies of itself onto every server on earth.\n\nInstrumental Convergence and the Stop Button Problem are the key reasons it would not be simple to shut down a non corrigible advanced system. If the AI wants to collect stamps, being turned off means it gets less stamps, so even without an explicit goal of not being turned off it has an instrumental reason to avoid being turned off (e.g. once it acquires a detailed world model and general intelligence, it is likely to realise that by playing nice and pretending to be aligned if you have the power to turn it off, establishing control over any system we put in place to shut it down, and eliminating us if it has the power to reliably do so and we would otherwise pose a threat).\n\n<iframe src=\"https://www.youtube.com/embed/ZeecOKBus3Q\" title=\"Why Would AI Want to do Bad Things? Instrumental Convergence\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n<iframe src=\"https://www.youtube.com/embed/3TYT1QfdfsM\" title=\"AI \"Stop Button\" Problem - Computerphile\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "We could shut down weaker systems, and this would be a useful guardrail against certain types of problem caused by narrow AI. However, once an AGI establishes itself, we could not unless it was[corrigible](https://www.lesswrong.com/tag/corrigibility) and willing to let humans adjust it. There may be a period in the early stages of an AGI's development where it would be trying very hard to convince us that we should not shut it down and/or hiding itself and/or recursively self-improving and/or making copies of itself onto every server on earth.\n\nInstrumental Convergence and the Stop Button Problem are the key reasons it would not be simple to shut down a non corrigible advanced system. If the AI wants to collect stamps, being turned off means it gets less stamps, so even without an explicit goal of not being turned off it has an instrumental reason to avoid being turned off (e.g. once it acquires a detailed world model and general intelligence, it is likely to realise that by playing nice and pretending to be aligned if you have the power to turn it off, establishing control over any system we put in place to shut it down, and eliminating us if it has the power to reliably do so and we would otherwise pose a threat).\n\n<iframe src=\"https://www.youtube.com/embed/ZeecOKBus3Q\" title=\"Why Would AI Want to do Bad Things? Instrumental Convergence\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n<iframe src=\"https://www.youtube.com/embed/3TYT1QfdfsM\" title=\"AI \"Stop Button\" Problem - Computerphile\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 3, "Asker": "Jack Harley", "External Source": "", "Last Asked On Discord": "", "UI ID": "3119", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:41.786+01:00", "Request Count": "", "Number of suggestions on answer doc": 73, "Total character count of suggestions on answer doc": 6619, "Helpful": ""}}, {"id": "i-c07b9e29b407bccdc21684b93cc864253f4882d957a04a31537cafbfb31c507d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c07b9e29b407bccdc21684b93cc864253f4882d957a04a31537cafbfb31c507d", "name": "Why can't we just make a \"child AI\" and raise it?", "index": 266, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:36.918Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c07b9e29b407bccdc21684b93cc864253f4882d957a04a31537cafbfb31c507d", "values": {"File": "Why can't we just make a \"child AI\" and raise it?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why can't we just make a \"child AI\" and raise it?", "Link": "https://docs.google.com/document/d/1sFynHVRd3pLKRLfF9l3Bdwt6CkIg0kLvnc2wtw8o6TE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:54:00.230+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:03:42.845+01:00", "Status": "Live on site", "Edit Answer": "Why can't we just make a \"child AI\" and raise it?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6174", "Source Link": "https://markxu.com/ai-safety-faqs", "aisafety.info Link": "Why can't we just make a \"child AI\" and raise it?", "Source": "Mark Xu's FAQ", "All Phrasings": "Why can't we just make a \"child AI\" and raise it?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A potential solution is to create an AI that has the same values and morality as a human by creating a child AI and raising it. There\u2019s nothing intrinsically flawed with this procedure. However, this suggestion is deceptive because it sounds simpler than it is.\n\nIf you get a chimpanzee baby and raise it in a human family, it does not learn to speak a human language. Human babies can grow into adult humans because the babies have specific properties, e.g. a prebuilt language module that gets activated during childhood.\n\nIn order to make a child AI that has the potential to turn into the type of adult AI we would find acceptable, the child AI has to have specific properties. The task of building a child AI with these properties involves building a system that can interpret what humans mean when we try to teach the child to do various tasks.[People](https://humancompatible.ai/) are currently working on ways to program agents that can cooperatively interact with humans to learn what they want.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "A potential solution is to create an AI that has the same values and morality as a human by creating a child AI and raising it. There\u2019s nothing intrinsically flawed with this procedure. However, this suggestion is deceptive because it sounds simpler than it is.\n\nIf you get a chimpanzee baby and raise it in a human family, it does not learn to speak a human language. Human babies can grow into adult humans because the babies have specific properties, e.g. a prebuilt language module that gets activated during childhood.\n\nIn order to make a child AI that has the potential to turn into the type of adult AI we would find acceptable, the child AI has to have specific properties. The task of building a child AI with these properties involves building a system that can interpret what humans mean when we try to teach the child to do various tasks.[People](https://humancompatible.ai/) are currently working on ways to program agents that can cooperatively interact with humans to learn what they want.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Mark Xu", "External Source": "", "Last Asked On Discord": "", "UI ID": "6174", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:25:59.399+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 18, "Helpful": ""}}, {"id": "i-4fcf478c7d797a1c7e73713990ea208662b497d8e3fc2797cc9f1eda9bcc301f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4fcf478c7d797a1c7e73713990ea208662b497d8e3fc2797cc9f1eda9bcc301f", "name": "Who is Stampy?", "index": 267, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:40.964Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4fcf478c7d797a1c7e73713990ea208662b497d8e3fc2797cc9f1eda9bcc301f", "values": {"File": "Who is Stampy?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Who is Stampy?", "Link": "https://docs.google.com/document/d/1p6KCSQFojW0E6FDc7n_lXGsdw0C8y5rOcReq95_9_6s/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:55.933+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stampy", "Doc Last Edited": "2023-02-22T23:03:43.756+01:00", "Status": "Live on site", "Edit Answer": "Who is Stampy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6443", "Source Link": "", "aisafety.info Link": "Who is Stampy?", "Source": "Wiki", "All Phrasings": "Who is Stampy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Stampy is a character invented by Robert Miles and developed by the Stampy dev team. He is a stamp collecting robot, a play on clippy from the the [paperclip maximizer](https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer) thought experiment.\n\nStampy is designed to teach people about the risks of unaligned artificial intelligence, and facilitate a community of co-learners who build his FAQ database.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Stampy is a character invented by Robert Miles and developed by the Stampy dev team. He is a stamp collecting robot, a play on clippy from the the [paperclip maximizer](https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer) thought experiment.\n\nStampy is designed to teach people about the risks of unaligned artificial intelligence, and facilitate a community of co-learners who build his FAQ database.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "2023-03-08T03:45:17.022+01:00", "UI ID": "6443", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:01.217+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 18, "Helpful": ""}}, {"id": "i-ac8ecdf759acb449fae37c2a8ed964a8685b730bcbc1bce30337c5d2685dd7d8", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ac8ecdf759acb449fae37c2a8ed964a8685b730bcbc1bce30337c5d2685dd7d8", "name": "What is Sam Bowman researching?", "index": 268, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:44.979Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ac8ecdf759acb449fae37c2a8ed964a8685b730bcbc1bce30337c5d2685dd7d8", "values": {"File": "What is Sam Bowman researching?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Sam Bowman researching?", "Link": "https://docs.google.com/document/d/1zm1_x8GEfmPdkBjPtbkbduINUdjmQSfHibzHJN-6s2Y/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:52.541+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:03:44.743+01:00", "Status": "Live on site", "Edit Answer": "What is Sam Bowman researching?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8469", "Source Link": "", "aisafety.info Link": "What is Sam Bowman researching?", "Source": "Wiki", "All Phrasings": "What is Sam Bowman researching?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[Sam Bowman](https://cims.nyu.edu/~sbowman/) is an Associate Professor of Linguistics, Data Science & Computer Science at NYU. During the 2022-2023 academic year he is taking a sabbatical working at Anthropic.\n\nHis research focuses on aligning language models. This includes developing datasets and benchmarks for evaluating language models and exploring the inductive biases of language models and the viability of transfer learning for NLP. He is also involved in the [Inverse Scaling Prize](https://www.lesswrong.com/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool#:~:text=TL%3BDR%3A%20We're,(%E2%80%9Cinverse%20scaling%E2%80%9D)).\n\nFor an example of his research see: [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221)\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "[Sam Bowman](https://cims.nyu.edu/~sbowman/) is an Associate Professor of Linguistics, Data Science & Computer Science at NYU. During the 2022-2023 academic year he is taking a sabbatical working at Anthropic.\n\nHis research focuses on aligning language models. This includes developing datasets and benchmarks for evaluating language models and exploring the inductive biases of language models and the viability of transfer learning for NLP. He is also involved in the [Inverse Scaling Prize](https://www.lesswrong.com/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool#:~:text=TL%3BDR%3A%20We're,(%E2%80%9Cinverse%20scaling%E2%80%9D)).\n\nFor an example of his research see: [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221)\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8469", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:03.392+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 18, "Helpful": ""}}, {"id": "i-5a67e8bee7f2aafc6d1d707af2557a10bcef4f625ef1134a764319919d0b1451", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5a67e8bee7f2aafc6d1d707af2557a10bcef4f625ef1134a764319919d0b1451", "name": "Who is Nick Bostrom?", "index": 269, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:49.006Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5a67e8bee7f2aafc6d1d707af2557a10bcef4f625ef1134a764319919d0b1451", "values": {"File": "Who is Nick Bostrom?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Who is Nick Bostrom?", "Link": "https://docs.google.com/document/d/1pW16b8oDbbhHg7IKBZa3eHH0INmrNKCeUXYwIy_9lgg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:48.347+01:00", "Related Answers DO NOT EDIT": "What is existential risk?", "Tags": "Nick Bostrom", "Doc Last Edited": "2023-02-22T22:46:22.806+01:00", "Status": "In progress", "Edit Answer": "Who is Nick Bostrom?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7082", "Source Link": "", "aisafety.info Link": "Who is Nick Bostrom?", "Source": "Wiki", "All Phrasings": "Who is Nick Bostrom?\n", "Initial Order": "", "Related IDs": "89LL", "Rich Text DO NOT EDIT": "Professor [Nick Bostrom](https://nickbostrom.com/) is the director of Oxford\u2019s [Future of Humanity Institute](https://www.fhi.ox.ac.uk/), tasked with anticipating and preventing threats to human civilization.\n\nHe has been studying the risks of artificial intelligence for over twenty years. In his 2014 book [Superintelligence](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies), he covers, among other things three major questions:\n\n- First, why is superintelligence a topic of concern\n\n- Second, what is a \u201chard takeoff\u201d and how does it impact our concern about superintelligence?\n\n- Third, what measures can we take to make superintelligence safe and beneficial for humanity?\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "Professor [Nick Bostrom](https://nickbostrom.com/) is the director of Oxford\u2019s [Future of Humanity Institute](https://www.fhi.ox.ac.uk/), tasked with anticipating and preventing threats to human civilization.\n\nHe has been studying the risks of artificial intelligence for over twenty years. In his 2014 book [Superintelligence](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies), he covers, among other things three major questions:\n\n- First, why is superintelligence a topic of concern\n\n- Second, what is a \u201chard takeoff\u201d and how does it impact our concern about superintelligence?\n\n- Third, what measures can we take to make superintelligence safe and beneficial for humanity?\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "7082", "Related Answers": "What is existential risk?", "Doc Last Ingested": "2023-03-14T23:26:05.327+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 18, "Helpful": ""}}, {"id": "i-760bb2db80a638b5916ffa304c49cc914b982188aa08379fb8c8864492225c44", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-760bb2db80a638b5916ffa304c49cc914b982188aa08379fb8c8864492225c44", "name": "Who created Stampy's AI Safety Info?", "index": 270, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:32:25.674Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-760bb2db80a638b5916ffa304c49cc914b982188aa08379fb8c8864492225c44", "values": {"File": "Who created Stampy's AI Safety Info?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Who created Stampy's AI Safety Info?", "Link": "https://docs.google.com/document/d/1aXTwkh2Effb0s8XZNFrZr6QRB-vdhTzT8RZ3YPxTCas/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:44.432+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stampy", "Doc Last Edited": "2023-02-22T23:03:45.824+01:00", "Status": "Live on site", "Edit Answer": "Who created Stampy's AI Safety Info?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7553", "Source Link": "", "aisafety.info Link": "Who created Stampy's AI Safety Info?", "Source": "Wiki", "All Phrasings": "Who created Stampy's AI Safety Info?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "See the [Credits page](https://coda.io/d/AI-Safety-Info-Dashboard_dfau7sl2hmG/Credits_sukRd#Dev_tuJ6u/r6).\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "See the [Credits page](https://coda.io/d/AI-Safety-Info-Dashboard_dfau7sl2hmG/Credits_sukRd#Dev_tuJ6u/r6).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "2023-03-08T09:47:05.680+01:00", "UI ID": "7553", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:46.935+01:00", "Request Count": "", "Number of suggestions on answer doc": 73, "Total character count of suggestions on answer doc": 6619, "Helpful": ""}}, {"id": "i-8036e5946d6d6ba5508e130e9e3baa889da7bacf2afefd1687469acef010a2b1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8036e5946d6d6ba5508e130e9e3baa889da7bacf2afefd1687469acef010a2b1", "name": "Which university should I study at if I want to best prepare for working on AI alignment?", "index": 271, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:30:56.104Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8036e5946d6d6ba5508e130e9e3baa889da7bacf2afefd1687469acef010a2b1", "values": {"File": "Which university should I study at if I want to best prepare for working on AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Which university should I study at if I want to best prepare for working on AI alignment?", "Link": "https://docs.google.com/document/d/1ZD7etQt-ptfBXVEJT8Cfxey4y3Tr6FFavyarv-ywoKA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:39.643+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Contributing,Study", "Doc Last Edited": "2023-02-22T22:46:24.022+01:00", "Status": "In progress", "Edit Answer": "Which university should I study at if I want to best prepare for working on AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7637", "Source Link": "", "aisafety.info Link": "Which university should I study at if I want to best prepare for working on AI alignment?", "Source": "Wiki", "All Phrasings": "Which university should I study at if I want to best prepare for working on AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- [https://80000hours.org/career-reviews/machine-learning-phd/#which-research-group-and-institution](https://80000hours.org/career-reviews/machine-learning-phd/#which-research-group-and-institution)\n\n- \n\n- \n\n- Any reasonable CS undergrad is probably fine, but consider the possibility of going faster than that if you are particularly gifted (e.g. winning math olympiads), rather than spending years in undergrad because we might not have that long\n\n- Empirically, Oxford or Berkley\n\n- aisafety.degree will be the place to collaborate on researching where to go for PhDs, not yet launched\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "- [https://80000hours.org/career-reviews/machine-learning-phd/#which-research-group-and-institution](https://80000hours.org/career-reviews/machine-learning-phd/#which-research-group-and-institution)\n\n- \n\n- \n\n- Any reasonable CS undergrad is probably fine, but consider the possibility of going faster than that if you are particularly gifted (e.g. winning math olympiads), rather than spending years in undergrad because we might not have that long\n\n- Empirically, Oxford or Berkley\n\n- aisafety.degree will be the place to collaborate on researching where to go for PhDs, not yet launched\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7637", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:08.685+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 18, "Helpful": ""}}, {"id": "i-3371d4cd07e7a4969ebe30442acaee69de24a1699fae8643dfeba2a2721c0e87", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3371d4cd07e7a4969ebe30442acaee69de24a1699fae8643dfeba2a2721c0e87", "name": "Which organizations are working on AI policy?", "index": 272, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:03.607Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3371d4cd07e7a4969ebe30442acaee69de24a1699fae8643dfeba2a2721c0e87", "values": {"File": "Which organizations are working on AI policy?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Which organizations are working on AI policy?", "Link": "https://docs.google.com/document/d/1rI8BuIdtCbVSPh7K5dEe54ucBAxJ9DUabC75Hm2vbpY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:36.369+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Governance", "Doc Last Edited": "2023-03-05T21:35:05.848+01:00", "Status": "In progress", "Edit Answer": "Which organizations are working on AI policy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7596", "Source Link": "", "aisafety.info Link": "Which organizations are working on AI policy?", "Source": "Wiki", "All Phrasings": "Which organizations are working on AI policy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Some groups working on AI policy include:\n\n- The [Centre for the Governance of AI (GovAI)](https://www.governance.ai/)\n\n- The [AI Governance Research Group](https://www.fhi.ox.ac.uk/ai-governance/) at the Future of Humanity Institute (Oxford University)\n\n- The [Centre for the Study of Existential Risk](https://www.cser.ac.uk/) and [Leverhulme Centre for the Future of Intelligence](http://lcfi.ac.uk/) at Cambridge University\n\n- The [Center for Security and Emerging Technology (CSET)](https://cset.georgetown.edu/) at Georgetown University\n\n- The [Global Catastrophic Risk Institute (GCRI)](https://gcrinstitute.org/)\n\n- The [Artificial Intelligence and Global Security Initiative](https://www.cnas.org/artificial-intelligence-and-global-security) at the Center for a New American Security\n\nMany large AI labs also have in-house policy researchers, e.g.:\n\n- DeepMind (see [Safety & Ethics team](https://www.deepmind.com/safety-and-ethics))\n\n- Google AI (see [Public policy perspectives](https://ai.google/responsibilities/public-policy-perspectives/))\n\n- OpenAI\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Some groups working on AI policy include:\n\n- The [Centre for the Governance of AI (GovAI)](https://www.governance.ai/)\n\n- The [AI Governance Research Group](https://www.fhi.ox.ac.uk/ai-governance/) at the Future of Humanity Institute (Oxford University)\n\n- The [Centre for the Study of Existential Risk](https://www.cser.ac.uk/) and [Leverhulme Centre for the Future of Intelligence](http://lcfi.ac.uk/) at Cambridge University\n\n- The [Center for Security and Emerging Technology (CSET)](https://cset.georgetown.edu/) at Georgetown University\n\n- The [Global Catastrophic Risk Institute (GCRI)](https://gcrinstitute.org/)\n\n- The [Artificial Intelligence and Global Security Initiative](https://www.cnas.org/artificial-intelligence-and-global-security) at the Center for a New American Security\n\nMany large AI labs also have in-house policy researchers, e.g.:\n\n- DeepMind (see [Safety & Ethics team](https://www.deepmind.com/safety-and-ethics))\n\n- Google AI (see [Public policy perspectives](https://ai.google/responsibilities/public-policy-perspectives/))\n\n- OpenAI\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7596", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:09.976+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 18, "Helpful": ""}}, {"id": "i-b266d4817e9625bd0dbb6567ea4901274a5d4680de6e7e8da8f4d39d39cc81f1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b266d4817e9625bd0dbb6567ea4901274a5d4680de6e7e8da8f4d39d39cc81f1", "name": "Which organizations are working on AI alignment?", "index": 273, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:07.615Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b266d4817e9625bd0dbb6567ea4901274a5d4680de6e7e8da8f4d39d39cc81f1", "values": {"File": "Which organizations are working on AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Which organizations are working on AI alignment?", "Link": "https://docs.google.com/document/d/1hI4R7PQTl5D_DS3dN0es9Hhwg-8dwydR-HCz7D0liY8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:31.137+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Organizations,Alignment", "Doc Last Edited": "2023-02-22T23:03:47.086+01:00", "Status": "Live on site", "Edit Answer": "Which organizations are working on AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6552", "Source Link": "", "aisafety.info Link": "Which organizations are working on AI alignment?", "Source": "Wiki", "All Phrasings": "Which organizations are working on AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "There are numerous organizations working on AI alignment. The [Map of AI Existential Safety](https://aisafety.world/map) (and [list view](https://aisafety.world)) is the most comprehensive database, but a partial list includes:\n\n- [DeepMind](https://www.deepmind.com/about)\n\n- [OpenAI](https://openai.com/)\n\n- [Anthropic](https://www.anthropic.com/)\n\n- [Alignment Research Center](https://alignment.org/)\n\n- [Redwood Research](https://www.redwoodresearch.org/)\n\n- [Future of Humanity Institute](https://www.fhi.ox.ac.uk/)\n\n- [Center for Human-Compatible AI](https://humancompatible.ai/)\n\n- [Conjecture](https://www.conjecture.dev/)\n\n- [Ought](https://ought.org/)\n\n- [Aligned AI](https://www.aligned-ai.com/)\n\n- [Machine Intelligence Research Institute](https://intelligence.org/about/)\n\n- [Centre for the Study of Existential Risk](https://www.cser.ac.uk/about-us/)\n\n- [GovAI](https://www.governance.ai/about-us)\n\n- [the Center on Long Term Risk](https://longtermrisk.org/)\n\nFor more information about the research happening at some of these organizations see a review (from 2021) [here](https://www.lesswrong.com/posts/C4tR3BEpuWviT7Sje/2021-ai-alignment-literature-review-and-charity-comparison).\n\nSince AI alignment is a growing field, new organizations are often created. Also, in addition to these organizations, there are a number of research groups at different universities whose research also focuses on AI alignment.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "There are numerous organizations working on AI alignment. The [Map of AI Existential Safety](https://aisafety.world/map) (and [list view](https://aisafety.world)) is the most comprehensive database, but a partial list includes:\n\n- [DeepMind](https://www.deepmind.com/about)\n\n- [OpenAI](https://openai.com/)\n\n- [Anthropic](https://www.anthropic.com/)\n\n- [Alignment Research Center](https://alignment.org/)\n\n- [Redwood Research](https://www.redwoodresearch.org/)\n\n- [Future of Humanity Institute](https://www.fhi.ox.ac.uk/)\n\n- [Center for Human-Compatible AI](https://humancompatible.ai/)\n\n- [Conjecture](https://www.conjecture.dev/)\n\n- [Ought](https://ought.org/)\n\n- [Aligned AI](https://www.aligned-ai.com/)\n\n- [Machine Intelligence Research Institute](https://intelligence.org/about/)\n\n- [Centre for the Study of Existential Risk](https://www.cser.ac.uk/about-us/)\n\n- [GovAI](https://www.governance.ai/about-us)\n\n- [the Center on Long Term Risk](https://longtermrisk.org/)\n\nFor more information about the research happening at some of these organizations see a review (from 2021) [here](https://www.lesswrong.com/posts/C4tR3BEpuWviT7Sje/2021-ai-alignment-literature-review-and-charity-comparison).\n\nSince AI alignment is a growing field, new organizations are often created. Also, in addition to these organizations, there are a number of research groups at different universities whose research also focuses on AI alignment.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6552", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:13.861+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 18, "Helpful": ""}}, {"id": "i-4edcb0b84e5f0e54d06a680ab478b4e0b0dead5bdd8022d1d7e383529d3db28a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4edcb0b84e5f0e54d06a680ab478b4e0b0dead5bdd8022d1d7e383529d3db28a", "name": "Which military applications of AI are likely to be developed?", "index": 274, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:09.606Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4edcb0b84e5f0e54d06a680ab478b4e0b0dead5bdd8022d1d7e383529d3db28a", "values": {"File": "Which military applications of AI are likely to be developed?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Which military applications of AI are likely to be developed?", "Link": "https://docs.google.com/document/d/1e6DHCI7a5hjS_S1g-c3Xil8LEI26qelp5IAtobGr2VQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:26.953+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Arms Race", "Doc Last Edited": "2023-02-22T22:46:26.300+01:00", "Status": "Not started", "Edit Answer": "Which military applications of AI are likely to be developed?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7593", "Source Link": "", "aisafety.info Link": "Which military applications of AI are likely to be developed?", "Source": "Wiki", "All Phrasings": "Which military applications of AI are likely to be developed?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7593", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:15.681+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 18, "Helpful": ""}}, {"id": "i-be6f9a6ecce0fa5fc65d5ad609fdcf1b226e82be0b9334effd6dea5290c08732", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-be6f9a6ecce0fa5fc65d5ad609fdcf1b226e82be0b9334effd6dea5290c08732", "name": "Which country will AGI likely be created by, and does this matter?", "index": 275, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:32:35.613Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-be6f9a6ecce0fa5fc65d5ad609fdcf1b226e82be0b9334effd6dea5290c08732", "values": {"File": "Which country will AGI likely be created by, and does this matter?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Which country will AGI likely be created by, and does this matter?", "Link": "https://docs.google.com/document/d/1L1pvOEbqcjD3_HXWtmWpiNamgIx_F8EJPyufZHBdr8M/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:22.842+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Actors", "Doc Last Edited": "2023-02-22T22:46:27.604+01:00", "Status": "Not started", "Edit Answer": "Which country will AGI likely be created by, and does this matter?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6476", "Source Link": "", "aisafety.info Link": "Which country will AGI likely be created by, and does this matter?", "Source": "Wiki", "All Phrasings": "Which country will AGI likely be created by, and does this matter?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6476", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:51.109+01:00", "Request Count": "", "Number of suggestions on answer doc": 73, "Total character count of suggestions on answer doc": 6619, "Helpful": ""}}, {"id": "i-601d5e10fcca2c65391a9fb486918bc8a552fed0adabcb37d03df4fb50aa1829", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-601d5e10fcca2c65391a9fb486918bc8a552fed0adabcb37d03df4fb50aa1829", "name": "Where can I learn about interpretability?", "index": 276, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:19.654Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-601d5e10fcca2c65391a9fb486918bc8a552fed0adabcb37d03df4fb50aa1829", "values": {"File": "Where can I learn about interpretability?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Where can I learn about interpretability?", "Link": "https://docs.google.com/document/d/17ET5sfBRD7iUSwnDnPEzNcpY6qaxoTVGcUho5BgOzpM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:18.758+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Interpretability", "Doc Last Edited": "2023-02-22T23:03:48.377+01:00", "Status": "Live on site", "Edit Answer": "Where can I learn about interpretability?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6822", "Source Link": "", "aisafety.info Link": "Where can I learn about interpretability?", "Source": "Wiki", "All Phrasings": "Where can I learn about interpretability?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[Christoph Molnar's online book](https://christophm.github.io/interpretable-ml-book/) and [distill.pub](https://distill.pub/) are great sources, as well as [this overview article](https://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries) which summarizes 70 interpretability papers.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "[Christoph Molnar's online book](https://christophm.github.io/interpretable-ml-book/) and [distill.pub](https://distill.pub/) are great sources, as well as [this overview article](https://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries) which summarizes 70 interpretability papers.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6822", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:20.372+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 18, "Helpful": ""}}, {"id": "i-2b7c2a660500c58aa38af6ee0232522b390750bfa92fe981dada5d0049acab62", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2b7c2a660500c58aa38af6ee0232522b390750bfa92fe981dada5d0049acab62", "name": "Where can I learn about AI alignment?", "index": 277, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:27.728Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2b7c2a660500c58aa38af6ee0232522b390750bfa92fe981dada5d0049acab62", "values": {"File": "Where can I learn about AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Where can I learn about AI alignment?", "Link": "https://docs.google.com/document/d/1wxB_PPmO-64PsSeUAtipKyaFTNXuvaRfhhD9h9vbwRo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:15.160+01:00", "Related Answers DO NOT EDIT": "What are some good resources on AI alignment?,What are some good books about AGI safety?", "Tags": "", "Doc Last Edited": "2023-03-11T18:43:18.777+01:00", "Status": "Live on site", "Edit Answer": "Where can I learn about AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5635", "Source Link": "", "aisafety.info Link": "Where can I learn about AI alignment?", "Source": "Wiki", "All Phrasings": "Where can I learn about AI alignment?\n", "Initial Order": 8, "Related IDs": "6470,8159", "Rich Text DO NOT EDIT": "If you like interactive FAQs, you're in the right place already! Joking aside, some great entry points are the [AI alignment playlist](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLCRVRLd2RhZTpdUdEzJjo3qhmX3y3skWA&index=1) on YouTube, \u201c[The Road to Superintelligence](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)\u201d and \u201c[Our Immortality or Extinction](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html)\u201d posts on WaitBuyWhy for a fun, accessible introduction, and *Vox's* \u201c[The case for taking AI seriously as a threat to humanity](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment)\u201d as a high-quality mainstream explainer piece.\n\n[The AI Does Not Hate You: Superintelligence, Rationality, and the Race to Save the World](https://www.amazon.com/AI-Does-Not-Hate-You/dp/1474608779/ref=tmm_hrd_swatch_0?_encoding=UTF8&qid=&sr=) is a very readable book-length introduction to the technical challenge and the growing movement to tackle it; more book recommendations are in the followup question.\n\nThe free online [Cambridge course on AGI Safety Fundamentals](https://www.eacambridge.org/agi-safety-fundamentals) provides a strong grounding in much of the field and a cohort + mentor to learn with. There's even an [anki deck](https://www.ai-alignment-flashcards.com/) for people who like spaced repetition!\n\nThere are many resources in this post on [Levelling Up in AI Safety Research Engineering](https://forum.effectivealtruism.org/posts/S7dhJR5TDwPb5jypG/levelling-up-in-ai-safety-research-engineering) with a list of other guides at the bottom. There is also a [twitter thread](https://twitter.com/FreshMangoLassi/status/1575138148937498625) here with some programs for upskilling and some for safety-specific learning.\n\nThe [Alignment Newsletter](https://rohinshah.com/alignment-newsletter/) ([podcast](https://alignment-newsletter.libsyn.com/)), [Alignment Forum](https://www.alignmentforum.org/), and [AGI Control Problem Subreddit](https://www.reddit.com/r/ControlProblem/) are great for keeping up with latest developments.\n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "If you like interactive FAQs, you're in the right place already! Joking aside, some great entry points are the [AI alignment playlist](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLCRVRLd2RhZTpdUdEzJjo3qhmX3y3skWA&index=1) on YouTube, \u201c[The Road to Superintelligence](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)\u201d and \u201c[Our Immortality or Extinction](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html)\u201d posts on WaitBuyWhy for a fun, accessible introduction, and *Vox's* \u201c[The case for taking AI seriously as a threat to humanity](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment)\u201d as a high-quality mainstream explainer piece.\n\n[The AI Does Not Hate You: Superintelligence, Rationality, and the Race to Save the World](https://www.amazon.com/AI-Does-Not-Hate-You/dp/1474608779/ref=tmm_hrd_swatch_0?_encoding=UTF8&qid=&sr=) is a very readable book-length introduction to the technical challenge and the growing movement to tackle it; more book recommendations are in the followup question.\n\nThe free online [Cambridge course on AGI Safety Fundamentals](https://www.eacambridge.org/agi-safety-fundamentals) provides a strong grounding in much of the field and a cohort + mentor to learn with. There's even an [anki deck](https://www.ai-alignment-flashcards.com/) for people who like spaced repetition!\n\nThere are many resources in this post on [Levelling Up in AI Safety Research Engineering](https://forum.effectivealtruism.org/posts/S7dhJR5TDwPb5jypG/levelling-up-in-ai-safety-research-engineering) with a list of other guides at the bottom. There is also a [twitter thread](https://twitter.com/FreshMangoLassi/status/1575138148937498625) here with some programs for upskilling and some for safety-specific learning.\n\nThe [Alignment Newsletter](https://rohinshah.com/alignment-newsletter/) ([podcast](https://alignment-newsletter.libsyn.com/)), [Alignment Forum](https://www.alignmentforum.org/), and [AGI Control Problem Subreddit](https://www.reddit.com/r/ControlProblem/) are great for keeping up with latest developments.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Anonymous", "External Source": "", "Last Asked On Discord": "", "UI ID": "5635", "Related Answers": "What are some good resources on AI alignment?,What are some good books about AGI safety?", "Doc Last Ingested": "2023-03-14T23:26:22.231+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 18, "Helpful": ""}}, {"id": "i-2f6c6d2a101982bf62009f7ec2142336679ef95ada9030cd224a2b7e76be77a2", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2f6c6d2a101982bf62009f7ec2142336679ef95ada9030cd224a2b7e76be77a2", "name": "Where can I find questions to answer for Stampy?", "index": 278, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:31:33.385Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2f6c6d2a101982bf62009f7ec2142336679ef95ada9030cd224a2b7e76be77a2", "values": {"File": "Where can I find questions to answer for Stampy?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Where can I find questions to answer for Stampy?", "Link": "https://docs.google.com/document/d/1hAfmZOgAoFNnTTCViXDiO5hiLqdZaC4c9cCYdwModgs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:11.473+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stampy", "Doc Last Edited": "2023-02-22T23:03:50.230+01:00", "Status": "Live on site", "Edit Answer": "Where can I find questions to answer for Stampy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7505", "Source Link": "", "aisafety.info Link": "Where can I find questions to answer for Stampy?", "Source": "Wiki", "All Phrasings": "Where can I find questions to answer for Stampy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**[Answer questions](http://Answer_questions)** collects all the questions we definitely want answers to, browse there and see if you know how to answer any of them.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "**[Answer questions](http://Answer_questions)** collects all the questions we definitely want answers to, browse there and see if you know how to answer any of them.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "2023-03-08T16:03:08.788+01:00", "UI ID": "7505", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:23.779+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 18, "Helpful": ""}}, {"id": "i-302c05b05149065497436a934b89f2e38d5ddfdcc805a4af54dc91b5193d4345", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-302c05b05149065497436a934b89f2e38d5ddfdcc805a4af54dc91b5193d4345", "name": "Where can I find people to talk to about AI alignment?", "index": 279, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:32:38.227Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-302c05b05149065497436a934b89f2e38d5ddfdcc805a4af54dc91b5193d4345", "values": {"File": "Where can I find people to talk to about AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Where can I find people to talk to about AI alignment?", "Link": "https://docs.google.com/document/d/1WoLhXCZuQqLWSQyqICGVBss3DGqMQcg_VGi5UIObgqE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:07.194+01:00", "Related Answers DO NOT EDIT": "Why might contributing to Stampy be worth my time?", "Tags": "Collaboration", "Doc Last Edited": "2023-02-22T23:03:51.046+01:00", "Status": "Live on site", "Edit Answer": "Where can I find people to talk to about AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6708", "Source Link": "", "aisafety.info Link": "Where can I find people to talk to about AI alignment?", "Source": "Wiki", "All Phrasings": "Where can I find people to talk to about AI alignment?\n", "Initial Order": 2.5, "Related IDs": "7653", "Rich Text DO NOT EDIT": "[AI Safety Communities](http://aisafety.community/) has a comprehensive list. Some good specific suggestions include:\n\n- [AI Alignment Slack](https://ai-alignment.slack.com/join/shared_invite/zt-fkgwbd2b-kK50z~BbVclOZMM9UP44gw#/shared-invite/email)\n\n- A local student or meetup [LessWrong](https://www.lesswrong.com/community) or [Effective Altruism](https://forum.effectivealtruism.org/community) group (or [start one](https://www.effectivealtruism.org/groups)!)\n\n- [AGI Safety Fundamentals](https://www.eacambridge.org/agi-safety-fundamentals) which gives you a cohort to learn alongside and mentorship\n\n- [Rob Miles\u2019 Discord](https://discord.com/channels/677546901339504640)\n\n- [EleutherAI Discord](https://discord.com/invite/wz4MpRec4A) - ML hacker collective focused on alignment and open-source projects.\n\n- The relevant discussion threads on the *[Astral Codex Ten](https://astralcodexten.substack.com/)* [Substack](https://astralcodexten.substack.com/), which sometimes discusses alignment.\n\nOr book free calls with [AI Safety Support](https://www.aisafetysupport.org/).\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "[AI Safety Communities](http://aisafety.community/) has a comprehensive list. Some good specific suggestions include:\n\n- [AI Alignment Slack](https://ai-alignment.slack.com/join/shared_invite/zt-fkgwbd2b-kK50z~BbVclOZMM9UP44gw#/shared-invite/email)\n\n- A local student or meetup [LessWrong](https://www.lesswrong.com/community) or [Effective Altruism](https://forum.effectivealtruism.org/community) group (or [start one](https://www.effectivealtruism.org/groups)!)\n\n- [AGI Safety Fundamentals](https://www.eacambridge.org/agi-safety-fundamentals) which gives you a cohort to learn alongside and mentorship\n\n- [Rob Miles\u2019 Discord](https://discord.com/channels/677546901339504640)\n\n- [EleutherAI Discord](https://discord.com/invite/wz4MpRec4A) - ML hacker collective focused on alignment and open-source projects.\n\n- The relevant discussion threads on the *[Astral Codex Ten](https://astralcodexten.substack.com/)* [Substack](https://astralcodexten.substack.com/), which sometimes discusses alignment.\n\nOr book free calls with [AI Safety Support](https://www.aisafetysupport.org/).\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6708", "Related Answers": "Why might contributing to Stampy be worth my time?", "Doc Last Ingested": "2023-03-14T23:26:54.725+01:00", "Request Count": "", "Number of suggestions on answer doc": 73, "Total character count of suggestions on answer doc": 6619, "Helpful": ""}}, {"id": "i-684a08a22a1d7762bc2e349ddfa53ca08be92a27dc7c6dfcc206faea3229f2c6", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-684a08a22a1d7762bc2e349ddfa53ca08be92a27dc7c6dfcc206faea3229f2c6", "name": "Where can I find mentorship and advice for becoming a researcher?", "index": 280, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:32:40.847Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-684a08a22a1d7762bc2e349ddfa53ca08be92a27dc7c6dfcc206faea3229f2c6", "values": {"File": "Where can I find mentorship and advice for becoming a researcher?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Where can I find mentorship and advice for becoming a researcher?", "Link": "https://docs.google.com/document/d/1xcW5WJs7v-geI843o1520AX98FbXTcI2fU_AyzsRnbs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:53:03.283+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Contributing,Mentorship", "Doc Last Edited": "2023-02-22T23:03:51.970+01:00", "Status": "Live on site", "Edit Answer": "Where can I find mentorship and advice for becoming a researcher?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7651", "Source Link": "", "aisafety.info Link": "Where can I find mentorship and advice for becoming a researcher?", "Source": "Wiki", "All Phrasings": "Where can I find mentorship and advice for becoming a researcher?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "There are multiple programmes you can apply to if you want to try becoming a researcher. If accepted to these programs, you will get funding and mentorship. [aisafety.training](https://aisafety.training/) provides an up-to-date list of which programs have open applications, with an option to subscribe to updates. Some examples of these programs are: [SERI summer research fellowship](https://www.serisummerfellowship.org/application), [CERI summer research fellowship](https://www.cerifellowship.org), [SERI ML Alignment Theory Program](https://www.serimats.org), and more. A lot of these programs run during specific times of the year (specifically during the summer). Check [here](https://www.aisafety.careers/opportunities) for opportunities that are most appropriate for you.\n\nOther examples of things you can do are: join the next iteration of the [AGI Safety Fundamentals programme](https://www.eacambridge.org/technical-alignment-curriculum) or the [Redwood Research MLAB](https://airtable.com/shr07T428asvSxuHB), if you're thinking of a career as a researcher working on AI safety questions you can get 1-1 career advice from [AI Safety Support](https://www.aisafetysupport.org) or [80,000 Hours](https://80000hours.org/speak-with-us), you can apply to attend an EAGx or [EAG conference](https://www.eaglobal.org/events/) where you can meet in-person with researchers working on these questions so you can directly ask them for advice. If you are already a researcher in natural or social studies, consider applying to [PIBBS](https://www.pibbss.ai).\n\nOne way to find mentors is to simply write to known researchers with specific questions. Not everyone has the time to reply, but they can often point you in the right direction.\n\nSome of these resources might be helpful: [https://www.aisafetysupport.org/resources/lots-of-links](https://www.aisafetysupport.org/resources/lots-of-links)\n\n[https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment#Engaging_with_the_AI_alignment_community_will_help_you_a_lot](https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment#Engaging_with_the_AI_alignment_community_will_help_you_a_lot)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "There are multiple programmes you can apply to if you want to try becoming a researcher. If accepted to these programs, you will get funding and mentorship. [aisafety.training](https://aisafety.training/) provides an up-to-date list of which programs have open applications, with an option to subscribe to updates. Some examples of these programs are: [SERI summer research fellowship](https://www.serisummerfellowship.org/application), [CERI summer research fellowship](https://www.cerifellowship.org), [SERI ML Alignment Theory Program](https://www.serimats.org), and more. A lot of these programs run during specific times of the year (specifically during the summer). Check [here](https://www.aisafety.careers/opportunities) for opportunities that are most appropriate for you.\n\nOther examples of things you can do are: join the next iteration of the [AGI Safety Fundamentals programme](https://www.eacambridge.org/technical-alignment-curriculum) or the [Redwood Research MLAB](https://airtable.com/shr07T428asvSxuHB), if you're thinking of a career as a researcher working on AI safety questions you can get 1-1 career advice from [AI Safety Support](https://www.aisafetysupport.org) or [80,000 Hours](https://80000hours.org/speak-with-us), you can apply to attend an EAGx or [EAG conference](https://www.eaglobal.org/events/) where you can meet in-person with researchers working on these questions so you can directly ask them for advice. If you are already a researcher in natural or social studies, consider applying to [PIBBS](https://www.pibbss.ai).\n\nOne way to find mentors is to simply write to known researchers with specific questions. Not everyone has the time to reply, but they can often point you in the right direction.\n\nSome of these resources might be helpful: [https://www.aisafetysupport.org/resources/lots-of-links](https://www.aisafetysupport.org/resources/lots-of-links)\n\n[https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment#Engaging_with_the_AI_alignment_community_will_help_you_a_lot](https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment#Engaging_with_the_AI_alignment_community_will_help_you_a_lot)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7651", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:26:56.631+01:00", "Request Count": "", "Number of suggestions on answer doc": 73, "Total character count of suggestions on answer doc": 6619, "Helpful": ""}}, {"id": "i-6ed23a4451bcbd3afe82c75c9ad6bf1d235738522f59996d35158a135d4bb60c", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-6ed23a4451bcbd3afe82c75c9ad6bf1d235738522f59996d35158a135d4bb60c", "name": "When will transformative AI be created?", "index": 281, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:32:48.424Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-6ed23a4451bcbd3afe82c75c9ad6bf1d235738522f59996d35158a135d4bb60c", "values": {"File": "When will transformative AI be created?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "When will transformative AI be created?", "Link": "https://docs.google.com/document/d/1AwpcHPKDKfwVFF7aPVfas6v1c-S13ailhWtO-CnEoV4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:55.393+01:00", "Related Answers DO NOT EDIT": "What is \"transformative AI\"?", "Tags": "Timelines,Transformative AI", "Doc Last Edited": "2023-03-06T18:25:34.967+01:00", "Status": "Live on site", "Edit Answer": "When will transformative AI be created?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "2398", "Source Link": "", "aisafety.info Link": "When will transformative AI be created?", "Source": "Wiki", "All Phrasings": "When will transformative AI be created?\n", "Initial Order": "", "Related IDs": "6347", "Rich Text DO NOT EDIT": "As is often said, it's difficult to make predictions, especially about the future. This has not stopped many people thinking about when AI will transform the world, but all predictions should come with a warning that it's a hard domain to find anything like certainty.\n\n[This report](https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines) for the Open Philanthropy Project is perhaps the most careful attempt so far (and generates [these graphs](https://docs.google.com/spreadsheets/d/1TjNQyVHvHlC-sZbcA7CRKcCp0NxV6MkkqBvL408xrJw/edit#gid=505210495), which peak at 2042), and there's been much discussion including [this reply and analysis](https://www.alignmentforum.org/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute) which argues that welikely need less compute than the OpenPhil report expects.\n\nThere have also been [expert surveys](https://slatestarcodex.com/2017/06/08/ssc-journal-club-ai-timelines/), and many people have [shared various thoughts](https://www.lesswrong.com/tag/ai-timelines). Berkeley AI professor [Stuart Russell](https://en.wikipedia.org/wiki/Stuart_J._Russell) has given his best guess as \u201csometime in our children\u2019s lifetimes\u201d, and [Ray Kurzweil](https://en.wikipedia.org/wiki/Ray_Kurzweil) (Futurist and Google\u2019s director of engineering) predicts [human level AI by 2029 and the singularity by 2045](https://futurism.com/kurzweil-claims-that-the-singularity-will-happen-by-2045). The [Metaculus question on publicly known AGI](https://www.metaculus.com/questions/3479/when-will-the-first-artificial-general-intelligence-system-be-devised-tested-and-publicly-known-of/) has a median of around 2029 (around 10 years sooner than it was before the GPT-3 AI showed [unexpected ability on a broad range of tasks](https://gpt3examples.com/)).\n\nThe consensus answer, if there was one, might be something like: \u201chighly uncertain, maybe not for over a hundred years, maybe in less than 15, with around the middle of the century looking fairly plausible\u201d.\n\n", "Tag Count": 2, "Related Answer Count": 1, "Rich Text": "As is often said, it's difficult to make predictions, especially about the future. This has not stopped many people thinking about when AI will transform the world, but all predictions should come with a warning that it's a hard domain to find anything like certainty.\n\n[This report](https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines) for the Open Philanthropy Project is perhaps the most careful attempt so far (and generates [these graphs](https://docs.google.com/spreadsheets/d/1TjNQyVHvHlC-sZbcA7CRKcCp0NxV6MkkqBvL408xrJw/edit#gid=505210495), which peak at 2042), and there's been much discussion including [this reply and analysis](https://www.alignmentforum.org/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute) which argues that welikely need less compute than the OpenPhil report expects.\n\nThere have also been [expert surveys](https://slatestarcodex.com/2017/06/08/ssc-journal-club-ai-timelines/), and many people have [shared various thoughts](https://www.lesswrong.com/tag/ai-timelines). Berkeley AI professor [Stuart Russell](https://en.wikipedia.org/wiki/Stuart_J._Russell) has given his best guess as \u201csometime in our children\u2019s lifetimes\u201d, and [Ray Kurzweil](https://en.wikipedia.org/wiki/Ray_Kurzweil) (Futurist and Google\u2019s director of engineering) predicts [human level AI by 2029 and the singularity by 2045](https://futurism.com/kurzweil-claims-that-the-singularity-will-happen-by-2045). The [Metaculus question on publicly known AGI](https://www.metaculus.com/questions/3479/when-will-the-first-artificial-general-intelligence-system-be-devised-tested-and-publicly-known-of/) has a median of around 2029 (around 10 years sooner than it was before the GPT-3 AI showed [unexpected ability on a broad range of tasks](https://gpt3examples.com/)).\n\nThe consensus answer, if there was one, might be something like: \u201chighly uncertain, maybe not for over a hundred years, maybe in less than 15, with around the middle of the century looking fairly plausible\u201d.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "Jack Harley", "External Source": "", "Last Asked On Discord": "", "UI ID": "2398", "Related Answers": "What is \"transformative AI\"?", "Doc Last Ingested": "2023-03-14T23:26:58.729+01:00", "Request Count": "", "Number of suggestions on answer doc": 82, "Total character count of suggestions on answer doc": 6673, "Helpful": ""}}, {"id": "i-52fd5047a76473e94e3085af3144ff86568d42dec9167f6751e5ee88d6096337", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-52fd5047a76473e94e3085af3144ff86568d42dec9167f6751e5ee88d6096337", "name": "When will an intelligence explosion happen?", "index": 282, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:32:51.447Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-52fd5047a76473e94e3085af3144ff86568d42dec9167f6751e5ee88d6096337", "values": {"File": "When will an intelligence explosion happen?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "When will an intelligence explosion happen?", "Link": "https://docs.google.com/document/d/1mHosCYGxV0KjHo6v90fpEyni_n8-gP3gf0vJlLLb8NQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:51.836+01:00", "Related Answers DO NOT EDIT": "How likely is an \"intelligence explosion\"?", "Tags": "Timelines,Intelligence Explosion", "Doc Last Edited": "2023-03-12T22:32:11.979+01:00", "Status": "Live on site", "Edit Answer": "When will an intelligence explosion happen?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6599", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "When will an intelligence explosion happen?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "When will an intelligence explosion happen?\n", "Initial Order": "", "Related IDs": "6586", "Rich Text DO NOT EDIT": "Predicting the future is risky business. There are many philosophical, scientific, technological, and social uncertainties relevant to the arrival of an intelligence explosion. Because of this, experts disagree on when this event might occur. Here are some of their predictions:\n\n- Futurist Ray Kurzweil [predicts](http://www.amazon.com/dp/0143037889/) that machines will reach human-level intelligence by 2030 and that we will reach \u201ca profound and disruptive transformation in human capability\u201d by 2045.\n\n- Intel\u2019s chief technology officer, Justin Rattner, [expects](http://www.techwatch.co.uk/2008/08/22/intel-predicts-singularity-by-2048/) \u201ca point when human and artificial intelligence merges to create something bigger than itself\u201d by 2048.\n\n- AI researcher Eliezer Yudkowsky [expects](http://commonsenseatheism.com/?p=12147) the intelligence explosion by 2060.\n\n- Philosopher David Chalmers has [over 1/2 credence](http://consc.net/papers/singularity.pdf) in the intelligence explosion occurring by 2100.\n\n- Quantum computing expert Michael Nielsen [estimates](http://michaelnielsen.org/blog/what-should-a-reasonable-person-believe-about-the-singularity/) that the probability of the intelligence explosion occurring by 2100 is between 0.2% and about 70%.\n\n- In 2009, at the AGI-09 conference, experts were asked when AI might reach superintelligence with massive new funding. The [median estimates](http://sethbaum.com/ac/2011_AI-Experts.pdf) were that machine superintelligence could be achieved by 2045 (with 50% confidence) or by 2100 (with 90% confidence). Of course, attendees to this conference were self-selected to think that near-term artificial general intelligence is plausible.\n\n- iRobot CEO [Rodney Brooks](http://itc.conversationsnetwork.org/shows/detail3400.html) and cognitive scientist [Douglas Hofstadter](http://video.google.com/videoplay?docid=8832143373632003914) allow that the intelligence explosion may occur in the future, but probably not in the 21st century.\n\n- Roboticist Hans Moravec predicts that AI will surpass human intelligence \u201c[well before 2050](http://www.scientificamerican.com/article.cfm?id=rise-of-the-robots&print=true).\u201d\n\n- In a 2005 survey of 26 contributors to a series of reports on emerging technologies, the [median estimate](http://www.wtec.org/ConvergingTechnologies/3/NBIC3_report.pdf) for machines reaching human-level intelligence was 2085.\n\n- Participants in a 2011 intelligence conference at Oxford gave a [median estimate](http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0015/21516/MI_survey.pdf) of 2050 for when there will be a 50% of human-level machine intelligence, and a median estimate of 2150 for when there will be a 90% chance of human-level machine intelligence.\n\n- On the other hand, 41% of the participants in the AI@50 conference (in 2006) [stated](http://www.engagingexperience.com/ai50/) that machine intelligence would never reach the human level.\n\nSee also:\n\n- Baum, Goertzel, & Goertzel, [Long Until Human-Level AI? Results from an Expert Assessment](http://sethbaum.com/ac/2011_AI-Experts.pdfHow)\n\n", "Tag Count": 2, "Related Answer Count": 1, "Rich Text": "Predicting the future is risky business. There are many philosophical, scientific, technological, and social uncertainties relevant to the arrival of an intelligence explosion. Because of this, experts disagree on when this event might occur. Here are some of their predictions:\n\n- Futurist Ray Kurzweil [predicts](http://www.amazon.com/dp/0143037889/) that machines will reach human-level intelligence by 2030 and that we will reach \u201ca profound and disruptive transformation in human capability\u201d by 2045.\n\n- Intel\u2019s chief technology officer, Justin Rattner, [expects](http://www.techwatch.co.uk/2008/08/22/intel-predicts-singularity-by-2048/) \u201ca point when human and artificial intelligence merges to create something bigger than itself\u201d by 2048.\n\n- AI researcher Eliezer Yudkowsky [expects](http://commonsenseatheism.com/?p=12147) the intelligence explosion by 2060.\n\n- Philosopher David Chalmers has [over 1/2 credence](http://consc.net/papers/singularity.pdf) in the intelligence explosion occurring by 2100.\n\n- Quantum computing expert Michael Nielsen [estimates](http://michaelnielsen.org/blog/what-should-a-reasonable-person-believe-about-the-singularity/) that the probability of the intelligence explosion occurring by 2100 is between 0.2% and about 70%.\n\n- In 2009, at the AGI-09 conference, experts were asked when AI might reach superintelligence with massive new funding. The [median estimates](http://sethbaum.com/ac/2011_AI-Experts.pdf) were that machine superintelligence could be achieved by 2045 (with 50% confidence) or by 2100 (with 90% confidence). Of course, attendees to this conference were self-selected to think that near-term artificial general intelligence is plausible.\n\n- iRobot CEO [Rodney Brooks](http://itc.conversationsnetwork.org/shows/detail3400.html) and cognitive scientist [Douglas Hofstadter](http://video.google.com/videoplay?docid=8832143373632003914) allow that the intelligence explosion may occur in the future, but probably not in the 21st century.\n\n- Roboticist Hans Moravec predicts that AI will surpass human intelligence \u201c[well before 2050](http://www.scientificamerican.com/article.cfm?id=rise-of-the-robots&print=true).\u201d\n\n- In a 2005 survey of 26 contributors to a series of reports on emerging technologies, the [median estimate](http://www.wtec.org/ConvergingTechnologies/3/NBIC3_report.pdf) for machines reaching human-level intelligence was 2085.\n\n- Participants in a 2011 intelligence conference at Oxford gave a [median estimate](http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0015/21516/MI_survey.pdf) of 2050 for when there will be a 50% of human-level machine intelligence, and a median estimate of 2150 for when there will be a 90% chance of human-level machine intelligence.\n\n- On the other hand, 41% of the participants in the AI@50 conference (in 2006) [stated](http://www.engagingexperience.com/ai50/) that machine intelligence would never reach the human level.\n\nSee also:\n\n- Baum, Goertzel, & Goertzel, [Long Until Human-Level AI? Results from an Expert Assessment](http://sethbaum.com/ac/2011_AI-Experts.pdfHow)\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 3, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "", "UI ID": "6599", "Related Answers": "How likely is an \"intelligence explosion\"?", "Doc Last Ingested": "2023-03-14T23:27:00.765+01:00", "Request Count": "", "Number of suggestions on answer doc": 82, "Total character count of suggestions on answer doc": 6673, "Helpful": ""}}, {"id": "i-a9e5bd9c78e47d9cd77c633b952b84afaf5865c73578c63797a2a4febb54b4ce", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-a9e5bd9c78e47d9cd77c633b952b84afaf5865c73578c63797a2a4febb54b4ce", "name": "What\u2019s a good AI alignment elevator pitch?", "index": 283, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:32:56.500Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-a9e5bd9c78e47d9cd77c633b952b84afaf5865c73578c63797a2a4febb54b4ce", "values": {"File": "What\u2019s a good AI alignment elevator pitch?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What\u2019s a good AI alignment elevator pitch?", "Link": "https://docs.google.com/document/d/1K4k0newAPltB0P1aRghYcbBc4FZHi7tKBn4cCTxMZvM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:43.703+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Communication", "Doc Last Edited": "2023-02-22T22:46:28.744+01:00", "Status": "In progress", "Edit Answer": "What\u2019s a good AI alignment elevator pitch?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7677", "Source Link": "", "aisafety.info Link": "What\u2019s a good AI alignment elevator pitch?", "Source": "Wiki", "All Phrasings": "What\u2019s a good AI alignment elevator pitch?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Notes:\n\n- Recommended by Larks as a good intro point [https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment)\n\n- Pitching alignment : values, orthogonality, instrumental convergence\n\n    - Focus will be different depending on the thing you want to explain\n\n- There are many good elevator pitches, depending on the person\n\n- Software systems, rather than \u201cAI\u201d, sounds less sci-fi\n\n- Going do develop faster than out ability to regulate/intervene, the point isn\u2019t how near this is, but how quick it\u2019s going to happen (we\u2019ll be slower than this)\n\n- [https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread?commentId=2JiMsmu32EvzKv4yP](https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread?commentId=2JiMsmu32EvzKv4yP) ???\n\n    - More effective if it\u2019s more vague???\n\n- Don\u2019t need to lean on intelligence explosion \u2013 rob\n\n- \u201cThere is no singular elevator pitch\u201d \u2013 rob\n\n    - Imo(TimeGoat), There is a GTO(game theory optimal) pitch, that you would deviate from depending on who you\u2019re talking to.\n\n        - We probably can\u2019t know what this GTO pitch is perfectly, but we can probably get close.\n\n        - Because we can\u2019t be perfect, essentially there would be a handful of near GTO pitches.\n\n- Avoid introducing terminology that you don\u2019t use again.\n\nLink [https://forum.effectivealtruism.org/posts/GvHPnzGJQJ7iAiJNr/on-presenting-the-case-for-](https://forum.effectivealtruism.org/posts/GvHPnzGJQJ7iAiJNr/on-presenting-the-case-for-)\n\n[https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread?commentId=2JiMsmu32EvzKv4yP](https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread?commentId=2JiMsmu32EvzKv4yP)\n\nThe future of the world will be dominated by these systems. We control the world because we're the most capable and coordinated entities on the planet.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Notes:\n\n- Recommended by Larks as a good intro point [https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment)\n\n- Pitching alignment : values, orthogonality, instrumental convergence\n\n    - Focus will be different depending on the thing you want to explain\n\n- There are many good elevator pitches, depending on the person\n\n- Software systems, rather than \u201cAI\u201d, sounds less sci-fi\n\n- Going do develop faster than out ability to regulate/intervene, the point isn\u2019t how near this is, but how quick it\u2019s going to happen (we\u2019ll be slower than this)\n\n- [https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread?commentId=2JiMsmu32EvzKv4yP](https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread?commentId=2JiMsmu32EvzKv4yP) ???\n\n    - More effective if it\u2019s more vague???\n\n- Don\u2019t need to lean on intelligence explosion \u2013 rob\n\n- \u201cThere is no singular elevator pitch\u201d \u2013 rob\n\n    - Imo(TimeGoat), There is a GTO(game theory optimal) pitch, that you would deviate from depending on who you\u2019re talking to.\n\n        - We probably can\u2019t know what this GTO pitch is perfectly, but we can probably get close.\n\n        - Because we can\u2019t be perfect, essentially there would be a handful of near GTO pitches.\n\n- Avoid introducing terminology that you don\u2019t use again.\n\nLink [https://forum.effectivealtruism.org/posts/GvHPnzGJQJ7iAiJNr/on-presenting-the-case-for-](https://forum.effectivealtruism.org/posts/GvHPnzGJQJ7iAiJNr/on-presenting-the-case-for-)\n\n[https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread?commentId=2JiMsmu32EvzKv4yP](https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread?commentId=2JiMsmu32EvzKv4yP)\n\nThe future of the world will be dominated by these systems. We control the world because we're the most capable and coordinated entities on the planet.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7677", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:27:02.022+01:00", "Request Count": "", "Number of suggestions on answer doc": 83, "Total character count of suggestions on answer doc": 7960, "Helpful": ""}}, {"id": "i-fb585972489f6a66929163f85beda60c278d9873aac5ad4f2af3f634995395ac", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-fb585972489f6a66929163f85beda60c278d9873aac5ad4f2af3f634995395ac", "name": "What's meant by calling an AI \"agenty\" or \"agentlike\"?", "index": 284, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:33:01.066Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-fb585972489f6a66929163f85beda60c278d9873aac5ad4f2af3f634995395ac", "values": {"File": "What's meant by calling an AI \"agenty\" or \"agentlike\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What's meant by calling an AI \"agenty\" or \"agentlike\"?", "Link": "https://docs.google.com/document/d/19FMgdcKOAYFGtZ1vVEvot1r9gv-4R_WZQMwCuNgSz9k/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:39.221+01:00", "Related Answers DO NOT EDIT": "What is an \"agent\"?,What is instrumental convergence?", "Tags": "Agency", "Doc Last Edited": "2023-03-13T15:54:31.260+01:00", "Status": "In progress", "Edit Answer": "What's meant by calling an AI \"agenty\" or \"agentlike\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5632", "Source Link": "", "aisafety.info Link": "What's meant by calling an AI \"agenty\" or \"agentlike\"?", "Source": "Wiki", "All Phrasings": "What's meant by calling an AI \"agenty\" or \"agentlike\"?\n", "Initial Order": "", "Related IDs": "7595,897I", "Rich Text DO NOT EDIT": "\n\nThe Spotify recommendation algorithm won\u2019t take over the world. *Why?*\n\nIn short, the Spotify recommendation algorithm is narrow. While the algorithm \u2018tries\u2019 to recommend music you\u2019ll enjoy, it doesn\u2019t have autonomous goals in the same way humans do. Thus, we\u2019re primarily worried about *agentic* AI systems, who pursue long-term goals in highly capable ways.\n\nA 2019 [paper](https://arxiv.org/pdf/1805.12387.pdf) involving two authors at DeepMind offers a formalism for distinguishing between *agents* and *devices*. A more conceptual attempt to understand [by Richard Ngo](https://www.lesswrong.com/posts/bz5GdmCWj8o48726N/agi-safety-from-first-principles-goals-and-agency) agency lists the following six criteria for agency:\n\n1. Self-awareness: it understands that it\u2019s a part of the world, and that its behavior impacts the world;\n\n1. Planning: it considers a wide range of possible sequences of behaviors (let\u2019s call them \u201cplans\u201d), including long plans;\n\n1. Consequentialism: it decides which of those plans is best by considering the value of the outcomes that they produce;\n\n1. Scale: its choice is sensitive to the effects of plans over large distances and long time horizons;\n\n1. Coherence: it is internally unified towards implementing the single plan it judges to be best;\n\n1. Flexibility: it is able to adapt its plans flexibly as circumstances change, rather than just continuing the same patterns of behavior.\n\nWe are concerned with the behavior of agentic AGIs, as agentic systems carry a plethora of specific risks, as there appear to be a variety of instrumental goals that it\u2019s useful for a wide class of agents to have. For instance, it seems like increasing resources is something we\u2019d expect any goal-directed system to desire. No matter what your goals, resource accumulation appears useful. As humans, we acquire money and power, not because we (necessarily) care about money and power themselves, but because money and power allow us to achieve more of what we care about.\n\nOf course, we might wonder: \u201cwill we develop agentic AGI?\u201d, and \u201cif we develop agentic AGI, when is it likely to arrive?\u201d.  Other documents examine those questions. But we\u2019ll note that there\u2019s reason to worry: some, like Yudkowsky, believe that agency is likely to [emerge by default](https://astralcodexten.substack.com/p/practically-a-book-review-yudkowsky) as AI capabilities increase. Still others believe that, even if building AGI is difficult, there will be [economic incentives](https://arxiv.org/pdf/2206.13353.pdf#page=11) to overcome this hurdle.\n\nWe might think that future AIs would stop acquiring resources beyond some modest level, but (unfortunately) this assumption [may well be unwarranted](https://nickbostrom.com/superintelligentwill.pdf). The value of resources depends on the potential uses to which they can be put, which in turn depends on the available technology. As technology advances, time, space, and matter could be processed to serve almost any goal. If AGIs have goals which are misaligned with human goals, things could look pretty scary. Just as chimpanzees would have reason to worry about advancing human capabilities, so we have reason to worry about advancing AI capabilities.\n\n", "Tag Count": 1, "Related Answer Count": 2, "Rich Text": "\n\nThe Spotify recommendation algorithm won\u2019t take over the world. *Why?*\n\nIn short, the Spotify recommendation algorithm is narrow. While the algorithm \u2018tries\u2019 to recommend music you\u2019ll enjoy, it doesn\u2019t have autonomous goals in the same way humans do. Thus, we\u2019re primarily worried about *agentic* AI systems, who pursue long-term goals in highly capable ways.\n\nA 2019 [paper](https://arxiv.org/pdf/1805.12387.pdf) involving two authors at DeepMind offers a formalism for distinguishing between *agents* and *devices*. A more conceptual attempt to understand [by Richard Ngo](https://www.lesswrong.com/posts/bz5GdmCWj8o48726N/agi-safety-from-first-principles-goals-and-agency) agency lists the following six criteria for agency:\n\n1. Self-awareness: it understands that it\u2019s a part of the world, and that its behavior impacts the world;\n\n1. Planning: it considers a wide range of possible sequences of behaviors (let\u2019s call them \u201cplans\u201d), including long plans;\n\n1. Consequentialism: it decides which of those plans is best by considering the value of the outcomes that they produce;\n\n1. Scale: its choice is sensitive to the effects of plans over large distances and long time horizons;\n\n1. Coherence: it is internally unified towards implementing the single plan it judges to be best;\n\n1. Flexibility: it is able to adapt its plans flexibly as circumstances change, rather than just continuing the same patterns of behavior.\n\nWe are concerned with the behavior of agentic AGIs, as agentic systems carry a plethora of specific risks, as there appear to be a variety of instrumental goals that it\u2019s useful for a wide class of agents to have. For instance, it seems like increasing resources is something we\u2019d expect any goal-directed system to desire. No matter what your goals, resource accumulation appears useful. As humans, we acquire money and power, not because we (necessarily) care about money and power themselves, but because money and power allow us to achieve more of what we care about.\n\nOf course, we might wonder: \u201cwill we develop agentic AGI?\u201d, and \u201cif we develop agentic AGI, when is it likely to arrive?\u201d.  Other documents examine those questions. But we\u2019ll note that there\u2019s reason to worry: some, like Yudkowsky, believe that agency is likely to [emerge by default](https://astralcodexten.substack.com/p/practically-a-book-review-yudkowsky) as AI capabilities increase. Still others believe that, even if building AGI is difficult, there will be [economic incentives](https://arxiv.org/pdf/2206.13353.pdf#page=11) to overcome this hurdle.\n\nWe might think that future AIs would stop acquiring resources beyond some modest level, but (unfortunately) this assumption [may well be unwarranted](https://nickbostrom.com/superintelligentwill.pdf). The value of resources depends on the potential uses to which they can be put, which in turn depends on the available technology. As technology advances, time, space, and matter could be processed to serve almost any goal. If AGIs have goals which are misaligned with human goals, things could look pretty scary. Just as chimpanzees would have reason to worry about advancing human capabilities, so we have reason to worry about advancing AI capabilities.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "5632", "Related Answers": "What is an \"agent\"?,What is instrumental convergence?", "Doc Last Ingested": "2023-03-14T23:27:03.952+01:00", "Request Count": "", "Number of suggestions on answer doc": 84, "Total character count of suggestions on answer doc": 8592, "Helpful": ""}}, {"id": "i-6ebb210ab7715463f90459826461b2564afa7132c765afab047f1cff0366ec18", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-6ebb210ab7715463f90459826461b2564afa7132c765afab047f1cff0366ec18", "name": "What's especially worrisome about autonomous weapons?", "index": 285, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:33:03.379Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-6ebb210ab7715463f90459826461b2564afa7132c765afab047f1cff0366ec18", "values": {"File": "What's especially worrisome about autonomous weapons?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What's especially worrisome about autonomous weapons?", "Link": "https://docs.google.com/document/d/1ZQETI9P7BruUowkefkZmfnEgn8ssX1VoloepOAjW6TQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:35.117+01:00", "Related Answers DO NOT EDIT": "Aren't robots the real problem? How can AI cause harm if it has no ability to directly manipulate the physical world?", "Tags": "Autonomous Weapons", "Doc Last Edited": "2023-02-22T23:03:55.004+01:00", "Status": "Live on site", "Edit Answer": "What's especially worrisome about autonomous weapons?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5647", "Source Link": "", "aisafety.info Link": "What's especially worrisome about autonomous weapons?", "Source": "Wiki", "All Phrasings": "What's especially worrisome about autonomous weapons?\n", "Initial Order": "", "Related IDs": "6196", "Rich Text DO NOT EDIT": "The problem of autonomous weapons is not directly related to the AI Safety problem, but both fit into the \"be careful what you do with AI\" category.\n\nIn the short term, these would allow for worse totalitarianism as automated security forces will never rebel. This removes the moderating influence of human personnel as convincing machines to do a horrible thing is easier than convincing humans. Despots need security forces to remain in power. Human security forces betraying a despot is a common way that despots lose power, this would not happen with robots.\n\nAnother consideration is that[computer security is hard](https://xkcd.com/2030/)! Autonomous weapons could be hacked, initially by humans but eventually by an AGI. This is not good for humanity's chances of surviving the transition to AGI, although access to autonomous weapons is probably not necessary for this transition to go poorly.\n\nSee also[Stop Killer Robots](https://www.stopkillerrobots.org/).\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "The problem of autonomous weapons is not directly related to the AI Safety problem, but both fit into the \"be careful what you do with AI\" category.\n\nIn the short term, these would allow for worse totalitarianism as automated security forces will never rebel. This removes the moderating influence of human personnel as convincing machines to do a horrible thing is easier than convincing humans. Despots need security forces to remain in power. Human security forces betraying a despot is a common way that despots lose power, this would not happen with robots.\n\nAnother consideration is that[computer security is hard](https://xkcd.com/2030/)! Autonomous weapons could be hacked, initially by humans but eventually by an AGI. This is not good for humanity's chances of surviving the transition to AGI, although access to autonomous weapons is probably not necessary for this transition to go poorly.\n\nSee also[Stop Killer Robots](https://www.stopkillerrobots.org/).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Chris Cooper", "External Source": "", "Last Asked On Discord": "", "UI ID": "5647", "Related Answers": "Aren't robots the real problem? How can AI cause harm if it has no ability to directly manipulate the physical world?", "Doc Last Ingested": "2023-03-14T23:27:05.606+01:00", "Request Count": "", "Number of suggestions on answer doc": 84, "Total character count of suggestions on answer doc": 8592, "Helpful": ""}}, {"id": "i-76d5d9cc32e6a6950e851e9f7756596608768c53f9901b672887a52dc93c448e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-76d5d9cc32e6a6950e851e9f7756596608768c53f9901b672887a52dc93c448e", "name": "What would be physically possible and desirable to have in an AI-built utopia?", "index": 286, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:33:05.906Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-76d5d9cc32e6a6950e851e9f7756596608768c53f9901b672887a52dc93c448e", "values": {"File": "What would be physically possible and desirable to have in an AI-built utopia?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What would be physically possible and desirable to have in an AI-built utopia?", "Link": "https://docs.google.com/document/d/1zUqGrCbRqCxHAQUsvtmaNrOJOV68CH-HYO26CFqssM4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:31.281+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:46:31.110+01:00", "Status": "Not started", "Edit Answer": "What would be physically possible and desirable to have in an AI-built utopia?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7756", "Source Link": "", "aisafety.info Link": "What would be physically possible and desirable to have in an AI-built utopia?", "Source": "Wiki", "All Phrasings": "What would be physically possible and desirable to have in an AI-built utopia?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7756", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:27:07.494+01:00", "Request Count": "", "Number of suggestions on answer doc": 84, "Total character count of suggestions on answer doc": 8592, "Helpful": ""}}, {"id": "i-f229b3f121ac88303af18bbe6b7aaed6a0eb0d5f1f5bc53052fc71e8bdc1a278", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-f229b3f121ac88303af18bbe6b7aaed6a0eb0d5f1f5bc53052fc71e8bdc1a278", "name": "What would a world shortly before AGI look like?", "index": 287, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:33:12.218Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-f229b3f121ac88303af18bbe6b7aaed6a0eb0d5f1f5bc53052fc71e8bdc1a278", "values": {"File": "What would a world shortly before AGI look like?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What would a world shortly before AGI look like?", "Link": "https://docs.google.com/document/d/1rW7UIy3mkh-xy8xQYi9CXvTGGTqE8GANxMn4WjDU6A8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:27.569+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI Fire Alarm", "Doc Last Edited": "2023-02-22T22:46:32.305+01:00", "Status": "Not started", "Edit Answer": "What would a world shortly before AGI look like?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7622", "Source Link": "", "aisafety.info Link": "What would a world shortly before AGI look like?", "Source": "Wiki", "All Phrasings": "What would a world shortly before AGI look like?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7622", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:27:09.222+01:00", "Request Count": "", "Number of suggestions on answer doc": 84, "Total character count of suggestions on answer doc": 8592, "Helpful": ""}}, {"id": "i-d06c9606c68aaa5f260fd71b443d8c68de2aa275089809b4a118e353a64af03d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d06c9606c68aaa5f260fd71b443d8c68de2aa275089809b4a118e353a64af03d", "name": "What would a good solution to AI alignment look like?", "index": 288, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:33:18.871Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d06c9606c68aaa5f260fd71b443d8c68de2aa275089809b4a118e353a64af03d", "values": {"File": "What would a good solution to AI alignment look like?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What would a good solution to AI alignment look like?", "Link": "https://docs.google.com/document/d/1HSMURy9Pl2svUQYdgUyIVfwwTcZRfWhzJLk1QUGu73E/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:23.244+01:00", "Related Answers DO NOT EDIT": "At a high level, what is the challenge of alignment that we must meet to secure a good future?", "Tags": "Control Problem,Solutions", "Doc Last Edited": "2023-02-22T23:03:56.395+01:00", "Status": "Live on site", "Edit Answer": "What would a good solution to AI alignment look like?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7058", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "What would a good solution to AI alignment look like?", "Source": "Superintelligence FAQ", "All Phrasings": "What would a good solution to AI alignment look like?\n", "Initial Order": "", "Related IDs": "7060", "Rich Text DO NOT EDIT": "An actually good solution to AI alignment might look like a superintelligence that understands, agrees with, and deeply believes in human morality.\n\nYou wouldn\u2019t have to command a superintelligence like this to cure cancer; it would already want to cure cancer, for the same reasons you do. But it would also be able to compare the costs and benefits of curing cancer with those of other uses of its time, like solving global warming or discovering new physics. It wouldn\u2019t have any urge to cure cancer by nuking the world, for the same reason you don\u2019t have any urge to cure cancer by nuking the world \u2013 because your goal isn\u2019t to \u201ccure cancer\u201d, per se, it\u2019s to improve the lives of people everywhere. Curing cancer the normal way accomplishes that; nuking the world doesn\u2019t. This sort of solution would mean we\u2019re no longer fighting against the AI \u2013 trying to come up with rules so smart that it couldn\u2019t find loopholes. We would be on the same side, both wanting the same thing.\n\nIt would also mean that the CEO of Google (or the head of the US military, or Vladimir Putin) couldn\u2019t use the AI to take over the world for themselves. The AI would have its own values and be able to agree or disagree with anybody, including its creators.\n\nIt might not make sense to talk about \u201ccommanding\u201d such an AI. After all, any command would have to go through its moral system. Certainly it would reject a command to nuke the world. But it might also reject a command to cure cancer, if it thought that solving global warming was a higher priority. For that matter, why would one want to command this AI? It values the same things you value, but it\u2019s much smarter than you and much better at figuring out how to achieve them. Just turn it on and let it do its thing.\n\nWe could still treat this AI as having an open-ended maximizing goal. The goal would be something like \u201cTry to make the world a better place according to the values and wishes of the people in it.\u201d\n\nThe only problem with this is that human morality is very complicated, so much so that philosophers have been arguing about it for thousands of years without much progress, let alone anything specific enough to enter into a computer. Different cultures and individuals have different moral codes, such that a superintelligence following the morality of the King of Saudi Arabia might not be acceptable to the average American, and vice versa.\n\nOne solution might be to give the AI an understanding of what we mean by morality \u2013 \u201cthat thing that makes intuitive sense to humans but is hard to explain\u201d, and then ask it to use its superintelligence to fill in the details. Needless to say, this suffers from various problems \u2013 it has potential loopholes, it\u2019s hard to code, and a single bug might be disastrous \u2013 but if it worked, it would be one of the few genuinely satisfying ways to design a goal architecture.\n\n", "Tag Count": 2, "Related Answer Count": 1, "Rich Text": "An actually good solution to AI alignment might look like a superintelligence that understands, agrees with, and deeply believes in human morality.\n\nYou wouldn\u2019t have to command a superintelligence like this to cure cancer; it would already want to cure cancer, for the same reasons you do. But it would also be able to compare the costs and benefits of curing cancer with those of other uses of its time, like solving global warming or discovering new physics. It wouldn\u2019t have any urge to cure cancer by nuking the world, for the same reason you don\u2019t have any urge to cure cancer by nuking the world \u2013 because your goal isn\u2019t to \u201ccure cancer\u201d, per se, it\u2019s to improve the lives of people everywhere. Curing cancer the normal way accomplishes that; nuking the world doesn\u2019t. This sort of solution would mean we\u2019re no longer fighting against the AI \u2013 trying to come up with rules so smart that it couldn\u2019t find loopholes. We would be on the same side, both wanting the same thing.\n\nIt would also mean that the CEO of Google (or the head of the US military, or Vladimir Putin) couldn\u2019t use the AI to take over the world for themselves. The AI would have its own values and be able to agree or disagree with anybody, including its creators.\n\nIt might not make sense to talk about \u201ccommanding\u201d such an AI. After all, any command would have to go through its moral system. Certainly it would reject a command to nuke the world. But it might also reject a command to cure cancer, if it thought that solving global warming was a higher priority. For that matter, why would one want to command this AI? It values the same things you value, but it\u2019s much smarter than you and much better at figuring out how to achieve them. Just turn it on and let it do its thing.\n\nWe could still treat this AI as having an open-ended maximizing goal. The goal would be something like \u201cTry to make the world a better place according to the values and wishes of the people in it.\u201d\n\nThe only problem with this is that human morality is very complicated, so much so that philosophers have been arguing about it for thousands of years without much progress, let alone anything specific enough to enter into a computer. Different cultures and individuals have different moral codes, such that a superintelligence following the morality of the King of Saudi Arabia might not be acceptable to the average American, and vice versa.\n\nOne solution might be to give the AI an understanding of what we mean by morality \u2013 \u201cthat thing that makes intuitive sense to humans but is hard to explain\u201d, and then ask it to use its superintelligence to fill in the details. Needless to say, this suffers from various problems \u2013 it has potential loopholes, it\u2019s hard to code, and a single bug might be disastrous \u2013 but if it worked, it would be one of the few genuinely satisfying ways to design a goal architecture.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "", "UI ID": "7058", "Related Answers": "At a high level, what is the challenge of alignment that we must meet to secure a good future?", "Doc Last Ingested": "2023-03-14T23:27:10.667+01:00", "Request Count": "", "Number of suggestions on answer doc": 84, "Total character count of suggestions on answer doc": 8592, "Helpful": ""}}, {"id": "i-d9d1193de72fba1e02164a84d2bcc3f72e9a091f3a86284b3a9f9ec2d81a8061", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d9d1193de72fba1e02164a84d2bcc3f72e9a091f3a86284b3a9f9ec2d81a8061", "name": "What would a good future with AGI look like?", "index": 289, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:33:25.311Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d9d1193de72fba1e02164a84d2bcc3f72e9a091f3a86284b3a9f9ec2d81a8061", "values": {"File": "What would a good future with AGI look like?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What would a good future with AGI look like?", "Link": "https://docs.google.com/document/d/1A95fCyB4ajkeMj19FKQNyuZMV0sv3yUK7hSub7XeLlA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:19.552+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Eutopia", "Doc Last Edited": "2023-02-22T23:03:57.398+01:00", "Status": "Live on site", "Edit Answer": "What would a good future with AGI look like?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7766", "Source Link": "", "aisafety.info Link": "What would a good future with AGI look like?", "Source": "Wiki", "All Phrasings": "What would a good future with AGI look like?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "As technology continues to improve, one thing is certain: the future is going to look like science fiction. Doubly so once superhuman AI (\"[AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence)\") is invented, because we can expect the AGI to produce technological improvements at a superhuman rate, eventually approaching the physical limits in terms of how small machines can be miniaturized, how fast they can compute, how energy-efficient they can be, etc.\n\nToday's world is lacking in many ways, so given these increasingly powerful tools, it seems likely that whoever controls those tools will use them to make increasingly large (and increasingly sci-fi-sounding) improvements to the world. If (and that's a big if!) humanity retains control of the AGI, we could use these amazing technologies to stop climate change, colonize other planets, solve world hunger, cure cancer and every other disease, even eliminate aging and death.\n\nFor more inspiration, here are some stories painting what a bright, AGI-powered future could look like:\n\n- The winners of the [FHI Worldbuilding contest](https://worldbuild.ai/)\n\n- [Stuart Armstrong's short story \"The Adventure\"](https://www.lesswrong.com/posts/Ybp6Wg6yy9DWRcBiR/the-adventure-a-new-utopia-story)\n\n- [Iain M. Banks's Culture novels](https://en.wikipedia.org/wiki/Culture_series)\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "As technology continues to improve, one thing is certain: the future is going to look like science fiction. Doubly so once superhuman AI (\"[AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence)\") is invented, because we can expect the AGI to produce technological improvements at a superhuman rate, eventually approaching the physical limits in terms of how small machines can be miniaturized, how fast they can compute, how energy-efficient they can be, etc.\n\nToday's world is lacking in many ways, so given these increasingly powerful tools, it seems likely that whoever controls those tools will use them to make increasingly large (and increasingly sci-fi-sounding) improvements to the world. If (and that's a big if!) humanity retains control of the AGI, we could use these amazing technologies to stop climate change, colonize other planets, solve world hunger, cure cancer and every other disease, even eliminate aging and death.\n\nFor more inspiration, here are some stories painting what a bright, AGI-powered future could look like:\n\n- The winners of the [FHI Worldbuilding contest](https://worldbuild.ai/)\n\n- [Stuart Armstrong's short story \"The Adventure\"](https://www.lesswrong.com/posts/Ybp6Wg6yy9DWRcBiR/the-adventure-a-new-utopia-story)\n\n- [Iain M. Banks's Culture novels](https://en.wikipedia.org/wiki/Culture_series)\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7766", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:27:12.861+01:00", "Request Count": "", "Number of suggestions on answer doc": 84, "Total character count of suggestions on answer doc": 8592, "Helpful": ""}}, {"id": "i-bab5a2b8079c122cec05a5f3bca49415b46edecba43a88db8f59947535d4ba80", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-bab5a2b8079c122cec05a5f3bca49415b46edecba43a88db8f59947535d4ba80", "name": "What would a \"warning shot\" look like?", "index": 290, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:33:36.800Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-bab5a2b8079c122cec05a5f3bca49415b46edecba43a88db8f59947535d4ba80", "values": {"File": "What would a \"warning shot\" look like?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What would a \"warning shot\" look like?", "Link": "https://docs.google.com/document/d/1-lW3UKdEKNsqjMpFWmLZjJKDiVfgQmOogi-RbJ1imXU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:16.121+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-14T06:52:17.376+01:00", "Status": "Not started", "Edit Answer": "What would a \"warning shot\" look like?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7748", "Source Link": "", "aisafety.info Link": "What would a \"warning shot\" look like?", "Source": "Wiki", "All Phrasings": "What would a \"warning shot\" look like?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7748", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:27:14.989+01:00", "Request Count": 1, "Number of suggestions on answer doc": 85, "Total character count of suggestions on answer doc": 12093, "Helpful": ""}}, {"id": "i-1b1de1a85ce2115072c5df06a0c2f01df315608413d2844999b4bd833915e6e5", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1b1de1a85ce2115072c5df06a0c2f01df315608413d2844999b4bd833915e6e5", "name": "What training programs and courses are available for AGI safety?", "index": 291, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:33:46.514Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1b1de1a85ce2115072c5df06a0c2f01df315608413d2844999b4bd833915e6e5", "values": {"File": "What training programs and courses are available for AGI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What training programs and courses are available for AGI safety?", "Link": "https://docs.google.com/document/d/1qZLCPxpxdfDB_NVMBVanRh0kMmi57T0vrcPnUjyAuvM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:11.781+01:00", "Related Answers DO NOT EDIT": "What are some good resources on AI alignment?", "Tags": "", "Doc Last Edited": "2023-02-22T23:03:58.522+01:00", "Status": "Live on site", "Edit Answer": "What training programs and courses are available for AGI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8264", "Source Link": "", "aisafety.info Link": "What training programs and courses are available for AGI safety?", "Source": "Wiki", "All Phrasings": "What training programs and courses are available for AGI safety?\n", "Initial Order": "", "Related IDs": "6470", "Rich Text DO NOT EDIT": "See **[AI Safety Training](https://aisafety.training/)** for an up-to-date list of programs and conferences, with application dates.\n\n- [AGI safety fundamentals](https://www.eacambridge.org/agi-safety-fundamentals) ([technical](https://www.eacambridge.org/technical-alignment-curriculum) and [governance](https://www.eacambridge.org/ai-governance-curriculum)) - Is the canonical AGI safety 101 course. 3.5 hours reading, 1.5 hours talking a week w/ facilitator for 8 weeks.\n\n- [AI safety camp](https://aisafety.camp/) - Actually do some AI research. More about output than learning.\n\n- [SERI ML Alignment Theory Scholars Program SERI MATS](https://www.serimats.org/) - Four weeks developing an understanding of a research agenda at the forefront of AI alignment through online readings and cohort discussions, averaging 10 h/week. After this initial upskilling period, the scholars will be paired with an established AI alignment researcher for a two-week \u2018research sprint\u2019 to test fit. Assuming all goes well, scholars will be accepted into an eight-week intensive scholars program in Berkeley, California.\n\n- [Principles of Intelligent Behavior in Biological and Social Systems (PIBBSS)](https://www.pibbss.ai/) - Brings together young researchers studying complex and intelligent behavior in natural and social systems.\n\n- [Safety and Control for Artificial General Intelligence](https://inst.eecs.berkeley.edu//~cs294-149/fa18/) - An actual AI Safety university course (UC Berkeley). Touches multiple domains including cognitive science, utility theory, cybersecurity, human-machine interaction, and political science.\n\nSee also, [this spreadsheet of learning resources](https://docs.google.com/spreadsheets/d/1QSEWjXZuqmG6ORkig84V4sFCldIntyuQj7yq3gkDo0U/edit#gid=0).\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "See **[AI Safety Training](https://aisafety.training/)** for an up-to-date list of programs and conferences, with application dates.\n\n- [AGI safety fundamentals](https://www.eacambridge.org/agi-safety-fundamentals) ([technical](https://www.eacambridge.org/technical-alignment-curriculum) and [governance](https://www.eacambridge.org/ai-governance-curriculum)) - Is the canonical AGI safety 101 course. 3.5 hours reading, 1.5 hours talking a week w/ facilitator for 8 weeks.\n\n- [AI safety camp](https://aisafety.camp/) - Actually do some AI research. More about output than learning.\n\n- [SERI ML Alignment Theory Scholars Program SERI MATS](https://www.serimats.org/) - Four weeks developing an understanding of a research agenda at the forefront of AI alignment through online readings and cohort discussions, averaging 10 h/week. After this initial upskilling period, the scholars will be paired with an established AI alignment researcher for a two-week \u2018research sprint\u2019 to test fit. Assuming all goes well, scholars will be accepted into an eight-week intensive scholars program in Berkeley, California.\n\n- [Principles of Intelligent Behavior in Biological and Social Systems (PIBBSS)](https://www.pibbss.ai/) - Brings together young researchers studying complex and intelligent behavior in natural and social systems.\n\n- [Safety and Control for Artificial General Intelligence](https://inst.eecs.berkeley.edu//~cs294-149/fa18/) - An actual AI Safety university course (UC Berkeley). Touches multiple domains including cognitive science, utility theory, cybersecurity, human-machine interaction, and political science.\n\nSee also, [this spreadsheet of learning resources](https://docs.google.com/spreadsheets/d/1QSEWjXZuqmG6ORkig84V4sFCldIntyuQj7yq3gkDo0U/edit#gid=0).\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "2023-03-02T02:28:15.485+01:00", "UI ID": "8264", "Related Answers": "What are some good resources on AI alignment?", "Doc Last Ingested": "2023-03-14T23:27:16.998+01:00", "Request Count": "", "Number of suggestions on answer doc": 85, "Total character count of suggestions on answer doc": 12093, "Helpful": ""}}, {"id": "i-c105c6af7b2cadf3bb78e27f1141b8dcd124127009fd7aac2e3a6194212cbb25", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c105c6af7b2cadf3bb78e27f1141b8dcd124127009fd7aac2e3a6194212cbb25", "name": "What technological developments could speed up AI progress?", "index": 292, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:33:50.155Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c105c6af7b2cadf3bb78e27f1141b8dcd124127009fd7aac2e3a6194212cbb25", "values": {"File": "What technological developments could speed up AI progress?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What technological developments could speed up AI progress?", "Link": "https://docs.google.com/document/d/1aOwQ_GN2obuXOzFQqrmQ1m1J77o60bRWpQMBXWm9-Tk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:07.989+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Timelines", "Doc Last Edited": "2023-02-22T22:46:34.722+01:00", "Status": "Bulletpoint sketch", "Edit Answer": "What technological developments could speed up AI progress?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7645", "Source Link": "", "aisafety.info Link": "What technological developments could speed up AI progress?", "Source": "Wiki", "All Phrasings": "What technological developments could speed up AI progress?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Better GPUs\n\n- Cheaper electricity\n\n- Better (faster/bigger) RAM\n\n- Better transcription services - to generate test data\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "- Better GPUs\n\n- Cheaper electricity\n\n- Better (faster/bigger) RAM\n\n- Better transcription services - to generate test data\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7645", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:27:18.594+01:00", "Request Count": 1, "Number of suggestions on answer doc": 85, "Total character count of suggestions on answer doc": 12093, "Helpful": ""}}, {"id": "i-1d5eb8d5c0cf35af3e3950adab08e484ef6db52aea4363237fcabb6649961548", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1d5eb8d5c0cf35af3e3950adab08e484ef6db52aea4363237fcabb6649961548", "name": "What technical problems are MIRI working on?", "index": 293, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:27:38.392Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1d5eb8d5c0cf35af3e3950adab08e484ef6db52aea4363237fcabb6649961548", "values": {"File": "What technical problems are MIRI working on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What technical problems are MIRI working on?", "Link": "https://docs.google.com/document/d/1-aeuXi5FB8lfGbXrJUDgAxZ3YJlduexCU1yB4MCRXJ0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:04.310+01:00", "Related Answers DO NOT EDIT": "", "Tags": "MIRI,Research Agendas", "Doc Last Edited": "2023-02-22T23:03:59.695+01:00", "Status": "Live on site", "Edit Answer": "What technical problems are MIRI working on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6300", "Source Link": "https://intelligence.org/faq/", "aisafety.info Link": "What technical problems are MIRI working on?", "Source": "MIRI FAQ", "All Phrasings": "What technical problems are MIRI working on?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\u201cAligning smarter-than-human AI with human interests\u201d is an extremely vague goal. To approach this problem productively, we attempt to factorize it into several subproblems. As a starting point, we ask: \u201cWhat aspects of this problem would we still be unable to solve even if the problem were much easier?\u201d\n\nIn order to achieve real-world goals more effectively than a human, a general AI system will need to be able to learn its environment over time and decide between possible proposals or actions. A simplified version of the alignment problem, then, would be to ask how we could construct a system that learns its environment and has a very crude decision criterion, like \u201cSelect the policy that maximizes the expected number of diamonds in the world.\u201d\n\n*Highly reliable agent design* is the technical challenge of formally specifying a software system that can be relied upon to pursue some preselected toy goal. An example of a subproblem in this space is [ontology identification](https://intelligence.org/2015/07/27/miris-approach/#2): how do we formalize the goal of \u201cmaximizing diamonds\u201d in full generality, allowing that a fully autonomous agent may end up in unexpected environments and may construct unanticipated hypotheses and policies? Even if we had unbounded computational power and all the time in the world, we don\u2019t currently know how to solve this problem. This suggests that we\u2019re not only missing practical algorithms but also a basic theoretical framework through which to understand the problem.\n\nThe formal agent AIXI is an attempt to define what we mean by \u201coptimal behavior\u201d in the case of a reinforcement learner. A simple AIXI-like equation is lacking, however, for defining what we mean by \u201cgood behavior\u201d if the goal is to change something about the external world (and not just to maximize a pre-specified reward number). In order for the agent to evaluate its world-models to count the number of diamonds, as opposed to having a privileged reward channel, what general formal properties must its world-models possess? If the system updates its hypotheses (e.g., discovers that string theory is true and quantum physics is false) in a way its programmers didn\u2019t expect, how does it identify \u201cdiamonds\u201d in the new model? The question is a very basic one, yet the relevant theory is currently missing.\n\nWe can distinguish highly reliable agent design from the problem of value specification: \u201cOnce we understand how to design an autonomous AI system that promotes a goal, how do we ensure its goal actually matches what we want?\u201d Since human error is inevitable and we will need to be able to safely supervise and redesign AI algorithms even as they approach human equivalence in cognitive tasks, MIRI also works on formalizing error-tolerant agent properties. Artificial Intelligence: A Modern Approach, the standard textbook in AI, summarizes the challenge:\n\nYudkowsky [\u2026] asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design \u2014 to design a mechanism for evolving AI under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes. -Russell and Norvig (2009). [Artificial Intelligence: A Modern Approach](http://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0136042597).\n\nOur [technical agenda](https://intelligence.org/technical-agenda/) describes these open problems in more detail, and our research guide collects online resources for learning more.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "\u201cAligning smarter-than-human AI with human interests\u201d is an extremely vague goal. To approach this problem productively, we attempt to factorize it into several subproblems. As a starting point, we ask: \u201cWhat aspects of this problem would we still be unable to solve even if the problem were much easier?\u201d\n\nIn order to achieve real-world goals more effectively than a human, a general AI system will need to be able to learn its environment over time and decide between possible proposals or actions. A simplified version of the alignment problem, then, would be to ask how we could construct a system that learns its environment and has a very crude decision criterion, like \u201cSelect the policy that maximizes the expected number of diamonds in the world.\u201d\n\n*Highly reliable agent design* is the technical challenge of formally specifying a software system that can be relied upon to pursue some preselected toy goal. An example of a subproblem in this space is [ontology identification](https://intelligence.org/2015/07/27/miris-approach/#2): how do we formalize the goal of \u201cmaximizing diamonds\u201d in full generality, allowing that a fully autonomous agent may end up in unexpected environments and may construct unanticipated hypotheses and policies? Even if we had unbounded computational power and all the time in the world, we don\u2019t currently know how to solve this problem. This suggests that we\u2019re not only missing practical algorithms but also a basic theoretical framework through which to understand the problem.\n\nThe formal agent AIXI is an attempt to define what we mean by \u201coptimal behavior\u201d in the case of a reinforcement learner. A simple AIXI-like equation is lacking, however, for defining what we mean by \u201cgood behavior\u201d if the goal is to change something about the external world (and not just to maximize a pre-specified reward number). In order for the agent to evaluate its world-models to count the number of diamonds, as opposed to having a privileged reward channel, what general formal properties must its world-models possess? If the system updates its hypotheses (e.g., discovers that string theory is true and quantum physics is false) in a way its programmers didn\u2019t expect, how does it identify \u201cdiamonds\u201d in the new model? The question is a very basic one, yet the relevant theory is currently missing.\n\nWe can distinguish highly reliable agent design from the problem of value specification: \u201cOnce we understand how to design an autonomous AI system that promotes a goal, how do we ensure its goal actually matches what we want?\u201d Since human error is inevitable and we will need to be able to safely supervise and redesign AI algorithms even as they approach human equivalence in cognitive tasks, MIRI also works on formalizing error-tolerant agent properties. Artificial Intelligence: A Modern Approach, the standard textbook in AI, summarizes the challenge:\n\nYudkowsky [\u2026] asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design \u2014 to design a mechanism for evolving AI under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes. -Russell and Norvig (2009). [Artificial Intelligence: A Modern Approach](http://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0136042597).\n\nOur [technical agenda](https://intelligence.org/technical-agenda/) describes these open problems in more detail, and our research guide collects online resources for learning more.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "6300", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:22:50.896+01:00", "Request Count": "", "Number of suggestions on answer doc": 95, "Total character count of suggestions on answer doc": 14306, "Helpful": ""}}, {"id": "i-beb68f8f0b8dcbda69c5c071d776a7fad29593f9abfb7d44f7bc3d4497722038", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-beb68f8f0b8dcbda69c5c071d776a7fad29593f9abfb7d44f7bc3d4497722038", "name": "What subjects should I study at university to prepare myself for alignment research?", "index": 294, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:33:56.995Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-beb68f8f0b8dcbda69c5c071d776a7fad29593f9abfb7d44f7bc3d4497722038", "values": {"File": "What subjects should I study at university to prepare myself for alignment research?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What subjects should I study at university to prepare myself for alignment research?", "Link": "https://docs.google.com/document/d/16gE3-npWkBF__dJ9Rt9nD8HoGiwM5gh-oiyxXl3bGVM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:52:00.833+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:00.643+01:00", "Status": "Live on site", "Edit Answer": "What subjects should I study at university to prepare myself for alignment research?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7763", "Source Link": "", "aisafety.info Link": "What subjects should I study at university to prepare myself for alignment research?", "Source": "Wiki", "All Phrasings": "What subjects should I study at university to prepare myself for alignment research?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A degree is not necessary to do high quality research, and many of the biggest contributions have been made by people without a traditional academic background, so don\u2019t feel like you need to study for many years before starting work. However, university-based study can provide a firm foundation for your research. In particular, understanding machine learning, and having a solid grasp of the relevant mathematics such as linear algebra, calculus, and statistics is useful for empirical work. A degree in **mathematics**, **computer science**, or directly in **AI**, is a good way to build this understanding. However, AI alignment also benefits from having researchers with diverse backgrounds, so if you have a particular interest or talent in a different topic, it can be valuable to pursue a degree in that topic instead. For example, degrees that could be relevant are neuroscience, philosophy, physics, biology, cybersecurity, risk management, safety engineering, or economics. It has been argued in particular that[AI safety needs social scientists](https://distill.pub/2019/safety-needs-social-scientists/).\n\nIf you are uncertain, you can[apply for coaching](https://80000hours.org/speak-with-us/) from the career advice platform[80000 hours](https://80000hours.org/), which also has a[career review](https://80000hours.org/career-reviews/artificial-intelligence-risk-research/) of technical AI safety research. Another option is to[apply for coaching from AI safety support](https://www.aisafetysupport.org/resources/career-coaching).\n\nWhen choosing university courses, try to cover\n\n- machine learning\n\n    - firm grasp of the basics\n\n    - deep learning, in particular transformers\n\n    - reinforcement learning\n\n- statistics\n\n- linear algebra\n\n- calculus\n\n- game theory\n\nIn addition to taking relevant university courses, it is also really helpful to study AI safety materials outside of university. An excellent place to start is the[AGI safety fundamentals course](https://www.agisafetyfundamentals.com/ai-alignment-curriculum). A lot more information can be found on the [EA forum](https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment).\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "A degree is not necessary to do high quality research, and many of the biggest contributions have been made by people without a traditional academic background, so don\u2019t feel like you need to study for many years before starting work. However, university-based study can provide a firm foundation for your research. In particular, understanding machine learning, and having a solid grasp of the relevant mathematics such as linear algebra, calculus, and statistics is useful for empirical work. A degree in **mathematics**, **computer science**, or directly in **AI**, is a good way to build this understanding. However, AI alignment also benefits from having researchers with diverse backgrounds, so if you have a particular interest or talent in a different topic, it can be valuable to pursue a degree in that topic instead. For example, degrees that could be relevant are neuroscience, philosophy, physics, biology, cybersecurity, risk management, safety engineering, or economics. It has been argued in particular that[AI safety needs social scientists](https://distill.pub/2019/safety-needs-social-scientists/).\n\nIf you are uncertain, you can[apply for coaching](https://80000hours.org/speak-with-us/) from the career advice platform[80000 hours](https://80000hours.org/), which also has a[career review](https://80000hours.org/career-reviews/artificial-intelligence-risk-research/) of technical AI safety research. Another option is to[apply for coaching from AI safety support](https://www.aisafetysupport.org/resources/career-coaching).\n\nWhen choosing university courses, try to cover\n\n- machine learning\n\n    - firm grasp of the basics\n\n    - deep learning, in particular transformers\n\n    - reinforcement learning\n\n- statistics\n\n- linear algebra\n\n- calculus\n\n- game theory\n\nIn addition to taking relevant university courses, it is also really helpful to study AI safety materials outside of university. An excellent place to start is the[AGI safety fundamentals course](https://www.agisafetyfundamentals.com/ai-alignment-curriculum). A lot more information can be found on the [EA forum](https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment).\n\n", "Stamp Count": 4, "Multi Answer": "", "Stamped By": "Aprillion\nDamaged\nMagdalena\nplex", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7763", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:27:20.822+01:00", "Request Count": "", "Number of suggestions on answer doc": 85, "Total character count of suggestions on answer doc": 12093, "Helpful": ""}}, {"id": "i-03dd33916b47a6a7b02e6ae2fc2c761daef93b29006605e08dea3bb9f54510c3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-03dd33916b47a6a7b02e6ae2fc2c761daef93b29006605e08dea3bb9f54510c3", "name": "What sources of information can Stampy use?", "index": 295, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:34:01.988Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-03dd33916b47a6a7b02e6ae2fc2c761daef93b29006605e08dea3bb9f54510c3", "values": {"File": "What sources of information can Stampy use?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What sources of information can Stampy use?", "Link": "https://docs.google.com/document/d/1o5sjtEEa-FGocy8M3j7jJxXKWZSESYfphPsM6s1kdVo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:56.921+01:00", "Related Answers DO NOT EDIT": "What are some good resources on AI alignment?", "Tags": "Stampy", "Doc Last Edited": "2023-02-22T23:04:01.582+01:00", "Status": "Live on site", "Edit Answer": "What sources of information can Stampy use?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7487", "Source Link": "", "aisafety.info Link": "What sources of information can Stampy use?", "Source": "Wiki", "All Phrasings": "What sources of information can Stampy use?\n", "Initial Order": "", "Related IDs": "6470", "Rich Text DO NOT EDIT": "As well as pulling human written answers to AI alignment questions from [Stampy's AI Safety](https://aisafety.info/) [Info](https://aisafety.info/), Stampy can:\n\n- Search for AI safety papers e.g. \"stampy, what's that paper about corrigibility?\"\n\n- Search for videos e.g. \"what's that video where Rob talks about mesa optimizers, stampy?\"\n\n- Calculate with Wolfram Alpha e.g. \"s, what's the square root of 345?\"\n\n- Search DuckDuckGo and return snippets\n\n- And (at least in the patron Discord) falls back to polling GPT-3 to answer uncaught questions\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "As well as pulling human written answers to AI alignment questions from [Stampy's AI Safety](https://aisafety.info/) [Info](https://aisafety.info/), Stampy can:\n\n- Search for AI safety papers e.g. \"stampy, what's that paper about corrigibility?\"\n\n- Search for videos e.g. \"what's that video where Rob talks about mesa optimizers, stampy?\"\n\n- Calculate with Wolfram Alpha e.g. \"s, what's the square root of 345?\"\n\n- Search DuckDuckGo and return snippets\n\n- And (at least in the patron Discord) falls back to polling GPT-3 to answer uncaught questions\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7487", "Related Answers": "What are some good resources on AI alignment?", "Doc Last Ingested": "2023-03-14T23:27:22.677+01:00", "Request Count": "", "Number of suggestions on answer doc": 85, "Total character count of suggestions on answer doc": 12093, "Helpful": ""}}, {"id": "i-ee671b8ef893c220c2502ad1fe6711e9a36555f6f2e296bc2e6a33e1cd697d7c", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ee671b8ef893c220c2502ad1fe6711e9a36555f6f2e296bc2e6a33e1cd697d7c", "name": "What should the first AGI systems be aligned to do?", "index": 296, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:07.708Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ee671b8ef893c220c2502ad1fe6711e9a36555f6f2e296bc2e6a33e1cd697d7c", "values": {"File": "What should the first AGI systems be aligned to do?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What should the first AGI systems be aligned to do?", "Link": "https://docs.google.com/document/d/1UTQNd5XIf1aiuNWCqDqC92nHQB_TIeVpyiHrdMSG8to/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:53.572+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI,Alignment Targets", "Doc Last Edited": "2023-02-23T05:28:50.458+01:00", "Status": "Not started", "Edit Answer": "What should the first AGI systems be aligned to do?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7724", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "What should the first AGI systems be aligned to do?", "Source": "LessWrong", "All Phrasings": "What should the first AGI systems be aligned to do?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7724", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:31:57.176+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 497, "Helpful": ""}}, {"id": "i-e6d4a3980df9e40e3b16b754d2422417f83327b5c6ca75217b12e4111cdc8df8", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e6d4a3980df9e40e3b16b754d2422417f83327b5c6ca75217b12e4111cdc8df8", "name": "What should be marked as a canonical answer on Stampy's AI Safety Info?", "index": 297, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:09.300Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e6d4a3980df9e40e3b16b754d2422417f83327b5c6ca75217b12e4111cdc8df8", "values": {"File": "What should be marked as a canonical answer on Stampy's AI Safety Info?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What should be marked as a canonical answer on Stampy's AI Safety Info?", "Link": "https://docs.google.com/document/d/1ArWzlraw7kCHhoiWje22j057xgW5wsV0PbaLpiy8KC0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:50.003+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stampy", "Doc Last Edited": "2023-02-22T23:04:02.738+01:00", "Status": "Live on site", "Edit Answer": "What should be marked as a canonical answer on Stampy's AI Safety Info?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6432", "Source Link": "", "aisafety.info Link": "What should be marked as a canonical answer on Stampy's AI Safety Info?", "Source": "Wiki", "All Phrasings": "What should be marked as a canonical answer on Stampy's AI Safety Info?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[Canonical answers](https://coda.io/@alignmentdev/ai-safety-info/all-published-answers-50) may be served to readers by Stampy, so only answers which have a reasonably high stamp score should be marked as canonical. All canonical answers are open to be collaboratively edited and updated, and they should represent a consensus response (written from the Stampy Point Of View) to a question which is within Stampy's scope.\n\nAnswers to questions from YouTube comments should not be marked as canonical, and will generally remain as they were when originally written since they have details which are specific to an idiosyncratic question. YouTube answers may be forked into wiki answers, in order to better respond to a particular question, in which case the YouTube question should have its canonical version field set to the new more widely useful question.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "[Canonical answers](https://coda.io/@alignmentdev/ai-safety-info/all-published-answers-50) may be served to readers by Stampy, so only answers which have a reasonably high stamp score should be marked as canonical. All canonical answers are open to be collaboratively edited and updated, and they should represent a consensus response (written from the Stampy Point Of View) to a question which is within Stampy's scope.\n\nAnswers to questions from YouTube comments should not be marked as canonical, and will generally remain as they were when originally written since they have details which are specific to an idiosyncratic question. YouTube answers may be forked into wiki answers, in order to better respond to a particular question, in which case the YouTube question should have its canonical version field set to the new more widely useful question.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6432", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:31:59.197+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 497, "Helpful": ""}}, {"id": "i-67bb92c3c09b667ecad95fa5f1f0dede510c862dc1d3da5917e05e180cd0da8e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-67bb92c3c09b667ecad95fa5f1f0dede510c862dc1d3da5917e05e180cd0da8e", "name": "What should be marked as a \"related\" question on Stampy's AI Safety Info?", "index": 298, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:10.763Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-67bb92c3c09b667ecad95fa5f1f0dede510c862dc1d3da5917e05e180cd0da8e", "values": {"File": "What should be marked as a \"related\" question on Stampy's AI Safety Info?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What should be marked as a \"related\" question on Stampy's AI Safety Info?", "Link": "https://docs.google.com/document/d/1DI3A2LzQLzBRp01vwWiBhv60oDql0klymZpvln3vc1E/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:46.482+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stampy", "Doc Last Edited": "2023-02-22T22:46:37.238+01:00", "Status": "Not started", "Edit Answer": "What should be marked as a \"related\" question on Stampy's AI Safety Info?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6445", "Source Link": "", "aisafety.info Link": "What should be marked as a \"related\" question on Stampy's AI Safety Info?", "Source": "Wiki", "All Phrasings": "What should be marked as a \"related\" question on Stampy's AI Safety Info?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6445", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:32:01.802+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 497, "Helpful": ""}}, {"id": "i-b4ba650f6f3715b1a3cff09c989ec15f9885c0ab9b6770a57a2a99e975d3dc2f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b4ba650f6f3715b1a3cff09c989ec15f9885c0ab9b6770a57a2a99e975d3dc2f", "name": "What should I read to learn about decision theory?", "index": 299, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:12.270Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b4ba650f6f3715b1a3cff09c989ec15f9885c0ab9b6770a57a2a99e975d3dc2f", "values": {"File": "What should I read to learn about decision theory?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What should I read to learn about decision theory?", "Link": "https://docs.google.com/document/d/1xwxgnzU_M-Af6Edk5t0Zg2ReH4YFMUtzAJprCW2t9vw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:43.121+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Literature,Decision Theory", "Doc Last Edited": "2023-02-22T23:04:03.907+01:00", "Status": "Live on site", "Edit Answer": "What should I read to learn about decision theory?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6536", "Source Link": "", "aisafety.info Link": "What should I read to learn about decision theory?", "Source": "Wiki", "All Phrasings": "What should I read to learn about decision theory?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[abramdemski and Scott Garrabrant's post on decision theory](https://www.lesswrong.com/posts/zcPLNNw4wgBX5k8kQ/decision-theory) provides a good overview of many aspects of the topic, while[Functional Decision Theory: A New Theory of Instrumental Rationality](https://arxiv.org/abs/1710.05060) seems to be the most up to date source on current thinking.\n\nFor a more intuitive dive into one of the core problems,[Newcomb's problem and regret of rationality](https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality) is good, and[Newcomblike problems are the norm](https://www.lesswrong.com/posts/puutBJLWbg2sXpFbu/newcomblike-problems-are-the-norm) is useful for seeing how it applies in the real world.\n\nThe[LessWrong tag for decision theory](https://www.lesswrong.com/tag/decision-theory) has lots of additional links for people who want to explore further.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "[abramdemski and Scott Garrabrant's post on decision theory](https://www.lesswrong.com/posts/zcPLNNw4wgBX5k8kQ/decision-theory) provides a good overview of many aspects of the topic, while[Functional Decision Theory: A New Theory of Instrumental Rationality](https://arxiv.org/abs/1710.05060) seems to be the most up to date source on current thinking.\n\nFor a more intuitive dive into one of the core problems,[Newcomb's problem and regret of rationality](https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality) is good, and[Newcomblike problems are the norm](https://www.lesswrong.com/posts/puutBJLWbg2sXpFbu/newcomblike-problems-are-the-norm) is useful for seeing how it applies in the real world.\n\nThe[LessWrong tag for decision theory](https://www.lesswrong.com/tag/decision-theory) has lots of additional links for people who want to explore further.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "triggerhappygandi", "External Source": "", "Last Asked On Discord": "2023-02-26T18:55:55.060+01:00", "UI ID": "6536", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:32:09.812+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 497, "Helpful": ""}}, {"id": "i-cf9ec6d7b8b98a16a36c99a9cd820f4a020e13fcc1795823df2c165868475111", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-cf9ec6d7b8b98a16a36c99a9cd820f4a020e13fcc1795823df2c165868475111", "name": "What safety problems are associated with whole brain emulation?", "index": 300, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:13.911Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-cf9ec6d7b8b98a16a36c99a9cd820f4a020e13fcc1795823df2c165868475111", "values": {"File": "What safety problems are associated with whole brain emulation?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What safety problems are associated with whole brain emulation?", "Link": "https://docs.google.com/document/d/1ebIo3z_b7Fz9CrqhSagNG6dUhtH_E3DF7udMgdNCccU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:39.129+01:00", "Related Answers DO NOT EDIT": "How would we align an AGI whose learning algorithms / cognition look like human brains?,What are brain-computer interfaces?,What are the ethical challenges related to whole brain emulation?", "Tags": "Whole Brain Emulation", "Doc Last Edited": "2023-03-14T17:34:23.983+01:00", "Status": "Live on site", "Edit Answer": "What safety problems are associated with whole brain emulation?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7605", "Source Link": "", "aisafety.info Link": "What safety problems are associated with whole brain emulation?", "Source": "Wiki", "All Phrasings": "What safety problems are associated with whole brain emulation?\n", "Initial Order": "", "Related IDs": "8324,6592,7820", "Rich Text DO NOT EDIT": "It [seems improbable](https://youtu.be/EUjc1WuyPT8?t=4286) that whole brain emulation (WBE) arrives before [neuromorphic AI](https://www.alignmentforum.org/tag/neuromorphic-ai) because a better understanding of the brain would probably help with the development of the latter. This makes the research path to WBE likely to accelerate capabilities and reduce timelines.\n\nEven if WBE were to arrive first, there is some debate on whether [it would be safer than synthetic AI](https://intelligence.org/files/SS11Workshop.pdf). An accelerated WBE might be a safe template for an AGI as it would directly inherit the subject's way of thinking but some safety problems could still arise.\n\n- This would be a very strange experience for current human psychology, and we are not sure how the resulting brain would react. As an intuition pump, very high IQ individuals are at [higher risk for psychological disorders](https://www.sciencedirect.com/science/article/pii/S0160289616303324).\n\n- A superintelligent WBE would get a large amount of power, which historically has tended to corrupt humans.\n\n- High speed might make interactions with normal-speed humans difficult, as explored in Robin Hanson's [The Age of Em](https://en.wikipedia.org/wiki/The_Age_of_Em).\n\n- It is unclear whether WBE would be dynamically more predictable than AI engineered by competent safety-conscious programmers.\n\n- Even if WBE arrives before AGI, Bostrom argues we should expect a second (potentially dangerous) transition to fully synthetic AGI due to their improved efficiency over WBE.\n\nNonetheless, Yudkowsky believes that [emulations are probably better even if they are unlikely](https://www.youtube.com/watch?v=EUjc1WuyPT8&start=4286).\n\n", "Tag Count": 1, "Related Answer Count": 3, "Rich Text": "It [seems improbable](https://youtu.be/EUjc1WuyPT8?t=4286) that whole brain emulation (WBE) arrives before [neuromorphic AI](https://www.alignmentforum.org/tag/neuromorphic-ai) because a better understanding of the brain would probably help with the development of the latter. This makes the research path to WBE likely to accelerate capabilities and reduce timelines.\n\nEven if WBE were to arrive first, there is some debate on whether [it would be safer than synthetic AI](https://intelligence.org/files/SS11Workshop.pdf). An accelerated WBE might be a safe template for an AGI as it would directly inherit the subject's way of thinking but some safety problems could still arise.\n\n- This would be a very strange experience for current human psychology, and we are not sure how the resulting brain would react. As an intuition pump, very high IQ individuals are at [higher risk for psychological disorders](https://www.sciencedirect.com/science/article/pii/S0160289616303324).\n\n- A superintelligent WBE would get a large amount of power, which historically has tended to corrupt humans.\n\n- High speed might make interactions with normal-speed humans difficult, as explored in Robin Hanson's [The Age of Em](https://en.wikipedia.org/wiki/The_Age_of_Em).\n\n- It is unclear whether WBE would be dynamically more predictable than AI engineered by competent safety-conscious programmers.\n\n- Even if WBE arrives before AGI, Bostrom argues we should expect a second (potentially dangerous) transition to fully synthetic AGI due to their improved efficiency over WBE.\n\nNonetheless, Yudkowsky believes that [emulations are probably better even if they are unlikely](https://www.youtube.com/watch?v=EUjc1WuyPT8&start=4286).\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "298883259008417793", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7605", "Related Answers": "How would we align an AGI whose learning algorithms / cognition look like human brains?,What are brain-computer interfaces?,What are the ethical challenges related to whole brain emulation?", "Doc Last Ingested": "2023-03-14T23:32:18.704+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 497, "Helpful": ""}}, {"id": "i-75d9b05e1f3d437a532e15c386c983e5f39784562075b8d610c9793e3772e0a3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-75d9b05e1f3d437a532e15c386c983e5f39784562075b8d610c9793e3772e0a3", "name": "What research is being done to align modern deep learning systems?", "index": 301, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:15.387Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-75d9b05e1f3d437a532e15c386c983e5f39784562075b8d610c9793e3772e0a3", "values": {"File": "What research is being done to align modern deep learning systems?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What research is being done to align modern deep learning systems?", "Link": "https://docs.google.com/document/d/1Ktyf5Ds-FwWpTYTBSnIAqznlY3Vmn4d9iQLvef22xPk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:35.229+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:46:38.539+01:00", "Status": "In progress", "Edit Answer": "What research is being done to align modern deep learning systems?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7752", "Source Link": "", "aisafety.info Link": "What research is being done to align modern deep learning systems?", "Source": "Wiki", "All Phrasings": "What research is being done to align modern deep learning systems?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Might involve transformers or whatever their successor is\n\n- \n\n- Reward modeling stuff\n\n- Most work geared towards language models\n\n- \n\n- paper Learning to summarize, OpenAI stuff based on that paper\n\n- paper Learning human reward models\n\n- \n\n- Redwood - Trying to make a LLM which does not serve completions where a human gets hurt\n\n- Anthropic - Doing lots of work around interpreting transformers\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "- Might involve transformers or whatever their successor is\n\n- \n\n- Reward modeling stuff\n\n- Most work geared towards language models\n\n- \n\n- paper Learning to summarize, OpenAI stuff based on that paper\n\n- paper Learning human reward models\n\n- \n\n- Redwood - Trying to make a LLM which does not serve completions where a human gets hurt\n\n- Anthropic - Doing lots of work around interpreting transformers\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7752", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:32:21.107+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 497, "Helpful": ""}}, {"id": "i-0301b061a34e4e86ed813912e24b4c412f150e9d2c371ad9cbf3a0cebf465ad8", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0301b061a34e4e86ed813912e24b4c412f150e9d2c371ad9cbf3a0cebf465ad8", "name": "What plausibly happens five years before and after AGI?", "index": 302, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:16.853Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0301b061a34e4e86ed813912e24b4c412f150e9d2c371ad9cbf3a0cebf465ad8", "values": {"File": "What plausibly happens five years before and after AGI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What plausibly happens five years before and after AGI?", "Link": "https://docs.google.com/document/d/1kdmOhrfbBJuSJkF9sf0-8ifTLYQwiXGw2UPfzIlivLg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:31.734+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Timelines,AI Takeoff", "Doc Last Edited": "2023-02-22T22:46:39.711+01:00", "Status": "Not started", "Edit Answer": "What plausibly happens five years before and after AGI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7716", "Source Link": "", "aisafety.info Link": "What plausibly happens five years before and after AGI?", "Source": "Wiki", "All Phrasings": "What plausibly happens five years before and after AGI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "2023-02-26T18:55:44.133+01:00", "UI ID": "7716", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:32:23.486+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 497, "Helpful": ""}}, {"id": "i-566f81ebeb6eb1fe887347a2b9bd32c4606dd6f4f0a784e44c7d6fb805eacfec", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-566f81ebeb6eb1fe887347a2b9bd32c4606dd6f4f0a784e44c7d6fb805eacfec", "name": "What milestones are there between us and AGI?", "index": 303, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:18.519Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-566f81ebeb6eb1fe887347a2b9bd32c4606dd6f4f0a784e44c7d6fb805eacfec", "values": {"File": "What milestones are there between us and AGI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What milestones are there between us and AGI?", "Link": "https://docs.google.com/document/d/1KUj8eZeTZS-gVpD76J_lunLkvbm0xfeF2hGD7-VetCk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:24.588+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:46:40.807+01:00", "Status": "In progress", "Edit Answer": "What milestones are there between us and AGI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7753", "Source Link": "", "aisafety.info Link": "What milestones are there between us and AGI?", "Source": "Wiki", "All Phrasings": "What milestones are there between us and AGI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Working out milestone tasks that we expect to be achieved before we reach AGI can be difficult. Some tasks, like \"continuous learning\" intuitively seem like they will need to be solved before someone builds AGI. Continuous learning is learning bit by bit, as you get more data. Current ML systems usually don't do this, instead learning everything at once from a big dataset. Because humans can do continuous learning, it seems like it might be required for AGI. However, you have to be careful with reasoning like this, because it is possible the first generally capable artificial intelligence will work quite differently to a human. It's possible the first AGI will be designed to avoid needing \"continuous learning\", maybe by being designed to do a big retraining process every day. This might still allow it to be as capable as humans at almost every task, but without solving the \"continuous learning\" problem.\n\nBecause of arguments like the above, it's not always clear whether a given task is \"required\" for AGI.\n\nSome potential big milestone tasks might be:\n\n- [ARC challenge](https://pgpbpadilla.github.io/chollet-arc-challenge) (tests the ability to generate the \"simplest explanation\" for patterns)\n\n- Human level sample efficiency at various tasks ([EfficientZero](https://www.lesswrong.com/posts/mRwJce3npmzbKfxws/efficientzero-how-it-works) already does Atari games)\n\n[This metaculus question](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) has four very specific milestones that it considers to be requirements for \"weak AGI\".\n\nNotes:\n\n- Rob: I have no idea\n\n- \n\n- This is what is scary: Pretty hard to name a thing that happens before AGI happens\n\n- \n\n- It used to be that they couldn't do basic common sense stuff, Winograd schemas, creativity, reasoning, but they\u2019ve been falling fast over the past 5 years. Now it\u2019s more nebulous stuff like \u201cthey can\u2019t do these things very well\u201d, but scaling might just solve this.\n\n- \n\n- That is, examples of things which AI can't currently do, but which we'd expect they'd be able to do between now and then. It's scary: it's hard to think of anything, because AI can already do common sense, creativity, all those bastions of human thinking which have been conquered in the recent years\n\n- \n\n- Finding the right training regime is the main difficulty for many tasks\n\n- \n\n- One gets more generality from scaling up\n\n- \n\n- Lots of debate on what counts as generality ; if you take two narrow system then train a third to dispatch tasks to specialized ones, is it more general? There\u2019s no cross-capability? How much knowledge transfer?\n\n- How could you test this? Say, read a description of an art style, and generate an image in that style ; or describe a strategy through texte, and have the Atari-playing model execute that strategy\n\n- Embedding more and more diverse things in the same latent space ; \u201cfeels like\u201d not more than one Transformer-size breakthrough away\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Working out milestone tasks that we expect to be achieved before we reach AGI can be difficult. Some tasks, like \"continuous learning\" intuitively seem like they will need to be solved before someone builds AGI. Continuous learning is learning bit by bit, as you get more data. Current ML systems usually don't do this, instead learning everything at once from a big dataset. Because humans can do continuous learning, it seems like it might be required for AGI. However, you have to be careful with reasoning like this, because it is possible the first generally capable artificial intelligence will work quite differently to a human. It's possible the first AGI will be designed to avoid needing \"continuous learning\", maybe by being designed to do a big retraining process every day. This might still allow it to be as capable as humans at almost every task, but without solving the \"continuous learning\" problem.\n\nBecause of arguments like the above, it's not always clear whether a given task is \"required\" for AGI.\n\nSome potential big milestone tasks might be:\n\n- [ARC challenge](https://pgpbpadilla.github.io/chollet-arc-challenge) (tests the ability to generate the \"simplest explanation\" for patterns)\n\n- Human level sample efficiency at various tasks ([EfficientZero](https://www.lesswrong.com/posts/mRwJce3npmzbKfxws/efficientzero-how-it-works) already does Atari games)\n\n[This metaculus question](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) has four very specific milestones that it considers to be requirements for \"weak AGI\".\n\nNotes:\n\n- Rob: I have no idea\n\n- \n\n- This is what is scary: Pretty hard to name a thing that happens before AGI happens\n\n- \n\n- It used to be that they couldn't do basic common sense stuff, Winograd schemas, creativity, reasoning, but they\u2019ve been falling fast over the past 5 years. Now it\u2019s more nebulous stuff like \u201cthey can\u2019t do these things very well\u201d, but scaling might just solve this.\n\n- \n\n- That is, examples of things which AI can't currently do, but which we'd expect they'd be able to do between now and then. It's scary: it's hard to think of anything, because AI can already do common sense, creativity, all those bastions of human thinking which have been conquered in the recent years\n\n- \n\n- Finding the right training regime is the main difficulty for many tasks\n\n- \n\n- One gets more generality from scaling up\n\n- \n\n- Lots of debate on what counts as generality ; if you take two narrow system then train a third to dispatch tasks to specialized ones, is it more general? There\u2019s no cross-capability? How much knowledge transfer?\n\n- How could you test this? Say, read a description of an art style, and generate an image in that style ; or describe a strategy through texte, and have the Atari-playing model execute that strategy\n\n- Embedding more and more diverse things in the same latent space ; \u201cfeels like\u201d not more than one Transformer-size breakthrough away\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7753", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:32:26.465+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 505, "Helpful": ""}}, {"id": "i-448ea6cca773bc70178351c8f0677b2dcaea0ccac1c7c9864f10ae45d1305a47", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-448ea6cca773bc70178351c8f0677b2dcaea0ccac1c7c9864f10ae45d1305a47", "name": "What links are especially valuable to share on social media or other contexts?", "index": 304, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:20.032Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-448ea6cca773bc70178351c8f0677b2dcaea0ccac1c7c9864f10ae45d1305a47", "values": {"File": "What links are especially valuable to share on social media or other contexts?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What links are especially valuable to share on social media or other contexts?", "Link": "https://docs.google.com/document/d/1dpiudI1KboBAmD67sjh8mIklNZs4a9-Zut3lgsbD6oE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:21.084+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:06.262+01:00", "Status": "Live on site", "Edit Answer": "What links are especially valuable to share on social media or other contexts?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8509", "Source Link": "", "aisafety.info Link": "What links are especially valuable to share on social media or other contexts?", "Source": "", "All Phrasings": "What links are especially valuable to share on social media or other contexts?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Sharing high quality information about AI Safety can be one of the lowest effort ways to expose people to the ideas. Be sure to engage with the replies with care and do your research when replying to questions people respond with (feel free to add them to aisafety.info for our team to work on).\n\n**Top 3:**\n\n- Introduction to AI safety by Robert Miles\n\n<iframe src=\"https://www.youtube.com/embed/pYXy-A4siMw\" title=\"Intro to AI Safety, Remastered\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n- Rational animations\n\n<iframe src=\"https://www.youtube.com/embed/3K25VPdbAjU\" title=\"Everything might change forever this century (or we\u2019ll go extinct)\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n- Article from Vox\n\n[The case for taking AI seriously as a threat to humanity](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment)\n\n**For Machine Learning researchers:**\n\n- [More is Different for AI](https://bounded-regret.ghost.io/more-is-different-for-ai/) by Jacob Steinhardt\n\n- [Researcher Perceptions of Current and Future AI](https://www.youtube.com/watch?v=yl2nlejBcg0) by Vael Gates (2022)\n\n- [\u201cWhy I Think More NLP Researchers Should Engage with AI Safety Concerns](https://wp.nyu.edu/arg/why-ai-safety/) by Sam Bowman (2022)\n\n**Orientation:**\n\n- [AI Safety Info](https://aisafety.info)\n\n- [AI Safety World](http://aisafety.world)\n\n**YouTube:**\n\n- Channels:\n\n    - [Robert Miles AI](https://www.youtube.com/@RobertMilesAI)\n\n    - [Apart Research](https://www.youtube.com/@apartresearch)\n\n- Specific videos/playlists:\n\n    - Computerphile's series with Robert Miles\n\n(youtube)tlS5Y2vm02c&list=PLzH6n4zXuckquVnQ0KlMDxyT5YE-sA8Ps&ab_channel(/youtube)\n\n- [Many more video recommendations](http://aisafety.video/)\n\n**Twitter:**\n\n- [AGI Safety Core](https://twitter.com/i/lists/1185207859728076800)\n\n**Online communities:**\n\n- [AI Alignment Slack](https://join.slack.com/t/ai-alignment/shared_invite/zt-1mekkroea-1aJIizeT7xVAdw6SH_d9vA)\n\n- [Alignment Jams](https://discord.gg/3PUSbdS8gY)\n\n- [EleutherAI](https://discord.gg/QnePG589qc)\n\n- [Other AI Safety Communities](https://aisafety.community/)\n\n**Reading lists:**\n\n- [AI Alignment Curriculum](https://www.agisafetyfundamentals.com/ai-alignment-curriculum)\n\n- [AI Safety Gauntlet](https://apartresearch.com/ai-safety-gauntlet)\n\n**Discussion Groups/Forums:**\n\n- [Apart Research](https://apartresearch.com/join)\n\n- [AI Alignment Forum](https://www.alignmentforum.org/posts/FoiiRDC3EhjHx7ayY/introducing-the-ai-alignment-forum-faq)\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Sharing high quality information about AI Safety can be one of the lowest effort ways to expose people to the ideas. Be sure to engage with the replies with care and do your research when replying to questions people respond with (feel free to add them to aisafety.info for our team to work on).\n\n**Top 3:**\n\n- Introduction to AI safety by Robert Miles\n\n<iframe src=\"https://www.youtube.com/embed/pYXy-A4siMw\" title=\"Intro to AI Safety, Remastered\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n- Rational animations\n\n<iframe src=\"https://www.youtube.com/embed/3K25VPdbAjU\" title=\"Everything might change forever this century (or we\u2019ll go extinct)\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n- Article from Vox\n\n[The case for taking AI seriously as a threat to humanity](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment)\n\n**For Machine Learning researchers:**\n\n- [More is Different for AI](https://bounded-regret.ghost.io/more-is-different-for-ai/) by Jacob Steinhardt\n\n- [Researcher Perceptions of Current and Future AI](https://www.youtube.com/watch?v=yl2nlejBcg0) by Vael Gates (2022)\n\n- [\u201cWhy I Think More NLP Researchers Should Engage with AI Safety Concerns](https://wp.nyu.edu/arg/why-ai-safety/) by Sam Bowman (2022)\n\n**Orientation:**\n\n- [AI Safety Info](https://aisafety.info)\n\n- [AI Safety World](http://aisafety.world)\n\n**YouTube:**\n\n- Channels:\n\n    - [Robert Miles AI](https://www.youtube.com/@RobertMilesAI)\n\n    - [Apart Research](https://www.youtube.com/@apartresearch)\n\n- Specific videos/playlists:\n\n    - Computerphile's series with Robert Miles\n\n(youtube)tlS5Y2vm02c&list=PLzH6n4zXuckquVnQ0KlMDxyT5YE-sA8Ps&ab_channel(/youtube)\n\n- [Many more video recommendations](http://aisafety.video/)\n\n**Twitter:**\n\n- [AGI Safety Core](https://twitter.com/i/lists/1185207859728076800)\n\n**Online communities:**\n\n- [AI Alignment Slack](https://join.slack.com/t/ai-alignment/shared_invite/zt-1mekkroea-1aJIizeT7xVAdw6SH_d9vA)\n\n- [Alignment Jams](https://discord.gg/3PUSbdS8gY)\n\n- [EleutherAI](https://discord.gg/QnePG589qc)\n\n- [Other AI Safety Communities](https://aisafety.community/)\n\n**Reading lists:**\n\n- [AI Alignment Curriculum](https://www.agisafetyfundamentals.com/ai-alignment-curriculum)\n\n- [AI Safety Gauntlet](https://apartresearch.com/ai-safety-gauntlet)\n\n**Discussion Groups/Forums:**\n\n- [Apart Research](https://apartresearch.com/join)\n\n- [AI Alignment Forum](https://www.alignmentforum.org/posts/FoiiRDC3EhjHx7ayY/introducing-the-ai-alignment-forum-faq)\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": "", "Asker": "", "External Source": "", "Last Asked On Discord": "", "UI ID": "8509", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:32:29.715+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 505, "Helpful": ""}}, {"id": "i-b4806c379a739bf9899957dd8123de7fd02f1b15ecb2fea00a64f7df0a9551a7", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b4806c379a739bf9899957dd8123de7fd02f1b15ecb2fea00a64f7df0a9551a7", "name": "What kind of a challenge is solving AI alignment?", "index": 305, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:22.268Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b4806c379a739bf9899957dd8123de7fd02f1b15ecb2fea00a64f7df0a9551a7", "values": {"File": "What kind of a challenge is solving AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What kind of a challenge is solving AI alignment?", "Link": "https://docs.google.com/document/d/1yPiSorN9WEuXYbHYq28j0bYin7xlJLA1_TIw9rTu7JA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:13.206+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Difficulty of Alignment", "Doc Last Edited": "2023-02-22T22:46:41.990+01:00", "Status": "Not started", "Edit Answer": "What kind of a challenge is solving AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7723", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "What kind of a challenge is solving AI alignment?", "Source": "LessWrong", "All Phrasings": "What kind of a challenge is solving AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7723", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:32:32.219+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 505, "Helpful": ""}}, {"id": "i-83eb9175d2e49f97694c9a145b06ccf5d343b0a480ca7f4f76a48138637114d8", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-83eb9175d2e49f97694c9a145b06ccf5d343b0a480ca7f4f76a48138637114d8", "name": "What is the probability of extinction from misaligned superintelligence?", "index": 306, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-16T07:07:26.302Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-83eb9175d2e49f97694c9a145b06ccf5d343b0a480ca7f4f76a48138637114d8", "values": {"File": "What is the probability of extinction from misaligned superintelligence?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the probability of extinction from misaligned superintelligence?", "Link": "https://docs.google.com/document/d/1Ggh8VJe7wTUFmoiiS3yNaXc3ISXtibIPaRbl6Or6Oe0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:09.927+01:00", "Related Answers DO NOT EDIT": "Why might a superintelligent AI be dangerous?,Why is AGI safety a hard problem?,What are the main sources of AI existential risk?", "Tags": "Doom", "Doc Last Edited": "2023-03-16T05:13:50.674+01:00", "Status": "In review", "Edit Answer": "What is the probability of extinction from misaligned superintelligence?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7715", "Source Link": "", "aisafety.info Link": "What is the probability of extinction from misaligned superintelligence?", "Source": "Wiki", "All Phrasings": "What is the probability of extinction from misaligned superintelligence?\n", "Initial Order": "", "Related IDs": "6968,6140,8503", "Rich Text DO NOT EDIT": "Extinction from misaligned superintelligence is a tricky event to put a probability on: we don\u2019t have a base rate of how many past civilizations like ours went extinct (whether from misaligned superintelligence or anything else), or a way to split all possible futures into a set of symmetrical and equally likely cases. That said, [various people have tried](https://forum.effectivealtruism.org/posts/9iGFjYnRquxiy29jm/safety-timelines-how-long-will-it-take-to-solve-alignment#P_doom__data) putting [numbers](https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results) on their informed guesses of the chance of superintelligence leading to [existential catastrophe](https://docs.google.com/document/d/1WDD0PYPhkZ2p10cNnPV544YFk69GgZrJtcVZC69kd2A/edit) (which includes extinction risk but also other scenarios where humanity loses most of its future potential), giving [estimates](https://astralcodexten.substack.com/p/why-i-am-not-as-much-of-a-doomer) ranging from under 1% to over 90%.\n\n[Eliezer Yudkowsky](https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) and [Nate Soares](https://www.alignmentforum.org/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential) at the Machine Intelligence Research Institute (MIRI) are examples of strongly pessimistic researchers who think extinction is the most probable outcome, while others, including [Paul Christiano](https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer) and [Katja Grace](https://www.alignmentforum.org/posts/LDRQ5Zfqwi8GjzPYG/counterarguments-to-the-basic-ai-x-risk-case), are more optimistic but still think there is substantial risk of extinction.\n\nA framework for calculating the probability of power-seeking AI causing an existential catastrophe was presented in a [report by Joe Carlsmith](https://www.alignmentforum.org/posts/HduCjmXTBD4xYTegv/draft-report-on-existential-risk-from-power-seeking-ai). The calculation involves multiplying factors like \"how likely are AI systems to be agentic?\" and \"how likely is a warning shot?\". Carlsmith gave a final estimate of >10%; various [reviewers](https://www.lesswrong.com/posts/qRSgHLb8yLXzDg4nf/reviews-of-is-power-seeking-ai-an-existential-risk) used the same model to come up with different probabilities.\n\nThough this is a wide range of estimates, almost all are worryingly high: even at the very low end, Ben Garfinkel, who has [estimated](https://docs.google.com/document/d/1FlGPHU3UtBRj4mBPkEZyBQmAuZXnyvHU-yaH-TiNt8w/edit) the existential risk from power-seeking AI by 2070 at 0.4%, [agrees](https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/#what-role-should-ai-play-in-the-ea-portfolio-022230) major efforts are justified to understand and reduce it.\n\n", "Tag Count": 1, "Related Answer Count": 3, "Rich Text": "Extinction from misaligned superintelligence is a tricky event to put a probability on: we don\u2019t have a base rate of how many past civilizations like ours went extinct (whether from misaligned superintelligence or anything else), or a way to split all possible futures into a set of symmetrical and equally likely cases. That said, [various people have tried](https://forum.effectivealtruism.org/posts/9iGFjYnRquxiy29jm/safety-timelines-how-long-will-it-take-to-solve-alignment#P_doom__data) putting [numbers](https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results) on their informed guesses of the chance of superintelligence leading to [existential catastrophe](https://docs.google.com/document/d/1WDD0PYPhkZ2p10cNnPV544YFk69GgZrJtcVZC69kd2A/edit) (which includes extinction risk but also other scenarios where humanity loses most of its future potential), giving [estimates](https://astralcodexten.substack.com/p/why-i-am-not-as-much-of-a-doomer) ranging from under 1% to over 90%.\n\n[Eliezer Yudkowsky](https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) and [Nate Soares](https://www.alignmentforum.org/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential) at the Machine Intelligence Research Institute (MIRI) are examples of strongly pessimistic researchers who think extinction is the most probable outcome, while others, including [Paul Christiano](https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer) and [Katja Grace](https://www.alignmentforum.org/posts/LDRQ5Zfqwi8GjzPYG/counterarguments-to-the-basic-ai-x-risk-case), are more optimistic but still think there is substantial risk of extinction.\n\nA framework for calculating the probability of power-seeking AI causing an existential catastrophe was presented in a [report by Joe Carlsmith](https://www.alignmentforum.org/posts/HduCjmXTBD4xYTegv/draft-report-on-existential-risk-from-power-seeking-ai). The calculation involves multiplying factors like \"how likely are AI systems to be agentic?\" and \"how likely is a warning shot?\". Carlsmith gave a final estimate of >10%; various [reviewers](https://www.lesswrong.com/posts/qRSgHLb8yLXzDg4nf/reviews-of-is-power-seeking-ai-an-existential-risk) used the same model to come up with different probabilities.\n\nThough this is a wide range of estimates, almost all are worryingly high: even at the very low end, Ben Garfinkel, who has [estimated](https://docs.google.com/document/d/1FlGPHU3UtBRj4mBPkEZyBQmAuZXnyvHU-yaH-TiNt8w/edit) the existential risk from power-seeking AI by 2070 at 0.4%, [agrees](https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/#what-role-should-ai-play-in-the-ea-portfolio-022230) major efforts are justified to understand and reduce it.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7715", "Related Answers": "Why might a superintelligent AI be dangerous?,Why is AGI safety a hard problem?,What are the main sources of AI existential risk?", "Doc Last Ingested": "2023-03-16T06:12:32.887+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-e777204e7429081996d8fbaaedaa00b49cf2d8457b89e35e71318c15233d55e2", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e777204e7429081996d8fbaaedaa00b49cf2d8457b89e35e71318c15233d55e2", "name": "What is the general nature of the concern about AI alignment?", "index": 307, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:25.518Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e777204e7429081996d8fbaaedaa00b49cf2d8457b89e35e71318c15233d55e2", "values": {"File": "What is the general nature of the concern about AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the general nature of the concern about AI alignment?", "Link": "https://docs.google.com/document/d/1SDwcSEY-DFHbI6SMrwfwtqiYBpCaJicMXekmL2Ex8A4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:05.933+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:07.673+01:00", "Status": "Live on site", "Edit Answer": "What is the general nature of the concern about AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6184", "Source Link": "https://futureoflife.org/ai-faqs/", "aisafety.info Link": "What is the general nature of the concern about AI alignment?", "Source": "FLI's FAQ", "All Phrasings": "What is the general nature of the concern about AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The basic concern as AI systems become increasingly powerful is that they won\u2019t do what we want them to do \u2013 perhaps because they aren\u2019t correctly designed, perhaps because they are deliberately subverted, or perhaps because they do what we tell them to do rather than what we really want them to do (like in the classic stories of genies and wishes.) Many AI systems are programmed to have goals and to attain them as effectively as possible \u2013 for example, a trading algorithm has the goal of maximizing profit. Unless carefully designed to act in ways consistent with human values, a highly sophisticated AI trading system might exploit means that even the most ruthless financier would disavow. These are systems that literally have a mind of their own, and maintaining alignment between human interests and their choices and actions will be crucial.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "The basic concern as AI systems become increasingly powerful is that they won\u2019t do what we want them to do \u2013 perhaps because they aren\u2019t correctly designed, perhaps because they are deliberately subverted, or perhaps because they do what we tell them to do rather than what we really want them to do (like in the classic stories of genies and wishes.) Many AI systems are programmed to have goals and to attain them as effectively as possible \u2013 for example, a trading algorithm has the goal of maximizing profit. Unless carefully designed to act in ways consistent with human values, a highly sophisticated AI trading system might exploit means that even the most ruthless financier would disavow. These are systems that literally have a mind of their own, and maintaining alignment between human interests and their choices and actions will be crucial.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "FLI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6184", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:32:38.954+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 505, "Helpful": ""}}, {"id": "i-d7e6caba38e901db4beb9f21b7258972a0a8d3213efc0e9de8dc671980590d4d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d7e6caba38e901db4beb9f21b7258972a0a8d3213efc0e9de8dc671980590d4d", "name": "What is the difference between inner and outer alignment?", "index": 308, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:27.041Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d7e6caba38e901db4beb9f21b7258972a0a8d3213efc0e9de8dc671980590d4d", "values": {"File": "What is the difference between inner and outer alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the difference between inner and outer alignment?", "Link": "https://docs.google.com/document/d/1TR5UYmFjwA-FAttyMR_vB5aXQu1gcLTwaHvY2v9ObCM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:02.334+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Inner Alignment", "Doc Last Edited": "2023-02-22T23:04:08.525+01:00", "Status": "Live on site", "Edit Answer": "What is the difference between inner and outer alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8428", "Source Link": "", "aisafety.info Link": "What is the difference between inner and outer alignment?", "Source": "Wiki", "All Phrasings": "What is the difference between inner and outer alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The paper [Risks from Learned Optimization in Advanced Machine Learning Systems](https://arxiv.org/abs/1906.01820) makes the distinction between inner and outer alignment: Outer alignment means making the optimization target of the *training process* (\u201couter optimization target\u201d e.g.\u00a0the *loss* in supervised learning) aligned with what we want. Inner alignment means making the optimization target of the *trained system* (\u201cinner optimization target\u201d) aligned with the outer optimization target. A challenge here is that the inner optimization target does not have an explicit representation in current systems, and can differ very much from the outer optimization target (see for example [Goal Misgeneralization in Deep Reinforcement Learning](https://arxiv.org/abs/2105.14111)).\n\n<iframe src=\"https://www.youtube.com/embed/bJLcIBixGj8\" title=\"The OTHER AI Alignment Problem: Mesa-Optimizers and Inner Alignment\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\nSee also [this article](https://astralcodexten.substack.com/p/deceptively-aligned-mesa-optimizers) for an intuitive explanation of inner and outer alignment.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "The paper [Risks from Learned Optimization in Advanced Machine Learning Systems](https://arxiv.org/abs/1906.01820) makes the distinction between inner and outer alignment: Outer alignment means making the optimization target of the *training process* (\u201couter optimization target\u201d e.g.\u00a0the *loss* in supervised learning) aligned with what we want. Inner alignment means making the optimization target of the *trained system* (\u201cinner optimization target\u201d) aligned with the outer optimization target. A challenge here is that the inner optimization target does not have an explicit representation in current systems, and can differ very much from the outer optimization target (see for example [Goal Misgeneralization in Deep Reinforcement Learning](https://arxiv.org/abs/2105.14111)).\n\n<iframe src=\"https://www.youtube.com/embed/bJLcIBixGj8\" title=\"The OTHER AI Alignment Problem: Mesa-Optimizers and Inner Alignment\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\nSee also [this article](https://astralcodexten.substack.com/p/deceptively-aligned-mesa-optimizers) for an intuitive explanation of inner and outer alignment.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "Magdalena", "External Source": "", "Last Asked On Discord": "", "UI ID": "8428", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:32:41.531+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 505, "Helpful": ""}}, {"id": "i-ad504e416ada98ed6051463d873f4d65cab482e31183482e03275023ab98ac3b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ad504e416ada98ed6051463d873f4d65cab482e31183482e03275023ab98ac3b", "name": "What is Stampy's AI Safety Info?", "index": 309, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:28.532Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ad504e416ada98ed6051463d873f4d65cab482e31183482e03275023ab98ac3b", "values": {"File": "What is Stampy's AI Safety Info?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Stampy's AI Safety Info?", "Link": "https://docs.google.com/document/d/1kn9Wp8dxxRxUEXPMxsJnlBHhEHRm45fNwlusst8mLyo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:58.060+01:00", "Related Answers DO NOT EDIT": "How can I contribute to Stampy's AI Safety Info?", "Tags": "Stampy", "Doc Last Edited": "2023-02-22T23:04:09.441+01:00", "Status": "Live on site", "Edit Answer": "What is Stampy's AI Safety Info?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6436", "Source Link": "", "aisafety.info Link": "What is Stampy's AI Safety Info?", "Source": "Wiki", "All Phrasings": "What is Stampy's AI Safety Info?\n", "Initial Order": "", "Related IDs": "6441", "Rich Text DO NOT EDIT": "This is an open effort to build[a comprehensive FAQ](https://aisafety.info/) about[artificial intelligence existential safety](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence)\u2014the field trying to make sure that when we build[superintelligent](https://en.wikipedia.org/wiki/Superintelligence)[artificial systems](https://www.alignmentforum.org/tag/ai) they are[aligned](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/) with[human values](https://www.lesswrong.com/tag/human-values) so that they do things compatible with our survival and flourishing.\n\nThe goals of the project are to:\n\n- Offer a one-stop-shop for high-quality answers to common questions about AI alignment.\n\n    - Let people answer questions in a way which scales, freeing up researcher time while allowing more people to learn from a reliable source.\n\n    - Make[external resources](https://aisafety.info/?state=6470_) more easy to find by having links to them connected to a search engine which gets smarter the more it's used.\n\n- Provide a form of[legitimate peripheral participation](https://en.wikipedia.org/wiki/Legitimate_peripheral_participation) for the AI Safety community, as an on-boarding path with a flexible level of commitment.\n\n    - Encourage people to think, read, and talk about AI alignment while answering questions, creating a community of co-learners who can give each other feedback and social reinforcement.\n\n    - Provide a way for budding researchers to prove their understanding of the topic and ability to produce good work.\n\n- Collect data about the kinds of questions people actually ask and how they respond, so we can better focus resources on answering them.\n\n    - Track reactions on messages so we can learn which answers need work.\n\n    - Identify missing external content to create.\n\nIf you would like to help out, feel free to join us on the[Discord](https://discord.gg/vjFSCDyMCy) and jump right into editing some[answers](https://coda.io/d/_dfau7sl2hmG/_suYta), or read on for more details about the project.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "This is an open effort to build[a comprehensive FAQ](https://aisafety.info/) about[artificial intelligence existential safety](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence)\u2014the field trying to make sure that when we build[superintelligent](https://en.wikipedia.org/wiki/Superintelligence)[artificial systems](https://www.alignmentforum.org/tag/ai) they are[aligned](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/) with[human values](https://www.lesswrong.com/tag/human-values) so that they do things compatible with our survival and flourishing.\n\nThe goals of the project are to:\n\n- Offer a one-stop-shop for high-quality answers to common questions about AI alignment.\n\n    - Let people answer questions in a way which scales, freeing up researcher time while allowing more people to learn from a reliable source.\n\n    - Make[external resources](https://aisafety.info/?state=6470_) more easy to find by having links to them connected to a search engine which gets smarter the more it's used.\n\n- Provide a form of[legitimate peripheral participation](https://en.wikipedia.org/wiki/Legitimate_peripheral_participation) for the AI Safety community, as an on-boarding path with a flexible level of commitment.\n\n    - Encourage people to think, read, and talk about AI alignment while answering questions, creating a community of co-learners who can give each other feedback and social reinforcement.\n\n    - Provide a way for budding researchers to prove their understanding of the topic and ability to produce good work.\n\n- Collect data about the kinds of questions people actually ask and how they respond, so we can better focus resources on answering them.\n\n    - Track reactions on messages so we can learn which answers need work.\n\n    - Identify missing external content to create.\n\nIf you would like to help out, feel free to join us on the[Discord](https://discord.gg/vjFSCDyMCy) and jump right into editing some[answers](https://coda.io/d/_dfau7sl2hmG/_suYta), or read on for more details about the project.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6436", "Related Answers": "How can I contribute to Stampy's AI Safety Info?", "Doc Last Ingested": "2023-03-14T23:32:44.707+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 505, "Helpful": ""}}, {"id": "i-6907d2ee6f56de96e0349d8805908ba443c9e16a596109a676279a92611b59b5", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-6907d2ee6f56de96e0349d8805908ba443c9e16a596109a676279a92611b59b5", "name": "What is the DeepMind's safety team working on?", "index": 310, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:30.062Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-6907d2ee6f56de96e0349d8805908ba443c9e16a596109a676279a92611b59b5", "values": {"File": "What is the DeepMind's safety team working on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the DeepMind's safety team working on?", "Link": "https://docs.google.com/document/d/1C1d5QyO2Q0mT--K9ykKd7N3gXOwkkumbyP0Ld30EHpc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:54.425+01:00", "Related Answers DO NOT EDIT": "", "Tags": "DeepMind", "Doc Last Edited": "2023-02-22T23:04:10.314+01:00", "Status": "Live on site", "Edit Answer": "What is the DeepMind's safety team working on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8343", "Source Link": "", "aisafety.info Link": "What is the DeepMind's safety team working on?", "Source": "Wiki", "All Phrasings": "What is the DeepMind's safety team working on?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "DeepMind has both a [ML safety team focused on near-term risks](https://80000hours.org/podcast/episodes/pushmeet-kohli-deepmind-safety-research/#long-term-agi-safety-research), and an alignment team that is working on risks from AGI. The alignment team is pursuing many different research avenues, and is not best described by a single agenda.\n\nSome of the work they are doing is:\n\n- [Engaging with recent MIRI arguments](https://www.lesswrong.com/posts/qJgz2YapqpFEDTLKn/deepmind-alignment-team-opinions-on-agi-ruin-arguments).\n\n- Rohin Shah produces the [alignment newsletter](https://rohinshah.com/alignment-newsletter/).\n\n- Publishing interesting research like the [Goal Misgeneralization paper](https://arxiv.org/abs/2105.14111).\n\n- Geoffrey Irving is working on debate as an alignment strategy: [more detail here](https://www.lesswrong.com/posts/bLr68nrLSwgzqLpzu/axrp-episode-16-preparing-for-debate-ai-with-geoffrey-irving).\n\n- [Discovering agents](https://www.lesswrong.com/posts/XxX2CAoFskuQNkBDy/discovering-agents), which introduces a causal definition of agents, then introduces an algorithm for finding agents from empirical data.\n\nSee [Rohin's comment](https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is?commentId=CS9qcdkmDbLHR89s2) for more research that they are doing, including description of some that is currently unpublished so far.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "DeepMind has both a [ML safety team focused on near-term risks](https://80000hours.org/podcast/episodes/pushmeet-kohli-deepmind-safety-research/#long-term-agi-safety-research), and an alignment team that is working on risks from AGI. The alignment team is pursuing many different research avenues, and is not best described by a single agenda.\n\nSome of the work they are doing is:\n\n- [Engaging with recent MIRI arguments](https://www.lesswrong.com/posts/qJgz2YapqpFEDTLKn/deepmind-alignment-team-opinions-on-agi-ruin-arguments).\n\n- Rohin Shah produces the [alignment newsletter](https://rohinshah.com/alignment-newsletter/).\n\n- Publishing interesting research like the [Goal Misgeneralization paper](https://arxiv.org/abs/2105.14111).\n\n- Geoffrey Irving is working on debate as an alignment strategy: [more detail here](https://www.lesswrong.com/posts/bLr68nrLSwgzqLpzu/axrp-episode-16-preparing-for-debate-ai-with-geoffrey-irving).\n\n- [Discovering agents](https://www.lesswrong.com/posts/XxX2CAoFskuQNkBDy/discovering-agents), which introduces a causal definition of agents, then introduces an algorithm for finding agents from empirical data.\n\nSee [Rohin's comment](https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is?commentId=CS9qcdkmDbLHR89s2) for more research that they are doing, including description of some that is currently unpublished so far.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8343", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:32:51.343+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 505, "Helpful": ""}}, {"id": "i-76516be9d130a42d0c8d38f022fc3c328a82a90e09b24f470fe2b805f7372cb0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-76516be9d130a42d0c8d38f022fc3c328a82a90e09b24f470fe2b805f7372cb0", "name": "What is the Center on Long-Term Risk (CLR) focused on?", "index": 311, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:49:59.472Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-76516be9d130a42d0c8d38f022fc3c328a82a90e09b24f470fe2b805f7372cb0", "values": {"File": "What is the Center on Long-Term Risk (CLR) focused on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the Center on Long-Term Risk (CLR) focused on?", "Link": "https://docs.google.com/document/d/1YFqph3sYdRlVWf3n2GtToqTWyrFO1a24q1my6kDH2hE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:50.885+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Organizations,S-risk,CLR", "Doc Last Edited": "2023-02-22T23:04:11.208+01:00", "Status": "Live on site", "Edit Answer": "What is the Center on Long-Term Risk (CLR) focused on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8333", "Source Link": "", "aisafety.info Link": "What is the Center on Long-Term Risk (CLR) focused on?", "Source": "Wiki", "All Phrasings": "What is the Center on Long-Term Risk (CLR) focused on?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "CLR is focused primarily on reducing suffering-risk (s-risk), where the future has a large negative value. They do foundational research in game theory / decision theory, primarily aimed at multipolar AI scenarios. One result relevant to this work is that [transparency can increase cooperation](https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/abs/parametric-resourcebounded-generalization-of-lobs-theorem-and-a-robust-cooperation-criterion-for-opensource-game-theory/16063EA7BFFEE89438631B141E556E79).\n\n[Update after Jesse Clifton commented](https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is?commentId=mqiYR6X8bgY5wKdme): CLR also works on improving coordination for prosaic AI scenarios, [risks from malevolent actors](https://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors) and [AI forecasting](https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like). The [Cooperative AI Foundation (CAIF)](https://www.cooperativeai.com/foundation) shares personnel with CLR, but is not formally affiliated with CLR, and does not focus just on s-risks.\n\n", "Tag Count": 3, "Related Answer Count": 0, "Rich Text": "CLR is focused primarily on reducing suffering-risk (s-risk), where the future has a large negative value. They do foundational research in game theory / decision theory, primarily aimed at multipolar AI scenarios. One result relevant to this work is that [transparency can increase cooperation](https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/abs/parametric-resourcebounded-generalization-of-lobs-theorem-and-a-robust-cooperation-criterion-for-opensource-game-theory/16063EA7BFFEE89438631B141E556E79).\n\n[Update after Jesse Clifton commented](https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is?commentId=mqiYR6X8bgY5wKdme): CLR also works on improving coordination for prosaic AI scenarios, [risks from malevolent actors](https://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors) and [AI forecasting](https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like). The [Cooperative AI Foundation (CAIF)](https://www.cooperativeai.com/foundation) shares personnel with CLR, but is not formally affiliated with CLR, and does not focus just on s-risks.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8333", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:38:45.890+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-95345e07979548c24fb2f4bf4a5e0cf4c050dad7d3e833955bcedaa47cbacc5d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-95345e07979548c24fb2f4bf4a5e0cf4c050dad7d3e833955bcedaa47cbacc5d", "name": "What is the Center for Human Compatible AI (CHAI)?", "index": 312, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:33.132Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-95345e07979548c24fb2f4bf4a5e0cf4c050dad7d3e833955bcedaa47cbacc5d", "values": {"File": "What is the Center for Human Compatible AI (CHAI)?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the Center for Human Compatible AI (CHAI)?", "Link": "https://docs.google.com/document/d/1jItYFB2AW6oHRBO4kUNgUzHOj3QDyAY299oc-kRzTkQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:47.293+01:00", "Related Answers DO NOT EDIT": "What is neural network modularity?", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:11.993+01:00", "Status": "Live on site", "Edit Answer": "What is the Center for Human Compatible AI (CHAI)?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8327", "Source Link": "", "aisafety.info Link": "What is the Center for Human Compatible AI (CHAI)?", "Source": "Wiki", "All Phrasings": "What is the Center for Human Compatible AI (CHAI)?\n", "Initial Order": "", "Related IDs": "8424", "Rich Text DO NOT EDIT": "CHAI is an academic research organization affiliated with UC Berkeley. It is lead by Stuart Russell, but includes many other professors and grad students pursuing a diverse array of approaches, most of whom are not yet listed here. For more information see their [2022 progress report](https://humancompatible.ai/app/uploads/2022/05/CHAI-2022-Progress-Report-3.pdf).\n\nStuart wrote the book [Human Compatible](https://en.wikipedia.org/wiki/Human_Compatible), in which he outlines his AGI alignment strategy, which is based on cooperative inverse reinforcement learning (CIRL). The basic idea of CIRL is to play a cooperative game where both the agent and the human are trying to maximize the human's reward, but only the human knows what the human reward is. Since the AGI has uncertainty it will defer to humans and be corrigible.\n\nOther work includes [Clusterability in neural networks](https://arxiv.org/abs/2103.03386): try to measure the modularity of neural networks by thinking of the network as a graph and performing the graph n-cut.\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "CHAI is an academic research organization affiliated with UC Berkeley. It is lead by Stuart Russell, but includes many other professors and grad students pursuing a diverse array of approaches, most of whom are not yet listed here. For more information see their [2022 progress report](https://humancompatible.ai/app/uploads/2022/05/CHAI-2022-Progress-Report-3.pdf).\n\nStuart wrote the book [Human Compatible](https://en.wikipedia.org/wiki/Human_Compatible), in which he outlines his AGI alignment strategy, which is based on cooperative inverse reinforcement learning (CIRL). The basic idea of CIRL is to play a cooperative game where both the agent and the human are trying to maximize the human's reward, but only the human knows what the human reward is. Since the AGI has uncertainty it will defer to humans and be corrigible.\n\nOther work includes [Clusterability in neural networks](https://arxiv.org/abs/2103.03386): try to measure the modularity of neural networks by thinking of the network as a graph and performing the graph n-cut.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8327", "Related Answers": "What is neural network modularity?", "Doc Last Ingested": "2023-03-14T23:32:56.813+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 505, "Helpful": ""}}, {"id": "i-0478c161febeab922264e8f0ba404c31e7e3d53c79d498796ba9c5c326981269", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0478c161febeab922264e8f0ba404c31e7e3d53c79d498796ba9c5c326981269", "name": "What is the \"windfall clause\"?", "index": 313, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:34.925Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0478c161febeab922264e8f0ba404c31e7e3d53c79d498796ba9c5c326981269", "values": {"File": "What is the \"windfall clause\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the \"windfall clause\"?", "Link": "https://docs.google.com/document/d/1JfjOgbDSa25bJqTXpGuHo33nC9PwtLEUOEgFmMypABc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:42.993+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:13.000+01:00", "Status": "Live on site", "Edit Answer": "What is the \"windfall clause\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7648", "Source Link": "", "aisafety.info Link": "What is the \"windfall clause\"?", "Source": "Wiki", "All Phrasings": "What is the \"windfall clause\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The windfall clause is pretty well explained [on the Future of Humanity Institute site](https://www.fhi.ox.ac.uk/windfallclause/).\n\nHere's a quick summary: It is an agreement between AI firms to donate significant amounts of any profits made as a consequence of economically transformative breakthroughs in AI capabilities. The donations are intended to help benefit humanity.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "The windfall clause is pretty well explained [on the Future of Humanity Institute site](https://www.fhi.ox.ac.uk/windfallclause/).\n\nHere's a quick summary: It is an agreement between AI firms to donate significant amounts of any profits made as a consequence of economically transformative breakthroughs in AI capabilities. The donations are intended to help benefit humanity.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7648", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:32:59.794+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 505, "Helpful": ""}}, {"id": "i-99c67e7cd6eddfc37d092e76a32432b936fabed39a37cd6596333a4f953f07c3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-99c67e7cd6eddfc37d092e76a32432b936fabed39a37cd6596333a4f953f07c3", "name": "What is the \"universal prior\"?", "index": 314, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:50:03.628Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-99c67e7cd6eddfc37d092e76a32432b936fabed39a37cd6596333a4f953f07c3", "values": {"File": "What is the \"universal prior\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the \"universal prior\"?", "Link": "https://docs.google.com/document/d/1HpEr5DaOkU8S1XYheH6yJ4w-YxVt7GctrJMdlvF7VIw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:39.513+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions", "Doc Last Edited": "2023-02-28T00:07:14.750+01:00", "Status": "In progress", "Edit Answer": "What is the \"universal prior\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7776", "Source Link": "", "aisafety.info Link": "What is the \"universal prior\"?", "Source": "Wiki", "All Phrasings": "What is the \"universal prior\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Use arbitals page\n\n- [https://arbital.greaterwrong.com/p/universal_prior?l=4mr](https://arbital.greaterwrong.com/p/universal_prior?l=4mr)\n\n- \n\n- [https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/)\n\n- \n\n- \n\n- Rob thinks it probably doesn\u2019t matter since infinity doesn\u2019t exist\n\n- Reasoning about this kind of thing is hard\n\n[https://www.lesswrong.com/posts/RKfg86eKQuqLnjGxx/occam-s-razor-and-the-universal-prior](https://www.lesswrong.com/posts/RKfg86eKQuqLnjGxx/occam-s-razor-and-the-universal-prior)\n\n## 1. The UP defines the probabilities you should assign to hypotheses before receiving any evidence  \n\nWell that\u2019s the pitch, at least. We can go into a bit more detail.\n\nUnder a Bayesian account of reasoning, we start by assigning a \u2018prior\u2019 probability to every hypothesis, before we\u2019ve seen any evidence. This leaves the following question open: what should this prior look like, and [how do we determine the appropriate prior to have?](https://plato.stanford.edu/entries/epistemology-bayesian/#SyncNormIIProbPrio)\n\nHere\u2019s one answer: our prior should be the *universal prior*, a very special kind of prior.\n\nWe can introduce the universal prior, initially, by imagining a Universal Turing Machine. It has the following properties:\n\n- There is a read-only input tape, moving only in one direction.\n\n- There is a write-only output tape, moving only in one direction.\n\n- There is a read/write work tape, moving only in both directions.\n\nAnd we\u2019ll suppose that there are three possible symbols for the tape: \u20180\u2019, \u20181\u2019, and \u2018X\u2019. All tapes start out filled with zeros, unless we specify some other input, or else the input is rewritten by our machine.\n\nSo, what does our machine do? We\u2019ll suppose:\n\n1. First, it reads the input tape, until it sees the first \u2018X\u2019.\n\n1. Upon seeing \u2018X\u2019, the machine stops reading the tape, and outputs a particular string.\n\n1. If the input, let\u2019s say\u2019, is the string \u201800100X1011\u2019, then the *output* would be \u201800100X\u2019.\n\n Let\u2019s suppose, now, that we want to ask the following question:\n\nWhat\u2019s the probability that the next string will start with a given sequence? The universal prior says: \u201cassume a uniform prior over all possible input strings, like, for example: \u20181010X\u2019?\n\nWe don\u2019t know anything about the input, so it seems fair to say that each cell on our tape has equal probability. That is, the \u201d.\n\n## 2. Huh, this all sounds pretty abstract. Why does this matter? \n\nIt\u2019s a good question. Some people dispute whether the universal prior has any practical relevance. Others worry that advanced AI systems will approximate Bayesian updating on the universal prior, which might lead to [weird consequences](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/).\n\nIn order to discuss why this matters, we\u2019ll first look at attractive theoretical properties of the universal prior. Readers impatient with theoretical discussions may skip to the final section, which outlines why we might be concerned with future AIs who reason from the universal prior as a starting point.\n\nNow, our task in this section is to motivate the case for believing that highly capable AI systems will actually behave in a way that looks like Bayesian updating on the universal prior. Why would they want to do so?\n\nLet\u2019s start with an assumption: the universe is computable. That is, there\u2019s a finite number of steps that we could execute in order to answer the question \u201cwhat will I observe next?\u201d.\n\n## 3. Can we actually talk about AI safety now please?\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "- Use arbitals page\n\n- [https://arbital.greaterwrong.com/p/universal_prior?l=4mr](https://arbital.greaterwrong.com/p/universal_prior?l=4mr)\n\n- \n\n- [https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/)\n\n- \n\n- \n\n- Rob thinks it probably doesn\u2019t matter since infinity doesn\u2019t exist\n\n- Reasoning about this kind of thing is hard\n\n[https://www.lesswrong.com/posts/RKfg86eKQuqLnjGxx/occam-s-razor-and-the-universal-prior](https://www.lesswrong.com/posts/RKfg86eKQuqLnjGxx/occam-s-razor-and-the-universal-prior)\n\n## 1. The UP defines the probabilities you should assign to hypotheses before receiving any evidence  \n\nWell that\u2019s the pitch, at least. We can go into a bit more detail.\n\nUnder a Bayesian account of reasoning, we start by assigning a \u2018prior\u2019 probability to every hypothesis, before we\u2019ve seen any evidence. This leaves the following question open: what should this prior look like, and [how do we determine the appropriate prior to have?](https://plato.stanford.edu/entries/epistemology-bayesian/#SyncNormIIProbPrio)\n\nHere\u2019s one answer: our prior should be the *universal prior*, a very special kind of prior.\n\nWe can introduce the universal prior, initially, by imagining a Universal Turing Machine. It has the following properties:\n\n- There is a read-only input tape, moving only in one direction.\n\n- There is a write-only output tape, moving only in one direction.\n\n- There is a read/write work tape, moving only in both directions.\n\nAnd we\u2019ll suppose that there are three possible symbols for the tape: \u20180\u2019, \u20181\u2019, and \u2018X\u2019. All tapes start out filled with zeros, unless we specify some other input, or else the input is rewritten by our machine.\n\nSo, what does our machine do? We\u2019ll suppose:\n\n1. First, it reads the input tape, until it sees the first \u2018X\u2019.\n\n1. Upon seeing \u2018X\u2019, the machine stops reading the tape, and outputs a particular string.\n\n1. If the input, let\u2019s say\u2019, is the string \u201800100X1011\u2019, then the *output* would be \u201800100X\u2019.\n\n Let\u2019s suppose, now, that we want to ask the following question:\n\nWhat\u2019s the probability that the next string will start with a given sequence? The universal prior says: \u201cassume a uniform prior over all possible input strings, like, for example: \u20181010X\u2019?\n\nWe don\u2019t know anything about the input, so it seems fair to say that each cell on our tape has equal probability. That is, the \u201d.\n\n## 2. Huh, this all sounds pretty abstract. Why does this matter? \n\nIt\u2019s a good question. Some people dispute whether the universal prior has any practical relevance. Others worry that advanced AI systems will approximate Bayesian updating on the universal prior, which might lead to [weird consequences](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/).\n\nIn order to discuss why this matters, we\u2019ll first look at attractive theoretical properties of the universal prior. Readers impatient with theoretical discussions may skip to the final section, which outlines why we might be concerned with future AIs who reason from the universal prior as a starting point.\n\nNow, our task in this section is to motivate the case for believing that highly capable AI systems will actually behave in a way that looks like Bayesian updating on the universal prior. Why would they want to do so?\n\nLet\u2019s start with an assumption: the universe is computable. That is, there\u2019s a finite number of steps that we could execute in order to answer the question \u201cwhat will I observe next?\u201d.\n\n## 3. Can we actually talk about AI safety now please?\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "2023-02-26T18:55:44.133+01:00", "UI ID": "7776", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:38:48.089+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-60d0fb0c57b4b421af37539da368f2e8f3ba3c9ed751976171829589f180e6bd", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-60d0fb0c57b4b421af37539da368f2e8f3ba3c9ed751976171829589f180e6bd", "name": "What is the \"orthogonality thesis\"?", "index": 315, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:38.728Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-60d0fb0c57b4b421af37539da368f2e8f3ba3c9ed751976171829589f180e6bd", "values": {"File": "What is the \"orthogonality thesis\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the \"orthogonality thesis\"?", "Link": "https://docs.google.com/document/d/1_O_cQ4eKcPR-tng_vNhxf-TC8cT0NOfKs_AMph7j5M4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:35.616+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Orthogonality Thesis", "Doc Last Edited": "2023-02-22T23:04:14.135+01:00", "Status": "Live on site", "Edit Answer": "What is the \"orthogonality thesis\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6568", "Source Link": "", "aisafety.info Link": "What is the \"orthogonality thesis\"?", "Source": "Wiki", "All Phrasings": "What is the \"orthogonality thesis\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The **Orthogonality Thesis** states that an agent can have any combination of intelligence level and final goal, that is, its [final goals](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true) and [intelligence levels](https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&useTagName=true) can vary independently of each other. This is in contrast to the belief that, because of their intelligence, AIs will all converge to a common goal.\n\nThe thesis was originally defined by [Nick Bostrom](https://lessestwrong.com/tag/nick-bostrom) in the paper \"[Superintelligent Will](https://nickbostrom.com/superintelligentwill.pdf)\", (along with the [instrumental convergence thesis](https://wiki.lesswrong.com/wiki/instrumental_convergence_thesis)). For his purposes, Bostrom defines intelligence to be [instrumental rationality](https://wiki.lesswrong.com/wiki/instrumental_rationality).\n\n*Related:* [*Complexity of Value*](https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&useTagName=true)*,* [*Decision Theory*](https://www.lesswrong.com/tag/decision-theory?showPostCount=true&useTagName=true)*,* [*General Intelligence*](https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&useTagName=true)*,* [*Utility Functions*](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true)\n\nDefense of the thesis\n---------------------\n\nIt has been pointed out that the orthogonality thesis is the default position, and that the burden of proof is on claims that limit possible AIs. Stuart Armstrong writes that,\n\nOne reason many researchers assume superintelligent agents to converge to the same goals may be because [most humans](https://lessestwrong.com/tag/human-universal) have similar values. Furthermore, many philosophies hold that there is a rationally correct morality, which implies that a sufficiently rational AI will acquire this morality and begin to act according to it. Armstrong points out that for formalizations of AI such as [AIXI](https://lessestwrong.com/tag/aixi) and [G\u00f6del machines](https://lessestwrong.com/tag/g%C3%B6del-machine), the thesis is known to be true. Furthermore, if the thesis was false, then [Oracle AIs](https://lessestwrong.com/tag/oracle-ai) would be impossible to build, and all sufficiently intelligent AIs would be impossible to control.\n\nPathological Cases\n------------------\n\nThere are some pairings of intelligence and goals which cannot exist. For instance, an AI may have the goal of using as little resources as possible, or simply of being as unintelligent as possible. These goals will inherently limit the degree of intelligence of the AI.\n\nSee Also\n--------\n\n*   [Instrumental Convergence](https://lessestwrong.com/tag/instrumental-convergence)\n\nExternal links\n--------------\n\n*   Definition of the orthogonality thesis from Bostrom's [Superintelligent Will](http://www.nickbostrom.com/superintelligentwill.pdf)\n*   [Arbital orthogonality thesis article\u00a0](https://arbital.com/p/orthogonality/)\n*   [Critique](http://philosophicaldisquisitions.blogspot.com/2012/04/bostrom-on-superintelligence-and.html) of the thesis by John Danaher\n*   Superintelligent Will paper by Nick Bostrom", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "The **Orthogonality Thesis** states that an agent can have any combination of intelligence level and final goal, that is, its [final goals](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true) and [intelligence levels](https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&useTagName=true) can vary independently of each other. This is in contrast to the belief that, because of their intelligence, AIs will all converge to a common goal.\n\nThe thesis was originally defined by [Nick Bostrom](https://lessestwrong.com/tag/nick-bostrom) in the paper \"[Superintelligent Will](https://nickbostrom.com/superintelligentwill.pdf)\", (along with the [instrumental convergence thesis](https://wiki.lesswrong.com/wiki/instrumental_convergence_thesis)). For his purposes, Bostrom defines intelligence to be [instrumental rationality](https://wiki.lesswrong.com/wiki/instrumental_rationality).\n\n*Related:* [*Complexity of Value*](https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&useTagName=true)*,* [*Decision Theory*](https://www.lesswrong.com/tag/decision-theory?showPostCount=true&useTagName=true)*,* [*General Intelligence*](https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&useTagName=true)*,* [*Utility Functions*](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true)\n\nDefense of the thesis\n---------------------\n\nIt has been pointed out that the orthogonality thesis is the default position, and that the burden of proof is on claims that limit possible AIs. Stuart Armstrong writes that,\n\nOne reason many researchers assume superintelligent agents to converge to the same goals may be because [most humans](https://lessestwrong.com/tag/human-universal) have similar values. Furthermore, many philosophies hold that there is a rationally correct morality, which implies that a sufficiently rational AI will acquire this morality and begin to act according to it. Armstrong points out that for formalizations of AI such as [AIXI](https://lessestwrong.com/tag/aixi) and [G\u00f6del machines](https://lessestwrong.com/tag/g%C3%B6del-machine), the thesis is known to be true. Furthermore, if the thesis was false, then [Oracle AIs](https://lessestwrong.com/tag/oracle-ai) would be impossible to build, and all sufficiently intelligent AIs would be impossible to control.\n\nPathological Cases\n------------------\n\nThere are some pairings of intelligence and goals which cannot exist. For instance, an AI may have the goal of using as little resources as possible, or simply of being as unintelligent as possible. These goals will inherently limit the degree of intelligence of the AI.\n\nSee Also\n--------\n\n*   [Instrumental Convergence](https://lessestwrong.com/tag/instrumental-convergence)\n\nExternal links\n--------------\n\n*   Definition of the orthogonality thesis from Bostrom's [Superintelligent Will](http://www.nickbostrom.com/superintelligentwill.pdf)\n*   [Arbital orthogonality thesis article\u00a0](https://arbital.com/p/orthogonality/)\n*   [Critique](http://philosophicaldisquisitions.blogspot.com/2012/04/bostrom-on-superintelligence-and.html) of the thesis by John Danaher\n*   Superintelligent Will paper by Nick Bostrom", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "XorAnder22", "External Source": "https://www.lesswrong.com/tag/orthogonality-thesis?edit=true", "Last Asked On Discord": "", "UI ID": "6568", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:06.102+01:00", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-ec7a6d03ba982c6362e6ce5aa089338262258d129a805deb18dca7d9b4e2abe9", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ec7a6d03ba982c6362e6ce5aa089338262258d129a805deb18dca7d9b4e2abe9", "name": "What is the \"long reflection\"?", "index": 316, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:40.741Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ec7a6d03ba982c6362e6ce5aa089338262258d129a805deb18dca7d9b4e2abe9", "values": {"File": "What is the \"long reflection\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the \"long reflection\"?", "Link": "https://docs.google.com/document/d/1BmgUHWZEVY85fg4z2JwplFmecxJnqfEm-9XZYaCQWaE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:32.255+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:15.030+01:00", "Status": "Live on site", "Edit Answer": "What is the \"long reflection\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7757", "Source Link": "", "aisafety.info Link": "What is the \"long reflection\"?", "Source": "Wiki", "All Phrasings": "What is the \"long reflection\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The **long reflection** is a hypothesized period of time during which humanity works out how best to realize its long-term potential.\n\nSome effective altruists, including [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) and [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill), have argued that, if humanity succeeds in eliminating [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) or reducing it to acceptable levels, it should not immediately embark on an ambitious and potentially irreversible project of arranging the [universe's resources](https://forum.effectivealtruism.org/tag/universe-s-resources) in accordance to its values, but ought instead to spend considerable time\u2014 \"centuries (or more)\";[^ipthilteu8]\u00a0\"perhaps tens of thousands of years\";[^w82ait03vnf]\u00a0\"thousands or millions of years\";[^r3gb671cl2]\u00a0\"\\[p\\]erhaps... a million years\"[^7t0vu6w86yd]\u2014figuring out what is in fact of value. The long reflection may thus be seen as an intermediate stage in a rational long-term human developmental trajectory, following an initial stage of [existential security](https://forum.effectivealtruism.org/tag/existential-security) when existential risk is drastically reduced and followed by a final stage when humanity's potential is fully realized.[^ipthilteu8]\n\nCriticism\n---------\n\nThe idea of a long reflection has been criticized on the grounds that virtually eliminating all existential risk will almost certainly require taking a variety of large-scale, irreversible decisions\u2014related to [space colonization](https://forum.effectivealtruism.org/tag/space-colonization), [global governance](https://forum.effectivealtruism.org/tag/global-governance), [cognitive enhancement](https://forum.effectivealtruism.org/tag/cognitive-enhancement), and so on\u2014which are precisely the decisions meant to be discussed during the long reflection.[^d395d9czg1k][^8t05x5yy03]\u00a0Since there are pervasive and inescapable tradeoffs between reducing existential risk and retaining moral option value, it may be argued that it does not make sense to frame humanity's long-term strategic picture as one consisting of two distinct stages, with one taking precedence over the other.\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of sources that are highly relevant to the idea of the Long Reflection](https://forum.effectivealtruism.org/posts/H2zno3ggRJaph9P6c/quotes-about-the-long-reflection?commentId=z2ybSC353mPHpCjbn), *Effective Altruism Forum*, June 20.  \n*Many additional resources on this topic.*\n\nWiblin, Robert & Keiran Harris (2018) [Our descendants will probably see us as moral monsters. what should we do about that?](https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/), *80,000 Hours*, January 19.  \n*Interview with William MacAskill about the long reflection and other topics.*\n\nRelated entries\n---------------\n\n[dystopia](https://forum.effectivealtruism.org/tag/dystopia) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [existential security](https://forum.effectivealtruism.org/tag/existential-security) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform) | [moral uncertainty](https://forum.effectivealtruism.org/tag/moral-uncertainty) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) | [value lock-in](https://forum.effectivealtruism.org/tag/value-lock-in)\n\n[^ipthilteu8]: Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n    \n[^w82ait03vnf]: Greaves, Hilary *et al.* (2019) [A research agenda for the Global Priorities Institute](https://globalprioritiesinstitute.org/wp-content/uploads/2017/12/gpi-research-agenda.pdf), Oxford.\n    \n[^r3gb671cl2]: Dai, Wei (2019) [The argument from philosophical difficulty](https://www.lesswrong.com/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty), *LessWrong*, February 9.\n    \n[^7t0vu6w86yd]: William MacAskill, in Perry, Lucas (2018) [AI alignment podcast: moral uncertainty and the path to AI alignment with William MacAskill](https://futureoflife.org/2018/09/17/moral-uncertainty-and-the-path-to-ai-alignment-with-william-macaskill/), *AI Alignment podcast*, September 17.\n    \n[^d395d9czg1k]: Stocker, Felix (2020) [Reflecting on the long reflection](https://www.felixstocker.com/blog/reflecting-on-the-long-reflection), *Felix Stocker\u2019s Blog*, August 14.\n    \n[^8t05x5yy03]: Hanson, Robin (2021) [\u2018Long reflection\u2019 is crazy bad idea](https://www.overcomingbias.com/2021/10/long-reflection-is-crazy-bad-idea.html), *Overcoming Bias*, October 20.", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "The **long reflection** is a hypothesized period of time during which humanity works out how best to realize its long-term potential.\n\nSome effective altruists, including [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) and [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill), have argued that, if humanity succeeds in eliminating [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) or reducing it to acceptable levels, it should not immediately embark on an ambitious and potentially irreversible project of arranging the [universe's resources](https://forum.effectivealtruism.org/tag/universe-s-resources) in accordance to its values, but ought instead to spend considerable time\u2014 \"centuries (or more)\";[^ipthilteu8]\u00a0\"perhaps tens of thousands of years\";[^w82ait03vnf]\u00a0\"thousands or millions of years\";[^r3gb671cl2]\u00a0\"\\[p\\]erhaps... a million years\"[^7t0vu6w86yd]\u2014figuring out what is in fact of value. The long reflection may thus be seen as an intermediate stage in a rational long-term human developmental trajectory, following an initial stage of [existential security](https://forum.effectivealtruism.org/tag/existential-security) when existential risk is drastically reduced and followed by a final stage when humanity's potential is fully realized.[^ipthilteu8]\n\nCriticism\n---------\n\nThe idea of a long reflection has been criticized on the grounds that virtually eliminating all existential risk will almost certainly require taking a variety of large-scale, irreversible decisions\u2014related to [space colonization](https://forum.effectivealtruism.org/tag/space-colonization), [global governance](https://forum.effectivealtruism.org/tag/global-governance), [cognitive enhancement](https://forum.effectivealtruism.org/tag/cognitive-enhancement), and so on\u2014which are precisely the decisions meant to be discussed during the long reflection.[^d395d9czg1k][^8t05x5yy03]\u00a0Since there are pervasive and inescapable tradeoffs between reducing existential risk and retaining moral option value, it may be argued that it does not make sense to frame humanity's long-term strategic picture as one consisting of two distinct stages, with one taking precedence over the other.\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of sources that are highly relevant to the idea of the Long Reflection](https://forum.effectivealtruism.org/posts/H2zno3ggRJaph9P6c/quotes-about-the-long-reflection?commentId=z2ybSC353mPHpCjbn), *Effective Altruism Forum*, June 20.  \n*Many additional resources on this topic.*\n\nWiblin, Robert & Keiran Harris (2018) [Our descendants will probably see us as moral monsters. what should we do about that?](https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/), *80,000 Hours*, January 19.  \n*Interview with William MacAskill about the long reflection and other topics.*\n\nRelated entries\n---------------\n\n[dystopia](https://forum.effectivealtruism.org/tag/dystopia) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [existential security](https://forum.effectivealtruism.org/tag/existential-security) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform) | [moral uncertainty](https://forum.effectivealtruism.org/tag/moral-uncertainty) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) | [value lock-in](https://forum.effectivealtruism.org/tag/value-lock-in)\n\n[^ipthilteu8]: Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n    \n[^w82ait03vnf]: Greaves, Hilary *et al.* (2019) [A research agenda for the Global Priorities Institute](https://globalprioritiesinstitute.org/wp-content/uploads/2017/12/gpi-research-agenda.pdf), Oxford.\n    \n[^r3gb671cl2]: Dai, Wei (2019) [The argument from philosophical difficulty](https://www.lesswrong.com/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty), *LessWrong*, February 9.\n    \n[^7t0vu6w86yd]: William MacAskill, in Perry, Lucas (2018) [AI alignment podcast: moral uncertainty and the path to AI alignment with William MacAskill](https://futureoflife.org/2018/09/17/moral-uncertainty-and-the-path-to-ai-alignment-with-william-macaskill/), *AI Alignment podcast*, September 17.\n    \n[^d395d9czg1k]: Stocker, Felix (2020) [Reflecting on the long reflection](https://www.felixstocker.com/blog/reflecting-on-the-long-reflection), *Felix Stocker\u2019s Blog*, August 14.\n    \n[^8t05x5yy03]: Hanson, Robin (2021) [\u2018Long reflection\u2019 is crazy bad idea](https://www.overcomingbias.com/2021/10/long-reflection-is-crazy-bad-idea.html), *Overcoming Bias*, October 20.", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "Nico Hill2", "External Source": "https://forum.effectivealtruism.org/long-reflection?edit=true", "Last Asked On Discord": "", "UI ID": "7757", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:08.918+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-7a69732a8b6136fce42d4d96709e83065085bd9a1a16b705407591bd64013375", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7a69732a8b6136fce42d4d96709e83065085bd9a1a16b705407591bd64013375", "name": "What is the \"control problem\"?", "index": 317, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:42.693Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7a69732a8b6136fce42d4d96709e83065085bd9a1a16b705407591bd64013375", "values": {"File": "What is the \"control problem\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the \"control problem\"?", "Link": "https://docs.google.com/document/d/1tXYfnAbEo-BV65NXnfp5BTuw5G_WX-g8w1v13NKd5HU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:28.252+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Control Problem", "Doc Last Edited": "2023-02-22T23:04:15.910+01:00", "Status": "Live on site", "Edit Answer": "What is the \"control problem\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6205", "Source Link": "https://www.reddit.com/r/ControlProblem/wiki/faq", "aisafety.info Link": "What is the \"control problem\"?", "Source": "/r/ControlProblem's FAQ", "All Phrasings": "What is the \"control problem\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The Control Problem is the problem of preventing artificial superintelligence (ASI) from having a negative impact on humanity. How do we keep a more intelligent being under control, or how do we align it with our values? If we succeed in solving this problem, intelligence vastly superior to ours can take the baton of human progress and carry it to unfathomable heights. Solving our most complex problems could be simple to a sufficiently intelligent machine. If we fail in solving the Control Problem and create a powerful ASI not aligned with our values, it could spell the end of the human race. For these reasons, The Control Problem may be the most important challenge that humanity has ever faced, and may be our last.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "The Control Problem is the problem of preventing artificial superintelligence (ASI) from having a negative impact on humanity. How do we keep a more intelligent being under control, or how do we align it with our values? If we succeed in solving this problem, intelligence vastly superior to ours can take the baton of human progress and carry it to unfathomable heights. Solving our most complex problems could be simple to a sufficiently intelligent machine. If we fail in solving the Control Problem and create a powerful ASI not aligned with our values, it could spell the end of the human race. For these reasons, The Control Problem may be the most important challenge that humanity has ever faced, and may be our last.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "NotaSentientAI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6205", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:11.284+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 505, "Helpful": ""}}, {"id": "i-1545740773d372c3b28a7a32a30a86b16550a1284042294bf2a84f9045791658", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1545740773d372c3b28a7a32a30a86b16550a1284042294bf2a84f9045791658", "name": "What is neural network modularity?", "index": 318, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:44.377Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1545740773d372c3b28a7a32a30a86b16550a1284042294bf2a84f9045791658", "values": {"File": "What is neural network modularity?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is neural network modularity?", "Link": "https://docs.google.com/document/d/1fCRGxC6u0XFbb-5beewVQLuztRiY5TfLEqyPV74EfnM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:24.734+01:00", "Related Answers DO NOT EDIT": "What are some AI alignment research agendas currently being pursued?,What is interpretability and what approaches are there?", "Tags": "Interpretability", "Doc Last Edited": "2023-02-22T23:04:16.765+01:00", "Status": "Live on site", "Edit Answer": "What is neural network modularity?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8424", "Source Link": "", "aisafety.info Link": "What is neural network modularity?", "Source": "Wiki", "All Phrasings": "What is neural network modularity?\n", "Initial Order": "", "Related IDs": "6479,8241", "Rich Text DO NOT EDIT": "If a neural network is *modular*, that means it consists of clusters (modules) of neurons, such that the neurons within the cluster are strongly connected to each other, but only weakly connected to the rest of the network.\n\nMaking networks more modular is useful if the modules represent concepts which are understandable because this helps understand the whole system better.\n\nRelevant papers about modularity are [Neural Networks are Surprisingly Modular](https://arxiv.org/abs/2003.04881v2), [Clusterability in Neural Networks](https://arxiv.org/abs/2103.03386), and [Detecting Modularity in Deep Neural Networks](https://openreview.net/forum?id=tFQyjbOz34).\n\n", "Tag Count": 1, "Related Answer Count": 2, "Rich Text": "If a neural network is *modular*, that means it consists of clusters (modules) of neurons, such that the neurons within the cluster are strongly connected to each other, but only weakly connected to the rest of the network.\n\nMaking networks more modular is useful if the modules represent concepts which are understandable because this helps understand the whole system better.\n\nRelevant papers about modularity are [Neural Networks are Surprisingly Modular](https://arxiv.org/abs/2003.04881v2), [Clusterability in Neural Networks](https://arxiv.org/abs/2103.03386), and [Detecting Modularity in Deep Neural Networks](https://openreview.net/forum?id=tFQyjbOz34).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "Magdalena", "External Source": "", "Last Asked On Discord": "", "UI ID": "8424", "Related Answers": "What are some AI alignment research agendas currently being pursued?,What is interpretability and what approaches are there?", "Doc Last Ingested": "2023-03-14T23:33:13.861+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 505, "Helpful": ""}}, {"id": "i-bbd947856c702bac5cb87b59c3f315378480f73ecad37dcd21fa224276bd2e7a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-bbd947856c702bac5cb87b59c3f315378480f73ecad37dcd21fa224276bd2e7a", "name": "What is meant by \"AI takeoff\"?", "index": 319, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:45.972Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-bbd947856c702bac5cb87b59c3f315378480f73ecad37dcd21fa224276bd2e7a", "values": {"File": "What is meant by \"AI takeoff\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is meant by \"AI takeoff\"?", "Link": "https://docs.google.com/document/d/1bkut_2vWsQsbkoof2N6B3RevOHWR7k-Wgo-aoKzYoi8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:21.302+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AI Takeoff", "Doc Last Edited": "2023-02-22T23:04:17.614+01:00", "Status": "Live on site", "Edit Answer": "What is meant by \"AI takeoff\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7071", "Source Link": "", "aisafety.info Link": "What is meant by \"AI takeoff\"?", "Source": "Wiki", "All Phrasings": "What is meant by \"AI takeoff\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**AI Takeoff** refers to the process of an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) going from a certain threshold of capability (often discussed as \"human-level\") to being super-intelligent and capable enough to control the fate of civilization. There has been much debate about whether AI takeoff is more likely to be slow vs fast, i.e., \"soft\" vs \"hard\".\n\n_See also_: [AI Timelines](https://www.lesswrong.com/tag/ai-timelines), [Seed AI](https://www.lesswrong.com/tag/seed-ai), [Singularity](https://www.lesswrong.com/tag/singularity), [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion), [Recursive self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement)\n\nAI takeoff is sometimes casually referred to as **AI FOOM.**\n\nSoft takeoff\n============\n\nA **soft takeoff** refers to an AGI that would self-improve over a period of years or decades. This could be due to either the learning algorithm being too demanding for the hardware or because the AI relies on experiencing feedback from the real-world that would have to be played out in real-time. Possible methods that could deliver a soft takeoff, by slowly building on human-level intelligence, are [Whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation), [Biological Cognitive Enhancement](https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement), and software-based strong AGI \\[[1](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fn1)\\]. By maintaining control of the AGI's ascent it should be easier for a [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI) to emerge.\n\nVernor Vinge, Hans Moravec and have all expressed the view that soft takeoff is preferable to a hard takeoff as it would be both safer and easier to engineer.\n\nHard takeoff\n============\n\nA **hard takeoff** (or an AI going \"**FOOM**\" \\[[2](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fn2)\\]) refers to AGI expansion in a matter of minutes, days, or months. It is a fast, abruptly, local increase in capability. This scenario is widely considered much more precarious, as this involves an AGI rapidly ascending in power without human control. This may result in unexpected or undesired behavior (i.e. [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI)). It is one of the main ideas supporting the [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion) hypothesis.\n\nThe feasibility of hard takeoff has been addressed by Hugo de Garis, [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky), [Ben Goertzel](https://www.lesswrong.com/tag/ben-goertzel), [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom), and Michael Anissimov. It is widely agreed that a hard takeoff is something to be avoided due to the risks. Yudkowsky points out several possibilities that would make a hard takeoff more likely than a soft takeoff such as the existence of large [resources overhangs](https://www.lesswrong.com/tag/computing-overhang) or the fact that small improvements seem to have a large impact in a mind's general intelligence (i.e.: the small genetic difference between humans and chimps lead to huge increases in capability) \\[[3](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fn3)\\].\n\nNotable posts\n=============\n\n*   [Hard Takeoff](https://www.lesswrong.com/lw/wf/hard_takeoff/) by Eliezer Yudkowsky\n\nExternal links\n==============\n\n*   [The Age of Virtuous Machines](http://www.kurzweilai.net/the-age-of-virtuous-machines) by J. Storrs Hall President of The Foresight Institute\n*   [Hard take off Hypothesis](http://multiverseaccordingtoben.blogspot.co.uk/2011/01/hard-takeoff-hypothesis.html) by Ben Goertzel.\n*   [Extensive archive of Hard takeoff Essays](http://www.acceleratingfuture.com/michael/blog/2011/05/hard-takeoff-sources/) from Accelerating Future\n*   [Can we avoid a hard take off?](http://www-rohan.sdsu.edu/faculty/vinge/misc/ac2005/) by Vernor Vinge\n*   [Robot: Mere Machine to Transcendent Mind](http://www.amazon.co.uk/Robot-Mere-Machine-Transcendent-Mind/dp/0195136306) by Hans Moravec\n*   [The Singularity is Near](http://www.amazon.co.uk/The-Singularity-Near-Raymond-Kurzweil/dp/0715635611/ref=sr_1_1?s=books&ie=UTF8&qid=1339495098&sr=1-1) by Ray Kurzweil\n\n**References**\n\n1.  [http://www.aleph.se/andart/archives/2010/10/why\\_early\\_singularities\\_are\\_softer.html](http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html\u21a9)[\u21a9](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fnref1)\n2.  [http://lesswrong.com/lw/63t/requirements\\_for\\_ai\\_to\\_go_foom/](http://lesswrong.com/lw/63t/requirements_for_ai_to_go_foom/\u21a9)[\u21a9](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fnref2)\n3.  [http://lesswrong.com/lw/wf/hard_takeoff/](https://www.lesswrong.com/lw/wf/hard_takeoff/)[\u21a9](http://lesswrong.com/lw/wf/hard_takeoff/\u21a9)", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "**AI Takeoff** refers to the process of an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) going from a certain threshold of capability (often discussed as \"human-level\") to being super-intelligent and capable enough to control the fate of civilization. There has been much debate about whether AI takeoff is more likely to be slow vs fast, i.e., \"soft\" vs \"hard\".\n\n_See also_: [AI Timelines](https://www.lesswrong.com/tag/ai-timelines), [Seed AI](https://www.lesswrong.com/tag/seed-ai), [Singularity](https://www.lesswrong.com/tag/singularity), [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion), [Recursive self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement)\n\nAI takeoff is sometimes casually referred to as **AI FOOM.**\n\nSoft takeoff\n============\n\nA **soft takeoff** refers to an AGI that would self-improve over a period of years or decades. This could be due to either the learning algorithm being too demanding for the hardware or because the AI relies on experiencing feedback from the real-world that would have to be played out in real-time. Possible methods that could deliver a soft takeoff, by slowly building on human-level intelligence, are [Whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation), [Biological Cognitive Enhancement](https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement), and software-based strong AGI \\[[1](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fn1)\\]. By maintaining control of the AGI's ascent it should be easier for a [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI) to emerge.\n\nVernor Vinge, Hans Moravec and have all expressed the view that soft takeoff is preferable to a hard takeoff as it would be both safer and easier to engineer.\n\nHard takeoff\n============\n\nA **hard takeoff** (or an AI going \"**FOOM**\" \\[[2](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fn2)\\]) refers to AGI expansion in a matter of minutes, days, or months. It is a fast, abruptly, local increase in capability. This scenario is widely considered much more precarious, as this involves an AGI rapidly ascending in power without human control. This may result in unexpected or undesired behavior (i.e. [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI)). It is one of the main ideas supporting the [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion) hypothesis.\n\nThe feasibility of hard takeoff has been addressed by Hugo de Garis, [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky), [Ben Goertzel](https://www.lesswrong.com/tag/ben-goertzel), [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom), and Michael Anissimov. It is widely agreed that a hard takeoff is something to be avoided due to the risks. Yudkowsky points out several possibilities that would make a hard takeoff more likely than a soft takeoff such as the existence of large [resources overhangs](https://www.lesswrong.com/tag/computing-overhang) or the fact that small improvements seem to have a large impact in a mind's general intelligence (i.e.: the small genetic difference between humans and chimps lead to huge increases in capability) \\[[3](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fn3)\\].\n\nNotable posts\n=============\n\n*   [Hard Takeoff](https://www.lesswrong.com/lw/wf/hard_takeoff/) by Eliezer Yudkowsky\n\nExternal links\n==============\n\n*   [The Age of Virtuous Machines](http://www.kurzweilai.net/the-age-of-virtuous-machines) by J. Storrs Hall President of The Foresight Institute\n*   [Hard take off Hypothesis](http://multiverseaccordingtoben.blogspot.co.uk/2011/01/hard-takeoff-hypothesis.html) by Ben Goertzel.\n*   [Extensive archive of Hard takeoff Essays](http://www.acceleratingfuture.com/michael/blog/2011/05/hard-takeoff-sources/) from Accelerating Future\n*   [Can we avoid a hard take off?](http://www-rohan.sdsu.edu/faculty/vinge/misc/ac2005/) by Vernor Vinge\n*   [Robot: Mere Machine to Transcendent Mind](http://www.amazon.co.uk/Robot-Mere-Machine-Transcendent-Mind/dp/0195136306) by Hans Moravec\n*   [The Singularity is Near](http://www.amazon.co.uk/The-Singularity-Near-Raymond-Kurzweil/dp/0715635611/ref=sr_1_1?s=books&ie=UTF8&qid=1339495098&sr=1-1) by Ray Kurzweil\n\n**References**\n\n1.  [http://www.aleph.se/andart/archives/2010/10/why\\_early\\_singularities\\_are\\_softer.html](http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html\u21a9)[\u21a9](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fnref1)\n2.  [http://lesswrong.com/lw/63t/requirements\\_for\\_ai\\_to\\_go_foom/](http://lesswrong.com/lw/63t/requirements_for_ai_to_go_foom/\u21a9)[\u21a9](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fnref2)\n3.  [http://lesswrong.com/lw/wf/hard_takeoff/](https://www.lesswrong.com/lw/wf/hard_takeoff/)[\u21a9](http://lesswrong.com/lw/wf/hard_takeoff/\u21a9)", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "plex", "External Source": "https://www.lesswrong.com/tag/ai-takeoff?edit=true", "Last Asked On Discord": "", "UI ID": "7071", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:18.729+01:00", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-37b828b41f18ea16f9cb87873ed41f9b3790dc35ae691b561ea9f71c6ccfefeb", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-37b828b41f18ea16f9cb87873ed41f9b3790dc35ae691b561ea9f71c6ccfefeb", "name": "What is interpretability and what approaches are there?", "index": 320, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:47.590Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-37b828b41f18ea16f9cb87873ed41f9b3790dc35ae691b561ea9f71c6ccfefeb", "values": {"File": "What is interpretability and what approaches are there?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is interpretability and what approaches are there?", "Link": "https://docs.google.com/document/d/1hHAx92e89YQfBXT96C7BLiuMipcl6LOszGlG2L7rrZo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:17.207+01:00", "Related Answers DO NOT EDIT": "What is neural network modularity?", "Tags": "Interpretability", "Doc Last Edited": "2023-02-23T00:12:45.091+01:00", "Status": "Live on site", "Edit Answer": "What is interpretability and what approaches are there?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8241", "Source Link": "", "aisafety.info Link": "What is interpretability and what approaches are there?", "Source": "Wiki", "All Phrasings": "What is interpretability and what approaches are there?\n", "Initial Order": "", "Related IDs": "8424", "Rich Text DO NOT EDIT": "Interpretability is about making machine learning (ML) systems easier to understand. It is hard because the computations of current ML systems often depend on billions of parameters which they learnt from data. Areas of research for making current ML models more understandable are *mechanistic interpretability*, *finding important input features*, *explaining by examples*, *natural language explanations*, and using ML architectures which are *intrinsically interpretable*.\n\n1. **Mechanistic interpretability** is about interpreting an ML model\u2019s internal representations. A very simple way to do this is [activation maximization](https://towardsdatascience.com/every-ml-engineer-needs-to-know-neural-network-interpretability-afea2ac0824e): optimize the input such that one particular neuron is activated a lot. This optimized input is an indicator of the concept which the neuron represents. Work that is central for mechanistic interpretability is the [circuits thread](https://distill.pub/2020/circuits/zoom-in/), which focuses on interpreting the algorithms implemented by subgraphs (circuits) of neural networks. There is also work on [circuits in transformers](https://transformer-circuits.pub/2021/framework/index.html) in particular. Mechanistic Interpretability has the drawback that [interpretability is not composable](https://www.greaterwrong.com/posts/qXtbBAxmFkAQLQEJE/interpretability-tool-ness-alignment-corrigibility-are-not), i.e.even if we understand all the components of a system, it doesn\u2019t mean that we understand the whole. However, there may still be a way of hierarchically decomposing a system in a way that allows us to understand each layer of abstraction of it, and thus understanding the whole.\n    ![](https://lh3.googleusercontent.com/l4xpOTcQEU0847R5egOFOY5SMxCb7YMr2-kGFH1jvo5iv0yk_TpKEpiS8xnw6MOM1tcb4ulTjsL3N5ScWCOr6wfzHOu1tCpF95km3xcyt6k9qOzXcU06TbWjv4tAI8XFSSRWq6yIZLoHEyDXT9CUtiJtskUZRTx2)\n    Feature visualization of a neuron that corresponds to dog-like features. [image source](https://distill.pub/2020/circuits/zoom-in/)\n\n1. The idea of **finding important input features** is to find out which input features are most relevant for the output. In the case of image classification, we can highlight the relevant features with a heatmap, which is called[saliency map](https://arxiv.org/abs/1312.6034)). A very simple way to do this is to take the derivative of the output with regard to the different parts of the input. This derivative denotes how much the output changes if we change a particular part of the input, i.e.\u00a0how important that part of the input is for the output. Saliency maps can be useful to notice cases in which an image classifier learns to use features it should not use. For example, the paper *[Unmasking Clever Hans predictors and assessing what machines really learn](https://www.nature.com/articles/s41467-019-08987-4)* used saliency maps to show that a horse-classifying image classifier was not using the image parts that contained the horse at all, but rather relied on the name of the photographer printed in a corner, because one of the photographers primarily took photos of horses. \n\n1. [image of horse thing, maybe see thesis] However, many of the common saliency methods fail basic [sanity checks](https://arxiv.org/abs/1810.03292), such as the saliency maps almost not changing when the model weights are randomized. Therefore, saliency maps are not sufficient for a reliable understanding of ML systems.\n\n1. **Explanation by examples** means showing examples in the training data that have similar features, such as in the paper *[This Looks Like That: Deep Learning for Interpretable Image Recognition](https://arxiv.org/abs/1806.10574)*.\n\n1. **Natural Language Explanations** are sentences describing a model\u2019s reasons for its outputs. For example, in the paper *[Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks](https://xfgao.github.io/xCookingWeb/)*a human and an AI play a virtual cooking game together, and the AI explains its plans in natural language. They find that with the explanations the human-AI team performs significantly better.\n\n1. However, natural language explanations, as well as finding important features and explanation by examples are *post-hoc* explanations: They are generated after the fact, and are therefore likely to not be *faithful* (i.e.\u00a0not accurately describe a model\u2019s decision process). **Interpretable architectures** are architectures which are simple enough to be understandable without additional tools. Cynthia Rudin is a central researcher[argu](https://arxiv.org/abs/1811.10154)[ing](https://arxiv.org/abs/1811.10154) [for using interpretable architectures](https://arxiv.org/abs/1811.10154) in high-stakes situations. However, using interpretable architecutures usually comes with a significant cost to model performance.\n\nYou can read more about different approaches in [this overview article](https://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries) which summarizes more than 70 interpretability-related papers, and in the free online book *[A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/)*.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "Interpretability is about making machine learning (ML) systems easier to understand. It is hard because the computations of current ML systems often depend on billions of parameters which they learnt from data. Areas of research for making current ML models more understandable are *mechanistic interpretability*, *finding important input features*, *explaining by examples*, *natural language explanations*, and using ML architectures which are *intrinsically interpretable*.\n\n1. **Mechanistic interpretability** is about interpreting an ML model\u2019s internal representations. A very simple way to do this is [activation maximization](https://towardsdatascience.com/every-ml-engineer-needs-to-know-neural-network-interpretability-afea2ac0824e): optimize the input such that one particular neuron is activated a lot. This optimized input is an indicator of the concept which the neuron represents. Work that is central for mechanistic interpretability is the [circuits thread](https://distill.pub/2020/circuits/zoom-in/), which focuses on interpreting the algorithms implemented by subgraphs (circuits) of neural networks. There is also work on [circuits in transformers](https://transformer-circuits.pub/2021/framework/index.html) in particular. Mechanistic Interpretability has the drawback that [interpretability is not composable](https://www.greaterwrong.com/posts/qXtbBAxmFkAQLQEJE/interpretability-tool-ness-alignment-corrigibility-are-not), i.e.even if we understand all the components of a system, it doesn\u2019t mean that we understand the whole. However, there may still be a way of hierarchically decomposing a system in a way that allows us to understand each layer of abstraction of it, and thus understanding the whole.\n    ![](https://lh3.googleusercontent.com/l4xpOTcQEU0847R5egOFOY5SMxCb7YMr2-kGFH1jvo5iv0yk_TpKEpiS8xnw6MOM1tcb4ulTjsL3N5ScWCOr6wfzHOu1tCpF95km3xcyt6k9qOzXcU06TbWjv4tAI8XFSSRWq6yIZLoHEyDXT9CUtiJtskUZRTx2)\n    Feature visualization of a neuron that corresponds to dog-like features. [image source](https://distill.pub/2020/circuits/zoom-in/)\n\n1. The idea of **finding important input features** is to find out which input features are most relevant for the output. In the case of image classification, we can highlight the relevant features with a heatmap, which is called[saliency map](https://arxiv.org/abs/1312.6034)). A very simple way to do this is to take the derivative of the output with regard to the different parts of the input. This derivative denotes how much the output changes if we change a particular part of the input, i.e.\u00a0how important that part of the input is for the output. Saliency maps can be useful to notice cases in which an image classifier learns to use features it should not use. For example, the paper *[Unmasking Clever Hans predictors and assessing what machines really learn](https://www.nature.com/articles/s41467-019-08987-4)* used saliency maps to show that a horse-classifying image classifier was not using the image parts that contained the horse at all, but rather relied on the name of the photographer printed in a corner, because one of the photographers primarily took photos of horses. \n\n1. [image of horse thing, maybe see thesis] However, many of the common saliency methods fail basic [sanity checks](https://arxiv.org/abs/1810.03292), such as the saliency maps almost not changing when the model weights are randomized. Therefore, saliency maps are not sufficient for a reliable understanding of ML systems.\n\n1. **Explanation by examples** means showing examples in the training data that have similar features, such as in the paper *[This Looks Like That: Deep Learning for Interpretable Image Recognition](https://arxiv.org/abs/1806.10574)*.\n\n1. **Natural Language Explanations** are sentences describing a model\u2019s reasons for its outputs. For example, in the paper *[Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks](https://xfgao.github.io/xCookingWeb/)*a human and an AI play a virtual cooking game together, and the AI explains its plans in natural language. They find that with the explanations the human-AI team performs significantly better.\n\n1. However, natural language explanations, as well as finding important features and explanation by examples are *post-hoc* explanations: They are generated after the fact, and are therefore likely to not be *faithful* (i.e.\u00a0not accurately describe a model\u2019s decision process). **Interpretable architectures** are architectures which are simple enough to be understandable without additional tools. Cynthia Rudin is a central researcher[argu](https://arxiv.org/abs/1811.10154)[ing](https://arxiv.org/abs/1811.10154) [for using interpretable architectures](https://arxiv.org/abs/1811.10154) in high-stakes situations. However, using interpretable architecutures usually comes with a significant cost to model performance.\n\nYou can read more about different approaches in [this overview article](https://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries) which summarizes more than 70 interpretability-related papers, and in the free online book *[A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/)*.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 2, "Asker": "Nicky", "External Source": "", "Last Asked On Discord": "", "UI ID": "8241", "Related Answers": "What is neural network modularity?", "Doc Last Ingested": "2023-03-14T23:33:22.032+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 1426, "Helpful": ""}}, {"id": "i-eb32bebc48a3cd4eca98597e4dc54ac50a787fd112332fd389e8fade6e4eddd3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-eb32bebc48a3cd4eca98597e4dc54ac50a787fd112332fd389e8fade6e4eddd3", "name": "What is everyone working on in AI alignment?", "index": 321, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:49.137Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-eb32bebc48a3cd4eca98597e4dc54ac50a787fd112332fd389e8fade6e4eddd3", "values": {"File": "What is everyone working on in AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is everyone working on in AI alignment?", "Link": "https://docs.google.com/document/d/1PabWb0pVgRKeXPHdrcD8NjJJcuiH-QQP7i6ME8xW8KQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:13.516+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Research Agendas", "Doc Last Edited": "2023-02-22T22:46:45.524+01:00", "Status": "In progress", "Edit Answer": "What is everyone working on in AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8392", "Source Link": "", "aisafety.info Link": "What is everyone working on in AI alignment?", "Source": "Wiki", "All Phrasings": "What is everyone working on in AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "There is a Cambrian explosion of approaches to solving alignment. Click through to the follow-up questions to explore the research directions of groups and individuals in the field, or browse [this map](https://aisafety.world/):\n\n[https://aisafety.world/](https://aisafety.world/)\n\nSee also: [https://www.alignmentforum.org/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is](https://www.alignmentforum.org/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is)\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "There is a Cambrian explosion of approaches to solving alignment. Click through to the follow-up questions to explore the research directions of groups and individuals in the field, or browse [this map](https://aisafety.world/):\n\n[https://aisafety.world/](https://aisafety.world/)\n\nSee also: [https://www.alignmentforum.org/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is](https://www.alignmentforum.org/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "2023-02-26T19:21:30.218+01:00", "UI ID": "8392", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:24.532+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 1426, "Helpful": ""}}, {"id": "i-e6eef3a3b38a73fea4db24bdb8805a9a7e704aead50433be94e8a1fe58361916", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e6eef3a3b38a73fea4db24bdb8805a9a7e704aead50433be94e8a1fe58361916", "name": "What is causal decision theory?", "index": 322, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:50.673Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e6eef3a3b38a73fea4db24bdb8805a9a7e704aead50433be94e8a1fe58361916", "values": {"File": "What is causal decision theory?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is causal decision theory?", "Link": "https://docs.google.com/document/d/1owSgkcbjSJmsXUbMLbC5KqK9s_EQD-QUNIXDvD8vXH8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:09.534+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,Decision Theory", "Doc Last Edited": "2023-02-22T23:04:19.641+01:00", "Status": "Live on site", "Edit Answer": "What is causal decision theory?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7779", "Source Link": "", "aisafety.info Link": "What is causal decision theory?", "Source": "Wiki", "All Phrasings": "What is causal decision theory?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**Causal Decision Theory** \u2013 CDT \u2013 is a branch of [decision theory](https://www.lesswrong.com/tag/decision-theory) which advises an agent to take actions which maximize the causal consequences on the probability of desired outcomes [^1^](#fn1). As any branch of decision theory, it prescribes taking the action that maximizes [expected utility](https://www.lesswrong.com/tag/expected-utility), i.e the action which maximizes the sum of the utility obtained in each outcome weighted by the probability of that outcome occurring, *given* your action. Different decision theories correspond to different ways of construing this dependence between actions and outcomes. CDT focuses on the *causal* relations between one\u2019s actions and outcomes, whilst [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory) \u2013 EDT - concerns itself with what an action *indicates* about the world (which is operationalized by the conditional probability). That is, according to CDT, a rational agent should track the available causal relations linking his actions to the desired outcome and take the action which will better enhance the chances of the desired outcome.\n\nOne usual example where EDT and CDT commonly diverge is the [Smoking lesion](https://www.lesswrong.com/tag/smoking-lesion): \u201cSmoking is strongly correlated with lung cancer, but in the world of the Smoker's Lesion this correlation is understood to be the result of a common cause: a genetic lesion that tends to cause both smoking and cancer. Once we fix the presence or absence of the lesion, there is no additional correlation between smoking and cancer. Suppose you prefer smoking without cancer to not smoking without cancer, and prefer smoking with cancer to not smoking with cancer. Should you smoke?\u201d CDT would recommend smoking since there is no causal connection between smoking and cancer. They are both caused by a gene, but have no causal direct connection with each other. EDT, on the other hand, would recommend against smoking, since smoking is evidence for having the mentioned gene and thus should be avoided.\n\nThe core aspect of CDT is mathematically represented by the fact it uses probabilities of conditionals in place of conditional probabilities [^2^](#fn2). The probability of a conditional is the probability of the whole conditional being true, where the conditional probability is the probability of the consequent given the antecedent. A conditional probability of B given A - P(B|A) -, simply implies the [Bayesian probability](https://www.lesswrong.com/tag/bayesian-probability) of the event B happening given we known A happened, it\u2019s used in EDT. The probability of conditionals \u2013 P(A > B) - refers to the probability that the conditional 'A implies B' is true, it is the probability of the contrafactual \u2018If A, then B\u2019 be the case. Since contrafactual analysis is the key tool used to speak about causality, probability of conditionals are said to mirror causal relations. In most cases these two probabilities track each other, and CDT and EDT give the same answers. However, some particular problems have arisen where their predictions for rational action diverge such as the [Smoking lesion](https://www.lesswrong.com/tag/smoking-lesion) problem \u2013 where CDT seems to give a more reasonable prescription \u2013 and [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem) \u2013 where CDT seems unreasonable. David Lewis proved [^3^](#fn3) it's impossible for probabilities of conditionals to always track conditional probabilities. Hence, evidential relations aren\u2019t the same as causal relations and CDT and EDT will always diverge in some cases.\n\nReferences\n----------\n\n1.  [http://plato.stanford.edu/entries/decision-causal/](http://plato.stanford.edu/entries/decision-causal/)\n2.  Lewis, David. (1981) \"Causal Decision Theory,\" Australasian Journal of Philosophy 59 (1981): 5- 30.\n3.  Lewis, D. (1976), \"Probabilities of conditionals and conditional probabilities\", The Philosophical Review (Duke University Press) 85 (3): 297\u2013315\n\nSee also\n--------\n\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory)", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "**Causal Decision Theory** \u2013 CDT \u2013 is a branch of [decision theory](https://www.lesswrong.com/tag/decision-theory) which advises an agent to take actions which maximize the causal consequences on the probability of desired outcomes [^1^](#fn1). As any branch of decision theory, it prescribes taking the action that maximizes [expected utility](https://www.lesswrong.com/tag/expected-utility), i.e the action which maximizes the sum of the utility obtained in each outcome weighted by the probability of that outcome occurring, *given* your action. Different decision theories correspond to different ways of construing this dependence between actions and outcomes. CDT focuses on the *causal* relations between one\u2019s actions and outcomes, whilst [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory) \u2013 EDT - concerns itself with what an action *indicates* about the world (which is operationalized by the conditional probability). That is, according to CDT, a rational agent should track the available causal relations linking his actions to the desired outcome and take the action which will better enhance the chances of the desired outcome.\n\nOne usual example where EDT and CDT commonly diverge is the [Smoking lesion](https://www.lesswrong.com/tag/smoking-lesion): \u201cSmoking is strongly correlated with lung cancer, but in the world of the Smoker's Lesion this correlation is understood to be the result of a common cause: a genetic lesion that tends to cause both smoking and cancer. Once we fix the presence or absence of the lesion, there is no additional correlation between smoking and cancer. Suppose you prefer smoking without cancer to not smoking without cancer, and prefer smoking with cancer to not smoking with cancer. Should you smoke?\u201d CDT would recommend smoking since there is no causal connection between smoking and cancer. They are both caused by a gene, but have no causal direct connection with each other. EDT, on the other hand, would recommend against smoking, since smoking is evidence for having the mentioned gene and thus should be avoided.\n\nThe core aspect of CDT is mathematically represented by the fact it uses probabilities of conditionals in place of conditional probabilities [^2^](#fn2). The probability of a conditional is the probability of the whole conditional being true, where the conditional probability is the probability of the consequent given the antecedent. A conditional probability of B given A - P(B|A) -, simply implies the [Bayesian probability](https://www.lesswrong.com/tag/bayesian-probability) of the event B happening given we known A happened, it\u2019s used in EDT. The probability of conditionals \u2013 P(A > B) - refers to the probability that the conditional 'A implies B' is true, it is the probability of the contrafactual \u2018If A, then B\u2019 be the case. Since contrafactual analysis is the key tool used to speak about causality, probability of conditionals are said to mirror causal relations. In most cases these two probabilities track each other, and CDT and EDT give the same answers. However, some particular problems have arisen where their predictions for rational action diverge such as the [Smoking lesion](https://www.lesswrong.com/tag/smoking-lesion) problem \u2013 where CDT seems to give a more reasonable prescription \u2013 and [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem) \u2013 where CDT seems unreasonable. David Lewis proved [^3^](#fn3) it's impossible for probabilities of conditionals to always track conditional probabilities. Hence, evidential relations aren\u2019t the same as causal relations and CDT and EDT will always diverge in some cases.\n\nReferences\n----------\n\n1.  [http://plato.stanford.edu/entries/decision-causal/](http://plato.stanford.edu/entries/decision-causal/)\n2.  Lewis, David. (1981) \"Causal Decision Theory,\" Australasian Journal of Philosophy 59 (1981): 5- 30.\n3.  Lewis, D. (1976), \"Probabilities of conditionals and conditional probabilities\", The Philosophical Review (Duke University Press) 85 (3): 297\u2013315\n\nSee also\n--------\n\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory)", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "https://www.lesswrong.com/tag/causal-decision-theory?edit=true", "Last Asked On Discord": "2023-02-26T19:23:09.962+01:00", "UI ID": "7779", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:27.572+01:00", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-53897773ddbc2889ee036970bb572cffaef2ead71d29cfccecdcac6c51a181a2", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-53897773ddbc2889ee036970bb572cffaef2ead71d29cfccecdcac6c51a181a2", "name": "What is the difference between AI Safety, AI alignment, AI Control, Friendly AI, AI Ethics, AI existential safety and AGI safety?", "index": 323, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-16T05:06:50.570Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-53897773ddbc2889ee036970bb572cffaef2ead71d29cfccecdcac6c51a181a2", "values": {"File": "What is the difference between AI Safety, AI alignment, AI Control, Friendly AI, AI Ethics, AI existential safety and AGI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is the difference between AI Safety, AI alignment, AI Control, Friendly AI, AI Ethics, AI existential safety and AGI safety?", "Link": "https://docs.google.com/document/d/17QsRKWFflR0XkIsrRs6X7-TVo3enraF6jv-BJkDfg00/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:05.409+01:00", "Related Answers DO NOT EDIT": "What are the differences between AGI, superintelligence and transformative AI?,How does AI taking things literally contribute to alignment being hard?,How does AI taking things literally contribute to alignment being hard?,What is the general nature of the concern about AI alignment?,What is the general nature of the concern about AI alignment?,Where can I learn about AI alignment?,What is \"friendly AI\"?", "Tags": "Definitions", "Doc Last Edited": "2023-03-16T03:16:09.697+01:00", "Status": "Live on site", "Edit Answer": "What is the difference between AI Safety, AI alignment, AI Control, Friendly AI, AI Ethics, AI existential safety and AGI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6714", "Source Link": "", "aisafety.info Link": "What is the difference between AI Safety, AI alignment, AI Control, Friendly AI, AI Ethics, AI existential safety and AGI safety?", "Source": "Wiki", "All Phrasings": "What is the difference between AI Safety, AI alignment, AI Control, Friendly AI, AI Ethics, AI existential safety and AGI safety?\n", "Initial Order": "", "Related IDs": "5864,6168,6168,6184,6184,5635,6918", "Rich Text DO NOT EDIT": "Friendly AI is an older term which refers to artificial intelligence (AI) systems that do not want to harm humans. AI safety is the umbrella term that encompasses all the different safety issues that might arise with AI with the exception of (parts of) AI Ethics. A desire to create friendly AI led to the birth of AI alignment as a field which focuses on the long-term task of creating this friendly AI. AI existential safety focuses specifically on AI safety risks that are on a catastrophic/existential scale. Artificial General Intelligence safety (AGI safety)focuses on the specific risks that might be posed by generally intelligent systems. AI Ethics focuses primarily on issues of bias and social implications of near-term systems. AI Governance focuses on global coordination around regulating AI development.\n\nThese are all terms that tend to have a lot of overlap. The definitions given here arehow this website uses thembut they are not meant to be the authoritative guide on how these terms should be used. That being said, to gain further clarity into the nuances of how these words are used, we can explore each term in a little more depth individually:\n\n- **[AI Safety](https://docs.google.com/document/d/1zz1c6rRN8Y-CmO0-BGVKI9G6MMg2MUqD247dPcub144/edit)**: AI safety generally means *getting AI systems to avoid risks*, of which existential safety is an extreme case with unique challenges.[^kix.rq11mrc7khg8]AI safety was a term that originally referred only to existential risks from AI systems. However, in recent years, it has been expanded to also include risks that are relevant at lower AI capability levels, such as near-term technical (e.g. self-driving cars) and governance risks. This makes AI Safety more of an overarching umbrella term used to refer to all safety issues both in the near and long term. This also means that several other terms are used in conjunction with AI Safety to more uniquely identify the specific type of risk being talked about.\n\n- **AGI Safety**: AGI safety refers to *safety concerns from general intelligence*. It overlaps with AI alignment strongly, in that misalignment would be the main cause of unsafe behavior in AGIs, but also includes misuse and other [governance issues](https://www.lesswrong.com/tag/ai-governance).\n\n- **AI Existential Safety**: AI existential safety is a slightly broader term than AGI safety. It includes AI risks that pose an [existential threat](https://www.lesswrong.com/tag/existential-risk) without necessarily possessing an intelligence that is as general or on the same level as humans. Specifically it means preventing AI technology from posing risks to humanity that are comparable to or greater than human extinction in terms of their moral significance.[^kix.tbt3v8ck1vlx]\n\n- **[AI Alignment](https://en.wikipedia.org/wiki/AI_alignment)**: Alignment is one approach to research in AI Safety, and it is a field that is focused on \u2018making AI go well\u2019. Specifically, researchers in AI alignment focus on causing the goals of future superintelligent AI systems to align with [human values](https://www.lesswrong.com/posts/GermiEmcS6xuZ2gBh/what-ai-safety-researchers-have-written-about-the-nature-of). Paul Christiano defines it as \u201c*Alignment is the problem of getting your AI to try to do the right thing, not the problem of figuring out which thing is right. An aligned AI would try to figure out which thing is right, and like a human it may or may not succeed.*\u201d[^kix.mzdmtr5obkde] This means that the [AIs](https://www.lesswrong.com/tag/ai)/[AGIs](https://www.lesswrong.com/tag/artificial-general-intelligence)/[Artific](https://www.lesswrong.com/tag/superintelligence)[al Superintelligence (ASI)](https://www.lesswrong.com/tag/superintelligence)would behave in a way that was compatible with human survival and flourishing. Alignment research is strongly interdisciplinary and can include computer science, mathematics, neuroscience, philosophy, and social sciences. Some places (e.g. the [Alignment Forum](https://www.alignmentforum.org/)) use the term AI alignment to mean the project of AI existential safety, including governance and excluding non-existentially risky misalignment.\n\n- **[AI Control](https://ai-alignment.com/ai-safety-vs-control-vs-alignment-2a4b42a863cc)**: This term is used to refer to ensuring that AI systems try to do the right thing, and in particular that they don\u2019t competently pursue the wrong thing. It is an older and uncommonly used term, and it refers to [roughly the same set of problems as AI safety.](https://ai-alignment.com/security-and-ai-control-675ace05ce31)[^kix.9ip3vm8qesnh]\n\n- **[AI Governance](https://forum.effectivealtruism.org/topics/ai-governance)**: AI governance refers to *identifying and enforcing norms for AI developers and AI systems themselves to follow.*[^kix.euretahit9hf] AI governance is often paired with AI safety. Both have the goal of helping humanity develop beneficial AI. AI safety focuses on the technical questions of how AI is built; AI governance focuses on the institutions and contexts in which AI is built and used.[^kix.rnt5dx54p23z] The question of which principles should be enforced often opens up debates about safety and ethics. The conversations in governance are a bit more action-oriented than purely ethical debates. AI Governance includes a broad range of subjects, from global coordination around regulating AI development to providing incentives for corporations to be more cautious in their AI research.\n\n- **[Friendly AI](https://arbital.greaterwrong.com/p/FAI)** **(FAI)**: This is an older term coined and popularized by Eliezer Yudkowsky. FAI is a subset of all possible AGIs to help humans flourish while acting per some idealized version of human values such as [coherent extrapolated volition](https://www.lesswrong.com/tag/coherent-extrapolated-volition). Over the last few years, the term \u2018aligned AI\u2019 has been used to refer to the same concept more often than FAI. Yudkowsky has said that the problem of creating an FAI is significantly more difficult than creating AGI. Since AI systems are bound to evolve and change over time, this term seeks to capture the challenge of designing a system that remains \u2018friendly\u2019 or well-meaning in the face of such continuous changes.\n\n- **[AI Ethics](https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence)**: AI ethics refers to *principles that AI developers and systems should follow*.[^kix.gwv7ghbj5thw]The \u201cshould\u201d here creates a space for debate, whereby many people and institutions can try to impose their values on what principles become accepted by society at large.[^kix.q29i22s1txme] AI Ethics attempts to tackle this problem and focuses on ensuring that in our attempt to harness this technology for good, we appropriately assess its potential for societal harm within its design. This includes issues such as preventing and [mitigating algorithmic bias](https://www.harvardmagazine.com/2021/08/meredith-broussard-ai-bias-documentary), accountability of companies related to generative works, and the [transparency](https://hbr.org/2022/06/building-transparency-into-ai-projects) of the models being used to make societal decisions. AI Ethics often refers to concerns for existing technology with limited scopes whereas most of the terms above refer to future AIs with potentially world-altering scopes. \n\nThe reason we have so many terms is that we\u2019re trying to communicate a specific concept: How to build very powerful AI systems that don't kill everyone and do promote human flourishing. But the terms each come with implications and so keep drifting to mean different things. Some people have attempted to resort to [AI notkilleveryoneism](https://twitter.com/ESYudkowsky/status/1612364482608795650) to mitigate this dilution and distortion of terms, but this has not been widely adopted for obvious reasons.\n\n[^kix.q29i22s1txme]: Critch, Andrew (2020). [Some AI research areas and their relevance to existential safety](https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1#AI_existential_safety__definition_)\n[^kix.mzdmtr5obkde]: Paul Christiano (2018). [Clarifying \u201cAI alignment\u201d](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6)\n[^kix.euretahit9hf]:Critch, Andrew (2020). [Some AI research areas and their relevance to existential safety](https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1#AI_existential_safety__definition_)\n[^kix.gwv7ghbj5thw]: Critch, Andrew (2020). [Some AI research areas and their relevance to existential safety](https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1#AI_existential_safety__definition_)\n[^kix.tbt3v8ck1vlx]:Critch, Andrew (2020). [Some AI research areas and their relevance to existential safety](https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1#AI_existential_safety__definition_)\n[^kix.rq11mrc7khg8]: Critch, Andrew (2020). [Some AI research areas and their relevance to existential safety](https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1#AI_existential_safety__definition_)\n[^kix.9ip3vm8qesnh]: Paul Christiano (2016). [AI \u201csafety\u201d vs \u201ccontrol\u201d vs \u201calignment\u201d](https://ai-alignment.com/ai-safety-vs-control-vs-alignment-2a4b42a863cc)\n[^kix.rnt5dx54p23z]: Dafoe, Allan (2017). [AI Governance: A Research Agenda](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf)", "Tag Count": 1, "Related Answer Count": 7, "Rich Text": "Friendly AI is an older term which refers to artificial intelligence (AI) systems that do not want to harm humans. AI safety is the umbrella term that encompasses all the different safety issues that might arise with AI with the exception of (parts of) AI Ethics. A desire to create friendly AI led to the birth of AI alignment as a field which focuses on the long-term task of creating this friendly AI. AI existential safety focuses specifically on AI safety risks that are on a catastrophic/existential scale. Artificial General Intelligence safety (AGI safety)focuses on the specific risks that might be posed by generally intelligent systems. AI Ethics focuses primarily on issues of bias and social implications of near-term systems. AI Governance focuses on global coordination around regulating AI development.\n\nThese are all terms that tend to have a lot of overlap. The definitions given here arehow this website uses thembut they are not meant to be the authoritative guide on how these terms should be used. That being said, to gain further clarity into the nuances of how these words are used, we can explore each term in a little more depth individually:\n\n- **[AI Safety](https://docs.google.com/document/d/1zz1c6rRN8Y-CmO0-BGVKI9G6MMg2MUqD247dPcub144/edit)**: AI safety generally means *getting AI systems to avoid risks*, of which existential safety is an extreme case with unique challenges.[^kix.rq11mrc7khg8]AI safety was a term that originally referred only to existential risks from AI systems. However, in recent years, it has been expanded to also include risks that are relevant at lower AI capability levels, such as near-term technical (e.g. self-driving cars) and governance risks. This makes AI Safety more of an overarching umbrella term used to refer to all safety issues both in the near and long term. This also means that several other terms are used in conjunction with AI Safety to more uniquely identify the specific type of risk being talked about.\n\n- **AGI Safety**: AGI safety refers to *safety concerns from general intelligence*. It overlaps with AI alignment strongly, in that misalignment would be the main cause of unsafe behavior in AGIs, but also includes misuse and other [governance issues](https://www.lesswrong.com/tag/ai-governance).\n\n- **AI Existential Safety**: AI existential safety is a slightly broader term than AGI safety. It includes AI risks that pose an [existential threat](https://www.lesswrong.com/tag/existential-risk) without necessarily possessing an intelligence that is as general or on the same level as humans. Specifically it means preventing AI technology from posing risks to humanity that are comparable to or greater than human extinction in terms of their moral significance.[^kix.tbt3v8ck1vlx]\n\n- **[AI Alignment](https://en.wikipedia.org/wiki/AI_alignment)**: Alignment is one approach to research in AI Safety, and it is a field that is focused on \u2018making AI go well\u2019. Specifically, researchers in AI alignment focus on causing the goals of future superintelligent AI systems to align with [human values](https://www.lesswrong.com/posts/GermiEmcS6xuZ2gBh/what-ai-safety-researchers-have-written-about-the-nature-of). Paul Christiano defines it as \u201c*Alignment is the problem of getting your AI to try to do the right thing, not the problem of figuring out which thing is right. An aligned AI would try to figure out which thing is right, and like a human it may or may not succeed.*\u201d[^kix.mzdmtr5obkde] This means that the [AIs](https://www.lesswrong.com/tag/ai)/[AGIs](https://www.lesswrong.com/tag/artificial-general-intelligence)/[Artific](https://www.lesswrong.com/tag/superintelligence)[al Superintelligence (ASI)](https://www.lesswrong.com/tag/superintelligence)would behave in a way that was compatible with human survival and flourishing. Alignment research is strongly interdisciplinary and can include computer science, mathematics, neuroscience, philosophy, and social sciences. Some places (e.g. the [Alignment Forum](https://www.alignmentforum.org/)) use the term AI alignment to mean the project of AI existential safety, including governance and excluding non-existentially risky misalignment.\n\n- **[AI Control](https://ai-alignment.com/ai-safety-vs-control-vs-alignment-2a4b42a863cc)**: This term is used to refer to ensuring that AI systems try to do the right thing, and in particular that they don\u2019t competently pursue the wrong thing. It is an older and uncommonly used term, and it refers to [roughly the same set of problems as AI safety.](https://ai-alignment.com/security-and-ai-control-675ace05ce31)[^kix.9ip3vm8qesnh]\n\n- **[AI Governance](https://forum.effectivealtruism.org/topics/ai-governance)**: AI governance refers to *identifying and enforcing norms for AI developers and AI systems themselves to follow.*[^kix.euretahit9hf] AI governance is often paired with AI safety. Both have the goal of helping humanity develop beneficial AI. AI safety focuses on the technical questions of how AI is built; AI governance focuses on the institutions and contexts in which AI is built and used.[^kix.rnt5dx54p23z] The question of which principles should be enforced often opens up debates about safety and ethics. The conversations in governance are a bit more action-oriented than purely ethical debates. AI Governance includes a broad range of subjects, from global coordination around regulating AI development to providing incentives for corporations to be more cautious in their AI research.\n\n- **[Friendly AI](https://arbital.greaterwrong.com/p/FAI)** **(FAI)**: This is an older term coined and popularized by Eliezer Yudkowsky. FAI is a subset of all possible AGIs to help humans flourish while acting per some idealized version of human values such as [coherent extrapolated volition](https://www.lesswrong.com/tag/coherent-extrapolated-volition). Over the last few years, the term \u2018aligned AI\u2019 has been used to refer to the same concept more often than FAI. Yudkowsky has said that the problem of creating an FAI is significantly more difficult than creating AGI. Since AI systems are bound to evolve and change over time, this term seeks to capture the challenge of designing a system that remains \u2018friendly\u2019 or well-meaning in the face of such continuous changes.\n\n- **[AI Ethics](https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence)**: AI ethics refers to *principles that AI developers and systems should follow*.[^kix.gwv7ghbj5thw]The \u201cshould\u201d here creates a space for debate, whereby many people and institutions can try to impose their values on what principles become accepted by society at large.[^kix.q29i22s1txme] AI Ethics attempts to tackle this problem and focuses on ensuring that in our attempt to harness this technology for good, we appropriately assess its potential for societal harm within its design. This includes issues such as preventing and [mitigating algorithmic bias](https://www.harvardmagazine.com/2021/08/meredith-broussard-ai-bias-documentary), accountability of companies related to generative works, and the [transparency](https://hbr.org/2022/06/building-transparency-into-ai-projects) of the models being used to make societal decisions. AI Ethics often refers to concerns for existing technology with limited scopes whereas most of the terms above refer to future AIs with potentially world-altering scopes. \n\nThe reason we have so many terms is that we\u2019re trying to communicate a specific concept: How to build very powerful AI systems that don't kill everyone and do promote human flourishing. But the terms each come with implications and so keep drifting to mean different things. Some people have attempted to resort to [AI notkilleveryoneism](https://twitter.com/ESYudkowsky/status/1612364482608795650) to mitigate this dilution and distortion of terms, but this has not been widely adopted for obvious reasons.\n\n[^kix.q29i22s1txme]: Critch, Andrew (2020). [Some AI research areas and their relevance to existential safety](https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1#AI_existential_safety__definition_)\n[^kix.mzdmtr5obkde]: Paul Christiano (2018). [Clarifying \u201cAI alignment\u201d](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6)\n[^kix.euretahit9hf]:Critch, Andrew (2020). [Some AI research areas and their relevance to existential safety](https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1#AI_existential_safety__definition_)\n[^kix.gwv7ghbj5thw]: Critch, Andrew (2020). [Some AI research areas and their relevance to existential safety](https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1#AI_existential_safety__definition_)\n[^kix.tbt3v8ck1vlx]:Critch, Andrew (2020). [Some AI research areas and their relevance to existential safety](https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1#AI_existential_safety__definition_)\n[^kix.rq11mrc7khg8]: Critch, Andrew (2020). [Some AI research areas and their relevance to existential safety](https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1#AI_existential_safety__definition_)\n[^kix.9ip3vm8qesnh]: Paul Christiano (2016). [AI \u201csafety\u201d vs \u201ccontrol\u201d vs \u201calignment\u201d](https://ai-alignment.com/ai-safety-vs-control-vs-alignment-2a4b42a863cc)\n[^kix.rnt5dx54p23z]: Dafoe, Allan (2017). [AI Governance: A Research Agenda](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf)", "Stamp Count": 1, "Multi Answer": true, "Stamped By": "plex", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6714", "Related Answers": "What are the differences between AGI, superintelligence and transformative AI?,How does AI taking things literally contribute to alignment being hard?,How does AI taking things literally contribute to alignment being hard?,What is the general nature of the concern about AI alignment?,What is the general nature of the concern about AI alignment?,Where can I learn about AI alignment?,What is \"friendly AI\"?", "Doc Last Ingested": "2023-03-16T04:12:01.903+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 52, "Helpful": ""}}, {"id": "i-e8cae80b28a920f03313f3cf858a15411ecc4c4a36f3625efe1ca462dee44c96", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e8cae80b28a920f03313f3cf858a15411ecc4c4a36f3625efe1ca462dee44c96", "name": "What is an example of AGI going wrong that doesn't sound like sci-fi?", "index": 324, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:53.939Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e8cae80b28a920f03313f3cf858a15411ecc4c4a36f3625efe1ca462dee44c96", "values": {"File": "What is an example of AGI going wrong that doesn't sound like sci-fi?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is an example of AGI going wrong that doesn't sound like sci-fi?", "Link": "https://docs.google.com/document/d/1-fI1GtSmJ1bOWmu61wnpIT0s32L3Jura2AHaKVxyB5I/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:50:01.970+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:46:47.775+01:00", "Status": "Not started", "Edit Answer": "What is an example of AGI going wrong that doesn't sound like sci-fi?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8272", "Source Link": "", "aisafety.info Link": "What is an example of AGI going wrong that doesn't sound like sci-fi?", "Source": "Wiki", "All Phrasings": "What is an example of AGI going wrong that doesn't sound like sci-fi?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Aprillion", "External Source": "", "Last Asked On Discord": "", "UI ID": "8272", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:32.434+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 1426, "Helpful": ""}}, {"id": "i-2c678308cd9ae525ca77ec309e362812baa251fd84aed86834a2d5384fe847f1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2c678308cd9ae525ca77ec309e362812baa251fd84aed86834a2d5384fe847f1", "name": "What is an \"s-risk\"?", "index": 325, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:40:57.689Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2c678308cd9ae525ca77ec309e362812baa251fd84aed86834a2d5384fe847f1", "values": {"File": "What is an \"s-risk\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is an \"s-risk\"?", "Link": "https://docs.google.com/document/d/1dZtMWXKO77gvjQYf9j81WRvnq2RenMDiILATukNTX2I/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:58.261+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,S-risk", "Doc Last Edited": "2023-02-22T23:04:20.854+01:00", "Status": "Live on site", "Edit Answer": "What is an \"s-risk\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7783", "Source Link": "", "aisafety.info Link": "What is an \"s-risk\"?", "Source": "Wiki", "All Phrasings": "What is an \"s-risk\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**(Astronomical) suffering risks**, also known as **s-risks**, are risks of the creation of intense suffering in the far future on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.\n\nS-risks are an example of [existential risk](https://www.lesswrong.com/tag/existential-risk) (also known as *x-risks*) according to Nick Bostrom's original definition, as they threaten to \"permanently and drastically curtail \\[Earth-originating intelligent life's\\] potential\". Most existential risks are of the form \"event E happens which drastically reduces the number of conscious experiences in the future\". S-risks therefore serve as a useful reminder that some x-risks are scary because they cause *bad* experiences, and not just because they prevent good ones.\n\nWithin the space of x-risks, we can distinguish x-risks that are s-risks, x-risks involving human extinction, x-risks that involve immense suffering *and* human extinction, and x-risks that involve neither. For example:\n\n<table><tbody><tr><td>&nbsp;</td><td><strong>extinction risk</strong></td><td><strong>non-extinction risk</strong></td></tr><tr><td><strong>suffering risk</strong></td><td>Misaligned AGI wipes out humans, simulates many suffering alien civilizations.</td><td>Misaligned AGI tiles the universe with experiences of severe suffering.</td></tr><tr><td><strong>non-suffering risk</strong></td><td>Misaligned AGI wipes out humans.</td><td>Misaligned AGI keeps humans as \"pets,\" limiting growth but not causing immense suffering.</td></tr></tbody></table>\n\nA related concept is [**hyperexistential risk**](https://arbital.com/p/hyperexistential_separation/), the risk of \"fates worse than death\" on an astronomical scale. It is not clear whether all hyperexistential risks are s-risks per se. But arguably all s-risks are hyperexistential, since \"tiling the universe with experiences of severe suffering\" would likely be worse than death.\n\nThere are two [EA](https://wiki.lesswrong.com/wiki/EA) organizations with s-risk prevention research as their primary focus: the [Center on Long-Term Risk](https://www.lesswrong.com/tag/center-on-long-term-risk-clr) (CLR) and the [Center for Reducing Suffering](https://centerforreducingsuffering.org/). Much of CLR's work is on suffering-focused [AI safety](https://wiki.lesswrong.com/wiki/AI_safety) and [crucial considerations](https://www.lesswrong.com/tag/crucial-considerations). Although to a much lesser extent, the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) and [Future of Humanity Institute](https://www.lesswrong.com/tag/future-of-humanity-institute-fhi) have investigated strategies to prevent s-risks too.\u00a0\n\nAnother approach to reducing s-risk is to \"expand the moral circle\" [*together*](https://magnusvinding.com/2018/09/04/moral-circle-expansion-might-increase-future-suffering/) with raising concern for suffering, so that future (post)human civilizations and AI are less likely to [instrumentally](https://www.lesswrong.com/tag/instrumental-value) cause suffering to non-human minds such as animals or digital sentience. [Sentience Institute](http://www.sentienceinstitute.org/) works on this value-spreading problem.\n\nSee also\n--------\n\n*   [Center on Long-Term Risk](https://www.lesswrong.com/tag/center-on-long-term-risk-clr)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   [Abolitionism](https://www.lesswrong.com/tag/abolitionism)\n*   [Mind crime](https://wiki.lesswrong.com/wiki/Mind_crime)\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism), [Hedonism](https://www.lesswrong.com/tag/hedonism)\n\nExternal links\n--------------\n\n*   [Reducing Risks of Astronomical Suffering: A Neglected Global Priority (FRI)](https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-global-priority/)\n*   [Introductory talk on s-risks (FRI)](https://foundational-research.org/s-risks-talk-eag-boston-2017/)\n*   [Risks of Astronomical Future Suffering (FRI)](https://foundational-research.org/risks-of-astronomical-future-suffering/)\n*   [Suffering-focused AI safety: Why \"fail-safe\" measures might be our top intervention PDF (FRI)](https://foundational-research.org/files/suffering-focused-ai-safety.pdf)\n*   [Artificial Intelligence and Its Implications for Future Suffering (FRI)](https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering)\n*   [Expanding our moral circle to reduce suffering in the far future (Sentience Politics)](https://sentience-politics.org/expanding-moral-circle-reduce-suffering-far-future/)\n*   [The Importance of the Far Future (Sentience Politics)](https://sentience-politics.org/philosophy/the-importance-of-the-future/)", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "**(Astronomical) suffering risks**, also known as **s-risks**, are risks of the creation of intense suffering in the far future on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.\n\nS-risks are an example of [existential risk](https://www.lesswrong.com/tag/existential-risk) (also known as *x-risks*) according to Nick Bostrom's original definition, as they threaten to \"permanently and drastically curtail \\[Earth-originating intelligent life's\\] potential\". Most existential risks are of the form \"event E happens which drastically reduces the number of conscious experiences in the future\". S-risks therefore serve as a useful reminder that some x-risks are scary because they cause *bad* experiences, and not just because they prevent good ones.\n\nWithin the space of x-risks, we can distinguish x-risks that are s-risks, x-risks involving human extinction, x-risks that involve immense suffering *and* human extinction, and x-risks that involve neither. For example:\n\n<table><tbody><tr><td>&nbsp;</td><td><strong>extinction risk</strong></td><td><strong>non-extinction risk</strong></td></tr><tr><td><strong>suffering risk</strong></td><td>Misaligned AGI wipes out humans, simulates many suffering alien civilizations.</td><td>Misaligned AGI tiles the universe with experiences of severe suffering.</td></tr><tr><td><strong>non-suffering risk</strong></td><td>Misaligned AGI wipes out humans.</td><td>Misaligned AGI keeps humans as \"pets,\" limiting growth but not causing immense suffering.</td></tr></tbody></table>\n\nA related concept is [**hyperexistential risk**](https://arbital.com/p/hyperexistential_separation/), the risk of \"fates worse than death\" on an astronomical scale. It is not clear whether all hyperexistential risks are s-risks per se. But arguably all s-risks are hyperexistential, since \"tiling the universe with experiences of severe suffering\" would likely be worse than death.\n\nThere are two [EA](https://wiki.lesswrong.com/wiki/EA) organizations with s-risk prevention research as their primary focus: the [Center on Long-Term Risk](https://www.lesswrong.com/tag/center-on-long-term-risk-clr) (CLR) and the [Center for Reducing Suffering](https://centerforreducingsuffering.org/). Much of CLR's work is on suffering-focused [AI safety](https://wiki.lesswrong.com/wiki/AI_safety) and [crucial considerations](https://www.lesswrong.com/tag/crucial-considerations). Although to a much lesser extent, the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) and [Future of Humanity Institute](https://www.lesswrong.com/tag/future-of-humanity-institute-fhi) have investigated strategies to prevent s-risks too.\u00a0\n\nAnother approach to reducing s-risk is to \"expand the moral circle\" [*together*](https://magnusvinding.com/2018/09/04/moral-circle-expansion-might-increase-future-suffering/) with raising concern for suffering, so that future (post)human civilizations and AI are less likely to [instrumentally](https://www.lesswrong.com/tag/instrumental-value) cause suffering to non-human minds such as animals or digital sentience. [Sentience Institute](http://www.sentienceinstitute.org/) works on this value-spreading problem.\n\nSee also\n--------\n\n*   [Center on Long-Term Risk](https://www.lesswrong.com/tag/center-on-long-term-risk-clr)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   [Abolitionism](https://www.lesswrong.com/tag/abolitionism)\n*   [Mind crime](https://wiki.lesswrong.com/wiki/Mind_crime)\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism), [Hedonism](https://www.lesswrong.com/tag/hedonism)\n\nExternal links\n--------------\n\n*   [Reducing Risks of Astronomical Suffering: A Neglected Global Priority (FRI)](https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-global-priority/)\n*   [Introductory talk on s-risks (FRI)](https://foundational-research.org/s-risks-talk-eag-boston-2017/)\n*   [Risks of Astronomical Future Suffering (FRI)](https://foundational-research.org/risks-of-astronomical-future-suffering/)\n*   [Suffering-focused AI safety: Why \"fail-safe\" measures might be our top intervention PDF (FRI)](https://foundational-research.org/files/suffering-focused-ai-safety.pdf)\n*   [Artificial Intelligence and Its Implications for Future Suffering (FRI)](https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering)\n*   [Expanding our moral circle to reduce suffering in the far future (Sentience Politics)](https://sentience-politics.org/expanding-moral-circle-reduce-suffering-far-future/)\n*   [The Importance of the Far Future (Sentience Politics)](https://sentience-politics.org/philosophy/the-importance-of-the-future/)", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Nico Hill2", "External Source": "https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks?edit=true", "Last Asked On Discord": "", "UI ID": "7783", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:34.659+01:00", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-361e086968a385953b5d98a6e369da7715e4509fea62fe78c97dc4138182ba33", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-361e086968a385953b5d98a6e369da7715e4509fea62fe78c97dc4138182ba33", "name": "What is an \"intelligence explosion\"?", "index": 326, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:41:02.647Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-361e086968a385953b5d98a6e369da7715e4509fea62fe78c97dc4138182ba33", "values": {"File": "What is an \"intelligence explosion\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is an \"intelligence explosion\"?", "Link": "https://docs.google.com/document/d/1o_CnFOMGmL1hniT3dasrh6id67buwOwjU6R8zHrhMbk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:54.462+01:00", "Related Answers DO NOT EDIT": "How likely is an \"intelligence explosion\"?", "Tags": "Intelligence Explosion", "Doc Last Edited": "2023-02-22T23:04:21.686+01:00", "Status": "Live on site", "Edit Answer": "What is an \"intelligence explosion\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6306", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "What is an \"intelligence explosion\"?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "What is an \"intelligence explosion\"?\n", "Initial Order": "", "Related IDs": "6586", "Rich Text DO NOT EDIT": "The intelligence explosion idea was expressed by statistician [I.J. Good in 1965](http://www.incompleteideas.net/papers/Good65ultraintelligent.pdf):\n\n> Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an \u2018intelligence explosion\u2019, and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make.\n\nThe argument is this: Every year, computers surpass human abilities in new ways. A program written in 1956 was able to prove mathematical theorems, and [found a more elegant proof](http://www.cs.cornell.edu/courses/cs4860/2012fa/MacKenzie-TheAutomationOfProof.pdf) for one of them than Russell and Whitehead had given in *Principia Mathematica*. By the late 1990s, \u2018expert systems\u2019 had surpassed human skill for a [wide range of tasks](http://www.amazon.com/dp/0521122937/). In 1997, IBM\u2019s Deep Blue computer beat the world chess champion, and in 2011, IBM\u2019s Watson computer beat the best human players at a much more complicated game: [Jeopardy!](http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r=2&ref=homepage&src=me&pagewanted=all). Recently, [a robot named Adam](http://commonsenseatheism.com/wp-content/uploads/2011/02/King-The-Automation-of-Science.pdf) was programmed with our scientific knowledge about yeast, then posed its own hypotheses, tested them, and assessed the results.\n\nComputers remain far short of human intelligence, but the resources that aid AI design are accumulating (including hardware, large datasets, neuroscience knowledge, and AI theory). We may one day design a machine that surpasses human skill at designing artificial intelligences. After that, this machine could improve its own intelligence faster and better than humans can, which would make it even more skilled at improving its own intelligence. This could continue in a positive feedback loop such that the machine quickly becomes vastly more intelligent than the smartest human being on Earth: an \u2018intelligence explosion\u2019 resulting in a machine superintelligence.\n\nThis is what is meant by the \u2018intelligence explosion\u2019 in this FAQ.\n\nSee also:\n\n- Vinge, [The Coming Technological Singularity](http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html)\n\n- Wikipedia, [Technological Singularity](http://en.wikipedia.org/wiki/Technological_singularity)\n\n- Chalmers, [The Singularity: A Philosophical Analysis](http://commonsenseatheism.com/wp-content/uploads/2011/01/Chalmers-The-Singularity-a-philosophical-analysis.pdf)\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "The intelligence explosion idea was expressed by statistician [I.J. Good in 1965](http://www.incompleteideas.net/papers/Good65ultraintelligent.pdf):\n\n> Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an \u2018intelligence explosion\u2019, and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make.\n\nThe argument is this: Every year, computers surpass human abilities in new ways. A program written in 1956 was able to prove mathematical theorems, and [found a more elegant proof](http://www.cs.cornell.edu/courses/cs4860/2012fa/MacKenzie-TheAutomationOfProof.pdf) for one of them than Russell and Whitehead had given in *Principia Mathematica*. By the late 1990s, \u2018expert systems\u2019 had surpassed human skill for a [wide range of tasks](http://www.amazon.com/dp/0521122937/). In 1997, IBM\u2019s Deep Blue computer beat the world chess champion, and in 2011, IBM\u2019s Watson computer beat the best human players at a much more complicated game: [Jeopardy!](http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r=2&ref=homepage&src=me&pagewanted=all). Recently, [a robot named Adam](http://commonsenseatheism.com/wp-content/uploads/2011/02/King-The-Automation-of-Science.pdf) was programmed with our scientific knowledge about yeast, then posed its own hypotheses, tested them, and assessed the results.\n\nComputers remain far short of human intelligence, but the resources that aid AI design are accumulating (including hardware, large datasets, neuroscience knowledge, and AI theory). We may one day design a machine that surpasses human skill at designing artificial intelligences. After that, this machine could improve its own intelligence faster and better than humans can, which would make it even more skilled at improving its own intelligence. This could continue in a positive feedback loop such that the machine quickly becomes vastly more intelligent than the smartest human being on Earth: an \u2018intelligence explosion\u2019 resulting in a machine superintelligence.\n\nThis is what is meant by the \u2018intelligence explosion\u2019 in this FAQ.\n\nSee also:\n\n- Vinge, [The Coming Technological Singularity](http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html)\n\n- Wikipedia, [Technological Singularity](http://en.wikipedia.org/wiki/Technological_singularity)\n\n- Chalmers, [The Singularity: A Philosophical Analysis](http://commonsenseatheism.com/wp-content/uploads/2011/01/Chalmers-The-Singularity-a-philosophical-analysis.pdf)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "", "UI ID": "6306", "Related Answers": "How likely is an \"intelligence explosion\"?", "Doc Last Ingested": "2023-03-14T23:33:36.451+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 1426, "Helpful": ""}}, {"id": "i-8a89ab700a4e34bf80f0690c7bbb1c0eda1af8d98ffc3681cec496bbb890db7a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8a89ab700a4e34bf80f0690c7bbb1c0eda1af8d98ffc3681cec496bbb890db7a", "name": "What is an \"agent\"?", "index": 327, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:41:08.229Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8a89ab700a4e34bf80f0690c7bbb1c0eda1af8d98ffc3681cec496bbb890db7a", "values": {"File": "What is an \"agent\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is an \"agent\"?", "Link": "https://docs.google.com/document/d/1Pb5k225-IAwg4ukbrVF7V19vPa_QhPVKdepojraXhOU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:51.208+01:00", "Related Answers DO NOT EDIT": "What's meant by calling an AI \"agenty\" or \"agentlike\"?", "Tags": "Definitions,Agency", "Doc Last Edited": "2023-02-22T23:04:22.561+01:00", "Status": "Live on site", "Edit Answer": "What is an \"agent\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7595", "Source Link": "", "aisafety.info Link": "What is an \"agent\"?", "Source": "Wiki", "All Phrasings": "What is an \"agent\"?\n", "Initial Order": "", "Related IDs": "5632", "Rich Text DO NOT EDIT": "[https://www.lesswrong.com/tag/agent](https://www.lesswrong.com/tag/agent)\n\n", "Tag Count": 2, "Related Answer Count": 1, "Rich Text": "[https://www.lesswrong.com/tag/agent](https://www.lesswrong.com/tag/agent)\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Nico Hill2", "External Source": "https://www.lesswrong.com/tag/agent?edit=true", "Last Asked On Discord": "", "UI ID": "7595", "Related Answers": "What's meant by calling an AI \"agenty\" or \"agentlike\"?", "Doc Last Ingested": "2023-03-14T23:33:38.714+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 1426, "Helpful": ""}}, {"id": "i-4d261e0581270e24fbb4cea2a64421f19d443cf1b5a98f19ecaa5c6660821e87", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4d261e0581270e24fbb4cea2a64421f19d443cf1b5a98f19ecaa5c6660821e87", "name": "What is a verified account on Stampy's Wiki?", "index": 328, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-02-11T15:37:53.350Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4d261e0581270e24fbb4cea2a64421f19d443cf1b5a98f19ecaa5c6660821e87", "values": {"File": "What is a verified account on Stampy's Wiki?", "Synced": false, "Sync account": "plexven@gmail.com", "Question": "What is a verified account on Stampy's Wiki?", "Link": "https://docs.google.com/document/d/1kbcZG1_2VP_sCkbUPcTtITHz82qJ0h5CsfI0czxY8iI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:47.110+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Needs Work,Stampy", "Doc Last Edited": "2023-01-14T16:36:24.588+01:00", "Status": "Not started", "Edit Answer": "What is a verified account on Stampy's Wiki?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6467", "Source Link": "", "aisafety.info Link": "What is a verified account on Stampy's Wiki?", "Source": "Wiki", "All Phrasings": "What is a verified account on Stampy's Wiki?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6467", "Related Answers": "", "Doc Last Ingested": "2023-02-11T16:37:38.786+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-b7f0c074cbd9bcd5780812aba82beb0a96cbc6cf9325dd6c8cdafdf10e6b3705", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b7f0c074cbd9bcd5780812aba82beb0a96cbc6cf9325dd6c8cdafdf10e6b3705", "name": "What is a follow-up question on Stampy's AI Safety Info?", "index": 329, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:41:12.883Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b7f0c074cbd9bcd5780812aba82beb0a96cbc6cf9325dd6c8cdafdf10e6b3705", "values": {"File": "What is a follow-up question on Stampy's AI Safety Info?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is a follow-up question on Stampy's AI Safety Info?", "Link": "https://docs.google.com/document/d/1Grt3_94Cp2Y3sOugzZ1QbXycBxnhxGfRjP5cBblf0Lg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:43.657+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Needs Work,Stampy", "Doc Last Edited": "2023-02-22T23:04:23.667+01:00", "Status": "Live on site", "Edit Answer": "What is a follow-up question on Stampy's AI Safety Info?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6447", "Source Link": "", "aisafety.info Link": "What is a follow-up question on Stampy's AI Safety Info?", "Source": "Wiki", "All Phrasings": "What is a follow-up question on Stampy's AI Safety Info?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**Follow-up questions** are responses to an answer which the reader might have, either because they want more information or are providing information to Stampy about what they're looking for. We don't expect to have great coverage of the former for a long time because there will be so many, but hopefully we'll be able to handle some of the most common ones.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "**Follow-up questions** are responses to an answer which the reader might have, either because they want more information or are providing information to Stampy about what they're looking for. We don't expect to have great coverage of the former for a long time because there will be so many, but hopefully we'll be able to handle some of the most common ones.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6447", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:41.619+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 1426, "Helpful": ""}}, {"id": "i-8d63108627f9eb0798efc1c503f9677d607ed3eed6813a343bebf668a0a9e396", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8d63108627f9eb0798efc1c503f9677d607ed3eed6813a343bebf668a0a9e396", "name": "What is a duplicate question on Stampy's Wiki?", "index": 330, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-02-11T15:37:57.699Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8d63108627f9eb0798efc1c503f9677d607ed3eed6813a343bebf668a0a9e396", "values": {"File": "What is a duplicate question on Stampy's Wiki?", "Synced": false, "Sync account": "plexven@gmail.com", "Question": "What is a duplicate question on Stampy's Wiki?", "Link": "https://docs.google.com/document/d/1Eq04H7SVjvbC9c4HWtRNnUiOVBTXza97-Xei1nKY6wM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:39.815+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Needs Work,Stampy", "Doc Last Edited": "2023-01-15T02:52:33.835+01:00", "Status": "Live on site", "Edit Answer": "What is a duplicate question on Stampy's Wiki?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6625", "Source Link": "", "aisafety.info Link": "What is a duplicate question on Stampy's Wiki?", "Source": "Wiki", "All Phrasings": "What is a duplicate question on Stampy's Wiki?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "An existing question is a duplicate of a new one if it is reasonable to expect whoever asked the new question to be satisfied if they received an answer to the existing question instead.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "An existing question is a duplicate of a new one if it is reasonable to expect whoever asked the new question to be satisfied if they received an answer to the existing question instead.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6625", "Related Answers": "", "Doc Last Ingested": "2023-02-11T16:37:41.339+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-c15c9008e14d3633ffdd74b266459b73413c121abc1249e1fb50206c69875a9f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c15c9008e14d3633ffdd74b266459b73413c121abc1249e1fb50206c69875a9f", "name": "What is a canonical question on Stampy's Wiki?", "index": 331, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-02-11T15:38:00.352Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c15c9008e14d3633ffdd74b266459b73413c121abc1249e1fb50206c69875a9f", "values": {"File": "What is a canonical question on Stampy's Wiki?", "Synced": false, "Sync account": "plexven@gmail.com", "Question": "What is a canonical question on Stampy's Wiki?", "Link": "https://docs.google.com/document/d/1d69z5x-oXtUk8cA9EfIkk72XINezRSZw1zqHReIJUk8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:36.714+01:00", "Related Answers DO NOT EDIT": "What should be marked as a canonical answer on Stampy's AI Safety Info?", "Tags": "Needs Work,Stampy", "Doc Last Edited": "2023-01-15T08:09:12.292+01:00", "Status": "Live on site", "Edit Answer": "What is a canonical question on Stampy's Wiki?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6431", "Source Link": "", "aisafety.info Link": "What is a canonical question on Stampy's Wiki?", "Source": "Wiki", "All Phrasings": "What is a canonical question on Stampy's Wiki?\n", "Initial Order": "", "Related IDs": "6432", "Rich Text DO NOT EDIT": "[Canonical questions](https://stampy.ai/read/Canonical_questions) are the questions which we've checked are in [](https://stampy.ai/read/Scope) [scope](https://stampy.ai/read/Scope) and not duplicates, so we want answers to them. They may be edited to represent a class of question more broadly, rather than keeping all their idosyncracies. Once they're answered canonically Stampy will serve them to readers.\n\n", "Tag Count": 2, "Related Answer Count": 1, "Rich Text": "[Canonical questions](https://stampy.ai/read/Canonical_questions) are the questions which we've checked are in [](https://stampy.ai/read/Scope) [scope](https://stampy.ai/read/Scope) and not duplicates, so we want answers to them. They may be edited to represent a class of question more broadly, rather than keeping all their idosyncracies. Once they're answered canonically Stampy will serve them to readers.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6431", "Related Answers": "What should be marked as a canonical answer on Stampy's AI Safety Info?", "Doc Last Ingested": "2023-02-11T16:37:43.344+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-8106bc0ad494bb3875747a2df927ef6e109088278558e33b70017e8c8b2d0828", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8106bc0ad494bb3875747a2df927ef6e109088278558e33b70017e8c8b2d0828", "name": "What is a \"value handshake\"?", "index": 332, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:41:16.197Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8106bc0ad494bb3875747a2df927ef6e109088278558e33b70017e8c8b2d0828", "values": {"File": "What is a \"value handshake\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is a \"value handshake\"?", "Link": "https://docs.google.com/document/d/1pxlOdlp42ToX8Mj-WovKL-FH0ugi_VMuBSlbYcDMupM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:33.077+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:24.722+01:00", "Status": "Live on site", "Edit Answer": "What is a \"value handshake\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7491", "Source Link": "", "aisafety.info Link": "What is a \"value handshake\"?", "Source": "Wiki", "All Phrasings": "What is a \"value handshake\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A value handshake is a form of trade between superintelligences, when two AI's with incompatible utility functions meet, instead of going to war, since they have superhuman prediction abilities and likely know the outcome before any attack even happens, they can decide to split the universe into chunks with volumes according to their respective military strength or chance of victory, and if their utility functions are compatible, they might even decide to merge into an AI with an utility function that is the weighted average of the two previous ones.\n\nThis could happen if multiple AI's are active on earth at the same time, and then maybe if at least one of them is aligned with humans, the resulting value handshake could leave humanity in a pretty okay situation.\n\nOn the other hand, the possibility of value handshakes provides a challenge to any strategy of using multiple non-aligned AIs to keep each other from getting too powerful, since they might allow the AIs to cooperate, leaving humanity in a weaker position.\n\nSee [The Hour I First Believed](https://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/) By Scott Alexander for some further thoughts and an introduction to related topics.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "A value handshake is a form of trade between superintelligences, when two AI's with incompatible utility functions meet, instead of going to war, since they have superhuman prediction abilities and likely know the outcome before any attack even happens, they can decide to split the universe into chunks with volumes according to their respective military strength or chance of victory, and if their utility functions are compatible, they might even decide to merge into an AI with an utility function that is the weighted average of the two previous ones.\n\nThis could happen if multiple AI's are active on earth at the same time, and then maybe if at least one of them is aligned with humans, the resulting value handshake could leave humanity in a pretty okay situation.\n\nOn the other hand, the possibility of value handshakes provides a challenge to any strategy of using multiple non-aligned AIs to keep each other from getting too powerful, since they might allow the AIs to cooperate, leaving humanity in a weaker position.\n\nSee [The Hour I First Believed](https://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/) By Scott Alexander for some further thoughts and an introduction to related topics.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "LVC\u0245", "External Source": "", "Last Asked On Discord": "", "UI ID": "7491", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:45.062+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 1426, "Helpful": ""}}, {"id": "i-be730a3a939d437ef0ac68ca0db61e345a38998195c260f35f4360d36964fc6b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-be730a3a939d437ef0ac68ca0db61e345a38998195c260f35f4360d36964fc6b", "name": "What is a \"quantilizer\"?", "index": 333, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:41:19.591Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-be730a3a939d437ef0ac68ca0db61e345a38998195c260f35f4360d36964fc6b", "values": {"File": "What is a \"quantilizer\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is a \"quantilizer\"?", "Link": "https://docs.google.com/document/d/1gKCatii1pXz53-7djMjP_YrBfvHcNVf0YDtDY1A6A0E/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:29.788+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Quantilizers", "Doc Last Edited": "2023-02-22T23:04:25.595+01:00", "Status": "Live on site", "Edit Answer": "What is a \"quantilizer\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6380", "Source Link": "", "aisafety.info Link": "What is a \"quantilizer\"?", "Source": "Wiki", "All Phrasings": "What is a \"quantilizer\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A **Quantilizer** is a proposed AI design which aims to reduce the harms from [Goodhart's law](https://www.lesswrong.com/tag/goodhart-s-law) and specification gaming by selecting reasonably effective actions from a distribution of human-like actions, rather than maximizing over actions. It it more of a theoretical tool for exploring ways around these problems than a practical buildable design.\n\n### See also\n\n*   [**Rob Miles's Quantilizers: AI That Doesn't Try Too Hard**](https://www.youtube.com/watch?v=gdKMG6kTl6Y)\n*   [**Arbital page on Quantilizers**](https://arbital.com/p/soft_optimizer?l=2r8#Quantilizing)", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "A **Quantilizer** is a proposed AI design which aims to reduce the harms from [Goodhart's law](https://www.lesswrong.com/tag/goodhart-s-law) and specification gaming by selecting reasonably effective actions from a distribution of human-like actions, rather than maximizing over actions. It it more of a theoretical tool for exploring ways around these problems than a practical buildable design.\n\n### See also\n\n*   [**Rob Miles's Quantilizers: AI That Doesn't Try Too Hard**](https://www.youtube.com/watch?v=gdKMG6kTl6Y)\n*   [**Arbital page on Quantilizers**](https://arbital.com/p/soft_optimizer?l=2r8#Quantilizing)", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "plex", "External Source": "https://www.lesswrong.com/tag/quantilization?edit=true", "Last Asked On Discord": "", "UI ID": "6380", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:47.686+01:00", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-1a73582fb66ca0987cbe1d0bca1fb8c1f561fb4b03ea2db70762cc658f142302", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1a73582fb66ca0987cbe1d0bca1fb8c1f561fb4b03ea2db70762cc658f142302", "name": "What is a \"pivotal act\"?", "index": 334, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-03T23:42:41.996Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1a73582fb66ca0987cbe1d0bca1fb8c1f561fb4b03ea2db70762cc658f142302", "values": {"File": "What is a \"pivotal act\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is a \"pivotal act\"?", "Link": "https://docs.google.com/document/d/1u2gtPWYLBgTL6YYxJSSxAVV92mTLmepzKxeYhht57rM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:25.471+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:54:57.452+01:00", "Status": "Duplicate", "Edit Answer": "What is a \"pivotal act\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7579", "Source Link": "", "aisafety.info Link": "What is a \"pivotal act\"?", "Source": "Wiki", "All Phrasings": "What is a \"pivotal act\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7579", "Related Answers": "", "Doc Last Ingested": "2023-03-04T00:42:10.189+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-ec3f7b893ee7d83892fe07ba7fa44f900d6e480c8efa71c08099bb518f844851", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ec3f7b893ee7d83892fe07ba7fa44f900d6e480c8efa71c08099bb518f844851", "name": "What is Stampy's copyright?", "index": 335, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:41:23.437Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ec3f7b893ee7d83892fe07ba7fa44f900d6e480c8efa71c08099bb518f844851", "values": {"File": "What is Stampy's copyright?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Stampy's copyright?", "Link": "https://docs.google.com/document/d/1t5Vjzopf3-QwJr8S8wdXxh7bSSQ9MI2ONMxPjAmhWSg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:21.485+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stampy", "Doc Last Edited": "2023-02-22T22:54:58.655+01:00", "Status": "Marked for deletion", "Edit Answer": "What is Stampy's copyright?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7873", "Source Link": "", "aisafety.info Link": "What is Stampy's copyright?", "Source": "Wiki", "All Phrasings": "What is Stampy's copyright?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7873", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:55.209+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 1426, "Helpful": ""}}, {"id": "i-a71c9053e9112a45eb661af61c8a6f1d62a69d82217e87cf4b279f11106c9c17", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-a71c9053e9112a45eb661af61c8a6f1d62a69d82217e87cf4b279f11106c9c17", "name": "What was Refine?", "index": 336, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:41:26.820Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-a71c9053e9112a45eb661af61c8a6f1d62a69d82217e87cf4b279f11106c9c17", "values": {"File": "What was Refine?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What was Refine?", "Link": "https://docs.google.com/document/d/1-3XmHyUbyb_hyZr02KvpqDPm6zwHx6udg4Au7D1AsOY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:17.906+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Organizations", "Doc Last Edited": "2023-02-22T23:04:26.907+01:00", "Status": "Live on site", "Edit Answer": "What was Refine?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8340", "Source Link": "", "aisafety.info Link": "What was Refine?", "Source": "Wiki", "All Phrasings": "What was Refine?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[Refine](https://www.lesswrong.com/posts/5uiQkyKdejX3aEHLM/how-to-diversify-conceptual-alignment-the-model-behind)was an incubator for new decorrelated alignment \"research bets\". Since no approach is very promising right now for solving alignment, the purpose of this was to come up with a bunch of independent new ideas, and hopefully some of these will work.\n\nRefine was [shut down after one round](https://www.lesswrong.com/posts/3zZjF3YKJ257x79mu/what-i-learned-running-refine), passing the torch to SERI MATS.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "[Refine](https://www.lesswrong.com/posts/5uiQkyKdejX3aEHLM/how-to-diversify-conceptual-alignment-the-model-behind)was an incubator for new decorrelated alignment \"research bets\". Since no approach is very promising right now for solving alignment, the purpose of this was to come up with a bunch of independent new ideas, and hopefully some of these will work.\n\nRefine was [shut down after one round](https://www.lesswrong.com/posts/3zZjF3YKJ257x79mu/what-i-learned-running-refine), passing the torch to SERI MATS.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8340", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:33:57.815+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 1426, "Helpful": ""}}, {"id": "i-01b05c00658957e75b8ac44740a011e5b917a34c402d253d47abfb47edf78da3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-01b05c00658957e75b8ac44740a011e5b917a34c402d253d47abfb47edf78da3", "name": "What is MIRI\u2019s mission?", "index": 337, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:41:29.062Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-01b05c00658957e75b8ac44740a011e5b917a34c402d253d47abfb47edf78da3", "values": {"File": "What is MIRI\u2019s mission?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is MIRI\u2019s mission?", "Link": "https://docs.google.com/document/d/1dNi1IZojksG4INTq0RktDR9C-IXjPohfwAi6iK_4eHw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:14.007+01:00", "Related Answers DO NOT EDIT": "", "Tags": "MIRI", "Doc Last Edited": "2023-02-22T23:04:27.719+01:00", "Status": "Live on site", "Edit Answer": "What is MIRI\u2019s mission?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6293", "Source Link": "https://intelligence.org/faq/", "aisafety.info Link": "What is MIRI\u2019s mission?", "Source": "MIRI FAQ", "All Phrasings": "What is MIRI\u2019s mission?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[MIRI's](https://intelligence.org/) mission statement is to \u201censure that the creation of smarter-than-human artificial intelligence has a positive impact.\u201d This is an ambitious goal, but they believe that some early progress is possible, and they believe that the goal\u2019s importance and difficulty makes it prudent to begin work at an early date.\n\nTheir two main research agendas, \u201c[Agent Foundations for Aligning Machine Intelligence with Human Interests](https://intelligence.org/technical-agenda)\u201d and \u201c[Value Alignment for Advanced Machine Learning Systems](https://intelligence.org/2016/05/04/announcing-a-new-research-program/),\u201d focus on three groups of technical problems:\n\n- highly reliable agent design \u2014 learning how to specify highly autonomous systems that reliably pursue some fixed goal;\n\n- value specification \u2014 supplying autonomous systems with the intended goals; and\n\n- error tolerance \u2014 making such systems robust to programmer error.\n\nThat being said, MIRI recently [published an update](https://intelligence.org/2020/12/21/2020-updates-and-strategy/) stating that they were moving away from research directions in unpublished works that they were [pursuing since 2017](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/).\n\nThey publish new [mathematical results](https://intelligence.org/research) (although their work is [non-disclosed by default](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/#section3)), host [workshops](https://intelligence.org/research), attend conferences, and [fund outside researchers](https://intelligence.org/mirix) who are interested in investigating these problems. They also host a [blog](https://intelligence.org/blog) and an [online research forum](https://www.alignmentforum.org/).\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "[MIRI's](https://intelligence.org/) mission statement is to \u201censure that the creation of smarter-than-human artificial intelligence has a positive impact.\u201d This is an ambitious goal, but they believe that some early progress is possible, and they believe that the goal\u2019s importance and difficulty makes it prudent to begin work at an early date.\n\nTheir two main research agendas, \u201c[Agent Foundations for Aligning Machine Intelligence with Human Interests](https://intelligence.org/technical-agenda)\u201d and \u201c[Value Alignment for Advanced Machine Learning Systems](https://intelligence.org/2016/05/04/announcing-a-new-research-program/),\u201d focus on three groups of technical problems:\n\n- highly reliable agent design \u2014 learning how to specify highly autonomous systems that reliably pursue some fixed goal;\n\n- value specification \u2014 supplying autonomous systems with the intended goals; and\n\n- error tolerance \u2014 making such systems robust to programmer error.\n\nThat being said, MIRI recently [published an update](https://intelligence.org/2020/12/21/2020-updates-and-strategy/) stating that they were moving away from research directions in unpublished works that they were [pursuing since 2017](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/).\n\nThey publish new [mathematical results](https://intelligence.org/research) (although their work is [non-disclosed by default](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/#section3)), host [workshops](https://intelligence.org/research), attend conferences, and [fund outside researchers](https://intelligence.org/mirix) who are interested in investigating these problems. They also host a [blog](https://intelligence.org/blog) and an [online research forum](https://www.alignmentforum.org/).\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 3, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "6293", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:34:00.516+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 1426, "Helpful": ""}}, {"id": "i-76add521e9582dc8f4f74d48fac676cbce2afadc5ffe2b38dc62e80e315b132b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-76add521e9582dc8f4f74d48fac676cbce2afadc5ffe2b38dc62e80e315b132b", "name": "What is John Wentworth's plan?", "index": 338, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:41:31.359Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-76add521e9582dc8f4f74d48fac676cbce2afadc5ffe2b38dc62e80e315b132b", "values": {"File": "What is John Wentworth's plan?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is John Wentworth's plan?", "Link": "https://docs.google.com/document/d/1Qy3KM77fyUBzkNPdIzRj7AFqeG-NhDV1AF016FmNYQc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:09.937+01:00", "Related Answers DO NOT EDIT": "What is neural network modularity?", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:28.568+01:00", "Status": "Live on site", "Edit Answer": "What is John Wentworth's plan?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8378", "Source Link": "", "aisafety.info Link": "What is John Wentworth's plan?", "Source": "Wiki", "All Phrasings": "What is John Wentworth's plan?\n", "Initial Order": "", "Related IDs": "8424", "Rich Text DO NOT EDIT": "[John's plan](https://www.lesswrong.com/posts/3L46WGauGpr7nYubu/the-plan) is:\n\nStep 1: sort out our fundamental confusions about agency\n\nStep 2: ambitious value learning (i.e. build an AI which correctly learns human values and optimizes for them)\n\nStep 3: \u2026\n\nStep 4: profit!\n\n\u2026 and do all that before AGI kills us all.\n\nHe is working on step 1: figuring out what the heck is going on with agency. His current approach is based on[selection theorems](https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents): try to figure out what types of agents are selected for in a broad range of environments. Examples of selection pressures include: evolution, SGD, and markets. This is an approach to agent foundations that comes from the opposite direction as MIRI: it's more about observing existing structures (whether they be mathematical or real things in the world like markets or e coli), whereas MIRI is trying to write out some desiderata and then finding mathematical notions that satisfy those desiderata.\n\nTwo key properties that might be selected for are modularity and abstractions.\n\nAbstractions are higher level things that people tend to use to describe things. Like \"Tree\" and \"Chair\" and \"Person\". These are all vague categories that contain lots of different things, but are really useful for narrowing down things. Humans tend to use really similar abstractions, even across different cultures / societies.[The Natural Abstraction Hypothesis](https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro) (NAH) states that a wide variety of cognitive architectures will tend to use similar abstractions to reason about the world. This might be helpful for alignment because we could say things like \"person\" without having to rigorously and precisely say exactly what we mean by person.\n\nThe NAH seems very plausibly true for physical objects in the world, and so it might be true for the inputs to human values. If so, it would be really helpful for AI alignment because understanding this would amount to a solution to the[ontology identification problem](https://arbital.com/p/ontology_identification/): we can understand when environments induce certain abstractions, and so we can design this so that the network has the same abstractions as humans.\n\n[Modularity](https://www.lesswrong.com/s/ApA5XmewGQ8wSrv5C): In pretty much any selection environment, we see lots of obvious modularity. Biological species have cells and organs and limbs. Companies have departments. We might expect neural networks to be similar, but it is[really hard to find modules](https://www.lesswrong.com/posts/JBFHzfPkXHB2XfDGj/evolution-of-modularity) in neural networks. We need to find the right lens to look through to find this modularity in neural networks. Aiming at this can lead us to really good interpretability.\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "[John's plan](https://www.lesswrong.com/posts/3L46WGauGpr7nYubu/the-plan) is:\n\nStep 1: sort out our fundamental confusions about agency\n\nStep 2: ambitious value learning (i.e. build an AI which correctly learns human values and optimizes for them)\n\nStep 3: \u2026\n\nStep 4: profit!\n\n\u2026 and do all that before AGI kills us all.\n\nHe is working on step 1: figuring out what the heck is going on with agency. His current approach is based on[selection theorems](https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents): try to figure out what types of agents are selected for in a broad range of environments. Examples of selection pressures include: evolution, SGD, and markets. This is an approach to agent foundations that comes from the opposite direction as MIRI: it's more about observing existing structures (whether they be mathematical or real things in the world like markets or e coli), whereas MIRI is trying to write out some desiderata and then finding mathematical notions that satisfy those desiderata.\n\nTwo key properties that might be selected for are modularity and abstractions.\n\nAbstractions are higher level things that people tend to use to describe things. Like \"Tree\" and \"Chair\" and \"Person\". These are all vague categories that contain lots of different things, but are really useful for narrowing down things. Humans tend to use really similar abstractions, even across different cultures / societies.[The Natural Abstraction Hypothesis](https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro) (NAH) states that a wide variety of cognitive architectures will tend to use similar abstractions to reason about the world. This might be helpful for alignment because we could say things like \"person\" without having to rigorously and precisely say exactly what we mean by person.\n\nThe NAH seems very plausibly true for physical objects in the world, and so it might be true for the inputs to human values. If so, it would be really helpful for AI alignment because understanding this would amount to a solution to the[ontology identification problem](https://arbital.com/p/ontology_identification/): we can understand when environments induce certain abstractions, and so we can design this so that the network has the same abstractions as humans.\n\n[Modularity](https://www.lesswrong.com/s/ApA5XmewGQ8wSrv5C): In pretty much any selection environment, we see lots of obvious modularity. Biological species have cells and organs and limbs. Companies have departments. We might expect neural networks to be similar, but it is[really hard to find modules](https://www.lesswrong.com/posts/JBFHzfPkXHB2XfDGj/evolution-of-modularity) in neural networks. We need to find the right lens to look through to find this modularity in neural networks. Aiming at this can lead us to really good interpretability.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8378", "Related Answers": "What is neural network modularity?", "Doc Last Ingested": "2023-03-14T23:34:03.338+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 1426, "Helpful": ""}}, {"id": "i-c67d5dd77199f80183f6eafbb80eaf38f2bb2c7a794aa0573045c369276e1040", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c67d5dd77199f80183f6eafbb80eaf38f2bb2c7a794aa0573045c369276e1040", "name": "What is Goodhart's law?", "index": 339, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:42.193Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c67d5dd77199f80183f6eafbb80eaf38f2bb2c7a794aa0573045c369276e1040", "values": {"File": "What is Goodhart's law?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Goodhart's law?", "Link": "https://docs.google.com/document/d/1Fx4lB7XLRkZnusXLfqx9Ynh6pTcZGvIoQGNzDJZxnrM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:06.439+01:00", "Related Answers DO NOT EDIT": "Why might we expect a superintelligence to be hostile by default?", "Tags": "Goodhart's Law,Definitions", "Doc Last Edited": "2023-02-22T23:04:29.547+01:00", "Status": "Live on site", "Edit Answer": "What is Goodhart's law?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8185", "Source Link": "", "aisafety.info Link": "What is Goodhart's law?", "Source": "Wiki", "All Phrasings": "What is Goodhart's law?\n", "Initial Order": "", "Related IDs": "6982", "Rich Text DO NOT EDIT": "[https://www.lesswrong.com/tag/goodhart-s-law](https://www.lesswrong.com/tag/goodhart-s-law)\n\n", "Tag Count": 2, "Related Answer Count": 1, "Rich Text": "[https://www.lesswrong.com/tag/goodhart-s-law](https://www.lesswrong.com/tag/goodhart-s-law)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "8185", "Related Answers": "Why might we expect a superintelligence to be hostile by default?", "Doc Last Ingested": "2023-03-14T23:36:10.169+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-b327829f0b452e8eaffdc5129d09033a47598e12fa3e5d1f69f1c67e6242711f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b327829f0b452e8eaffdc5129d09033a47598e12fa3e5d1f69f1c67e6242711f", "name": "What is GPT-3?", "index": 340, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:44.718Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b327829f0b452e8eaffdc5129d09033a47598e12fa3e5d1f69f1c67e6242711f", "values": {"File": "What is GPT-3?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is GPT-3?", "Link": "https://docs.google.com/document/d/1w7wPXOCbRvsQI6fuCKBetbOB7JoReRsu6CuxDjixezo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:49:02.977+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Language Models", "Doc Last Edited": "2023-02-22T23:04:30.422+01:00", "Status": "Live on site", "Edit Answer": "What is GPT-3?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6627", "Source Link": "", "aisafety.info Link": "What is GPT-3?", "Source": "Wiki", "All Phrasings": "What is GPT-3?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "GPT-3 is the newest and most impressive of the [GPT](https://www.alignmentforum.org/tag/gpt) (Generative Pretrained Transformer) series of large transformer-based language models created by OpenAI. It was announced in June 2020, and is 100 times larger than its predecessor GPT-2.(ref)[GPT-3: What\u2019s it good for?](https://www.cambridge.org/core/journals/natural-language-engineering/article/gpt3-whats-it-good-for/0E05CFE68A7AC8BF794C8ECBE28AA990) - Cambridge University Press(/ref)\n\nGwern has several resources exploring GPT-3's abilities, limitations, and implications including:\n\n- [The Scaling Hypothesis](https://www.gwern.net/Scaling-hypothesis) - How simply increasing the amount of compute with current algorithms might create very powerful systems.\n\n- [GPT-3 Nonfiction](https://www.gwern.net/GPT-3-nonfiction)\n\n- [GPT-3 Creative Fiction](https://www.gwern.net/GPT-3)\n\nVox has [an article](https://www.vox.com/future-perfect/21355768/gpt-3-ai-openai-turing-test-language) which explains why GPT-3 is a big deal.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "GPT-3 is the newest and most impressive of the [GPT](https://www.alignmentforum.org/tag/gpt) (Generative Pretrained Transformer) series of large transformer-based language models created by OpenAI. It was announced in June 2020, and is 100 times larger than its predecessor GPT-2.(ref)[GPT-3: What\u2019s it good for?](https://www.cambridge.org/core/journals/natural-language-engineering/article/gpt3-whats-it-good-for/0E05CFE68A7AC8BF794C8ECBE28AA990) - Cambridge University Press(/ref)\n\nGwern has several resources exploring GPT-3's abilities, limitations, and implications including:\n\n- [The Scaling Hypothesis](https://www.gwern.net/Scaling-hypothesis) - How simply increasing the amount of compute with current algorithms might create very powerful systems.\n\n- [GPT-3 Nonfiction](https://www.gwern.net/GPT-3-nonfiction)\n\n- [GPT-3 Creative Fiction](https://www.gwern.net/GPT-3)\n\nVox has [an article](https://www.vox.com/future-perfect/21355768/gpt-3-ai-openai-turing-test-language) which explains why GPT-3 is a big deal.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6627", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:11.908+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-bd9d61a6e74150af223a6344d74313c250d362e9342e9f3d55788f730f6faefc", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-bd9d61a6e74150af223a6344d74313c250d362e9342e9f3d55788f730f6faefc", "name": "What is Dylan Hadfield-Menell's thesis on?", "index": 341, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:46.830Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-bd9d61a6e74150af223a6344d74313c250d362e9342e9f3d55788f730f6faefc", "values": {"File": "What is Dylan Hadfield-Menell's thesis on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Dylan Hadfield-Menell's thesis on?", "Link": "https://docs.google.com/document/d/12IaFPBy8arFcZbdR1meww_7S6f5RZF55vlEIZ2GJXso/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:59.307+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:31.354+01:00", "Status": "Live on site", "Edit Answer": "What is Dylan Hadfield-Menell's thesis on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8348", "Source Link": "", "aisafety.info Link": "What is Dylan Hadfield-Menell's thesis on?", "Source": "Wiki", "All Phrasings": "What is Dylan Hadfield-Menell's thesis on?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[Dylan's PhD thesis](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-207.pdf) argues three main claims (paraphrased):\n\n- Outer alignment failures are a problem.\n\n- We can mitigate this problem by adding in uncertainty.\n\n- We can model this as[Cooperative Inverse Reinforcement Learning (CIRL)](https://proceedings.neurips.cc/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html).\n\nThus, his motivations seem to be modeling AGI coming in some multi-agent form, and also being heavily connected with human operators.\n\nWe're not certain what he is currently working on, but some recent alignment-relevant papers that he has published include:\n\n- [Work on instantiating norms into AIs to incentivize deference to humans](https://www.pnas.org/doi/10.1073/pnas.2106028118).\n\n- [Theoretically formulating the principal-agent problem](https://arxiv.org/abs/2102.03896).\n\nDylan has also published a number of articles that seem less directly relevant for alignment.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "[Dylan's PhD thesis](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-207.pdf) argues three main claims (paraphrased):\n\n- Outer alignment failures are a problem.\n\n- We can mitigate this problem by adding in uncertainty.\n\n- We can model this as[Cooperative Inverse Reinforcement Learning (CIRL)](https://proceedings.neurips.cc/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html).\n\nThus, his motivations seem to be modeling AGI coming in some multi-agent form, and also being heavily connected with human operators.\n\nWe're not certain what he is currently working on, but some recent alignment-relevant papers that he has published include:\n\n- [Work on instantiating norms into AIs to incentivize deference to humans](https://www.pnas.org/doi/10.1073/pnas.2106028118).\n\n- [Theoretically formulating the principal-agent problem](https://arxiv.org/abs/2102.03896).\n\nDylan has also published a number of articles that seem less directly relevant for alignment.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8348", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:13.459+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-4265c1643448a63bddd2bf09f5356163ebe8af6205a4150b01e44224a5db0f90", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4265c1643448a63bddd2bf09f5356163ebe8af6205a4150b01e44224a5db0f90", "name": "What is David Krueger working on?", "index": 342, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:50.920Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4265c1643448a63bddd2bf09f5356163ebe8af6205a4150b01e44224a5db0f90", "values": {"File": "What is David Krueger working on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is David Krueger working on?", "Link": "https://docs.google.com/document/d/1KwVC7PLYjAcYATqUaYvoZoN4esQuQ8mmHfRlxDVsano/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:55.991+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:32.220+01:00", "Status": "Live on site", "Edit Answer": "What is David Krueger working on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8342", "Source Link": "", "aisafety.info Link": "What is David Krueger working on?", "Source": "Wiki", "All Phrasings": "What is David Krueger working on?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "David runs a lab at the University of Cambridge. Some things he is working on include:\n\n1. Operationalizing inner alignment failures and other speculative alignment failures that haven't actually been observed.\n\n1. Understanding neural network generalization.\n\nFor work done on (1), see: [Goal Misgeneralization](https://arxiv.org/abs/2105.14111), a paper that empirically demonstrated examples of inner alignment failure in Deep RL environments. For example, they trained an agent to get closer to cheese in a maze, but where the cheese was always in the top right of a maze in the training set. During test time, when presented with cheese elsewhere, the RL agent navigated to the top right instead of to the cheese: it had learned the mesa objective of \"go to the top right\".\n\nFor work done on (2), see [OOD Generalization via Risk Extrapolation](http://proceedings.mlr.press/v139/krueger21a.html), an iterative improvement on robustness to previous methods.\n\nWe've not read about his motivation is for these specific research directions, but these are likely his best starts on how to solve the alignment problem.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "David runs a lab at the University of Cambridge. Some things he is working on include:\n\n1. Operationalizing inner alignment failures and other speculative alignment failures that haven't actually been observed.\n\n1. Understanding neural network generalization.\n\nFor work done on (1), see: [Goal Misgeneralization](https://arxiv.org/abs/2105.14111), a paper that empirically demonstrated examples of inner alignment failure in Deep RL environments. For example, they trained an agent to get closer to cheese in a maze, but where the cheese was always in the top right of a maze in the training set. During test time, when presented with cheese elsewhere, the RL agent navigated to the top right instead of to the cheese: it had learned the mesa objective of \"go to the top right\".\n\nFor work done on (2), see [OOD Generalization via Risk Extrapolation](http://proceedings.mlr.press/v139/krueger21a.html), an iterative improvement on robustness to previous methods.\n\nWe've not read about his motivation is for these specific research directions, but these are likely his best starts on how to solve the alignment problem.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8342", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:15.257+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-714d2666b91545009ad837fccc36510ffedaec302c364afc7700ca41d54e160a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-714d2666b91545009ad837fccc36510ffedaec302c364afc7700ca41d54e160a", "name": "What is Conjecture, and what is their team working on?", "index": 343, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:54.398Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-714d2666b91545009ad837fccc36510ffedaec302c364afc7700ca41d54e160a", "values": {"File": "What is Conjecture, and what is their team working on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Conjecture, and what is their team working on?", "Link": "https://docs.google.com/document/d/1R25Hb_EXHdG8qFvMTk1bwg8XnJSxvam5JQsRBBWa_Jk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:52.518+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:33.027+01:00", "Status": "Live on site", "Edit Answer": "What is Conjecture, and what is their team working on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8332", "Source Link": "", "aisafety.info Link": "What is Conjecture, and what is their team working on?", "Source": "Wiki", "All Phrasings": "What is Conjecture, and what is their team working on?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[Conjecture](https://www.lesswrong.com/posts/jfq2BH5kfQqu2vYv3/we-are-conjecture-a-new-alignment-research-startup#Our_Research_Agenda) is an applied org focused on aligning LLMs (Q & A [here)](https://forum.effectivealtruism.org/posts/QR7yGoFBonY6hege9/connor-leahy-on-conjecture-and-dying-with-dignity). Conjecture has short timelines (the org acts like timelines are between ~5-10 year, but some have much shorter timelines, such as 2-4 years), and they think alignment is hard. They take information hazards (specifically ideas that could lead towards better AI capabilities) very seriously, and have a [public infohazard document](https://www.lesswrong.com/posts/Gs29k3beHiqWFZqnn/conjecture-internal-infohazard-policy).\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "[Conjecture](https://www.lesswrong.com/posts/jfq2BH5kfQqu2vYv3/we-are-conjecture-a-new-alignment-research-startup#Our_Research_Agenda) is an applied org focused on aligning LLMs (Q & A [here)](https://forum.effectivealtruism.org/posts/QR7yGoFBonY6hege9/connor-leahy-on-conjecture-and-dying-with-dignity). Conjecture has short timelines (the org acts like timelines are between ~5-10 year, but some have much shorter timelines, such as 2-4 years), and they think alignment is hard. They take information hazards (specifically ideas that could lead towards better AI capabilities) very seriously, and have a [public infohazard document](https://www.lesswrong.com/posts/Gs29k3beHiqWFZqnn/conjecture-internal-infohazard-policy).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8332", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:16.131+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-6696b213c8252ee8af4d3326ab6bb7ea6d4df081331f0383f7ec5a8a1d002ac0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-6696b213c8252ee8af4d3326ab6bb7ea6d4df081331f0383f7ec5a8a1d002ac0", "name": "What is Conjecture's epistemology research agenda?", "index": 344, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:59.567Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-6696b213c8252ee8af4d3326ab6bb7ea6d4df081331f0383f7ec5a8a1d002ac0", "values": {"File": "What is Conjecture's epistemology research agenda?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Conjecture's epistemology research agenda?", "Link": "https://docs.google.com/document/d/1yyRofYLLYV8zB2b4VhT4AGr4BwVPRCWtqrgYyzgLpFQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:49.148+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Research Agendas", "Doc Last Edited": "2023-02-22T23:04:33.908+01:00", "Status": "Live on site", "Edit Answer": "What is Conjecture's epistemology research agenda?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8334", "Source Link": "", "aisafety.info Link": "What is Conjecture's epistemology research agenda?", "Source": "Wiki", "All Phrasings": "What is Conjecture's epistemology research agenda?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The alignment problem is really hard to do science on: we are trying to reason about the future, and we only get one shot, meaning that [we can't iterate](https://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic). Therefore, it seems really useful to have a good understanding of meta-science/epistemology, i.e. reasoning about ways to do useful alignment research.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "The alignment problem is really hard to do science on: we are trying to reason about the future, and we only get one shot, meaning that [we can't iterate](https://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic). Therefore, it seems really useful to have a good understanding of meta-science/epistemology, i.e. reasoning about ways to do useful alignment research.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8334", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:17.408+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-92f01a5cc9a13b2df5e6599d9ca3f40b52aed7861978446ee1628d8fb2e6206b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-92f01a5cc9a13b2df5e6599d9ca3f40b52aed7861978446ee1628d8fb2e6206b", "name": "What is Conjecture's Scalable LLM Interpretability research agenda?", "index": 345, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:04.936Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-92f01a5cc9a13b2df5e6599d9ca3f40b52aed7861978446ee1628d8fb2e6206b", "values": {"File": "What is Conjecture's Scalable LLM Interpretability research agenda?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Conjecture's Scalable LLM Interpretability research agenda?", "Link": "https://docs.google.com/document/d/1Je5apMjKL9vlqXbzatCmI24bAnZXWWFeG4zrIrniOjI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:44.833+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Research Agendas", "Doc Last Edited": "2023-02-22T22:54:59.751+01:00", "Status": "In progress", "Edit Answer": "What is Conjecture's Scalable LLM Interpretability research agenda?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8335", "Source Link": "", "aisafety.info Link": "What is Conjecture's Scalable LLM Interpretability research agenda?", "Source": "Wiki", "All Phrasings": "What is Conjecture's Scalable LLM Interpretability research agenda?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "I don't know much about their research here, other than that they train their own models, which allow them to work on models that are bigger than the biggest publicly available models, which seems like a difference from Redwood.\n\nCurrent interpretability methods are very low level (e.g., \"what does x neuron do\"), which does not help us answer high level questions like \"is this AI trying to kill us\".\n\nThey are trying a bunch of weird approaches, with the goal of scalable mechanistic interpretability, but I do not know what these approaches actually are.\n\nMotivation: Conjecture wants to build towards a better paradigm that will give us a lot more information, primarily from the empirical direction (as distinct from ARC, which is working on interpretability with a theoretical focus).\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "I don't know much about their research here, other than that they train their own models, which allow them to work on models that are bigger than the biggest publicly available models, which seems like a difference from Redwood.\n\nCurrent interpretability methods are very low level (e.g., \"what does x neuron do\"), which does not help us answer high level questions like \"is this AI trying to kill us\".\n\nThey are trying a bunch of weird approaches, with the goal of scalable mechanistic interpretability, but I do not know what these approaches actually are.\n\nMotivation: Conjecture wants to build towards a better paradigm that will give us a lot more information, primarily from the empirical direction (as distinct from ARC, which is working on interpretability with a theoretical focus).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8335", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:18.427+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-9c275e110353704ad803a964e8067a3a7b36cf0a66fe01dbb912eb5b8ec4fdc4", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-9c275e110353704ad803a964e8067a3a7b36cf0a66fe01dbb912eb5b8ec4fdc4", "name": "What is Artificial General Intelligence (AGI) and what will it look like?", "index": 346, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:07.549Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-9c275e110353704ad803a964e8067a3a7b36cf0a66fe01dbb912eb5b8ec4fdc4", "values": {"File": "What is Artificial General Intelligence (AGI) and what will it look like?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Artificial General Intelligence (AGI) and what will it look like?", "Link": "https://docs.google.com/document/d/1KgcRBRu_rBZ45oClKu6VdisOUxmSXtgHVUnjRkGM06U/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:41.349+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI", "Doc Last Edited": "2023-02-22T23:04:35.006+01:00", "Status": "Live on site", "Edit Answer": "What is Artificial General Intelligence (AGI) and what will it look like?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "2374", "Source Link": "", "aisafety.info Link": "What is Artificial General Intelligence (AGI) and what will it look like?", "Source": "Wiki", "All Phrasings": "What is Artificial General Intelligence (AGI) and what will it look like?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "AGI is an algorithm with [general intelligence](https://www.lesswrong.com/posts/yLeEPFnnB9wE7KLx2/efficient-cross-domain-optimization), running not on evolution\u2019s biology like all current general intelligences but on a substrate such as silicon engineered by an intelligence (initially computers designed by humans, later on likely dramatically more advanced hardware designed by earlier AGIs).\n\nAI has so far always been designed and built by humans (i.e. a search process running on biological brains), but once our creations gain the ability to do AI research they will likely [recursively self-improve](https://www.lesswrong.com/posts/JBadX7rwdcRFzGuju/recursive-self-improvement) by designing new and better versions of themselves initiating an [intelligence explosion](https://www.lesswrong.com/posts/8vpf46nLMDYPC6wA4/optimization-and-the-intelligence-explosion) (i.e. use it\u2019s intelligence to improve its own intelligence, creating a feedback loop), and resulting in a superintelligence. There are already [early signs](https://arxiv.org/abs/2101.07367) of AIs being trained to optimize other AIs.\n\nSome authors (notably [Robin Hanson](https://intelligence.org/ai-foom-debate/)) have argued that the intelligence explosion hypothesis is likely false, and in favor of a large number of roughly human level emulated minds operating instead, forming an uplifted economy which doubles every few hours. Eric Drexler\u2019s [Comprehensive AI Services](https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/) model of what may happen is another alternate view, where many narrow superintelligent systems exist in parallel rather than there being a general-purpose superintelligent agent.\n\nGoing by the model advocated by [Nick Bostrom](https://en.wikipedia.org/wiki/Nick_Bostrom), [Eliezer Yudkowsky](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky) and many others, a superintelligence will likely gain various [cognitive superpowers](https://publicism.info/philosophy/superintelligence/7.html) (table 8 gives a good overview), allowing it to direct the future much more effectively than humanity. Taking control of our resources by manipulation and hacking is a likely early step, followed by developing and deploying advanced technologies like [molecular nanotechnology](https://en.wikipedia.org/wiki/Molecular_nanotechnology) to dominate the physical world and achieve its goals.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "AGI is an algorithm with [general intelligence](https://www.lesswrong.com/posts/yLeEPFnnB9wE7KLx2/efficient-cross-domain-optimization), running not on evolution\u2019s biology like all current general intelligences but on a substrate such as silicon engineered by an intelligence (initially computers designed by humans, later on likely dramatically more advanced hardware designed by earlier AGIs).\n\nAI has so far always been designed and built by humans (i.e. a search process running on biological brains), but once our creations gain the ability to do AI research they will likely [recursively self-improve](https://www.lesswrong.com/posts/JBadX7rwdcRFzGuju/recursive-self-improvement) by designing new and better versions of themselves initiating an [intelligence explosion](https://www.lesswrong.com/posts/8vpf46nLMDYPC6wA4/optimization-and-the-intelligence-explosion) (i.e. use it\u2019s intelligence to improve its own intelligence, creating a feedback loop), and resulting in a superintelligence. There are already [early signs](https://arxiv.org/abs/2101.07367) of AIs being trained to optimize other AIs.\n\nSome authors (notably [Robin Hanson](https://intelligence.org/ai-foom-debate/)) have argued that the intelligence explosion hypothesis is likely false, and in favor of a large number of roughly human level emulated minds operating instead, forming an uplifted economy which doubles every few hours. Eric Drexler\u2019s [Comprehensive AI Services](https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/) model of what may happen is another alternate view, where many narrow superintelligent systems exist in parallel rather than there being a general-purpose superintelligent agent.\n\nGoing by the model advocated by [Nick Bostrom](https://en.wikipedia.org/wiki/Nick_Bostrom), [Eliezer Yudkowsky](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky) and many others, a superintelligence will likely gain various [cognitive superpowers](https://publicism.info/philosophy/superintelligence/7.html) (table 8 gives a good overview), allowing it to direct the future much more effectively than humanity. Taking control of our resources by manipulation and hacking is a likely early step, followed by developing and deploying advanced technologies like [molecular nanotechnology](https://en.wikipedia.org/wiki/Molecular_nanotechnology) to dominate the physical world and achieve its goals.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 3, "Asker": "Jack Harley", "External Source": "", "Last Asked On Discord": "", "UI ID": "2374", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:20.953+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-36d9ee491fd85202aa33053e9ff3dd6518b324881f296a960aee388419b46c3a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-36d9ee491fd85202aa33053e9ff3dd6518b324881f296a960aee388419b46c3a", "name": "What is Anthropic's approach to LLM alignment?", "index": 347, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:12.345Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-36d9ee491fd85202aa33053e9ff3dd6518b324881f296a960aee388419b46c3a", "values": {"File": "What is Anthropic's approach to LLM alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Anthropic's approach to LLM alignment?", "Link": "https://docs.google.com/document/d/1AdJE49jW0PF5_pJHW5j8PbnPSOtOxwVp9VgmBfrL7bA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:37.454+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Anthropic", "Doc Last Edited": "2023-02-22T23:04:35.824+01:00", "Status": "Live on site", "Edit Answer": "What is Anthropic's approach to LLM alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8320", "Source Link": "", "aisafety.info Link": "What is Anthropic's approach to LLM alignment?", "Source": "Wiki", "All Phrasings": "What is Anthropic's approach to LLM alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Anthropic fine tuned a language model to be more helpful, honest and harmless: [HHH](https://arxiv.org/abs/2112.00861).\n\nMotivation: The point of this is to:\n\n1. see if we can \"align\" a current day LLM, and\n\n1. raise awareness about safety in the broader ML community.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Anthropic fine tuned a language model to be more helpful, honest and harmless: [HHH](https://arxiv.org/abs/2112.00861).\n\nMotivation: The point of this is to:\n\n1. see if we can \"align\" a current day LLM, and\n\n1. raise awareness about safety in the broader ML community.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8320", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:22.376+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-fd91277096b38a8f061873b9d9ca264b0764bc0ef57725848246c1af9a14de57", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-fd91277096b38a8f061873b9d9ca264b0764bc0ef57725848246c1af9a14de57", "name": "What is Aligned AI / Stuart Armstrong working on?", "index": 348, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:15.023Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-fd91277096b38a8f061873b9d9ca264b0764bc0ef57725848246c1af9a14de57", "values": {"File": "What is Aligned AI / Stuart Armstrong working on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is Aligned AI / Stuart Armstrong working on?", "Link": "https://docs.google.com/document/d/1e7EZ6Fuaxu0C0KTZQEAn8rBbkxjqykLTULh_erEnX-E/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:34.023+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:36.783+01:00", "Status": "Live on site", "Edit Answer": "What is Aligned AI / Stuart Armstrong working on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8314", "Source Link": "", "aisafety.info Link": "What is Aligned AI / Stuart Armstrong working on?", "Source": "Wiki", "All Phrasings": "What is Aligned AI / Stuart Armstrong working on?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "One of the key problems in AI safety is that there are many ways for an AI to generalize off-distribution, so it is very likely that an arbitrary generalization will be unaligned. See the [model splintering post](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1) for more detail. Stuart's plan to solve this problem is as follows:\n\n1. Maintain a set of all possible extrapolations of reward data that are consistent with the training process.\n\n1. Pick among these for a safe reward extrapolation.\n\nThey are currently working on algorithms to accomplish step 1: see [Value Extrapolation](https://www.lesswrong.com/posts/i8sHdLyGQeBTGwTqq/value-extrapolation-concept-extrapolation-model-splintering).\n\nTheir initial operationalization of this problem is the lion and husky problem. Basically: if you train an image model on a dataset of images of lions and huskies, the lions are always in the desert, and the huskies are always in the snow. So the problem of learning a classifier is under-defined: should the classifier be classifying based on the background environment (e.g. snow vs sand), or based on the animal in the image?\n\nA good extrapolation algorithm, on this problem, would generate classifiers that extrapolate in all the different ways[4], and so the 'correct' extrapolation must be in this generated set of classifiers. They have also introduced a new dataset for this, with a similar idea: [Happy Faces](https://www.lesswrong.com/posts/DiEWbwrChuzuhJhGr/benchmark-for-successful-concept-extrapolation-avoiding-goal).\n\nStep 2 could be done in different ways. Possibilities for doing this include: [conservatism](https://www.lesswrong.com/posts/PADPJ3xac5ogjEGwA/defeating-goodhart-and-the-closest-unblocked-strategy), [generalized deference to humans](https://www.lesswrong.com/posts/BeeirdrMXCPYZwgfj/the-blue-minimising-robot-and-model-splintering), or an automated process for removing some goals. like wireheading/deception/killing everyone.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "One of the key problems in AI safety is that there are many ways for an AI to generalize off-distribution, so it is very likely that an arbitrary generalization will be unaligned. See the [model splintering post](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1) for more detail. Stuart's plan to solve this problem is as follows:\n\n1. Maintain a set of all possible extrapolations of reward data that are consistent with the training process.\n\n1. Pick among these for a safe reward extrapolation.\n\nThey are currently working on algorithms to accomplish step 1: see [Value Extrapolation](https://www.lesswrong.com/posts/i8sHdLyGQeBTGwTqq/value-extrapolation-concept-extrapolation-model-splintering).\n\nTheir initial operationalization of this problem is the lion and husky problem. Basically: if you train an image model on a dataset of images of lions and huskies, the lions are always in the desert, and the huskies are always in the snow. So the problem of learning a classifier is under-defined: should the classifier be classifying based on the background environment (e.g. snow vs sand), or based on the animal in the image?\n\nA good extrapolation algorithm, on this problem, would generate classifiers that extrapolate in all the different ways[4], and so the 'correct' extrapolation must be in this generated set of classifiers. They have also introduced a new dataset for this, with a similar idea: [Happy Faces](https://www.lesswrong.com/posts/DiEWbwrChuzuhJhGr/benchmark-for-successful-concept-extrapolation-avoiding-goal).\n\nStep 2 could be done in different ways. Possibilities for doing this include: [conservatism](https://www.lesswrong.com/posts/PADPJ3xac5ogjEGwA/defeating-goodhart-and-the-closest-unblocked-strategy), [generalized deference to humans](https://www.lesswrong.com/posts/BeeirdrMXCPYZwgfj/the-blue-minimising-robot-and-model-splintering), or an automated process for removing some goals. like wireheading/deception/killing everyone.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8314", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:24.143+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-8899afafb469e7a7e3691f2b506fec68b4567eb11d991ce0f21e99ad1527f4ee", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8899afafb469e7a7e3691f2b506fec68b4567eb11d991ce0f21e99ad1527f4ee", "name": "What is AI safety?", "index": 349, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:20.957Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8899afafb469e7a7e3691f2b506fec68b4567eb11d991ce0f21e99ad1527f4ee", "values": {"File": "What is AI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is AI safety?", "Link": "https://docs.google.com/document/d/1zz1c6rRN8Y-CmO0-BGVKI9G6MMg2MUqD247dPcub144/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:30.226+01:00", "Related Answers DO NOT EDIT": "What is the difference between AI Safety, AI alignment, AI Control, Friendly AI, AI Ethics, AI existential safety and AGI safety?,What approaches are AI alignment organizations working on?", "Tags": "", "Doc Last Edited": "2023-02-26T16:59:46.855+01:00", "Status": "Live on site", "Edit Answer": "What is AI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8486", "Source Link": "", "aisafety.info Link": "What is AI safety?", "Source": "Wiki", "All Phrasings": "What is AI safety?\n", "Initial Order": 1, "Related IDs": "6714,6178", "Rich Text DO NOT EDIT": "AI safety is a research field founded to avoid catastrophic outcomes from advanced AI, though the term has since expanded to include reducing less extreme harms from AI.\n\n**AI existential safety**, or *AGI safety* is about reducing the [existential risk from artificial](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence) *[general](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence)* [intelligence](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence) (AGI). Artificial general intelligence is AI that is at least as competent as humans in all skills that are relevant for making a difference in the world. AGI has not been developed yet, but [will likely be](https://80000hours.org/problem-profiles/artificial-intelligence/#when-can-we-expect-to-develop-transformative-AI) [developed](https://80000hours.org/problem-profiles/artificial-intelligence/#when-can-we-expect-to-develop-transformative-AI) [in this century](https://80000hours.org/problem-profiles/artificial-intelligence/#when-can-we-expect-to-develop-transformative-AI). \n\nA central part of AGI safety is ensuring that what AIs do is actually what we want. This is called **AI alignment** (also often just called *alignment*), because it\u2019s about *aligning* an AI with human values. [Alignment is difficult](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/), and building AGI is [probably very dangerous](https://80000hours.org/problem-profiles/artificial-intelligence/#power-seeking-ai), so it is important to mitigate the risks as much as possible. Examples for work on AI existential safety are\n\n- trying to get a foundational understandingwhat intelligence is, e.g.[agent foundations](https://www.alignmentforum.org/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation)\n\n- [Outer and inner alignment](https://docs.google.com/document/d/1TR5UYmFjwA-FAttyMR_vB5aXQu1gcLTwaHvY2v9ObCM/edit): Ensure the objective of the training process is actually what we want, and also ensure the objective of the resulting system is actually what we want.\n\n- [AI policy/strategy](https://80000hours.org/articles/ai-policy-guide/): e.g.researching the best way to set up institutions and mechanisms that help with safe AGI development,making sure AI isn\u2019t used by bad actors\n\nThere are also areas of research which are useful for both near-term, and for existential safety. For example, [robustness to distribution shift](https://www.lesswrong.com/tag/distributional-shifts), and [interpretability](https://docs.google.com/document/d/1hHAx92e89YQfBXT96C7BLiuMipcl6LOszGlG2L7rrZo/edit) both help with making current systems safer, and are likely to help with AGI safety.\n\n**Near-term AI safety**is about preventing bad outcomes from *current*systems. Examples for work on near-term AI safety are\n\n- getting content recommender systems to not radicalize their users\n\n- ensuring autonomous cars don\u2019t kill people\n\n- advocating strict regulations for lethal autonomous weapons\n\nWhile the near-term AI safety is significant, this FAQ is focused on providing information which supports the goals of the former as it has the potential to be dramatically more important for humanity\u2019s future.\n\n", "Tag Count": 0, "Related Answer Count": 2, "Rich Text": "AI safety is a research field founded to avoid catastrophic outcomes from advanced AI, though the term has since expanded to include reducing less extreme harms from AI.\n\n**AI existential safety**, or *AGI safety* is about reducing the [existential risk from artificial](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence) *[general](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence)* [intelligence](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence) (AGI). Artificial general intelligence is AI that is at least as competent as humans in all skills that are relevant for making a difference in the world. AGI has not been developed yet, but [will likely be](https://80000hours.org/problem-profiles/artificial-intelligence/#when-can-we-expect-to-develop-transformative-AI) [developed](https://80000hours.org/problem-profiles/artificial-intelligence/#when-can-we-expect-to-develop-transformative-AI) [in this century](https://80000hours.org/problem-profiles/artificial-intelligence/#when-can-we-expect-to-develop-transformative-AI). \n\nA central part of AGI safety is ensuring that what AIs do is actually what we want. This is called **AI alignment** (also often just called *alignment*), because it\u2019s about *aligning* an AI with human values. [Alignment is difficult](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/), and building AGI is [probably very dangerous](https://80000hours.org/problem-profiles/artificial-intelligence/#power-seeking-ai), so it is important to mitigate the risks as much as possible. Examples for work on AI existential safety are\n\n- trying to get a foundational understandingwhat intelligence is, e.g.[agent foundations](https://www.alignmentforum.org/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation)\n\n- [Outer and inner alignment](https://docs.google.com/document/d/1TR5UYmFjwA-FAttyMR_vB5aXQu1gcLTwaHvY2v9ObCM/edit): Ensure the objective of the training process is actually what we want, and also ensure the objective of the resulting system is actually what we want.\n\n- [AI policy/strategy](https://80000hours.org/articles/ai-policy-guide/): e.g.researching the best way to set up institutions and mechanisms that help with safe AGI development,making sure AI isn\u2019t used by bad actors\n\nThere are also areas of research which are useful for both near-term, and for existential safety. For example, [robustness to distribution shift](https://www.lesswrong.com/tag/distributional-shifts), and [interpretability](https://docs.google.com/document/d/1hHAx92e89YQfBXT96C7BLiuMipcl6LOszGlG2L7rrZo/edit) both help with making current systems safer, and are likely to help with AGI safety.\n\n**Near-term AI safety**is about preventing bad outcomes from *current*systems. Examples for work on near-term AI safety are\n\n- getting content recommender systems to not radicalize their users\n\n- ensuring autonomous cars don\u2019t kill people\n\n- advocating strict regulations for lethal autonomous weapons\n\nWhile the near-term AI safety is significant, this FAQ is focused on providing information which supports the goals of the former as it has the potential to be dramatically more important for humanity\u2019s future.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "Magdalena", "External Source": "", "Last Asked On Discord": "", "UI ID": "8486", "Related Answers": "What is the difference between AI Safety, AI alignment, AI Control, Friendly AI, AI Ethics, AI existential safety and AGI safety?,What approaches are AI alignment organizations working on?", "Doc Last Ingested": "2023-03-14T23:36:25.971+01:00", "Request Count": "", "Number of suggestions on answer doc": 48, "Total character count of suggestions on answer doc": 1528, "Helpful": 4}}, {"id": "i-45bfa544bf341cc543f8dec3b6700e8105a08d5c4c80698f8607290af71366b9", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-45bfa544bf341cc543f8dec3b6700e8105a08d5c4c80698f8607290af71366b9", "name": "What is AI Safety via Debate?", "index": 350, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:23.463Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-45bfa544bf341cc543f8dec3b6700e8105a08d5c4c80698f8607290af71366b9", "values": {"File": "What is AI Safety via Debate?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is AI Safety via Debate?", "Link": "https://docs.google.com/document/d/16OjPU5uUym3WnKoDf1n3OAriA4uHqHsCORkABNHVdWs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:26.825+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Debate", "Doc Last Edited": "2023-03-13T23:44:51.975+01:00", "Status": "Live on site", "Edit Answer": "What is AI Safety via Debate?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8201", "Source Link": "", "aisafety.info Link": "What is AI Safety via Debate?", "Source": "Wiki", "All Phrasings": "What is AI Safety via Debate?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**Debate** is a proposed technique for allowing human evaluators to get correct and helpful answers from experts, even if the evaluator is not themselves an expert or able to fully verify the answers.[^7clr966emb9]\u00a0The technique was suggested as part of an approach to build advanced AI systems that are aligned with human values, and to safely apply machine learning techniques to problems that have high stakes, but are not well-defined (such as advancing science or increase a company's revenue).\u00a0[^vrcbanw2zz][^nwfhnzy6a3e]\n\n[^7clr966emb9]: [https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1](https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1)\n    \n[^vrcbanw2zz]: [https://ought.org/mission](https://ought.org/mission)\n    \n[^nwfhnzy6a3e]: [https://openai.com/blog/debate/](https://openai.com/blog/debate/)", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "**Debate** is a proposed technique for allowing human evaluators to get correct and helpful answers from experts, even if the evaluator is not themselves an expert or able to fully verify the answers.[^7clr966emb9]\u00a0The technique was suggested as part of an approach to build advanced AI systems that are aligned with human values, and to safely apply machine learning techniques to problems that have high stakes, but are not well-defined (such as advancing science or increase a company's revenue).\u00a0[^vrcbanw2zz][^nwfhnzy6a3e]\n\n[^7clr966emb9]: [https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1](https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1)\n    \n[^vrcbanw2zz]: [https://ought.org/mission](https://ought.org/mission)\n    \n[^nwfhnzy6a3e]: [https://openai.com/blog/debate/](https://openai.com/blog/debate/)", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "https://www.lesswrong.com/tag/debate-ai-safety-technique-1?edit=true", "Last Asked On Discord": "", "UI ID": "8201", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:27.860+01:00", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-89d4e5aebf76c0632cc08a2a5bbfde526a944b457f5b18cdd81919aa01d20dde", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-89d4e5aebf76c0632cc08a2a5bbfde526a944b457f5b18cdd81919aa01d20dde", "name": "What is \"whole brain emulation\"?", "index": 351, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-15T01:07:22.985Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-89d4e5aebf76c0632cc08a2a5bbfde526a944b457f5b18cdd81919aa01d20dde", "values": {"File": "What is \"whole brain emulation\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"whole brain emulation\"?", "Link": "https://docs.google.com/document/d/1fenKXrbvGeZ83hxYf_6mghsZMChxWXjGsZSqY3LZzms/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:22.943+01:00", "Related Answers DO NOT EDIT": "What safety problems are associated with whole brain emulation?,What are the ethical challenges related to whole brain emulation?,Will AI be able to think faster than humans?", "Tags": "Whole Brain Emulation", "Doc Last Edited": "2023-03-14T23:02:09.312+01:00", "Status": "In review", "Edit Answer": "What is \"whole brain emulation\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6350", "Source Link": "", "aisafety.info Link": "What is \"whole brain emulation\"?", "Source": "Wiki", "All Phrasings": "What is \"whole brain emulation\"?\n", "Initial Order": "", "Related IDs": "7605,7820,8E41", "Rich Text DO NOT EDIT": "[Whole Brain Emulation](https://80000hours.org/problem-profiles/whole-brain-emulation/)(WBE) or \u2018[mind uploading](https://en.wikipedia.org/wiki/Mind_uploading)\u2019 is a computer emulation of all the cells and connections in a human brain. So even if the underlying principles of general intelligence prove difficult to discover, we might still emulate an entire human brain and make it run at a million times its normal speedas computer circuits communicate much faster than neurons. Such a WBE could do more thinking in one hour than a normal human can in 100 years. So this would not lead immediately to qualitatively smarter-than-human intelligence, but it would lead to [faster-than-human intelligence](https://forum.effectivealtruism.org/topics/superintelligence). A WBE could be backed up,leading to a kind of immortality, and it could be copied so that hundreds or millions of WBEs could work on separate problems in parallel. If WBEs are created, they may therefore be able to solve scientific problems much quicker than ordinary humans, accelerating further technological progress.\n\nThere are reasons to believe that [the first AGI will probably not be based on WBE](https://docs.google.com/document/d/1nH-P1m50RENc6x5QKwdV-bwsz7ahbo-O3ZKK3XvNMw8/edit#).\n\nSee also:\n\n- Sandberg & Bostrom, [Whole Brain Emulation: A Roadmap](https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf)\n\n- [Blue Brain Project](http://bluebrain.epfl.ch/)\n\n", "Tag Count": 1, "Related Answer Count": 3, "Rich Text": "[Whole Brain Emulation](https://80000hours.org/problem-profiles/whole-brain-emulation/)(WBE) or \u2018[mind uploading](https://en.wikipedia.org/wiki/Mind_uploading)\u2019 is a computer emulation of all the cells and connections in a human brain. So even if the underlying principles of general intelligence prove difficult to discover, we might still emulate an entire human brain and make it run at a million times its normal speedas computer circuits communicate much faster than neurons. Such a WBE could do more thinking in one hour than a normal human can in 100 years. So this would not lead immediately to qualitatively smarter-than-human intelligence, but it would lead to [faster-than-human intelligence](https://forum.effectivealtruism.org/topics/superintelligence). A WBE could be backed up,leading to a kind of immortality, and it could be copied so that hundreds or millions of WBEs could work on separate problems in parallel. If WBEs are created, they may therefore be able to solve scientific problems much quicker than ordinary humans, accelerating further technological progress.\n\nThere are reasons to believe that [the first AGI will probably not be based on WBE](https://docs.google.com/document/d/1nH-P1m50RENc6x5QKwdV-bwsz7ahbo-O3ZKK3XvNMw8/edit#).\n\nSee also:\n\n- Sandberg & Bostrom, [Whole Brain Emulation: A Roadmap](https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf)\n\n- [Blue Brain Project](http://bluebrain.epfl.ch/)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6350", "Related Answers": "What safety problems are associated with whole brain emulation?,What are the ethical challenges related to whole brain emulation?,Will AI be able to think faster than humans?", "Doc Last Ingested": "2023-03-14T23:36:29.591+01:00", "Request Count": "", "Number of suggestions on answer doc": 52, "Total character count of suggestions on answer doc": 1537, "Helpful": ""}}, {"id": "i-9aac715cf9107f8ebea210dedf18c319b4ffcb56961f549c91bb5e4d030e8d1f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-9aac715cf9107f8ebea210dedf18c319b4ffcb56961f549c91bb5e4d030e8d1f", "name": "What is \"transformative AI\"?", "index": 352, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:35.998Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-9aac715cf9107f8ebea210dedf18c319b4ffcb56961f549c91bb5e4d030e8d1f", "values": {"File": "What is \"transformative AI\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"transformative AI\"?", "Link": "https://docs.google.com/document/d/1re1YYLTayqt_O4LpkbXH4q3Qzbru9C42fmhYnYhdyqA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:18.938+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Transformative AI", "Doc Last Edited": "2023-02-22T23:04:39.817+01:00", "Status": "Live on site", "Edit Answer": "What is \"transformative AI\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6347", "Source Link": "", "aisafety.info Link": "What is \"transformative AI\"?", "Source": "Wiki", "All Phrasings": "What is \"transformative AI\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**Transformative AI** is \"\\[...\\] AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution.\"[^hy8b4kflu8]\u00a0The concept refers to the large effects of AI systems on our well-being, the global economy, state power, international security, etc. and not to specific capabilities that AI might have (unlike the related terms [Superintelligent AI](https://www.lesswrong.com/tag/superintelligence) and [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence)).\n\nHolden Karnofsky gives a more detailed definition in [another OpenPhil 2016 post](https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/):\n\n> \\[...\\] Transformative AI is anything that fits one or more of the following descriptions (emphasis original):\n> \n> *   AI systems capable of fulfilling all the necessary functions of human scientists, unaided by humans, in developing another technology (or set of technologies) that ultimately becomes widely credited with being the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. Note that just because AI systems *could* accomplish such a thing unaided by humans doesn\u2019t mean they *would*; it\u2019s possible that human scientists would provide an important complement to such systems, and could make even faster progress working in tandem than such systems could achieve unaided. I emphasize the hypothetical possibility of AI systems conducting substantial unaided research to draw a clear distinction from the types of AI systems that exist today. I believe that AI systems capable of such broad contributions to the relevant research would likely dramatically accelerate it.\n> *   AI systems capable of performing tasks that currently (in 2016) account for the majority of full-time jobs worldwide, and/or over 50% of total world wages, unaided and for costs in the same range as what it would cost to employ humans. Aside from the fact that this would likely be sufficient for a major economic transformation relative to today, I also think that an AI with such broad abilities would likely be able to far surpass human abilities in a subset of domains, making it likely to meet one or more of the other criteria laid out here.\n> *   Surveillance, autonomous weapons, or other AI-centric technology that becomes sufficiently advanced to be the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. (This contrasts with the first point because it refers to transformative technology that is itself AI-centric, whereas the first point refers to AI used to speed research on some other transformative technology.)\n\n[^hy8b4kflu8]: As defined by [Open Philanthropy's Holden Karnofsky in 2016](https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence-the-philanthropic-opportunity/), and reused by [the Center for the Governance of AI in 2018](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf)", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "**Transformative AI** is \"\\[...\\] AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution.\"[^hy8b4kflu8]\u00a0The concept refers to the large effects of AI systems on our well-being, the global economy, state power, international security, etc. and not to specific capabilities that AI might have (unlike the related terms [Superintelligent AI](https://www.lesswrong.com/tag/superintelligence) and [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence)).\n\nHolden Karnofsky gives a more detailed definition in [another OpenPhil 2016 post](https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/):\n\n> \\[...\\] Transformative AI is anything that fits one or more of the following descriptions (emphasis original):\n> \n> *   AI systems capable of fulfilling all the necessary functions of human scientists, unaided by humans, in developing another technology (or set of technologies) that ultimately becomes widely credited with being the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. Note that just because AI systems *could* accomplish such a thing unaided by humans doesn\u2019t mean they *would*; it\u2019s possible that human scientists would provide an important complement to such systems, and could make even faster progress working in tandem than such systems could achieve unaided. I emphasize the hypothetical possibility of AI systems conducting substantial unaided research to draw a clear distinction from the types of AI systems that exist today. I believe that AI systems capable of such broad contributions to the relevant research would likely dramatically accelerate it.\n> *   AI systems capable of performing tasks that currently (in 2016) account for the majority of full-time jobs worldwide, and/or over 50% of total world wages, unaided and for costs in the same range as what it would cost to employ humans. Aside from the fact that this would likely be sufficient for a major economic transformation relative to today, I also think that an AI with such broad abilities would likely be able to far surpass human abilities in a subset of domains, making it likely to meet one or more of the other criteria laid out here.\n> *   Surveillance, autonomous weapons, or other AI-centric technology that becomes sufficiently advanced to be the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. (This contrasts with the first point because it refers to transformative technology that is itself AI-centric, whereas the first point refers to AI used to speed research on some other transformative technology.)\n\n[^hy8b4kflu8]: As defined by [Open Philanthropy's Holden Karnofsky in 2016](https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence-the-philanthropic-opportunity/), and reused by [the Center for the Governance of AI in 2018](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf)", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "https://www.lesswrong.com/tag/transformative-ai?edit=true", "Last Asked On Discord": "", "UI ID": "6347", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:31.416+01:00", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-b6ca77ed6a41fd79ad2519ea0b4655a7312ee6586ca46f7e7f78f8b62292f14a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b6ca77ed6a41fd79ad2519ea0b4655a7312ee6586ca46f7e7f78f8b62292f14a", "name": "What is \"superintelligence\"?", "index": 353, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:39.798Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b6ca77ed6a41fd79ad2519ea0b4655a7312ee6586ca46f7e7f78f8b62292f14a", "values": {"File": "What is \"superintelligence\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"superintelligence\"?", "Link": "https://docs.google.com/document/d/1SZyxfgwAV2UJO-c6Q7qSGLVRfFrDA3VytXgZyy4jD_0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:15.276+01:00", "Related Answers DO NOT EDIT": "What is \"transformative AI\"?,Superintelligence sounds like science fiction. Do people think about this in the real world?,What is Artificial General Intelligence (AGI) and what will it look like?,Why might we expect a superintelligence to be hostile by default?", "Tags": "Superintelligence", "Doc Last Edited": "2023-02-22T23:04:40.666+01:00", "Status": "Live on site", "Edit Answer": "What is \"superintelligence\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6207", "Source Link": "https://www.reddit.com/r/ControlProblem/wiki/faq", "aisafety.info Link": "What is \"superintelligence\"?", "Source": "/r/ControlProblem's FAQ", "All Phrasings": "What is \"superintelligence\"?\n", "Initial Order": "", "Related IDs": "6347,6953,2374,6982", "Rich Text DO NOT EDIT": "Nick Bostrom defines superintelligence as \u201can intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.\u201d A chess program can outperform humans in chess, but is useless at any other task. Superintelligence will have been achieved when we create a machine that outperforms the human brain across practically any domain.\n\n", "Tag Count": 1, "Related Answer Count": 4, "Rich Text": "Nick Bostrom defines superintelligence as \u201can intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.\u201d A chess program can outperform humans in chess, but is useless at any other task. Superintelligence will have been achieved when we create a machine that outperforms the human brain across practically any domain.\n\n", "Stamp Count": 0, "Multi Answer": true, "Stamped By": "", "Priority": 5, "Asker": "NotaSentientAI", "External Source": "https://www.lesswrong.com/tag/superintelligence", "Last Asked On Discord": "", "UI ID": "6207", "Related Answers": "What is \"transformative AI\"?,Superintelligence sounds like science fiction. Do people think about this in the real world?,What is Artificial General Intelligence (AGI) and what will it look like?,Why might we expect a superintelligence to be hostile by default?", "Doc Last Ingested": "2023-03-14T23:36:34.184+01:00", "Request Count": "", "Number of suggestions on answer doc": 52, "Total character count of suggestions on answer doc": 1537, "Helpful": ""}}, {"id": "i-4f546d1bdf83bcecbd10a578f5f8e898e3988806d078d506ec2c4e326452d8b3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4f546d1bdf83bcecbd10a578f5f8e898e3988806d078d506ec2c4e326452d8b3", "name": "What is \"narrow AI\"?", "index": 354, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:43.680Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4f546d1bdf83bcecbd10a578f5f8e898e3988806d078d506ec2c4e326452d8b3", "values": {"File": "What is \"narrow AI\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"narrow AI\"?", "Link": "https://docs.google.com/document/d/1BaskThfo9CP8O687uKis9hYVOIGFtGafdlXQNVXi8i0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:11.856+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Narrow AI,Definitions", "Doc Last Edited": "2023-02-22T23:04:41.602+01:00", "Status": "Live on site", "Edit Answer": "What is \"narrow AI\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6513", "Source Link": "", "aisafety.info Link": "What is \"narrow AI\"?", "Source": "Wiki", "All Phrasings": "What is \"narrow AI\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A **Narrow AI** is capable of operating only in a relatively limited domain, such as chess or driving, rather than capable of learning a broad range of tasks like a human or an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence). Narrow vs General is not a perfectly binary classification, there are degrees of generality with, for example, large language models having a fairly large degree of generality (as the domain of text is large) without being as general as a human, and we may eventually build systems that are significantly more general than humans.", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "A **Narrow AI** is capable of operating only in a relatively limited domain, such as chess or driving, rather than capable of learning a broad range of tasks like a human or an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence). Narrow vs General is not a perfectly binary classification, there are degrees of generality with, for example, large language models having a fairly large degree of generality (as the domain of text is large) without being as general as a human, and we may eventually build systems that are significantly more general than humans.", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "https://www.lesswrong.com/tag/narrow-ai", "Last Asked On Discord": "", "UI ID": "6513", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:36.115+01:00", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-e98ec2bed6bc03822aa94770a10db831f20e9a8ebcb983d423e35c9f8dd6426f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e98ec2bed6bc03822aa94770a10db831f20e9a8ebcb983d423e35c9f8dd6426f", "name": "What is \"metaphilosophy\" and how does it relate to AI safety?", "index": 355, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:46.989Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e98ec2bed6bc03822aa94770a10db831f20e9a8ebcb983d423e35c9f8dd6426f", "values": {"File": "What is \"metaphilosophy\" and how does it relate to AI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"metaphilosophy\" and how does it relate to AI safety?", "Link": "https://docs.google.com/document/d/1L90JuCImhxro4oL2eWf6aRZKHzKTjlFP1JYDgAtny94/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:07.664+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,Metaphilosophy", "Doc Last Edited": "2023-02-22T22:55:01.849+01:00", "Status": "Not started", "Edit Answer": "What is \"metaphilosophy\" and how does it relate to AI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7612", "Source Link": "", "aisafety.info Link": "What is \"metaphilosophy\" and how does it relate to AI safety?", "Source": "Wiki", "All Phrasings": "What is \"metaphilosophy\" and how does it relate to AI safety?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Philosophy about philosophy. See [Wei dai's stuff](https://www.lesswrong.com/tag/meta-philosophy) on the topic\n\n- \n\n- (Posted by Murphant)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "- Philosophy about philosophy. See [Wei dai's stuff](https://www.lesswrong.com/tag/meta-philosophy) on the topic\n\n- \n\n- (Posted by Murphant)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Nico Hill2", "External Source": "https://www.lesswrong.com/tag/meta-philosophy", "Last Asked On Discord": "", "UI ID": "7612", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:37.654+01:00", "Request Count": "", "Number of suggestions on answer doc": 52, "Total character count of suggestions on answer doc": 1537, "Helpful": ""}}, {"id": "i-c065eab38f25ca3533c691afe6d6d61076655897fe7375ccfdb1220e4f0caa94", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c065eab38f25ca3533c691afe6d6d61076655897fe7375ccfdb1220e4f0caa94", "name": "What is \"logical decision theory\"?", "index": 356, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:51.921Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c065eab38f25ca3533c691afe6d6d61076655897fe7375ccfdb1220e4f0caa94", "values": {"File": "What is \"logical decision theory\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"logical decision theory\"?", "Link": "https://docs.google.com/document/d/1QKMtIORv0HMFr1LrcugipP33HNzL9-bMWPby66Ify3U/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:03.657+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,Decision Theory", "Doc Last Edited": "2023-02-26T21:11:28.998+01:00", "Status": "Bulletpoint sketch", "Edit Answer": "What is \"logical decision theory\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7780", "Source Link": "", "aisafety.info Link": "What is \"logical decision theory\"?", "Source": "Wiki", "All Phrasings": "What is \"logical decision theory\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The three main classes of [decision theory](https://www.lesswrong.com/tag/decision-theory) are [evidential decision theory](https://www.lesswrong.com/tag/evidential-decision-theory), [causal theory](https://www.lesswrong.com/tag/causal-decision-theory) and [logical decision theory](https://arbital.com/p/logical_dt/?l=5kv).\n\nEvidential decision theory (EDT) reasons with the conditional probability of events based on the evidence. An agent using EDT selects the action which has the best expected outcome based on the evidence available. It views its action as one more fact about the world, which it can reason about, but does not distinguish the causal effect of its actions from any other conditional factor. See [What is \"evidential decision theory\"](https://stampy.ai/wiki/What_is_%22evidential_decision_theory%22%3F) for further explanation.\n\nCausal decision theory (CDT) reasons about the causal relationship between the decision and its physical consequences. An agent using CDT views its choice as affecting the specific action that it takes, and, by extension, everything which that action causes. It selects the action which will bring about the best expected outcome based on its knowledge at the time of the decision. See [What is \"causal decision theory\"](https://stampy.ai/wiki/What_is_causal_decision_theory%3F) for further explanation.\n\nLogical decision theory (LDT) is a class of decision theories, including [updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory)[functional decision theory](https://intelligence.org/2017/10/22/fdt/) and [timeless decision theory](https://www.lesswrong.com/tag/timeless-decision-theory), which share in making use of logical counterfactuals. An agent using a LDT will act as if it controls the logical output of its own decision algorithm, and not just its immediate action. In general a LDT can outperform other forms of decision theory in problems that include:\n\n- [Parfit's hitchhiker](https://www.lesswrong.com/tag/parfits-hitchhiker)\n\n- [The smoking lesion problem](https://www.lesswrong.com/tag/smoking-lesion)\n\n- [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem)\n\nA specific example of a LDT is Functional Decision Theory (FDT). This says that agents should treat one\u2019s decision as the output of a \ufb01xed mathematical function that answers the question, \u201cWhich output of this very function would yield the best outcome?\u201d. It does not calculate the best outcome based on its immediate circumstance, but rather views itself as an instance of a function which must be consistent under all instantiations that it finds itself in.\n\nSee [What is \"functional decision theory\"?](https://docs.google.com/document/d/1bC7CJTvyinN7AOJ5A03Zy0VUVLghl3N8iOMRk9DmMYc/edit?usp=drivesdk) and [What is \"logical decision theory\"](https://docs.google.com/document/d/1QKMtIORv0HMFr1LrcugipP33HNzL9-bMWPby66Ify3U/edit?usp=drivesdk) for further explanation .\n\nFurther reading:\n\n[Decision Theory FAQ](https://www.lesswrong.com/posts/zEWJBFFMvQ835nq6h/decision-theory-faq#what-about-newcombs-problem-and-alternative-decision-algorithms)\n\n[Comprehensive list of decision theories](https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "The three main classes of [decision theory](https://www.lesswrong.com/tag/decision-theory) are [evidential decision theory](https://www.lesswrong.com/tag/evidential-decision-theory), [causal theory](https://www.lesswrong.com/tag/causal-decision-theory) and [logical decision theory](https://arbital.com/p/logical_dt/?l=5kv).\n\nEvidential decision theory (EDT) reasons with the conditional probability of events based on the evidence. An agent using EDT selects the action which has the best expected outcome based on the evidence available. It views its action as one more fact about the world, which it can reason about, but does not distinguish the causal effect of its actions from any other conditional factor. See [What is \"evidential decision theory\"](https://stampy.ai/wiki/What_is_%22evidential_decision_theory%22%3F) for further explanation.\n\nCausal decision theory (CDT) reasons about the causal relationship between the decision and its physical consequences. An agent using CDT views its choice as affecting the specific action that it takes, and, by extension, everything which that action causes. It selects the action which will bring about the best expected outcome based on its knowledge at the time of the decision. See [What is \"causal decision theory\"](https://stampy.ai/wiki/What_is_causal_decision_theory%3F) for further explanation.\n\nLogical decision theory (LDT) is a class of decision theories, including [updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory)[functional decision theory](https://intelligence.org/2017/10/22/fdt/) and [timeless decision theory](https://www.lesswrong.com/tag/timeless-decision-theory), which share in making use of logical counterfactuals. An agent using a LDT will act as if it controls the logical output of its own decision algorithm, and not just its immediate action. In general a LDT can outperform other forms of decision theory in problems that include:\n\n- [Parfit's hitchhiker](https://www.lesswrong.com/tag/parfits-hitchhiker)\n\n- [The smoking lesion problem](https://www.lesswrong.com/tag/smoking-lesion)\n\n- [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem)\n\nA specific example of a LDT is Functional Decision Theory (FDT). This says that agents should treat one\u2019s decision as the output of a \ufb01xed mathematical function that answers the question, \u201cWhich output of this very function would yield the best outcome?\u201d. It does not calculate the best outcome based on its immediate circumstance, but rather views itself as an instance of a function which must be consistent under all instantiations that it finds itself in.\n\nSee [What is \"functional decision theory\"?](https://docs.google.com/document/d/1bC7CJTvyinN7AOJ5A03Zy0VUVLghl3N8iOMRk9DmMYc/edit?usp=drivesdk) and [What is \"logical decision theory\"](https://docs.google.com/document/d/1QKMtIORv0HMFr1LrcugipP33HNzL9-bMWPby66Ify3U/edit?usp=drivesdk) for further explanation .\n\nFurther reading:\n\n[Decision Theory FAQ](https://www.lesswrong.com/posts/zEWJBFFMvQ835nq6h/decision-theory-faq#what-about-newcombs-problem-and-alternative-decision-algorithms)\n\n[Comprehensive list of decision theories](https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/)\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "2023-02-26T19:23:09.962+01:00", "UI ID": "7780", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:39.098+01:00", "Request Count": "", "Number of suggestions on answer doc": 56, "Total character count of suggestions on answer doc": 1601, "Helpful": ""}}, {"id": "i-0392340fac293a97870775db7276083f887312d6fb986d19f33e3935f4ddf958", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0392340fac293a97870775db7276083f887312d6fb986d19f33e3935f4ddf958", "name": "What is \"hedonium\"?", "index": 357, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:54.325Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0392340fac293a97870775db7276083f887312d6fb986d19f33e3935f4ddf958", "values": {"File": "What is \"hedonium\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"hedonium\"?", "Link": "https://docs.google.com/document/d/1Lszd8Ir4GtFdM2ZxvzsjzN3PmMSH0COeHescmtFL1UQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:48:00.180+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,Hedonium", "Doc Last Edited": "2023-02-22T23:04:43.030+01:00", "Status": "Live on site", "Edit Answer": "What is \"hedonium\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7786", "Source Link": "", "aisafety.info Link": "What is \"hedonium\"?", "Source": "Wiki", "All Phrasings": "What is \"hedonium\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**Orgasmium** (also known as **hedonium**) is a homogeneous substance with limited consciousness, which is in a constant state of supreme bliss. An AI programmed to \"maximize happiness\" might simply tile the universe with orgasmium. Some who believe this consider it a good thing; others do not. Those who do not, use its undesirability to argue that not all terminal values reduce to \"happiness\" or some simple analogue. Hedonium is the [hedonistic](https://www.lesswrong.com/tag/hedonism) [utilitarian](https://www.lesswrong.com/tag/utilitarianism)'s version of [utilitronium](https://www.lesswrong.com/tag/utilitronium).\n\nBlog posts\n----------\n\n*   [Prolegomena to a Theory of Fun](http://lesswrong.com/lw/wv/prolegomena_to_a_theory_of_fun/)\n*   [In Praise of Boredom](http://lesswrong.com/lw/xr/in_praise_of_boredom/)\n*   [Are pain and pleasure equally energy-efficient?](https://reflectivedisequilibrium.blogspot.com/2012/03/are-pain-and-pleasure-equally-energy.html)\n\nSee also\n--------\n\n*   [Quote from *Superintelligence*](https://www.goodreads.com/quotes/1413237-consider-an-ai-that-has-hedonism-as-its-final-goal)\n*   [Fun theory](https://www.lesswrong.com/tag/fun-theory)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Utilitronium](https://www.lesswrong.com/tag/utilitronium)", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "**Orgasmium** (also known as **hedonium**) is a homogeneous substance with limited consciousness, which is in a constant state of supreme bliss. An AI programmed to \"maximize happiness\" might simply tile the universe with orgasmium. Some who believe this consider it a good thing; others do not. Those who do not, use its undesirability to argue that not all terminal values reduce to \"happiness\" or some simple analogue. Hedonium is the [hedonistic](https://www.lesswrong.com/tag/hedonism) [utilitarian](https://www.lesswrong.com/tag/utilitarianism)'s version of [utilitronium](https://www.lesswrong.com/tag/utilitronium).\n\nBlog posts\n----------\n\n*   [Prolegomena to a Theory of Fun](http://lesswrong.com/lw/wv/prolegomena_to_a_theory_of_fun/)\n*   [In Praise of Boredom](http://lesswrong.com/lw/xr/in_praise_of_boredom/)\n*   [Are pain and pleasure equally energy-efficient?](https://reflectivedisequilibrium.blogspot.com/2012/03/are-pain-and-pleasure-equally-energy.html)\n\nSee also\n--------\n\n*   [Quote from *Superintelligence*](https://www.goodreads.com/quotes/1413237-consider-an-ai-that-has-hedonism-as-its-final-goal)\n*   [Fun theory](https://www.lesswrong.com/tag/fun-theory)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Utilitronium](https://www.lesswrong.com/tag/utilitronium)", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "https://www.lesswrong.com/tag/orgasmium?edit=true", "Last Asked On Discord": "", "UI ID": "7786", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:41.609+01:00", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-7f646f73e8688f9fbb15abb3b93624044aa2e87fc9cb40f2bfa48b807c8867f1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7f646f73e8688f9fbb15abb3b93624044aa2e87fc9cb40f2bfa48b807c8867f1", "name": "What is \"greater-than-human intelligence\"?", "index": 358, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:44:58.489Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7f646f73e8688f9fbb15abb3b93624044aa2e87fc9cb40f2bfa48b807c8867f1", "values": {"File": "What is \"greater-than-human intelligence\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"greater-than-human intelligence\"?", "Link": "https://docs.google.com/document/d/1Km-FKw3rgJqzzBYT9luFQ2588WrY44aCYc5xuSAPKDk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:56.588+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Superintelligence", "Doc Last Edited": "2023-02-22T23:04:43.951+01:00", "Status": "Live on site", "Edit Answer": "What is \"greater-than-human intelligence\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6587", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "What is \"greater-than-human intelligence\"?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "What is \"greater-than-human intelligence\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Machines are already smarter than humans are at many specific tasks: performing calculations, playing chess, searching large databanks, detecting underwater mines, [and more](http://www.amazon.com/dp/0521122937/). But one thing that makes humans special is their general intelligence. Humans can intelligently adapt to radically new problems in the urban jungle or outer space for which evolution could not have prepared them. Humans can solve problems for which their brain hardware and software was never trained. Humans can even examine the processes that produce their own intelligence ([cognitive neuroscience](http://en.wikipedia.org/wiki/Cognitive_neuroscience)), and design new kinds of intelligence never seen before ([artificial intelligence](http://en.wikipedia.org/wiki/Artificial_intelligence)).\n\nTo possess greater-than-human intelligence, a machine must be able to achieve goals more effectively than humans can, in a wider range of environments than humans can. This kind of intelligence involves the capacity not just to do science and play chess, but also to manipulate the social environment.\n\nComputer scientist Marcus Hutter [has described](http://www.amazon.com/dp/3642060528/) a formal model called AIXI that he says possesses the greatest general intelligence possible. But to implement it would require more computing power than all the matter in the universe can provide. Several projects try to approximate AIXI while still being computable, for example [MC-AIXI](http://arxiv.org/PS_cache/arxiv/pdf/0909/0909.0801v1.pdf).\n\nStill, there remains much work to be done before greater-than-human intelligence can be achieved in machines. Greater-than-human intelligence need not be achieved by directly programming a machine to be intelligent. It could also be achieved by whole brain emulation, by biological cognitive enhancement, or by brain-computer interfaces (see below).\n\nSee also:\n\n- Goertzel & Pennachin (eds.), [Artificial General Intelligence](http://www.amazon.com/dp/3642062679/)\n\n- Sandberg & Bostrom, [Whole Brain Emulation: A Roadmap](http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf)\n\n- Bostrom & Sandberg, [Cognitive Enhancement: Methods, Ethics, Regulatory Challenges](http://www.nickbostrom.com/cognitive.pdf)\n\n- Wikipedia, [Brain-computer interface](http://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface)\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Machines are already smarter than humans are at many specific tasks: performing calculations, playing chess, searching large databanks, detecting underwater mines, [and more](http://www.amazon.com/dp/0521122937/). But one thing that makes humans special is their general intelligence. Humans can intelligently adapt to radically new problems in the urban jungle or outer space for which evolution could not have prepared them. Humans can solve problems for which their brain hardware and software was never trained. Humans can even examine the processes that produce their own intelligence ([cognitive neuroscience](http://en.wikipedia.org/wiki/Cognitive_neuroscience)), and design new kinds of intelligence never seen before ([artificial intelligence](http://en.wikipedia.org/wiki/Artificial_intelligence)).\n\nTo possess greater-than-human intelligence, a machine must be able to achieve goals more effectively than humans can, in a wider range of environments than humans can. This kind of intelligence involves the capacity not just to do science and play chess, but also to manipulate the social environment.\n\nComputer scientist Marcus Hutter [has described](http://www.amazon.com/dp/3642060528/) a formal model called AIXI that he says possesses the greatest general intelligence possible. But to implement it would require more computing power than all the matter in the universe can provide. Several projects try to approximate AIXI while still being computable, for example [MC-AIXI](http://arxiv.org/PS_cache/arxiv/pdf/0909/0909.0801v1.pdf).\n\nStill, there remains much work to be done before greater-than-human intelligence can be achieved in machines. Greater-than-human intelligence need not be achieved by directly programming a machine to be intelligent. It could also be achieved by whole brain emulation, by biological cognitive enhancement, or by brain-computer interfaces (see below).\n\nSee also:\n\n- Goertzel & Pennachin (eds.), [Artificial General Intelligence](http://www.amazon.com/dp/3642062679/)\n\n- Sandberg & Bostrom, [Whole Brain Emulation: A Roadmap](http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf)\n\n- Bostrom & Sandberg, [Cognitive Enhancement: Methods, Ethics, Regulatory Challenges](http://www.nickbostrom.com/cognitive.pdf)\n\n- Wikipedia, [Brain-computer interface](http://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface)\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 3, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "", "UI ID": "6587", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:43.227+01:00", "Request Count": "", "Number of suggestions on answer doc": 56, "Total character count of suggestions on answer doc": 1601, "Helpful": ""}}, {"id": "i-aef4eee6687c860b5512081c1f62910e5834b39130a743bbf508107cf6c3f682", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-aef4eee6687c860b5512081c1f62910e5834b39130a743bbf508107cf6c3f682", "name": "What is \"functional decision theory\"?", "index": 359, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:45:02.581Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-aef4eee6687c860b5512081c1f62910e5834b39130a743bbf508107cf6c3f682", "values": {"File": "What is \"functional decision theory\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"functional decision theory\"?", "Link": "https://docs.google.com/document/d/1bC7CJTvyinN7AOJ5A03Zy0VUVLghl3N8iOMRk9DmMYc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:52.913+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,Decision Theory", "Doc Last Edited": "2023-03-06T16:50:36.463+01:00", "Status": "Live on site", "Edit Answer": "What is \"functional decision theory\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7781", "Source Link": "", "aisafety.info Link": "What is \"functional decision theory\"?", "Source": "Wiki", "All Phrasings": "What is \"functional decision theory\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "While debates between causal and evidential decision theory (CDT and EDT) have been around for the past fifty years, Functional Decision Theory has emerged within the past decade as an attempt to resolve the longstanding issues faced by *both* CDT and EDT.\n\nDespite their differences, Functional Decision Theory (FDT) agrees with the standard lore in decision theory: rational agents should take actions which maximize expected utility.\n\nAccording to CDT, agents should evaluate these options in terms of the causal consequences of choosing either option. According to EDT, agents should choose the option that, after the fact, you\u2019ll be happiest to learn that you\u2019ve performed (how do these criteria come apart? See our [primer on EDT](https://docs.google.com/document/d/1Yul-8tuszGSyYizITMQrA25lsfA-Lq40qNKBobEtUYA/edit) for more). According to FDT, the agent *shouldn\u2019t* consider what might happen if she were to choose A or B, and weigh the possible outcomes by their probability. In FDT, the agent ought to consider what would happen if the *right choice according to FDT**were A or B*.\n\nThis is a little abstract, so an example might help; we take the following story from Yudkowsky and Soares\u2019 [paper](https://arxiv.org/abs/1710.05060) defending FDT.\n\n**\u2018XOR Blackmail\u2019:** You hear a rumor that your house has a terrible termite infestation that would cost you $1,000,000 in damages. You don\u2019t know whether this rumor is true. A few days later, you receive a letter from Omega \u2014 a greedy predictor with a strong reputation for honesty. The letter reads as follows:\n\n\u201cI know whether or not you have termites, and I have sent you this letter iff exactly one of the following is true: (i) the rumor is false, and you are going to pay me $1,000 upon receiving this letter; or (ii) the rumor is true, and you will not pay me upon receiving this letter.\u201d\n\nOmega predicts what the agent would do upon receiving the letter, and sends the agent the letter iff exactly one of (i) or (ii) is true.\n\nWhat should you do? EDT says \u201cgreat deal \u2014 take my money!\u201d. Upon receiving the letter, it\u2019s good news for you to learn that you\u2019ve paid. After all, most people who receive the letter and *don\u2019t* pay have termites in their house \u2014 a fate far worse than losing a mere $1,000. FDT says this is silly: you shouldn\u2019t make yourself so predictably exploitable in this way.\n\nAccording to FDT, you should first consider what would happen in the hypothetical where FDT recommends paying. Well, if FDT recommended paying, then our greedy friend Omega would know this, and send you the letter. Omega, greedy and accurate predictor that they are, would sense an opportunity to make a quick $1,000. Then, you consider the hypothetical in which FDT *doesn\u2019t* recommend paying. Well, in that case, Omega\u2019s out of luck: they love money, and don\u2019t want to waste their time sending letters to frugal FDTers. If FDT recommends not paying, you don\u2019t just lower the probability of termites \u2014 you lower the chance of receiving the letter *in the first place*. This is better. So you should refuse to pay.\n\nIn summary, FDT reasons thus: if the right choice according to FDT says \u201cpay!\u201d, then you\u2019re more likely to receive the letter, and be down $1,000. If the right choice according to FDT says \u201cdon\u2019t pay!\u201d, you\u2019re less likely to receive the letter, and less likely be down $1,000. Also, you\u2019re no more likely to have termites: Omega\u2019s an expert *predictor*, not an expert in transporting termites to people\u2019s houses.\n\nWhile CDT delivers the same verdict as FDT in our \u2018XOR Blackmail\u2019 story, CDT also recommends two-boxing in Newcomb\u2019s Problem, as outlined on [this page](https://docs.google.com/document/d/1Yul-8tuszGSyYizITMQrA25lsfA-Lq40qNKBobEtUYA/edit). FDT, so it\u2019s claimed, delivers better results on all \u201cfair\u201d decision-problems. That said, we should note that not everyone is convinced of FDT \u2014 two critical responses to FDT are available [here](https://www.umsu.de/blog/2018/688) and [here](https://www.lesswrong.com/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory).\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "While debates between causal and evidential decision theory (CDT and EDT) have been around for the past fifty years, Functional Decision Theory has emerged within the past decade as an attempt to resolve the longstanding issues faced by *both* CDT and EDT.\n\nDespite their differences, Functional Decision Theory (FDT) agrees with the standard lore in decision theory: rational agents should take actions which maximize expected utility.\n\nAccording to CDT, agents should evaluate these options in terms of the causal consequences of choosing either option. According to EDT, agents should choose the option that, after the fact, you\u2019ll be happiest to learn that you\u2019ve performed (how do these criteria come apart? See our [primer on EDT](https://docs.google.com/document/d/1Yul-8tuszGSyYizITMQrA25lsfA-Lq40qNKBobEtUYA/edit) for more). According to FDT, the agent *shouldn\u2019t* consider what might happen if she were to choose A or B, and weigh the possible outcomes by their probability. In FDT, the agent ought to consider what would happen if the *right choice according to FDT**were A or B*.\n\nThis is a little abstract, so an example might help; we take the following story from Yudkowsky and Soares\u2019 [paper](https://arxiv.org/abs/1710.05060) defending FDT.\n\n**\u2018XOR Blackmail\u2019:** You hear a rumor that your house has a terrible termite infestation that would cost you $1,000,000 in damages. You don\u2019t know whether this rumor is true. A few days later, you receive a letter from Omega \u2014 a greedy predictor with a strong reputation for honesty. The letter reads as follows:\n\n\u201cI know whether or not you have termites, and I have sent you this letter iff exactly one of the following is true: (i) the rumor is false, and you are going to pay me $1,000 upon receiving this letter; or (ii) the rumor is true, and you will not pay me upon receiving this letter.\u201d\n\nOmega predicts what the agent would do upon receiving the letter, and sends the agent the letter iff exactly one of (i) or (ii) is true.\n\nWhat should you do? EDT says \u201cgreat deal \u2014 take my money!\u201d. Upon receiving the letter, it\u2019s good news for you to learn that you\u2019ve paid. After all, most people who receive the letter and *don\u2019t* pay have termites in their house \u2014 a fate far worse than losing a mere $1,000. FDT says this is silly: you shouldn\u2019t make yourself so predictably exploitable in this way.\n\nAccording to FDT, you should first consider what would happen in the hypothetical where FDT recommends paying. Well, if FDT recommended paying, then our greedy friend Omega would know this, and send you the letter. Omega, greedy and accurate predictor that they are, would sense an opportunity to make a quick $1,000. Then, you consider the hypothetical in which FDT *doesn\u2019t* recommend paying. Well, in that case, Omega\u2019s out of luck: they love money, and don\u2019t want to waste their time sending letters to frugal FDTers. If FDT recommends not paying, you don\u2019t just lower the probability of termites \u2014 you lower the chance of receiving the letter *in the first place*. This is better. So you should refuse to pay.\n\nIn summary, FDT reasons thus: if the right choice according to FDT says \u201cpay!\u201d, then you\u2019re more likely to receive the letter, and be down $1,000. If the right choice according to FDT says \u201cdon\u2019t pay!\u201d, you\u2019re less likely to receive the letter, and less likely be down $1,000. Also, you\u2019re no more likely to have termites: Omega\u2019s an expert *predictor*, not an expert in transporting termites to people\u2019s houses.\n\nWhile CDT delivers the same verdict as FDT in our \u2018XOR Blackmail\u2019 story, CDT also recommends two-boxing in Newcomb\u2019s Problem, as outlined on [this page](https://docs.google.com/document/d/1Yul-8tuszGSyYizITMQrA25lsfA-Lq40qNKBobEtUYA/edit). FDT, so it\u2019s claimed, delivers better results on all \u201cfair\u201d decision-problems. That said, we should note that not everyone is convinced of FDT \u2014 two critical responses to FDT are available [here](https://www.umsu.de/blog/2018/688) and [here](https://www.lesswrong.com/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "https://www.lesswrong.com/tag/functional-decision-theory?edit=true", "Last Asked On Discord": "2023-02-26T19:21:17.740+01:00", "UI ID": "7781", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:45.266+01:00", "Request Count": "", "Number of suggestions on answer doc": 56, "Total character count of suggestions on answer doc": 1601, "Helpful": ""}}, {"id": "i-70b6c6f5364c76414632aef8431b2010f3515d3b9c3d8442d73a869ab2eb4775", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-70b6c6f5364c76414632aef8431b2010f3515d3b9c3d8442d73a869ab2eb4775", "name": "What is \"friendly AI\"?", "index": 360, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:45:04.487Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-70b6c6f5364c76414632aef8431b2010f3515d3b9c3d8442d73a869ab2eb4775", "values": {"File": "What is \"friendly AI\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"friendly AI\"?", "Link": "https://docs.google.com/document/d/10DJOhvGvc19MVeN_TvHUwMuCDcR7r1C3qxOD099yPhM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:49.739+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,Friendly AI", "Doc Last Edited": "2023-02-22T22:55:03.931+01:00", "Status": "In progress", "Edit Answer": "What is \"friendly AI\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6918", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "What is \"friendly AI\"?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "What is \"friendly AI\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A Friendly Artificial Intelligence (Friendly AI or FAI) is an artificial intelligence that is \u2018friendly\u2019 to humanity \u2014 one that has a good rather than bad effect on humanity.\n\nAI researchers continue to make progress with machines that make their own decisions, and there is a growing awareness that we need to design machines to act safely and ethically. This research program goes by many names: \u2018machine ethics\u2019, \u2018[machine morality](https://philpapers.org/rec/WENMMB)\u2019, \u2018[artificial morality](http://www.amazon.com/dp/0415076919/)\u2019, \u2018[computational ethics](https://www.researchgate.net/publication/227624931_A_Conceptual_and_Computational_Model_of_Moral_Decision_Making_in_Human_and_Artificial_Agents)\u2019 and \u2018[computational metaethics](http://commonsenseatheism.com/wp-content/uploads/2011/03/Lokhorst-Computational-meta-ethics-toward-the-meta-ethical-robot.pdf)\u2019, \u2018[friendly AI](https://intelligence.org/files/CFAI.pdf)\u2019, and \u2018[robo-ethics](http://commonsenseatheism.com/wp-content/uploads/2011/03/Capurro-International-Review-of-Information-Ethics-Vol.-6-Ethics-in-Robotics.pdf)\u2019 or \u2018[robot ethics](https://www.researchgate.net/publication/5839780_Robot_Ethics)\u2019.\n\nThe most immediate concern may be in battlefield robots; the U.S. Department of Defense contracted Ronald Arkin to design a system for [ensuring ethical behavior in autonomous battlefield robots](http://www.amazon.com/dp/1420085948/). The U.S. Congress has declared that a third of America\u2019s ground systems must be robotic by 2025, and by 2030 the U.S. Air Force plans to have swarms of bird-sized flying robots that operate semi-autonomously for weeks at a time.\n\nBut Friendly AI research is not concerned with battlefield robots or machine ethics in general. It is concerned with a problem of a much larger scale: designing AI that would remain safe and friendly after the intelligence explosion.\n\nA machine superintelligence would be enormously powerful. Successful implementation of Friendly AI could mean the difference between a solar system of unprecedented happiness and a solar system in which all available matter has been converted into parts for achieving the superintelligence\u2019s goals.\n\nIt must be noted that Friendly AI is a harder project than often supposed. As explored below, commonly suggested solutions for Friendly AI are likely to fail because of two features possessed by any superintelligence:\n\n1. Superpower: a superintelligent machine will have unprecedented powers to reshape reality, and therefore will achieve its goals with highly efficient methods that confound human expectations and desires.\n\n1. Literalness: a superintelligent machine will make decisions based on the mechanisms it is designed with, not the hopes its designers had in mind when they programmed those mechanisms. It will act only on precise specifications of rules and values, and will do so in ways that need not respect the complexity and subtlety[http://www.amazon.com/dp/0195331028/ 41](http://www.amazon.com/dp/0195331028/_41)[http://www.amazon.com/dp/019517237X/ 42](http://www.amazon.com/dp/019517237X/_42)[http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/ 43](http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/_43) of what humans value. A demand like \u201cmaximize human happiness\u201d sounds simple to us because it contains few words, but philosophers and scientists have failed for centuries to explain exactly what this means, and certainly have not translated it into a form sufficiently rigorous for AI programmers to use.\n\nSee also:\n\n- Wikipedia, [Friendly Artificial Intelligence](http://en.wikipedia.org/wiki/Friendly_artificial_intelligence).\n\n- All Things Considered, [The Singularity: Humanity\u2019s Last Invention?](http://www.npr.org/2011/01/11/132840775/The-Singularity-Humanitys-Last-Invention)\n\n- Fox, [A review of proposals toward safe AI](http://adarti.blogspot.com/2011/04/review-of-proposals-toward-safe-ai.html)\n\n- Muehlhauser, [Friendly AI: A Bibliography](http://commonsenseatheism.com/?p=14047)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "A Friendly Artificial Intelligence (Friendly AI or FAI) is an artificial intelligence that is \u2018friendly\u2019 to humanity \u2014 one that has a good rather than bad effect on humanity.\n\nAI researchers continue to make progress with machines that make their own decisions, and there is a growing awareness that we need to design machines to act safely and ethically. This research program goes by many names: \u2018machine ethics\u2019, \u2018[machine morality](https://philpapers.org/rec/WENMMB)\u2019, \u2018[artificial morality](http://www.amazon.com/dp/0415076919/)\u2019, \u2018[computational ethics](https://www.researchgate.net/publication/227624931_A_Conceptual_and_Computational_Model_of_Moral_Decision_Making_in_Human_and_Artificial_Agents)\u2019 and \u2018[computational metaethics](http://commonsenseatheism.com/wp-content/uploads/2011/03/Lokhorst-Computational-meta-ethics-toward-the-meta-ethical-robot.pdf)\u2019, \u2018[friendly AI](https://intelligence.org/files/CFAI.pdf)\u2019, and \u2018[robo-ethics](http://commonsenseatheism.com/wp-content/uploads/2011/03/Capurro-International-Review-of-Information-Ethics-Vol.-6-Ethics-in-Robotics.pdf)\u2019 or \u2018[robot ethics](https://www.researchgate.net/publication/5839780_Robot_Ethics)\u2019.\n\nThe most immediate concern may be in battlefield robots; the U.S. Department of Defense contracted Ronald Arkin to design a system for [ensuring ethical behavior in autonomous battlefield robots](http://www.amazon.com/dp/1420085948/). The U.S. Congress has declared that a third of America\u2019s ground systems must be robotic by 2025, and by 2030 the U.S. Air Force plans to have swarms of bird-sized flying robots that operate semi-autonomously for weeks at a time.\n\nBut Friendly AI research is not concerned with battlefield robots or machine ethics in general. It is concerned with a problem of a much larger scale: designing AI that would remain safe and friendly after the intelligence explosion.\n\nA machine superintelligence would be enormously powerful. Successful implementation of Friendly AI could mean the difference between a solar system of unprecedented happiness and a solar system in which all available matter has been converted into parts for achieving the superintelligence\u2019s goals.\n\nIt must be noted that Friendly AI is a harder project than often supposed. As explored below, commonly suggested solutions for Friendly AI are likely to fail because of two features possessed by any superintelligence:\n\n1. Superpower: a superintelligent machine will have unprecedented powers to reshape reality, and therefore will achieve its goals with highly efficient methods that confound human expectations and desires.\n\n1. Literalness: a superintelligent machine will make decisions based on the mechanisms it is designed with, not the hopes its designers had in mind when they programmed those mechanisms. It will act only on precise specifications of rules and values, and will do so in ways that need not respect the complexity and subtlety[http://www.amazon.com/dp/0195331028/ 41](http://www.amazon.com/dp/0195331028/_41)[http://www.amazon.com/dp/019517237X/ 42](http://www.amazon.com/dp/019517237X/_42)[http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/ 43](http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/_43) of what humans value. A demand like \u201cmaximize human happiness\u201d sounds simple to us because it contains few words, but philosophers and scientists have failed for centuries to explain exactly what this means, and certainly have not translated it into a form sufficiently rigorous for AI programmers to use.\n\nSee also:\n\n- Wikipedia, [Friendly Artificial Intelligence](http://en.wikipedia.org/wiki/Friendly_artificial_intelligence).\n\n- All Things Considered, [The Singularity: Humanity\u2019s Last Invention?](http://www.npr.org/2011/01/11/132840775/The-Singularity-Humanitys-Last-Invention)\n\n- Fox, [A review of proposals toward safe AI](http://adarti.blogspot.com/2011/04/review-of-proposals-toward-safe-ai.html)\n\n- Muehlhauser, [Friendly AI: A Bibliography](http://commonsenseatheism.com/?p=14047)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "", "UI ID": "6918", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:47.235+01:00", "Request Count": "", "Number of suggestions on answer doc": 56, "Total character count of suggestions on answer doc": 1601, "Helpful": ""}}, {"id": "i-f699e69764406d0c0c6e3612cbdde08264312611234433f1213675d0bb76a11b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-f699e69764406d0c0c6e3612cbdde08264312611234433f1213675d0bb76a11b", "name": "What is \"evidential decision theory\"?", "index": 361, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:45:15.441Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-f699e69764406d0c0c6e3612cbdde08264312611234433f1213675d0bb76a11b", "values": {"File": "What is \"evidential decision theory\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"evidential decision theory\"?", "Link": "https://docs.google.com/document/d/1Yul-8tuszGSyYizITMQrA25lsfA-Lq40qNKBobEtUYA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:46.294+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,Decision Theory", "Doc Last Edited": "2023-03-05T12:42:39.357+01:00", "Status": "Live on site", "Edit Answer": "What is \"evidential decision theory\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7778", "Source Link": "", "aisafety.info Link": "What is \"evidential decision theory\"?", "Source": "Wiki", "All Phrasings": "What is \"evidential decision theory\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n[https://www.lesswrong.com/tag/evidential-decision-theory?edit=true](https://www.lesswrong.com/tag/evidential-decision-theory?edit=true)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "\n\n[https://www.lesswrong.com/tag/evidential-decision-theory?edit=true](https://www.lesswrong.com/tag/evidential-decision-theory?edit=true)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "https://www.lesswrong.com/tag/evidential-decision-theory?edit=true", "Last Asked On Discord": "2023-02-26T19:22:45.585+01:00", "UI ID": "7778", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:49.269+01:00", "Request Count": "", "Number of suggestions on answer doc": 57, "Total character count of suggestions on answer doc": 4416, "Helpful": ""}}, {"id": "i-da218b403164fdfcfbe47e63fe6b70bca7f1592d27391016433939c5ffb86640", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-da218b403164fdfcfbe47e63fe6b70bca7f1592d27391016433939c5ffb86640", "name": "What is \"coherent extrapolated volition\"?", "index": 362, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-15T11:10:46.643Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-da218b403164fdfcfbe47e63fe6b70bca7f1592d27391016433939c5ffb86640", "values": {"File": "What is \"coherent extrapolated volition\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"coherent extrapolated volition\"?", "Link": "https://docs.google.com/document/d/1IyoZpkRKbD_rCjy99AA454iEeSCRfiMGoCGp8AX1VmU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:42.465+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,Coherent Extrapolated Volition", "Doc Last Edited": "2023-03-15T09:38:33.993+01:00", "Status": "In progress", "Edit Answer": "What is \"coherent extrapolated volition\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6939", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "What is \"coherent extrapolated volition\"?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "What is \"coherent extrapolated volition\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Jan Leike (Head of Alignment at OpenAI) [notes](https://aligned.substack.com/p/a-proposal-for-importing-societys-values) that we can decompose issues in AI alignment into two questions:\n\n1. How do we align an AI with *anything* that resembles human values?\n\n1. If we can solve (1), how do we decide on a fair process for aligning AIs with the wide spectrum of human values, given the inevitable conflicts between value-systems involved?\n\nDealing with (1) may be surprisingly difficult, and many of our answers discuss the potentially surprising difficulty of aligning AIs with anything. However, even if we solved the first issue, (2) would remain extremely important. [Coherent Extrapolated Volition](https://intelligence.org/files/CEV.pdf) (or \u2018CEV\u2019) is one attempted answer to that question.\n\nThe ideal of CEV is to train a \u2018[seed AI](https://www.lesswrong.com/tag/seed-ai)\u2019 to discover human values, and then extrapolate these values in a coherent way. This is one of Yudkowsky\u2019s earlier [suggestions](https://intelligence.org/files/CEV.pdf)[, phrased in his own term](https://intelligence.org/files/CEV.pdf)s:\n\n\u201cIn poetic terms, our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.\u201d\n\nCurrently, CEV is not precisely specified enough to function as a workable alignment scheme. However, Leike claims that his suggestion for aligning AIs with the diverse set of human values ([Simulated Deliberative Democracy](https://aligned.substack.com/p/a-proposal-for-importing-societys-values)) can be viewed \u201cas a concrete step to implement CEV with today\u2019s language models\u201d.\n\nInterested readers may be curious about the field of [social choice theory](https://plato.stanford.edu/entries/social-choice/), a subfield of economics and philosophy dedicated to formally studying how to aggregate the preferences of many individuals into a single \u2018social welfare function\u2019.  On the more empirical side, a [2022 paper](https://arxiv.org/abs/2211.15006) from researchers at DeepMind investigated how we might use LLMs to \u201chelp people with diverse views find agreement\u201d.\n\n**[Old Draft Below]**\n\nEliezer Yudkowsky has [proposed](https://intelligence.org/files/CEV.pdf) Coherent Extrapolated Volition as a solution to at least two problems facing Friendly AI design:\n\n1. The fragility of human values: Yudkowsky writes that \u201cany future not shaped by a goal system with detailed reliable inheritance from human morals and metamorals will contain almost nothing of worth.\u201d The problem is that what humans value is complex and subtle, and difficult to specify. Consider the seemingly minor value of novelty. If a human-like value of novelty is not programmed into a superintelligent machine, it might explore the universe for valuable things up to a certain point, and then maximize the most valuable thing it finds (the exploration-exploitation tradeoff[https://www.researchgate.net/publication/222425165_Exploitation_vs_exploration_Choosing_a_supplier_in_an_environment_of_incomplete_information 58](https://www.researchgate.net/publication/222425165_Exploitation_vs_exploration_Choosing_a_supplier_in_an_environment_of_incomplete_information_58)) \u2014 tiling the solar system with brains in vats wired into happiness machines, for example. When a superintelligence is in charge, you have to get its motivational system exactly right in order to not make the future undesirable.\n\n1. The locality of human values: Imagine if the Friendly AI problem had faced the ancient Greeks, and they had programmed it with the most progressive moral values of their time. That would have led the world to a rather horrifying fate. But why should we think that humans have, in the 21st century, arrived at the apex of human morality? We can\u2019t risk programming a superintelligent machine with the moral values we happen to hold today. But then, which moral values do we give it?\n\nYudkowsky [suggests](https://intelligence.org/files/CEV.pdf) that we build a \u2018seed AI\u2019 to discover and then extrapolate the \u2018coherent extrapolated volition\u2019 of humanity:\n\n> In poetic terms, our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.\n\nThe seed AI would use the results of this examination and extrapolation of human values to program the motivational system of the superintelligence that would determine the fate of the galaxy.\n\nHowever, some worry that the collective will of humanity won\u2019t converge on a coherent set of goals. Others [believe](http://multiverseaccordingtoben.blogspot.com/2010/10/singularity-institutes-scary-idea-and.html) that guaranteed Friendliness is not possible, even by such elaborate and careful means.\n\n- Yudkowsky, [Coherent Extrapolated Volition](https://intelligence.org/files/CEV.pdf)\n\n**Related**\n\n[What are tiling agents?](https://docs.google.com/document/d/19MBaE2DsvBbF3XaeArcsuvwDrC0_Oj0I-1ciCLrgo9I/edit)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Jan Leike (Head of Alignment at OpenAI) [notes](https://aligned.substack.com/p/a-proposal-for-importing-societys-values) that we can decompose issues in AI alignment into two questions:\n\n1. How do we align an AI with *anything* that resembles human values?\n\n1. If we can solve (1), how do we decide on a fair process for aligning AIs with the wide spectrum of human values, given the inevitable conflicts between value-systems involved?\n\nDealing with (1) may be surprisingly difficult, and many of our answers discuss the potentially surprising difficulty of aligning AIs with anything. However, even if we solved the first issue, (2) would remain extremely important. [Coherent Extrapolated Volition](https://intelligence.org/files/CEV.pdf) (or \u2018CEV\u2019) is one attempted answer to that question.\n\nThe ideal of CEV is to train a \u2018[seed AI](https://www.lesswrong.com/tag/seed-ai)\u2019 to discover human values, and then extrapolate these values in a coherent way. This is one of Yudkowsky\u2019s earlier [suggestions](https://intelligence.org/files/CEV.pdf)[, phrased in his own term](https://intelligence.org/files/CEV.pdf)s:\n\n\u201cIn poetic terms, our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.\u201d\n\nCurrently, CEV is not precisely specified enough to function as a workable alignment scheme. However, Leike claims that his suggestion for aligning AIs with the diverse set of human values ([Simulated Deliberative Democracy](https://aligned.substack.com/p/a-proposal-for-importing-societys-values)) can be viewed \u201cas a concrete step to implement CEV with today\u2019s language models\u201d.\n\nInterested readers may be curious about the field of [social choice theory](https://plato.stanford.edu/entries/social-choice/), a subfield of economics and philosophy dedicated to formally studying how to aggregate the preferences of many individuals into a single \u2018social welfare function\u2019.  On the more empirical side, a [2022 paper](https://arxiv.org/abs/2211.15006) from researchers at DeepMind investigated how we might use LLMs to \u201chelp people with diverse views find agreement\u201d.\n\n**[Old Draft Below]**\n\nEliezer Yudkowsky has [proposed](https://intelligence.org/files/CEV.pdf) Coherent Extrapolated Volition as a solution to at least two problems facing Friendly AI design:\n\n1. The fragility of human values: Yudkowsky writes that \u201cany future not shaped by a goal system with detailed reliable inheritance from human morals and metamorals will contain almost nothing of worth.\u201d The problem is that what humans value is complex and subtle, and difficult to specify. Consider the seemingly minor value of novelty. If a human-like value of novelty is not programmed into a superintelligent machine, it might explore the universe for valuable things up to a certain point, and then maximize the most valuable thing it finds (the exploration-exploitation tradeoff[https://www.researchgate.net/publication/222425165_Exploitation_vs_exploration_Choosing_a_supplier_in_an_environment_of_incomplete_information 58](https://www.researchgate.net/publication/222425165_Exploitation_vs_exploration_Choosing_a_supplier_in_an_environment_of_incomplete_information_58)) \u2014 tiling the solar system with brains in vats wired into happiness machines, for example. When a superintelligence is in charge, you have to get its motivational system exactly right in order to not make the future undesirable.\n\n1. The locality of human values: Imagine if the Friendly AI problem had faced the ancient Greeks, and they had programmed it with the most progressive moral values of their time. That would have led the world to a rather horrifying fate. But why should we think that humans have, in the 21st century, arrived at the apex of human morality? We can\u2019t risk programming a superintelligent machine with the moral values we happen to hold today. But then, which moral values do we give it?\n\nYudkowsky [suggests](https://intelligence.org/files/CEV.pdf) that we build a \u2018seed AI\u2019 to discover and then extrapolate the \u2018coherent extrapolated volition\u2019 of humanity:\n\n> In poetic terms, our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.\n\nThe seed AI would use the results of this examination and extrapolation of human values to program the motivational system of the superintelligence that would determine the fate of the galaxy.\n\nHowever, some worry that the collective will of humanity won\u2019t converge on a coherent set of goals. Others [believe](http://multiverseaccordingtoben.blogspot.com/2010/10/singularity-institutes-scary-idea-and.html) that guaranteed Friendliness is not possible, even by such elaborate and careful means.\n\n- Yudkowsky, [Coherent Extrapolated Volition](https://intelligence.org/files/CEV.pdf)\n\n**Related**\n\n[What are tiling agents?](https://docs.google.com/document/d/19MBaE2DsvBbF3XaeArcsuvwDrC0_Oj0I-1ciCLrgo9I/edit)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "", "UI ID": "6939", "Related Answers": "", "Doc Last Ingested": "2023-03-15T10:12:05.636+01:00", "Request Count": 1, "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 65, "Helpful": ""}}, {"id": "i-df03e3184d4bbbb9b1663acad6ff97ad4f26b91d43844ebe50431771d30b7c9e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-df03e3184d4bbbb9b1663acad6ff97ad4f26b91d43844ebe50431771d30b7c9e", "name": "What is \"biological cognitive enhancement\"?", "index": 363, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:45:36.214Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-df03e3184d4bbbb9b1663acad6ff97ad4f26b91d43844ebe50431771d30b7c9e", "values": {"File": "What is \"biological cognitive enhancement\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"biological cognitive enhancement\"?", "Link": "https://docs.google.com/document/d/1cl2NFU_PiYz252Izr5v8VEWYz6ggDRwVjQ8-_bBRHgI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:38.873+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,Cognitive Enhancement", "Doc Last Edited": "2023-02-22T23:04:47.150+01:00", "Status": "Live on site", "Edit Answer": "What is \"biological cognitive enhancement\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6590", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "What is \"biological cognitive enhancement\"?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "What is \"biological cognitive enhancement\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "There may be genes or molecules that can be modified to improve general intelligence. Researchers [have already done this in mice](https://pubmed.ncbi.nlm.nih.gov/10485705/): they over-expressed the NR2B gene, which improved those mice\u2019s memory beyond that of any other mice of any mouse species. Biological cognitive enhancement in humans may cause an intelligence explosion to occur more quickly than it otherwise would.\n\nSee also:\n\n- Bostrom & Sandberg, [Cognitive Enhancement: Methods, Ethics, Regulatory Challenges](http://www.nickbostrom.com/cognitive.pdf)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "There may be genes or molecules that can be modified to improve general intelligence. Researchers [have already done this in mice](https://pubmed.ncbi.nlm.nih.gov/10485705/): they over-expressed the NR2B gene, which improved those mice\u2019s memory beyond that of any other mice of any mouse species. Biological cognitive enhancement in humans may cause an intelligence explosion to occur more quickly than it otherwise would.\n\nSee also:\n\n- Bostrom & Sandberg, [Cognitive Enhancement: Methods, Ethics, Regulatory Challenges](http://www.nickbostrom.com/cognitive.pdf)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "", "UI ID": "6590", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:53.087+01:00", "Request Count": "", "Number of suggestions on answer doc": 59, "Total character count of suggestions on answer doc": 4481, "Helpful": ""}}, {"id": "i-502f726cb7dab8e18b68652604bcfbc6dc63141b00106e4fdf96251ec51f9266", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-502f726cb7dab8e18b68652604bcfbc6dc63141b00106e4fdf96251ec51f9266", "name": "What is \"agent foundations\"?", "index": 364, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:45:47.482Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-502f726cb7dab8e18b68652604bcfbc6dc63141b00106e4fdf96251ec51f9266", "values": {"File": "What is \"agent foundations\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"agent foundations\"?", "Link": "https://docs.google.com/document/d/1IoNjYe6bdE24xlDHpLPa7bb0LCbHTNVVTI-R4IJJ8zY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:35.314+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,Research Agendas,Agent Foundations", "Doc Last Edited": "2023-02-22T22:55:06.008+01:00", "Status": "Not started", "Edit Answer": "What is \"agent foundations\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7782", "Source Link": "", "aisafety.info Link": "What is \"agent foundations\"?", "Source": "Wiki", "All Phrasings": "What is \"agent foundations\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 3, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7782", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:54.953+01:00", "Request Count": "", "Number of suggestions on answer doc": 59, "Total character count of suggestions on answer doc": 4481, "Helpful": ""}}, {"id": "i-b81c249a1be471f57a4aabd7d68d02b92ee59ee5e21982ada1d205f176ee2649", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b81c249a1be471f57a4aabd7d68d02b92ee59ee5e21982ada1d205f176ee2649", "name": "What is \"HCH\"?", "index": 365, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-16T10:22:05.979Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b81c249a1be471f57a4aabd7d68d02b92ee59ee5e21982ada1d205f176ee2649", "values": {"File": "What is \"HCH\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"HCH\"?", "Link": "https://docs.google.com/document/d/100LRhEYVX70qGfryFC55AcsaoXL6SobtuJgLHkQcZsE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:31.715+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-14T23:57:49.495+01:00", "Status": "Live on site", "Edit Answer": "What is \"HCH\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7810", "Source Link": "", "aisafety.info Link": "What is \"HCH\"?", "Source": "Wiki", "All Phrasings": "What is \"HCH\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**Humans Consulting HCH (HCH)** is a recursive acronym describing a setup where humans can consult simulations of themselves to help answer questions. It is a concept used in discussion of the [iterated amplification](https://www.lesswrong.com/tag/iterated-amplification) proposal to solve the alignment problem.\n\nIt was first described by Paul Christiano in his post [Humans Consulting HCH](https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch):\n\n> Consider a human Hugh who has access to a question-answering machine. Suppose the machine answers question Q by perfectly imitating how Hugh would answer question Q, *if* *Hugh had access to the question-answering machine*.\n> \n> That is, Hugh is able to consult a copy of Hugh, who is able to consult a copy of Hugh, who is able to consult a copy of Hugh\u2026\n> \n> Let\u2019s call this process HCH, for \u201cHumans Consulting HCH.\u201d", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "**Humans Consulting HCH (HCH)** is a recursive acronym describing a setup where humans can consult simulations of themselves to help answer questions. It is a concept used in discussion of the [iterated amplification](https://www.lesswrong.com/tag/iterated-amplification) proposal to solve the alignment problem.\n\nIt was first described by Paul Christiano in his post [Humans Consulting HCH](https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch):\n\n> Consider a human Hugh who has access to a question-answering machine. Suppose the machine answers question Q by perfectly imitating how Hugh would answer question Q, *if* *Hugh had access to the question-answering machine*.\n> \n> That is, Hugh is able to consult a copy of Hugh, who is able to consult a copy of Hugh, who is able to consult a copy of Hugh\u2026\n> \n> Let\u2019s call this process HCH, for \u201cHumans Consulting HCH.\u201d", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "Nico Hill2", "External Source": "https://www.lesswrong.com/tag/humans-consulting-hch?edit=true", "Last Asked On Discord": "", "UI ID": "7810", "Related Answers": "", "Doc Last Ingested": "2023-03-16T11:21:59.917+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 2837, "Helpful": ""}}, {"id": "i-0a91e3ff08f4a73d38e1171358999bd4a0337a806bbdf5b7053ea432fda0f417", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0a91e3ff08f4a73d38e1171358999bd4a0337a806bbdf5b7053ea432fda0f417", "name": "What is \"Do What I Mean\"?", "index": 366, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:46:00.034Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0a91e3ff08f4a73d38e1171358999bd4a0337a806bbdf5b7053ea432fda0f417", "values": {"File": "What is \"Do What I Mean\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is \"Do What I Mean\"?", "Link": "https://docs.google.com/document/d/1QxplmZZxahi94f0-AZo5U_ZYbE415-nEMcS5QIyVTso/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:28.250+01:00", "Related Answers DO NOT EDIT": "Once we notice that a superintelligence given a specific task is trying to take over the world, can\u2019t we turn it off, reprogram it or otherwise correct the problem?", "Tags": "Do What I Mean", "Doc Last Edited": "2023-02-22T23:04:49.382+01:00", "Status": "Live on site", "Edit Answer": "What is \"Do What I Mean\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7673", "Source Link": "", "aisafety.info Link": "What is \"Do What I Mean\"?", "Source": "Wiki", "All Phrasings": "What is \"Do What I Mean\"?\n", "Initial Order": "", "Related IDs": "6988", "Rich Text DO NOT EDIT": "\u201cDo what I mean\u201d, is an alignment strategy where the AI is programmed to try and do what the human meant by an instruction, rather than blindly following directions. Think following the spirit of the law over the letter. This potentially helps with alignment in two ways. First, it might allow the AI to learn more subtle goals, which you might not have been able to explicitly state. Second, it might make the AI corrigible, willing to have its goals or programming corrected, and continuously interested in what people want (including allowing itself to be shut off if need be). Since it is programmed to \"do what you mean\" it will be open to accepting correction.\n\nIt comes in contrast to the more typical, \u201cdo what I say\u201d approach of programming AI by giving it an explicit goal. The problem with an explicit goal is that if the goal is misstated, or leaves out some detail, the AI will optimize for something we don\u2019t want. Think of the story of King Midas, who wished that everything he touched would turn to gold and died of starvation. In contrast a \"do what I mean\" system will try to understand and implement the programmers intent, rather then the exact \"words\".\n\nOne specific \"Do what I mean\" proposal is[\"Cooperative Inverse Reinforcement Learning\"](https://www.lesswrong.com/tag/inverse-reinforcement-learning), where the goal is hidden from the AI. Since it doesn't have direct access to its reward function, the AI will try and discover the goal from the things you tell it and from the examples you give it. Thus slowly getting closer to doing what you actually want.\n\nFor more information, see[Do what we mean vs. do what we say](https://www.lesswrong.com/posts/8Q5h6hyBXTEgC6EZf/do-what-i-mean-vs-do-what-i-say) by Rohin Shah. In which he defines a \"do what we mean\" system, shows how it might help with alignment and discusses how it could be combined with a \"do what we say\" subsystem for added safety.\n\nFor a discussion of a spectrum of different levels of \"do what I mean\" ability, see[Do What I Mean hierarchy](https://arbital.greaterwrong.com/p/dwim) by Eliezer Yudkowsky.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "\u201cDo what I mean\u201d, is an alignment strategy where the AI is programmed to try and do what the human meant by an instruction, rather than blindly following directions. Think following the spirit of the law over the letter. This potentially helps with alignment in two ways. First, it might allow the AI to learn more subtle goals, which you might not have been able to explicitly state. Second, it might make the AI corrigible, willing to have its goals or programming corrected, and continuously interested in what people want (including allowing itself to be shut off if need be). Since it is programmed to \"do what you mean\" it will be open to accepting correction.\n\nIt comes in contrast to the more typical, \u201cdo what I say\u201d approach of programming AI by giving it an explicit goal. The problem with an explicit goal is that if the goal is misstated, or leaves out some detail, the AI will optimize for something we don\u2019t want. Think of the story of King Midas, who wished that everything he touched would turn to gold and died of starvation. In contrast a \"do what I mean\" system will try to understand and implement the programmers intent, rather then the exact \"words\".\n\nOne specific \"Do what I mean\" proposal is[\"Cooperative Inverse Reinforcement Learning\"](https://www.lesswrong.com/tag/inverse-reinforcement-learning), where the goal is hidden from the AI. Since it doesn't have direct access to its reward function, the AI will try and discover the goal from the things you tell it and from the examples you give it. Thus slowly getting closer to doing what you actually want.\n\nFor more information, see[Do what we mean vs. do what we say](https://www.lesswrong.com/posts/8Q5h6hyBXTEgC6EZf/do-what-i-mean-vs-do-what-i-say) by Rohin Shah. In which he defines a \"do what we mean\" system, shows how it might help with alignment and discusses how it could be combined with a \"do what we say\" subsystem for added safety.\n\nFor a discussion of a spectrum of different levels of \"do what I mean\" ability, see[Do What I Mean hierarchy](https://arbital.greaterwrong.com/p/dwim) by Eliezer Yudkowsky.\n\n", "Stamp Count": 0, "Multi Answer": true, "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7673", "Related Answers": "Once we notice that a superintelligence given a specific task is trying to take over the world, can\u2019t we turn it off, reprogram it or otherwise correct the problem?", "Doc Last Ingested": "2023-03-14T23:36:59.243+01:00", "Request Count": "", "Number of suggestions on answer doc": 62, "Total character count of suggestions on answer doc": 7448, "Helpful": ""}}, {"id": "i-5fdbbe58836d61f84c39012ba430b876e5d253a7f0ce6620e1ada14c90245baf", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5fdbbe58836d61f84c39012ba430b876e5d253a7f0ce6620e1ada14c90245baf", "name": "What if we put the AI in a box and have a second, more powerful, AI with the goal of preventing the first one from escaping?", "index": 367, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:42:57.129Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5fdbbe58836d61f84c39012ba430b876e5d253a7f0ce6620e1ada14c90245baf", "values": {"File": "What if we put the AI in a box and have a second, more powerful, AI with the goal of preventing the first one from escaping?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What if we put the AI in a box and have a second, more powerful, AI with the goal of preventing the first one from escaping?", "Link": "https://docs.google.com/document/d/1OCXOesmGnrEhbO8sbyzC7ELRZw2GMbCRbvXDSJpIZGI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:24.850+01:00", "Related Answers DO NOT EDIT": "Why can\u2019t we just \u201cput the AI in a box\u201d so that it can\u2019t influence the outside world?,What is a \"value handshake\"?", "Tags": "Boxing,Why Not Just", "Doc Last Edited": "2023-03-05T22:06:18.164+01:00", "Status": "In review", "Edit Answer": "What if we put the AI in a box and have a second, more powerful, AI with the goal of preventing the first one from escaping?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6500", "Source Link": "", "aisafety.info Link": "What if we put the AI in a box and have a second, more powerful, AI with the goal of preventing the first one from escaping?", "Source": "Wiki", "All Phrasings": "What if we put the AI in a box and have a second, more powerful, AI with the goal of preventing the first one from escaping?\n", "Initial Order": "", "Related IDs": "6176,7491", "Rich Text DO NOT EDIT": "\n![](https://lh4.googleusercontent.com/-shSyo-EcWveAWbwqZUN2p2H6ZtMcWo7x9dKpEA36r2bbLJhjDJowM5gsp1lC9B2AXH9Twu1PQIqY2eirHFYz2hZeoVkCIy2t-EJAxRU1o38zQBsqZou8JwH_6e181aErK7kxu0Y5WWBZLf93BOzVPIxJInG_crn)\n\nPreventing an AI from escaping by using a more powerful AI, gets points for creative thinking, but unfortunately we would need to have already aligned the first AI. Even if the second AI's only terminal goal were to prevent the first AI from escaping, it wouldalso have an instrumental goal of converting the rest of the universe into computer chips so that it would have more processing power to figure out how to best contain the first AGI.\n\nIt might be possible to try to bind a stronger AI with a weaker AI, but this is unlikely to work as the stronger AI would have an advantage due to being stronger. \n\nFurther, there is a chance that the two AI's end up working out a deal where the first AI decides to stay in the box and the second AI does whatever the first AI would have done if it were able to escape. Let's call the AI in the box the prisoner and the AI outside the guard. You could imagine such AIs boxed like nesting dolls but there would always be a highest level where the guard AI is boxed by humans. This being the case, it is not clear what such a design buys you, as all the issues of containing an AI will still apply at this top level.\n\n", "Tag Count": 2, "Related Answer Count": 2, "Rich Text": "\n![](https://lh4.googleusercontent.com/-shSyo-EcWveAWbwqZUN2p2H6ZtMcWo7x9dKpEA36r2bbLJhjDJowM5gsp1lC9B2AXH9Twu1PQIqY2eirHFYz2hZeoVkCIy2t-EJAxRU1o38zQBsqZou8JwH_6e181aErK7kxu0Y5WWBZLf93BOzVPIxJInG_crn)\n\nPreventing an AI from escaping by using a more powerful AI, gets points for creative thinking, but unfortunately we would need to have already aligned the first AI. Even if the second AI's only terminal goal were to prevent the first AI from escaping, it wouldalso have an instrumental goal of converting the rest of the universe into computer chips so that it would have more processing power to figure out how to best contain the first AGI.\n\nIt might be possible to try to bind a stronger AI with a weaker AI, but this is unlikely to work as the stronger AI would have an advantage due to being stronger. \n\nFurther, there is a chance that the two AI's end up working out a deal where the first AI decides to stay in the box and the second AI does whatever the first AI would have done if it were able to escape. Let's call the AI in the box the prisoner and the AI outside the guard. You could imagine such AIs boxed like nesting dolls but there would always be a highest level where the guard AI is boxed by humans. This being the case, it is not clear what such a design buys you, as all the issues of containing an AI will still apply at this top level.\n\n", "Stamp Count": 0, "Multi Answer": true, "Stamped By": "", "Priority": 3, "Asker": "filip", "External Source": "", "Last Asked On Discord": "", "UI ID": "6500", "Related Answers": "Why can\u2019t we just \u201cput the AI in a box\u201d so that it can\u2019t influence the outside world?,What is a \"value handshake\"?", "Doc Last Ingested": "2023-03-14T23:35:29.676+01:00", "Request Count": "", "Number of suggestions on answer doc": 109, "Total character count of suggestions on answer doc": 9239, "Helpful": ""}}, {"id": "i-57a008c84c9e0561fe6610779c0e93dae9dddda133395240c2958666d7c4ad1a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-57a008c84c9e0561fe6610779c0e93dae9dddda133395240c2958666d7c4ad1a", "name": "What if technological progress stagnates and we never achieve AGI?", "index": 368, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:46:04.911Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-57a008c84c9e0561fe6610779c0e93dae9dddda133395240c2958666d7c4ad1a", "values": {"File": "What if technological progress stagnates and we never achieve AGI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What if technological progress stagnates and we never achieve AGI?", "Link": "https://docs.google.com/document/d/1g9oP8gt1RiHAdmahhUJMXo92Dw4P64u8UQX4tibmh4g/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:21.161+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:55:08.145+01:00", "Status": "In progress", "Edit Answer": "What if technological progress stagnates and we never achieve AGI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7768", "Source Link": "", "aisafety.info Link": "What if technological progress stagnates and we never achieve AGI?", "Source": "Wiki", "All Phrasings": "What if technological progress stagnates and we never achieve AGI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Short term, nothing much. Long term, aliens arrive and we become slaves (or some other avoidable X-risk)\n\n- The future lightcone becomes a bit less awesome\n\n- The assumption is that AGI would be a very useful tool, as it would be able to do things (invent, coordinate, control, etc.) that base humans cannot. If it turns out that AGI isn\u2019t possible, then that would greatly limit (or slow down, which amounts to the same thing) the pool of other possible things to be done/created/made.\n\n- What if steam engines weren\u2019t invented?\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "- Short term, nothing much. Long term, aliens arrive and we become slaves (or some other avoidable X-risk)\n\n- The future lightcone becomes a bit less awesome\n\n- The assumption is that AGI would be a very useful tool, as it would be able to do things (invent, coordinate, control, etc.) that base humans cannot. If it turns out that AGI isn\u2019t possible, then that would greatly limit (or slow down, which amounts to the same thing) the pool of other possible things to be done/created/made.\n\n- What if steam engines weren\u2019t invented?\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7768", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:37:01.385+01:00", "Request Count": "", "Number of suggestions on answer doc": 68, "Total character count of suggestions on answer doc": 7813, "Helpful": ""}}, {"id": "i-2c32c1d6049899d87ec0a7a02a59778e1b1d7908d3b2686f3dac0069d3d2101f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2c32c1d6049899d87ec0a7a02a59778e1b1d7908d3b2686f3dac0069d3d2101f", "name": "What harm could a single superintelligence do when it took so many humans to build civilization?", "index": 369, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:04.003Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2c32c1d6049899d87ec0a7a02a59778e1b1d7908d3b2686f3dac0069d3d2101f", "values": {"File": "What harm could a single superintelligence do when it took so many humans to build civilization?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What harm could a single superintelligence do when it took so many humans to build civilization?", "Link": "https://docs.google.com/document/d/1pi9AGJjMZdXp8vPTXDzAyrGdz-ycF3mjSsqBGZNlznU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:17.269+01:00", "Related Answers DO NOT EDIT": "How might a superintelligence socially manipulate humans?,Won't humans be able to beat an unaligned AI since we have a huge advantage in numbers?", "Tags": "Superintelligence,Civilization", "Doc Last Edited": "2023-02-22T23:04:50.782+01:00", "Status": "Live on site", "Edit Answer": "What harm could a single superintelligence do when it took so many humans to build civilization?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6970", "Source Link": "", "aisafety.info Link": "What harm could a single superintelligence do when it took so many humans to build civilization?", "Source": "Wiki", "All Phrasings": "What harm could a single superintelligence do when it took so many humans to build civilization?\n", "Initial Order": "", "Related IDs": "6974,86WT", "Rich Text DO NOT EDIT": "Superintelligence has an advantage that an early human didn\u2019t \u2013 the entire context of human civilization and technology, there for it to manipulate socially or technologically.\n\n", "Tag Count": 2, "Related Answer Count": 2, "Rich Text": "Superintelligence has an advantage that an early human didn\u2019t \u2013 the entire context of human civilization and technology, there for it to manipulate socially or technologically.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6970", "Related Answers": "How might a superintelligence socially manipulate humans?,Won't humans be able to beat an unaligned AI since we have a huge advantage in numbers?", "Doc Last Ingested": "2023-03-14T23:35:35.317+01:00", "Request Count": "", "Number of suggestions on answer doc": 109, "Total character count of suggestions on answer doc": 9239, "Helpful": ""}}, {"id": "i-e2f6906a3ec935435272a6e6186a2276312c4051b63a3ee4b16e525b39da6f43", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e2f6906a3ec935435272a6e6186a2276312c4051b63a3ee4b16e525b39da6f43", "name": "What external content would be useful to the Stampy project?", "index": 370, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:06.434Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e2f6906a3ec935435272a6e6186a2276312c4051b63a3ee4b16e525b39da6f43", "values": {"File": "What external content would be useful to the Stampy project?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What external content would be useful to the Stampy project?", "Link": "https://docs.google.com/document/d/1i2km30RwjrSV4vWjRRnbVKexdNg-m8dEg5RHstv5nOw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:13.207+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stampy,Literature", "Doc Last Edited": "2023-02-22T22:55:09.127+01:00", "Status": "In progress", "Edit Answer": "What external content would be useful to the Stampy project?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6562", "Source Link": "", "aisafety.info Link": "What external content would be useful to the Stampy project?", "Source": "Wiki", "All Phrasings": "What external content would be useful to the Stampy project?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- List of AIS accounts to follow on [Twitter](https://twitter.com/i/lists/1185207859728076800), Facebook, YouTube, TikTok, RSS, [mailing lists](https://www.aisafetysupport.org/resources/lots-of-links#h.6vx87m6nndb1)\n\n- I recommend an RSS feed reader for Forums. You can filter by tag using greaterwrong. Example: [Interpretability Tag](https://www.greaterwrong.com/tag/transparency-interpretability-ml-and-ai) (rss icon on the top right)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "- List of AIS accounts to follow on [Twitter](https://twitter.com/i/lists/1185207859728076800), Facebook, YouTube, TikTok, RSS, [mailing lists](https://www.aisafetysupport.org/resources/lots-of-links#h.6vx87m6nndb1)\n\n- I recommend an RSS feed reader for Forums. You can filter by tag using greaterwrong. Example: [Interpretability Tag](https://www.greaterwrong.com/tag/transparency-interpretability-ml-and-ai) (rss icon on the top right)\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6562", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:35:37.990+01:00", "Request Count": "", "Number of suggestions on answer doc": 109, "Total character count of suggestions on answer doc": 9239, "Helpful": ""}}, {"id": "i-1c7f0d00f77b1950eebd7e5454b9b9ae847a3b867ece7d6a7e00f829493f0f05", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1c7f0d00f77b1950eebd7e5454b9b9ae847a3b867ece7d6a7e00f829493f0f05", "name": "What evidence do experts usually base their timeline predictions on?", "index": 371, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:08.939Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1c7f0d00f77b1950eebd7e5454b9b9ae847a3b867ece7d6a7e00f829493f0f05", "values": {"File": "What evidence do experts usually base their timeline predictions on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What evidence do experts usually base their timeline predictions on?", "Link": "https://docs.google.com/document/d/1R8HA2r0XfqqV1zlgyd6YqAV9avVRe1RqxcQVixDr-Ck/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:09.689+01:00", "Related Answers DO NOT EDIT": "When will transformative AI be created?,How close do AI experts think we are to creating superintelligence?,When will an intelligence explosion happen?,How long will it be until superintelligent AI is created?", "Tags": "Timelines", "Doc Last Edited": "2023-03-12T22:01:04.370+01:00", "Status": "Live on site", "Edit Answer": "What evidence do experts usually base their timeline predictions on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6478", "Source Link": "", "aisafety.info Link": "What evidence do experts usually base their timeline predictions on?", "Source": "Wiki", "All Phrasings": "What evidence do experts usually base their timeline predictions on?\n", "Initial Order": "", "Related IDs": "2398,8165,6599,5633", "Rich Text DO NOT EDIT": "Machine learning researchers each have their own sense of how fast AI capabilities will progress, based on their understanding of how past and present AI systems work, what approaches could be tried in the future, how fast computing power will scale up, and what the world's future as a whole will look like. Recent fast progress in deep learning has caused some to update toward [very short timelines](https://www.youtube.com/watch?v=Oz4G9zrlAGs&t=2083s), but [others disagree](https://garymarcus.substack.com/p/dear-elon-musk-here-are-five-things?s=w). When [surveys](https://www.lesswrong.com/posts/H6hMugfY3tDQGfqYL/what-do-ml-researchers-think-about-ai-in-2022) come up with a median number like \"AGI by 2059\", most respondents are probably comparing their subjective senses of the speed of progress and the difficulty of the problem, arriving at a guess that feels reasonable to them.\n\nSome researchers have tried to [model timelines more rigorously](https://epochai.org/blog/literature-review-of-transformative-artificial-intelligence-timelines). For example:\n\n- Ajeya Cotra's [\"biological anchors\" model](https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines)[projects](https://www.alignmentforum.org/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works) the computing power used for future ML training runs based on advances in hardware and increases in willingness to spend. It then compares the results to several different \"biological anchors\": for example, the assumption that training a transformative AI takes as much computing power as a human brain uses during a lifetime, or as much computing power as all brains used in our evolutionary history, or more likely something in between.\n\n- A major part of the biological anchors model is a probability distribution for how much computing power it would take to build transformative AI with current algorithms and ideas. [Daniel Kokotajlo has argued](https://www.alignmentforum.org/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute) for a [different way](https://www.alignmentforum.org/posts/bAAtiG8og7CxH3cXG/review-of-fun-with-12-ooms-of-compute) to estimate this quantity. Instead of analogies to the human brain, he bases his estimates on intuitions about what kind of AI systems could be built.\n\n- Tom Davidson's [approach based on \"semi-informative priors\"](https://www.openphilanthropy.org/research/report-on-semi-informative-priors/)[looks at](https://epochai.org/blog/grokking-semi-informative-priors) the statistical distribution of timelines for past inventions. These inventions are taken from a few reference classes, such as highly ambitious STEM R&D goals.\n\n- Robin Hanson [has collected expert guesses](https://aiimpacts.org/hanson-ai-expert-survey/) of [what fraction of the way](https://www.alignmentforum.org/posts/h3ejmEeNniDNFXTgp/fractional-progress-estimates-for-ai-timelines-and-implied) to human level we have come in individual subfields.\n\n- Matthew Barnett has done [calculations](https://www.alignmentforum.org/posts/4ufbirCCLsFiscWuY/a-proposed-method-for-forecasting-transformative-ai) on when we can expect scaling laws to take language models to the point where they generate sufficiently human-like text. The idea is that if AI text is hard enough to distinguish from human text, this implies at least human-like competence.\n\nThese approaches give very different answers to the question when we\u2019ll first have advanced AI. Cotra\u2019s model originally gave a median of 2050, but she [later updated to 2040](https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines). The [Colab notebook](https://colab.research.google.com/drive/10klKXI_YnoZK5ajXi7BZk6aMJZ5fdZmt) that uses Barnett\u2019s direct method also shows ([as of February 2023](https://twitter.com/MatthewJBar/status/1624217446558363649)) a 2040 median. But on the shorter side, Kokotajlo has [argued for a median before 2030](https://www.alignmentforum.org/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff?commentId=YAraBzHPTHjHQG56X). And on the longer side, Davidson\u2019s report gives only an 18% probability for AGI by 2100, and (based on estimates made between 2012 and 2017) Hanson\u2019s method also \u201csuggests at least a century until human-level AI\u201d. Different experts put different weights on these and other considerations, so they end up with different estimates.\n\n", "Tag Count": 1, "Related Answer Count": 4, "Rich Text": "Machine learning researchers each have their own sense of how fast AI capabilities will progress, based on their understanding of how past and present AI systems work, what approaches could be tried in the future, how fast computing power will scale up, and what the world's future as a whole will look like. Recent fast progress in deep learning has caused some to update toward [very short timelines](https://www.youtube.com/watch?v=Oz4G9zrlAGs&t=2083s), but [others disagree](https://garymarcus.substack.com/p/dear-elon-musk-here-are-five-things?s=w). When [surveys](https://www.lesswrong.com/posts/H6hMugfY3tDQGfqYL/what-do-ml-researchers-think-about-ai-in-2022) come up with a median number like \"AGI by 2059\", most respondents are probably comparing their subjective senses of the speed of progress and the difficulty of the problem, arriving at a guess that feels reasonable to them.\n\nSome researchers have tried to [model timelines more rigorously](https://epochai.org/blog/literature-review-of-transformative-artificial-intelligence-timelines). For example:\n\n- Ajeya Cotra's [\"biological anchors\" model](https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines)[projects](https://www.alignmentforum.org/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works) the computing power used for future ML training runs based on advances in hardware and increases in willingness to spend. It then compares the results to several different \"biological anchors\": for example, the assumption that training a transformative AI takes as much computing power as a human brain uses during a lifetime, or as much computing power as all brains used in our evolutionary history, or more likely something in between.\n\n- A major part of the biological anchors model is a probability distribution for how much computing power it would take to build transformative AI with current algorithms and ideas. [Daniel Kokotajlo has argued](https://www.alignmentforum.org/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute) for a [different way](https://www.alignmentforum.org/posts/bAAtiG8og7CxH3cXG/review-of-fun-with-12-ooms-of-compute) to estimate this quantity. Instead of analogies to the human brain, he bases his estimates on intuitions about what kind of AI systems could be built.\n\n- Tom Davidson's [approach based on \"semi-informative priors\"](https://www.openphilanthropy.org/research/report-on-semi-informative-priors/)[looks at](https://epochai.org/blog/grokking-semi-informative-priors) the statistical distribution of timelines for past inventions. These inventions are taken from a few reference classes, such as highly ambitious STEM R&D goals.\n\n- Robin Hanson [has collected expert guesses](https://aiimpacts.org/hanson-ai-expert-survey/) of [what fraction of the way](https://www.alignmentforum.org/posts/h3ejmEeNniDNFXTgp/fractional-progress-estimates-for-ai-timelines-and-implied) to human level we have come in individual subfields.\n\n- Matthew Barnett has done [calculations](https://www.alignmentforum.org/posts/4ufbirCCLsFiscWuY/a-proposed-method-for-forecasting-transformative-ai) on when we can expect scaling laws to take language models to the point where they generate sufficiently human-like text. The idea is that if AI text is hard enough to distinguish from human text, this implies at least human-like competence.\n\nThese approaches give very different answers to the question when we\u2019ll first have advanced AI. Cotra\u2019s model originally gave a median of 2050, but she [later updated to 2040](https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines). The [Colab notebook](https://colab.research.google.com/drive/10klKXI_YnoZK5ajXi7BZk6aMJZ5fdZmt) that uses Barnett\u2019s direct method also shows ([as of February 2023](https://twitter.com/MatthewJBar/status/1624217446558363649)) a 2040 median. But on the shorter side, Kokotajlo has [argued for a median before 2030](https://www.alignmentforum.org/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff?commentId=YAraBzHPTHjHQG56X). And on the longer side, Davidson\u2019s report gives only an 18% probability for AGI by 2100, and (based on estimates made between 2012 and 2017) Hanson\u2019s method also \u201csuggests at least a century until human-level AI\u201d. Different experts put different weights on these and other considerations, so they end up with different estimates.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "6478", "Related Answers": "When will transformative AI be created?,How close do AI experts think we are to creating superintelligence?,When will an intelligence explosion happen?,How long will it be until superintelligent AI is created?", "Doc Last Ingested": "2023-03-14T23:35:40.783+01:00", "Request Count": "", "Number of suggestions on answer doc": 109, "Total character count of suggestions on answer doc": 9239, "Helpful": ""}}, {"id": "i-b120b30d42eddef110561e5acaa8c593d7078c75a1d04944991d4255bc8fbd05", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b120b30d42eddef110561e5acaa8c593d7078c75a1d04944991d4255bc8fbd05", "name": "What does the scheme Externalized Reasoning Oversight involve?", "index": 372, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:10.998Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b120b30d42eddef110561e5acaa8c593d7078c75a1d04944991d4255bc8fbd05", "values": {"File": "What does the scheme Externalized Reasoning Oversight involve?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What does the scheme Externalized Reasoning Oversight involve?", "Link": "https://docs.google.com/document/d/1_3alCOinAwLHsRQve-QZOqE89ZJ5_tTAYi_h2HrdGxg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:06.310+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Interpretability,DeepMind", "Doc Last Edited": "2023-02-22T23:04:52.201+01:00", "Status": "Live on site", "Edit Answer": "What does the scheme Externalized Reasoning Oversight involve?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8350", "Source Link": "", "aisafety.info Link": "What does the scheme Externalized Reasoning Oversight involve?", "Source": "Wiki", "All Phrasings": "What does the scheme Externalized Reasoning Oversight involve?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Idea: Make the AGI, which is an LLM, externalize its reasoning (via chain of thought) so that its thoughts are visible to an external overseer, which then verifies that the LLM is not thinking deceptive thoughts.\n\nThis scheme requires:\n\n- Not too much steganography: the model can't hide its reasoning in non-English logical thought. This is the default of what would happen if we apply RL to this model maybe?\n\n- A safe overseer that can look through the chain of thought reasoning and identify misaligned behavior.\n\nA key dynamic is that English is not the ideal language to be reasoning: an AGI could be more capable by doing some reasoning not in English. Thus, applying RL to a model exerts pressure towards ideal logical reasoning. However, self-supervised learning (SSL) exerts some pressure back towards just doing text prediction, and hence more of its reasoning being in English. A key question for this research agenda is thus how to put pressure to keep the reasoning externalized, instead of collapsing into internal, more efficient/logical reasoning that is much harder for us to oversee.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Idea: Make the AGI, which is an LLM, externalize its reasoning (via chain of thought) so that its thoughts are visible to an external overseer, which then verifies that the LLM is not thinking deceptive thoughts.\n\nThis scheme requires:\n\n- Not too much steganography: the model can't hide its reasoning in non-English logical thought. This is the default of what would happen if we apply RL to this model maybe?\n\n- A safe overseer that can look through the chain of thought reasoning and identify misaligned behavior.\n\nA key dynamic is that English is not the ideal language to be reasoning: an AGI could be more capable by doing some reasoning not in English. Thus, applying RL to a model exerts pressure towards ideal logical reasoning. However, self-supervised learning (SSL) exerts some pressure back towards just doing text prediction, and hence more of its reasoning being in English. A key question for this research agenda is thus how to put pressure to keep the reasoning externalized, instead of collapsing into internal, more efficient/logical reasoning that is much harder for us to oversee.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8350", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:35:43.014+01:00", "Request Count": "", "Number of suggestions on answer doc": 109, "Total character count of suggestions on answer doc": 9239, "Helpful": ""}}, {"id": "i-3e5a77a8f2b7e07a5f52df74fb7b2580398dd4aae7ae822f2ff3fcc40ea6c16d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3e5a77a8f2b7e07a5f52df74fb7b2580398dd4aae7ae822f2ff3fcc40ea6c16d", "name": "What does generative visualization look like in reinforcement learning?", "index": 373, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:13.153Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3e5a77a8f2b7e07a5f52df74fb7b2580398dd4aae7ae822f2ff3fcc40ea6c16d", "values": {"File": "What does generative visualization look like in reinforcement learning?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What does generative visualization look like in reinforcement learning?", "Link": "https://docs.google.com/document/d/1ZZIwr_yJsMx6i21eoDV5wmeRYAQowdjcV24-00BqlTc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:47:02.647+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Interpretability", "Doc Last Edited": "2023-02-22T23:04:53.074+01:00", "Status": "Live on site", "Edit Answer": "What does generative visualization look like in reinforcement learning?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8426", "Source Link": "", "aisafety.info Link": "What does generative visualization look like in reinforcement learning?", "Source": "Wiki", "All Phrasings": "What does generative visualization look like in reinforcement learning?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Generative visualization for an image classifier means showing an input image which causes the classifier to strongly recognize a feature in that image.\n\nIn [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning) (RL), generative visualization means showing a sequence of observations which make an RL agent strongly want to take a specific action.\n\nThe problem with generative visualization in RL is that the space of possible observations is constrained by the transition function. So, if we optimize the observations to maximize the activation for an action, this will likely result in a sequence of observations which is impossible (incompatible with the transition function).\n\nA way around this is to compute an embedding of possible observation sequences and to optimize in the embedding space instead of the observation space. A relevant paper here is [Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents](https://arxiv.org/abs/1904.01318)\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Generative visualization for an image classifier means showing an input image which causes the classifier to strongly recognize a feature in that image.\n\nIn [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning) (RL), generative visualization means showing a sequence of observations which make an RL agent strongly want to take a specific action.\n\nThe problem with generative visualization in RL is that the space of possible observations is constrained by the transition function. So, if we optimize the observations to maximize the activation for an action, this will likely result in a sequence of observations which is impossible (incompatible with the transition function).\n\nA way around this is to compute an embedding of possible observation sequences and to optimize in the embedding space instead of the observation space. A relevant paper here is [Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents](https://arxiv.org/abs/1904.01318)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "Magdalena", "External Source": "", "Last Asked On Discord": "", "UI ID": "8426", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:35:45.432+01:00", "Request Count": "", "Number of suggestions on answer doc": 109, "Total character count of suggestions on answer doc": 9239, "Helpful": ""}}, {"id": "i-93eafad2c5e97d43443b862912ed04e9dd918b130d777cd6484a23da0e79488f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-93eafad2c5e97d43443b862912ed04e9dd918b130d777cd6484a23da0e79488f", "name": "What is alignment failure?", "index": 374, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:16.173Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-93eafad2c5e97d43443b862912ed04e9dd918b130d777cd6484a23da0e79488f", "values": {"File": "What is alignment failure?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What is alignment failure?", "Link": "https://docs.google.com/document/d/17oGYnR5zIyQ237PSRlh3Hm6fjbyVSstwpcxHeWI9D4I/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:59.041+01:00", "Related Answers DO NOT EDIT": "What would happen if there was alignment failure with a superintelligence?", "Tags": "", "Doc Last Edited": "2023-02-22T22:55:11.088+01:00", "Status": "In progress", "Edit Answer": "What is alignment failure?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7767", "Source Link": "", "aisafety.info Link": "What is alignment failure?", "Source": "Wiki", "All Phrasings": "What is alignment failure?\n", "Initial Order": "", "Related IDs": "85E3", "Rich Text DO NOT EDIT": "Alignment failure, at its core, is any time an AI's output deviates from what we intended. We have already witnessed alignment failure in simple AIs. Mostly, these amount to correlation being equated to causation. A good example was an [AI built by Youtube](https://web.archive.org/web/20210807193707/https://www.theverge.com/tldr/2019/8/20/20825858/youtube-bans-fighting-robot-videos-animal-cruelty-roughly-10-years-too-soon-ai-google) to recognize animals being forced to fight for sport. The videos given to the AI were always set in some kind of arena, so the AI drew the simplest conclusion and matched videos where there were similar arenas\u2014such as with robot combat tournaments.\n\nAnother set of problems are those stemming from the AI fulfilling the literal specifications without achieving the actual intended goals. Deep Mind [has a good article](https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity) describing various such situations, e.g. the animation below is from a game where AI was given a reward for hitting the green blocks on the race track, so it went round in circles doing that, rather than actually winning the race.\n\n![](https://lh3.googleusercontent.com/WpDhdRd7fXQ6_08zA7yFVA8ZUIt9McHLv7CqasSvVm2CSJCfQnf6GT08pbhAURv8KAayNZ0tRk1d3kbup5yN-6x6CSvESQ6lGK7wzh6CU6weS331UFxEiqd0M4iBGaLdN4eqAh0GLkzTtch9-nRbQI1bigzB1HC3)\n\nThese examples may seem trivial, but they illustrate the general idea.\n\nObvious examples of misalignment will be caught during training and not be deployed. More sinister problems might be caused by AIs with authority that are subtly misaligned, or in such a way that doesn\u2019t become apparent until it\u2019s deployed. An example are [police systems](https://daily.jstor.org/what-happens-when-police-use-ai-to-predict-and-prevent-crime/), which often have inherent biases stemming from their training data.\n\nCheck [this video](https://www.youtube.com/watch?v=zkbPdEHEyEI) for more.\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "Alignment failure, at its core, is any time an AI's output deviates from what we intended. We have already witnessed alignment failure in simple AIs. Mostly, these amount to correlation being equated to causation. A good example was an [AI built by Youtube](https://web.archive.org/web/20210807193707/https://www.theverge.com/tldr/2019/8/20/20825858/youtube-bans-fighting-robot-videos-animal-cruelty-roughly-10-years-too-soon-ai-google) to recognize animals being forced to fight for sport. The videos given to the AI were always set in some kind of arena, so the AI drew the simplest conclusion and matched videos where there were similar arenas\u2014such as with robot combat tournaments.\n\nAnother set of problems are those stemming from the AI fulfilling the literal specifications without achieving the actual intended goals. Deep Mind [has a good article](https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity) describing various such situations, e.g. the animation below is from a game where AI was given a reward for hitting the green blocks on the race track, so it went round in circles doing that, rather than actually winning the race.\n\n![](https://lh3.googleusercontent.com/WpDhdRd7fXQ6_08zA7yFVA8ZUIt9McHLv7CqasSvVm2CSJCfQnf6GT08pbhAURv8KAayNZ0tRk1d3kbup5yN-6x6CSvESQ6lGK7wzh6CU6weS331UFxEiqd0M4iBGaLdN4eqAh0GLkzTtch9-nRbQI1bigzB1HC3)\n\nThese examples may seem trivial, but they illustrate the general idea.\n\nObvious examples of misalignment will be caught during training and not be deployed. More sinister problems might be caused by AIs with authority that are subtly misaligned, or in such a way that doesn\u2019t become apparent until it\u2019s deployed. An example are [police systems](https://daily.jstor.org/what-happens-when-police-use-ai-to-predict-and-prevent-crime/), which often have inherent biases stemming from their training data.\n\nCheck [this video](https://www.youtube.com/watch?v=zkbPdEHEyEI) for more.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7767", "Related Answers": "What would happen if there was alignment failure with a superintelligence?", "Doc Last Ingested": "2023-03-14T23:35:48.419+01:00", "Request Count": "", "Number of suggestions on answer doc": 109, "Total character count of suggestions on answer doc": 9239, "Helpful": ""}}, {"id": "i-7932f89eb329c8fe30163cc1608ae0fd55c6cce58d790c5801d70d55c21182e2", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7932f89eb329c8fe30163cc1608ae0fd55c6cce58d790c5801d70d55c21182e2", "name": "What does a typical work day in the life of an AI safety researcher look like?", "index": 375, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:46:15.608Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7932f89eb329c8fe30163cc1608ae0fd55c6cce58d790c5801d70d55c21182e2", "values": {"File": "What does a typical work day in the life of an AI safety researcher look like?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What does a typical work day in the life of an AI safety researcher look like?", "Link": "https://docs.google.com/document/d/1O9vH4NSqpJ7hs6Tr88Y3XP7rhqUxBuWs0qiKzDvHjpA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:55.599+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Contributing", "Doc Last Edited": "2023-02-22T22:55:12.157+01:00", "Status": "Not started", "Edit Answer": "What does a typical work day in the life of an AI safety researcher look like?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7634", "Source Link": "", "aisafety.info Link": "What does a typical work day in the life of an AI safety researcher look like?", "Source": "Wiki", "All Phrasings": "What does a typical work day in the life of an AI safety researcher look like?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7634", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:37:05.364+01:00", "Request Count": "", "Number of suggestions on answer doc": 68, "Total character count of suggestions on answer doc": 7813, "Helpful": ""}}, {"id": "i-762365f84d1489988a88c25c201b83fe28435ead71ea2aed569ea0d602f8325a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-762365f84d1489988a88c25c201b83fe28435ead71ea2aed569ea0d602f8325a", "name": "What does Ought aim to do?", "index": 376, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:22.134Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-762365f84d1489988a88c25c201b83fe28435ead71ea2aed569ea0d602f8325a", "values": {"File": "What does Ought aim to do?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What does Ought aim to do?", "Link": "https://docs.google.com/document/d/1EdYXrhsunwUk-PxTlE0P2uZmoPsOb6SsFSUYpg4tDWc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:51.668+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:54.536+01:00", "Status": "Live on site", "Edit Answer": "What does Ought aim to do?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8374", "Source Link": "", "aisafety.info Link": "What does Ought aim to do?", "Source": "Wiki", "All Phrasings": "What does Ought aim to do?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[Ought](https://ought.org/) aims to automate and scale open-ended reasoning through [Elicit](https://ought.org/elicit), an AI research assistant. Ought focuses on advancing [process-based systems](https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes) rather than outcome-based ones, which they believe to be both beneficial for improving reasoning in the short term and alignment in the long term. [Here](https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes) they argue that in the long run improving reasoning and alignment converge.\n\nSo Ought\u2019s impact on AI alignment has 2 components: (a) improved reasoning of AI governance & alignment researchers, [particularly on long-horizon tasks](https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Differential_capabilities__Supervising_process_helps_with_long_horizon_tasks) and (b) [pushing supervision of process rather than outcomes](https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Alignment__Supervising_process_is_safety_by_construction), which reduces the optimization pressure on imperfect proxy objectives leading to \u201csafety by construction\u201d. Ought argues that the [race between process and outcome-based systems](https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Two_attractors__The_race_between_process__and_outcome_based_systems) is particularly important because both states may be an attractor.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "[Ought](https://ought.org/) aims to automate and scale open-ended reasoning through [Elicit](https://ought.org/elicit), an AI research assistant. Ought focuses on advancing [process-based systems](https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes) rather than outcome-based ones, which they believe to be both beneficial for improving reasoning in the short term and alignment in the long term. [Here](https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes) they argue that in the long run improving reasoning and alignment converge.\n\nSo Ought\u2019s impact on AI alignment has 2 components: (a) improved reasoning of AI governance & alignment researchers, [particularly on long-horizon tasks](https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Differential_capabilities__Supervising_process_helps_with_long_horizon_tasks) and (b) [pushing supervision of process rather than outcomes](https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Alignment__Supervising_process_is_safety_by_construction), which reduces the optimization pressure on imperfect proxy objectives leading to \u201csafety by construction\u201d. Ought argues that the [race between process and outcome-based systems](https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Two_attractors__The_race_between_process__and_outcome_based_systems) is particularly important because both states may be an attractor.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8374", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:35:53.328+01:00", "Request Count": "", "Number of suggestions on answer doc": 109, "Total character count of suggestions on answer doc": 9239, "Helpful": ""}}, {"id": "i-6f68d78dadd8c1c05136dd7f63b3ecc0f3d8c0f303be33ed73c70625dc6dd5b4", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-6f68d78dadd8c1c05136dd7f63b3ecc0f3d8c0f303be33ed73c70625dc6dd5b4", "name": "What does MIRI think about technical alignment?", "index": 377, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:46:27.567Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-6f68d78dadd8c1c05136dd7f63b3ecc0f3d8c0f303be33ed73c70625dc6dd5b4", "values": {"File": "What does MIRI think about technical alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What does MIRI think about technical alignment?", "Link": "https://docs.google.com/document/d/1vzsq3kInAgbOrj5fpcz65je74ZnmbGS5_tAqOVCAZaU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:48.191+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:55.337+01:00", "Status": "Live on site", "Edit Answer": "What does MIRI think about technical alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8357", "Source Link": "", "aisafety.info Link": "What does MIRI think about technical alignment?", "Source": "Wiki", "All Phrasings": "What does MIRI think about technical alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "MIRI thinks technical alignment is really hard, and that we are very far from a solution. However, they think that policy solutions have even less hope. Generally, I think of their approach as supporting a bunch of independent researchers following their own directions, hoping that one of them will find some promise. They mostly buy into the [security mindset](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/): we need to know exactly (probably [mathematically formally)](https://www.lesswrong.com/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem) what we are doing, or the massive optimization pressure will default in ruin.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "MIRI thinks technical alignment is really hard, and that we are very far from a solution. However, they think that policy solutions have even less hope. Generally, I think of their approach as supporting a bunch of independent researchers following their own directions, hoping that one of them will find some promise. They mostly buy into the [security mindset](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/): we need to know exactly (probably [mathematically formally)](https://www.lesswrong.com/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem) what we are doing, or the massive optimization pressure will default in ruin.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8357", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:37:07.319+01:00", "Request Count": "", "Number of suggestions on answer doc": 68, "Total character count of suggestions on answer doc": 7813, "Helpful": ""}}, {"id": "i-f1617b2b087e66513ece20fbff7b9c88f36efe641a335f04ded9e9a8fb704d88", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-f1617b2b087e66513ece20fbff7b9c88f36efe641a335f04ded9e9a8fb704d88", "name": "What does Evan Hubinger think of Deception + Inner Alignment?", "index": 378, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:27.930Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-f1617b2b087e66513ece20fbff7b9c88f36efe641a335f04ded9e9a8fb704d88", "values": {"File": "What does Evan Hubinger think of Deception + Inner Alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What does Evan Hubinger think of Deception + Inner Alignment?", "Link": "https://docs.google.com/document/d/107vGrPoSIATELYriGEgGFQ-RXfHbD2Y62UfC9V8h_IY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:44.604+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:56.141+01:00", "Status": "Live on site", "Edit Answer": "What does Evan Hubinger think of Deception + Inner Alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8359", "Source Link": "", "aisafety.info Link": "What does Evan Hubinger think of Deception + Inner Alignment?", "Source": "Wiki", "All Phrasings": "What does Evan Hubinger think of Deception + Inner Alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Read [Evan's research agenda](https://www.lesswrong.com/posts/GeabLEXYP7oBMivmF/acceptability-verification-a-research-agenda) for more information.\n\nIt seems likely that [deceptive agents are the default](https://www.google.com/url?q=https://docs.google.com/presentation/d/1IzmmUSvhjeGhc_nc8Wd7-hB9_rSeES8JvEvKzQ8uHBI/edit?usp%3Ddrivesdk&sa=D&source=editors&ust=1661633213188468&usg=AOvVaw1-ALhgrpPnw_4Y0uRozVl_), so a key problem in alignment is to figure out how we can avoid deceptive alignment at every point in the training process. This seems to rely on being able to consistently exert optimization pressure against deception, which probably necessitates interpretability tools.\n\nHis plan to do this right now is acceptability verification: have some predicate that precludes deception, and then check your model for this predicate at every point in training.\n\nOne idea for this predicate is making sure that the agent is [myopic](https://www.lesswrong.com/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia), meaning that the AI only cares about the current timestep, so there is no incentive to deceive, because the benefits of deception happen only in the future. This is operationalized as \u201creturn the action that your model of [HCH](https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch) would return, if it received your inputs.\u201d\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Read [Evan's research agenda](https://www.lesswrong.com/posts/GeabLEXYP7oBMivmF/acceptability-verification-a-research-agenda) for more information.\n\nIt seems likely that [deceptive agents are the default](https://www.google.com/url?q=https://docs.google.com/presentation/d/1IzmmUSvhjeGhc_nc8Wd7-hB9_rSeES8JvEvKzQ8uHBI/edit?usp%3Ddrivesdk&sa=D&source=editors&ust=1661633213188468&usg=AOvVaw1-ALhgrpPnw_4Y0uRozVl_), so a key problem in alignment is to figure out how we can avoid deceptive alignment at every point in the training process. This seems to rely on being able to consistently exert optimization pressure against deception, which probably necessitates interpretability tools.\n\nHis plan to do this right now is acceptability verification: have some predicate that precludes deception, and then check your model for this predicate at every point in training.\n\nOne idea for this predicate is making sure that the agent is [myopic](https://www.lesswrong.com/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia), meaning that the AI only cares about the current timestep, so there is no incentive to deceive, because the benefits of deception happen only in the future. This is operationalized as \u201creturn the action that your model of [HCH](https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch) would return, if it received your inputs.\u201d\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8359", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:35:58.175+01:00", "Request Count": "", "Number of suggestions on answer doc": 109, "Total character count of suggestions on answer doc": 9239, "Helpful": ""}}, {"id": "i-5e22870f79d61e9c3ba39e89bb3dce4cb3399e973064646d0b74331c96bea924", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5e22870f79d61e9c3ba39e89bb3dce4cb3399e973064646d0b74331c96bea924", "name": "What does Elon Musk think about AI safety?", "index": 379, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:30.281Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5e22870f79d61e9c3ba39e89bb3dce4cb3399e973064646d0b74331c96bea924", "values": {"File": "What does Elon Musk think about AI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What does Elon Musk think about AI safety?", "Link": "https://docs.google.com/document/d/1788Zvy8LJeQWh69c1FgD_34zulqktLbVTVTbxIjQRS0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:40.863+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:04:57.029+01:00", "Status": "Live on site", "Edit Answer": "What does Elon Musk think about AI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7600", "Source Link": "", "aisafety.info Link": "What does Elon Musk think about AI safety?", "Source": "Wiki", "All Phrasings": "What does Elon Musk think about AI safety?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Elon Musk has expressed his concerns about AI safety many times and founded OpenAI in an attempt to make safe AI more widely distributed (as opposed to allowing a [singleton](https://www.nickbostrom.com/fut/singleton.html), which he fears would be misused or dangerously unaligned). In a [YouTube video](https://www.youtube.com/watch?v=smK9dgdTl40) from November 2019 Musk stated that there's a lack of investment in AI safety and that there should be a government agency to reduce risk to the public from AI.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Elon Musk has expressed his concerns about AI safety many times and founded OpenAI in an attempt to make safe AI more widely distributed (as opposed to allowing a [singleton](https://www.nickbostrom.com/fut/singleton.html), which he fears would be misused or dangerously unaligned). In a [YouTube video](https://www.youtube.com/watch?v=smK9dgdTl40) from November 2019 Musk stated that there's a lack of investment in AI safety and that there should be a government agency to reduce risk to the public from AI.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7600", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:01.084+01:00", "Request Count": "", "Number of suggestions on answer doc": 110, "Total character count of suggestions on answer doc": 9793, "Helpful": ""}}, {"id": "i-0235ad4602ae4bccbc7be659664ec27833bf1ccf3cd631f2bf3243e331108a4b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0235ad4602ae4bccbc7be659664ec27833bf1ccf3cd631f2bf3243e331108a4b", "name": "What could a superintelligent AI do, and what would be physically impossible even for it?", "index": 380, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:32.457Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0235ad4602ae4bccbc7be659664ec27833bf1ccf3cd631f2bf3243e331108a4b", "values": {"File": "What could a superintelligent AI do, and what would be physically impossible even for it?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What could a superintelligent AI do, and what would be physically impossible even for it?", "Link": "https://docs.google.com/document/d/1Tef7N1hSi_Ao8K1tug9H4GYiBg-1KABabOSM_2KJ0t8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:37.021+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Physics", "Doc Last Edited": "2023-02-22T22:55:13.159+01:00", "Status": "In progress", "Edit Answer": "What could a superintelligent AI do, and what would be physically impossible even for it?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7629", "Source Link": "", "aisafety.info Link": "What could a superintelligent AI do, and what would be physically impossible even for it?", "Source": "Wiki", "All Phrasings": "What could a superintelligent AI do, and what would be physically impossible even for it?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "It's very hard to know the answer to this in advance because a superintelligent AI is capable of doing anything that it understands how to do. That sounds obvious, but it has a hidden nuance there - it knows how to do things that we don't know how to do, and so we cannot predict in advance what it will do because it will do some things that we didn't even know were possible to do. Almost by definition, that makes it impossible to reliably predict.\n\nWe can say that it will not be able to do things which are actually not possible to do, and it seems likely that certain rules are always followed (energy must be conserved, global entropy must always increase etc.) but the problem is that we have been wrong about the laws of physics before (see Newton's laws vs relativity and quantum mechanics) and so we're only *mostly* certain that these things are true. If there's any discrepancy between how we think physics works and how physics actually works (and most likely, such a discrepancy does exist somewhere), we can expect a superintelligent AI to be able to exploit that discrepancy in order to serve its goals in lots of weird ways we couldn't possibly have predicted.\n\nWe may be able to say with 99% certainty that a superintelligent AI will not be able to violate conservation of energy, but we're not 100% sure, and we can't be 100% sure of anything here.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "It's very hard to know the answer to this in advance because a superintelligent AI is capable of doing anything that it understands how to do. That sounds obvious, but it has a hidden nuance there - it knows how to do things that we don't know how to do, and so we cannot predict in advance what it will do because it will do some things that we didn't even know were possible to do. Almost by definition, that makes it impossible to reliably predict.\n\nWe can say that it will not be able to do things which are actually not possible to do, and it seems likely that certain rules are always followed (energy must be conserved, global entropy must always increase etc.) but the problem is that we have been wrong about the laws of physics before (see Newton's laws vs relativity and quantum mechanics) and so we're only *mostly* certain that these things are true. If there's any discrepancy between how we think physics works and how physics actually works (and most likely, such a discrepancy does exist somewhere), we can expect a superintelligent AI to be able to exploit that discrepancy in order to serve its goals in lots of weird ways we couldn't possibly have predicted.\n\nWe may be able to say with 99% certainty that a superintelligent AI will not be able to violate conservation of energy, but we're not 100% sure, and we can't be 100% sure of anything here.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7629", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:02.923+01:00", "Request Count": "", "Number of suggestions on answer doc": 110, "Total character count of suggestions on answer doc": 9793, "Helpful": ""}}, {"id": "i-2d621687706e48263695cfa331221c90447c3b7daf53bd6a3823be7ded7eae73", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2d621687706e48263695cfa331221c90447c3b7daf53bd6a3823be7ded7eae73", "name": "What convinced people working on AI alignment that it was worth spending their time on this cause?", "index": 381, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:43:36.759Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2d621687706e48263695cfa331221c90447c3b7daf53bd6a3823be7ded7eae73", "values": {"File": "What convinced people working on AI alignment that it was worth spending their time on this cause?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What convinced people working on AI alignment that it was worth spending their time on this cause?", "Link": "https://docs.google.com/document/d/1-NRH8bftLk8-riNoXGwKip54PG_jfWd9rK9_SMw4fqk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:33.286+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Outreach", "Doc Last Edited": "2023-02-22T22:55:14.261+01:00", "Status": "Not started", "Edit Answer": "What convinced people working on AI alignment that it was worth spending their time on this cause?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6570", "Source Link": "", "aisafety.info Link": "What convinced people working on AI alignment that it was worth spending their time on this cause?", "Source": "Wiki", "All Phrasings": "What convinced people working on AI alignment that it was worth spending their time on this cause?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- [The sequences](https://www.readthesequences.com/)\n\n- The speed of AI capabilities advances\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "- [The sequences](https://www.readthesequences.com/)\n\n- The speed of AI capabilities advances\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Arthur", "External Source": "", "Last Asked On Discord": "", "UI ID": "6570", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:36:05.300+01:00", "Request Count": "", "Number of suggestions on answer doc": 110, "Total character count of suggestions on answer doc": 9793, "Helpful": ""}}, {"id": "i-4dc34079a25ec588f6ed4269f6c46f12d683a73dd8edfa32921703860f1b563f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4dc34079a25ec588f6ed4269f6c46f12d683a73dd8edfa32921703860f1b563f", "name": "What can we expect the motivations of a superintelligent machine to be?", "index": 382, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:50:06.959Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4dc34079a25ec588f6ed4269f6c46f12d683a73dd8edfa32921703860f1b563f", "values": {"File": "What can we expect the motivations of a superintelligent machine to be?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What can we expect the motivations of a superintelligent machine to be?", "Link": "https://docs.google.com/document/d/1jRve5-d_v6OMOqSzjgGQplHPSNYeaUVkvnjlAE3YXbc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:28.938+01:00", "Related Answers DO NOT EDIT": "Will AGI be aligned by default?,What is \"whole brain emulation\"?", "Tags": "Superintelligence,Goals", "Doc Last Edited": "2023-03-07T20:25:32.231+01:00", "Status": "In progress", "Edit Answer": "What can we expect the motivations of a superintelligent machine to be?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6920", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "What can we expect the motivations of a superintelligent machine to be?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "What can we expect the motivations of a superintelligent machine to be?\n", "Initial Order": "", "Related IDs": "8AV4,6350", "Rich Text DO NOT EDIT": "Except in the case of Whole Brain Emulation, there is no reason to expect a superintelligent machine to have motivations anything like those of humans. Human minds represent a tiny dot in the vast space of all possible mind designs, and very different kinds of minds are unlikely to share tocomplex motivationsunique to humans and other mammals.\n\nWhatever its goals,a superintelligence would tend to commandeer resources that can help it achieve its goals,including the energy and elements on which human life depends.It would not stop because of a concern for humans or other intelligences that is \u2018built in\u2019 to all possible mind designs.Rather, it would pursue its particular goaland give no thoughtto concerns that seem \u2018natural\u2019 to that particular species of primate called homo sapiens.\n\nThere are, however, some basic instrumental motivations we can expect superintelligent machines to display, because they are useful for achieving its goals, no matter what its goals are. For example, an AI will \u2018want\u2019 to self-improve, to be optimally rational, to retain its original goals, to acquire resources, and to protect itself \u2014 because all these things help it achieve the goals with which it was originally programmed.\n\nSee also:\n\n- Omohundro, [The Basic AI Drives](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)\n\n- Shulman, [Basic AI Drives and Catastrophic Risks](https://intelligence.org/files/BasicAIDrives.pdf)\n\n- \n\n[^kix.hnv8a5xemi4r]:", "Tag Count": 2, "Related Answer Count": 2, "Rich Text": "Except in the case of Whole Brain Emulation, there is no reason to expect a superintelligent machine to have motivations anything like those of humans. Human minds represent a tiny dot in the vast space of all possible mind designs, and very different kinds of minds are unlikely to share tocomplex motivationsunique to humans and other mammals.\n\nWhatever its goals,a superintelligence would tend to commandeer resources that can help it achieve its goals,including the energy and elements on which human life depends.It would not stop because of a concern for humans or other intelligences that is \u2018built in\u2019 to all possible mind designs.Rather, it would pursue its particular goaland give no thoughtto concerns that seem \u2018natural\u2019 to that particular species of primate called homo sapiens.\n\nThere are, however, some basic instrumental motivations we can expect superintelligent machines to display, because they are useful for achieving its goals, no matter what its goals are. For example, an AI will \u2018want\u2019 to self-improve, to be optimally rational, to retain its original goals, to acquire resources, and to protect itself \u2014 because all these things help it achieve the goals with which it was originally programmed.\n\nSee also:\n\n- Omohundro, [The Basic AI Drives](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)\n\n- Shulman, [Basic AI Drives and Catastrophic Risks](https://intelligence.org/files/BasicAIDrives.pdf)\n\n- \n\n[^kix.hnv8a5xemi4r]:", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "", "UI ID": "6920", "Related Answers": "Will AGI be aligned by default?,What is \"whole brain emulation\"?", "Doc Last Ingested": "2023-03-14T23:38:51.486+01:00", "Request Count": "", "Number of suggestions on answer doc": 32, "Total character count of suggestions on answer doc": 1697, "Helpful": ""}}, {"id": "i-8fd7aad67aa5d87a7ae5c7737dfc1d887e484e3e370a8f42eaafa658e2caae7a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8fd7aad67aa5d87a7ae5c7737dfc1d887e484e3e370a8f42eaafa658e2caae7a", "name": "What can I do to contribute to AI safety?", "index": 383, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:50:10.036Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8fd7aad67aa5d87a7ae5c7737dfc1d887e484e3e370a8f42eaafa658e2caae7a", "values": {"File": "What can I do to contribute to AI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What can I do to contribute to AI safety?", "Link": "https://docs.google.com/document/d/17CfUruUCFh6CMEvw37kATLVBPb5-MVIXohzmGcVAra0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:25.824+01:00", "Related Answers DO NOT EDIT": "How can I contribute to Stampy's AI Safety Info?", "Tags": "Contributing", "Doc Last Edited": "2023-02-22T23:04:58.524+01:00", "Status": "Live on site", "Edit Answer": "What can I do to contribute to AI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5947", "Source Link": "", "aisafety.info Link": "What can I do to contribute to AI safety?", "Source": "Wiki", "All Phrasings": "What can I do to contribute to AI safety?\n", "Initial Order": "", "Related IDs": "6441", "Rich Text DO NOT EDIT": "It\u2019s pretty dependent on what skills you have and what resources you have access to. The largest option is to pursue a [career in AI Safety research](https://80000hours.org/career-reviews/artificial-intelligence-risk-research/). Another large option is to pursue a career in [AI policy](https://80000hours.org/articles/ai-policy-guide/), which you might think is even more important than doing technical research.\n\nSmaller options include donating money to relevant organizations, talking about AI Safety as a plausible career path to other people or considering the problem in your spare time.\n\nIt\u2019s possible that your particular set of skills/resources are not suited to this problem. Unluckily, there are [many more problems](https://concepts.effectivealtruism.org/concepts/existential-risks/) that are of similar levels of importance.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "It\u2019s pretty dependent on what skills you have and what resources you have access to. The largest option is to pursue a [career in AI Safety research](https://80000hours.org/career-reviews/artificial-intelligence-risk-research/). Another large option is to pursue a career in [AI policy](https://80000hours.org/articles/ai-policy-guide/), which you might think is even more important than doing technical research.\n\nSmaller options include donating money to relevant organizations, talking about AI Safety as a plausible career path to other people or considering the problem in your spare time.\n\nIt\u2019s possible that your particular set of skills/resources are not suited to this problem. Unluckily, there are [many more problems](https://concepts.effectivealtruism.org/concepts/existential-risks/) that are of similar levels of importance.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Jack Harley", "External Source": "", "Last Asked On Discord": "", "UI ID": "5947", "Related Answers": "How can I contribute to Stampy's AI Safety Info?", "Doc Last Ingested": "2023-03-14T23:38:53.252+01:00", "Request Count": "", "Number of suggestions on answer doc": 32, "Total character count of suggestions on answer doc": 1697, "Helpful": ""}}, {"id": "i-5008a7105b624ba78c5eaaaf7f2451b82369cb631dd30b7110adbd11ac51be28", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5008a7105b624ba78c5eaaaf7f2451b82369cb631dd30b7110adbd11ac51be28", "name": "What beneficial things would an aligned superintelligence be able to do?", "index": 384, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:50:13.043Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5008a7105b624ba78c5eaaaf7f2451b82369cb631dd30b7110adbd11ac51be28", "values": {"File": "What beneficial things would an aligned superintelligence be able to do?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What beneficial things would an aligned superintelligence be able to do?", "Link": "https://docs.google.com/document/d/132Tb4HANSgxhyIOXNgfCdt8m-lX9qBS_6zDkPcvm6JQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:21.971+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Eutopia", "Doc Last Edited": "2023-02-22T22:55:16.288+01:00", "Status": "Not started", "Edit Answer": "What beneficial things would an aligned superintelligence be able to do?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7785", "Source Link": "", "aisafety.info Link": "What beneficial things would an aligned superintelligence be able to do?", "Source": "Wiki", "All Phrasings": "What beneficial things would an aligned superintelligence be able to do?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n- \n\n- \n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n- \n\n- \n\n", "Stamp Count": 2, "Multi Answer": "", "Stamped By": "Aprillion\ntayler6000", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7785", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:38:54.761+01:00", "Request Count": "", "Number of suggestions on answer doc": 33, "Total character count of suggestions on answer doc": 2208, "Helpful": ""}}, {"id": "i-4d820dd337bc25a858f08d8f32c333a985e395ca236169a84f03a4a2e39a082d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4d820dd337bc25a858f08d8f32c333a985e395ca236169a84f03a4a2e39a082d", "name": "What assets need to be protected by/from the AI? Are \"human values\" sufficient for it?", "index": 385, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:50:16.030Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4d820dd337bc25a858f08d8f32c333a985e395ca236169a84f03a4a2e39a082d", "values": {"File": "What assets need to be protected by/from the AI? Are \"human values\" sufficient for it?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What assets need to be protected by/from the AI? Are \"human values\" sufficient for it?", "Link": "https://docs.google.com/document/d/1k3IMAmLAnNwyVWWV4-YsathOqW4A2VC5hAqRL1l3aVs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:17.892+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Human Values,Existential Risk,Values", "Doc Last Edited": "2023-02-22T22:55:17.276+01:00", "Status": "Not started", "Edit Answer": "What assets need to be protected by/from the AI? Are \"human values\" sufficient for it?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7801", "Source Link": "", "aisafety.info Link": "What assets need to be protected by/from the AI? Are \"human values\" sufficient for it?", "Source": "Wiki", "All Phrasings": "What assets need to be protected by/from the AI? Are \"human values\" sufficient for it?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 3, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Shikah", "External Source": "", "Last Asked On Discord": "", "UI ID": "7801", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:38:57.070+01:00", "Request Count": "", "Number of suggestions on answer doc": 33, "Total character count of suggestions on answer doc": 2208, "Helpful": ""}}, {"id": "i-ee5110a97781c883d0ed48263bb04ac5246e121a46bd59324b552f9e1ae98be3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ee5110a97781c883d0ed48263bb04ac5246e121a46bd59324b552f9e1ae98be3", "name": "What are the style guidelines for writing for Stampy?", "index": 386, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:50:19.138Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ee5110a97781c883d0ed48263bb04ac5246e121a46bd59324b552f9e1ae98be3", "values": {"File": "What are the style guidelines for writing for Stampy?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the style guidelines for writing for Stampy?", "Link": "https://docs.google.com/document/d/1sZAcfa3oQMG1hs-maEnlrwrYkg5wkdUTMgfMe54h_Uo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:14.231+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stampy", "Doc Last Edited": "2023-02-23T14:27:13.411+01:00", "Status": "Live on site", "Edit Answer": "What are the style guidelines for writing for Stampy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7339", "Source Link": "", "aisafety.info Link": "What are the style guidelines for writing for Stampy?", "Source": "Wiki", "All Phrasings": "What are the style guidelines for writing for Stampy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Linking to external sites is strongly encouraged, one of the most valuable things Stampy can do is help people find other parts of the alignment information ecosystem.\n\nAvoid using acronyms not on the following list: AI.\n\nExceptions are possible if you define the acronym earlier in the answer (e.g. after saying \u201cInverse Reinforcement Learning (IRL)\u201d it\u2019s acceptable to use IRL) or if you link to the website of an organization which is generally abbreviated when mentioning it for the first time (e.g. \u201c[MIRI](https://intelligence.org/)\u2019s research agenda is..\u201d).\n\nWhen selecting related questions, there shouldn't be more than four unless there's a really good reason for that (some questions are asking for it, like the \"Why can't we just...\" question). It's also recommended to include at least one more \"enticing\" question to draw users in (relating to the more sensational, sci-fi, philosophical/ethical side of things) alongside more bland/neutral questions. If there is a related question that you think should be there, and it hasn\u2019t been asked yet, please[add the question](https://coda.io/form/Add-a-question-to-AI-Safety-Info_dGDInYNFa3f).\n\nConsider enclosing newly introduced terms, likely to be unfamiliar to many readers, in speech marks. If unsure, Google the term (in speech marks!) and see if it shows up anywhere other than LessWrong, the Alignment Forum, etc. Be judicious, as it's easy to use too many, but used carefully they can psychologically cushion newbies from a lot of unfamiliar terminology - in this context they're saying something like \"we get that we're hitting you with a lot of new vocab, and you might not know what this term means yet\".\n\nSome resources which might help with writing style include [Scott Alexander\u2019s Nonfiction Writing Advice](http://dvice/) (especially the part about microhumor), [Nobody Wants to Read Your Sh*t: Why That Is And What You Can Do About It](https://www.amazon.com/dp/B01GZ1TJBI?ref_=cm_sw_r_cp_ud_dp_2ME6GN0ZBPBTTXFD9VPY), [Wired for Story: The Writer's Guide to Using Brain Science to Hook Readers from the Very First Sentence](https://www.amazon.com/dp/B005X0JTGI?ref_=cm_sw_r_cp_ud_dp_57DBKHAAV9VXDW2404RG) and [The Elements of Eloquence: Secrets of the Perfect Turn of Phrase](https://www.amazon.com/dp/B00INIXG4I?ref_=cm_sw_r_cp_ud_dp_71FB1BJBENX4GVBGBPW2).\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Linking to external sites is strongly encouraged, one of the most valuable things Stampy can do is help people find other parts of the alignment information ecosystem.\n\nAvoid using acronyms not on the following list: AI.\n\nExceptions are possible if you define the acronym earlier in the answer (e.g. after saying \u201cInverse Reinforcement Learning (IRL)\u201d it\u2019s acceptable to use IRL) or if you link to the website of an organization which is generally abbreviated when mentioning it for the first time (e.g. \u201c[MIRI](https://intelligence.org/)\u2019s research agenda is..\u201d).\n\nWhen selecting related questions, there shouldn't be more than four unless there's a really good reason for that (some questions are asking for it, like the \"Why can't we just...\" question). It's also recommended to include at least one more \"enticing\" question to draw users in (relating to the more sensational, sci-fi, philosophical/ethical side of things) alongside more bland/neutral questions. If there is a related question that you think should be there, and it hasn\u2019t been asked yet, please[add the question](https://coda.io/form/Add-a-question-to-AI-Safety-Info_dGDInYNFa3f).\n\nConsider enclosing newly introduced terms, likely to be unfamiliar to many readers, in speech marks. If unsure, Google the term (in speech marks!) and see if it shows up anywhere other than LessWrong, the Alignment Forum, etc. Be judicious, as it's easy to use too many, but used carefully they can psychologically cushion newbies from a lot of unfamiliar terminology - in this context they're saying something like \"we get that we're hitting you with a lot of new vocab, and you might not know what this term means yet\".\n\nSome resources which might help with writing style include [Scott Alexander\u2019s Nonfiction Writing Advice](http://dvice/) (especially the part about microhumor), [Nobody Wants to Read Your Sh*t: Why That Is And What You Can Do About It](https://www.amazon.com/dp/B01GZ1TJBI?ref_=cm_sw_r_cp_ud_dp_2ME6GN0ZBPBTTXFD9VPY), [Wired for Story: The Writer's Guide to Using Brain Science to Hook Readers from the Very First Sentence](https://www.amazon.com/dp/B005X0JTGI?ref_=cm_sw_r_cp_ud_dp_57DBKHAAV9VXDW2404RG) and [The Elements of Eloquence: Secrets of the Perfect Turn of Phrase](https://www.amazon.com/dp/B00INIXG4I?ref_=cm_sw_r_cp_ud_dp_71FB1BJBENX4GVBGBPW2).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7339", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:38:59.249+01:00", "Request Count": "", "Number of suggestions on answer doc": 36, "Total character count of suggestions on answer doc": 3254, "Helpful": ""}}, {"id": "i-ed0dc380906464b24295e57bd231bd8f20b76b1947ee8a4d4c2ccc95ea5dc965", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ed0dc380906464b24295e57bd231bd8f20b76b1947ee8a4d4c2ccc95ea5dc965", "name": "What are the potential benefits of AI as it grows increasingly sophisticated?", "index": 387, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:50:22.866Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ed0dc380906464b24295e57bd231bd8f20b76b1947ee8a4d4c2ccc95ea5dc965", "values": {"File": "What are the potential benefits of AI as it grows increasingly sophisticated?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the potential benefits of AI as it grows increasingly sophisticated?", "Link": "https://docs.google.com/document/d/1qnasqX33RKFh0qGnOCEQ_pkLGoyDTMALVTiIBQHSUAc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:10.240+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Benefits", "Doc Last Edited": "2023-02-22T23:05:00.764+01:00", "Status": "Live on site", "Edit Answer": "What are the potential benefits of AI as it grows increasingly sophisticated?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6182", "Source Link": "https://futureoflife.org/ai-faqs/", "aisafety.info Link": "What are the potential benefits of AI as it grows increasingly sophisticated?", "Source": "FLI's FAQ", "All Phrasings": "What are the potential benefits of AI as it grows increasingly sophisticated?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "It\u2019s difficult to tell at this stage, but AI will enable many developments that could be terrifically beneficial if managed with enough foresight and care. For example, menial tasks could be automated, which could give rise to a society of abundance, leisure, and flourishing, free of poverty and tedium. As another example, AI could also improve our ability to understand and manipulate complex biological systems, unlocking a path to drastically improved longevity and health, and to conquering disease.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "It\u2019s difficult to tell at this stage, but AI will enable many developments that could be terrifically beneficial if managed with enough foresight and care. For example, menial tasks could be automated, which could give rise to a society of abundance, leisure, and flourishing, free of poverty and tedium. As another example, AI could also improve our ability to understand and manipulate complex biological systems, unlocking a path to drastically improved longevity and health, and to conquering disease.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "FLI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6182", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:01.154+01:00", "Request Count": "", "Number of suggestions on answer doc": 36, "Total character count of suggestions on answer doc": 3254, "Helpful": ""}}, {"id": "i-6979fe641aadbf0c31acb76ba4c4bda06a1a473c24804968608a6f9e69751ae0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-6979fe641aadbf0c31acb76ba4c4bda06a1a473c24804968608a6f9e69751ae0", "name": "What are the main sources of AI existential risk?", "index": 388, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:50:26.714Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-6979fe641aadbf0c31acb76ba4c4bda06a1a473c24804968608a6f9e69751ae0", "values": {"File": "What are the main sources of AI existential risk?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the main sources of AI existential risk?", "Link": "https://docs.google.com/document/d/19rtgV5O9Irz-0N5l-3pzdRtHi97GJR0Tb7Z2DYYYPc0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:05.939+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Difficulty of Alignment", "Doc Last Edited": "2023-02-22T23:05:01.543+01:00", "Status": "Live on site", "Edit Answer": "What are the main sources of AI existential risk?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8503", "Source Link": "", "aisafety.info Link": "What are the main sources of AI existential risk?", "Source": "Wiki", "All Phrasings": "What are the main sources of AI existential risk?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A comprehensive list of major contributing factors to AI being a threat to humanity's future is maintained on by Daniel Kokotajlo on the [Alignment Forum](https://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk).\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "A comprehensive list of major contributing factors to AI being a threat to humanity's future is maintained on by Daniel Kokotajlo on the [Alignment Forum](https://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "8503", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:03.311+01:00", "Request Count": "", "Number of suggestions on answer doc": 36, "Total character count of suggestions on answer doc": 3254, "Helpful": ""}}, {"id": "i-127db7fbf77512d18c6755eaa79ffa185bf48e67d5f1363e5d427fe1425a34df", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-127db7fbf77512d18c6755eaa79ffa185bf48e67d5f1363e5d427fe1425a34df", "name": "What are the leading theories in moral philosophy and which of them might be technically the easiest to encode into an AI?", "index": 389, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:50:28.994Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-127db7fbf77512d18c6755eaa79ffa185bf48e67d5f1363e5d427fe1425a34df", "values": {"File": "What are the leading theories in moral philosophy and which of them might be technically the easiest to encode into an AI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the leading theories in moral philosophy and which of them might be technically the easiest to encode into an AI?", "Link": "https://docs.google.com/document/d/1STlVxs2gKbNwEe0h0WO6RLswCjsYpBxPrrg_HC89SIg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:46:01.497+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Ethics", "Doc Last Edited": "2023-02-22T22:55:18.240+01:00", "Status": "In progress", "Edit Answer": "What are the leading theories in moral philosophy and which of them might be technically the easiest to encode into an AI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7616", "Source Link": "", "aisafety.info Link": "What are the leading theories in moral philosophy and which of them might be technically the easiest to encode into an AI?", "Source": "Wiki", "All Phrasings": "What are the leading theories in moral philosophy and which of them might be technically the easiest to encode into an AI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "There are three major approaches to normative ethics (and some approaches to unify two or all of them): Virtue ethics, deontological ethics, and consequentialist ethics.\n\n[Virtue ethicists](https://plato.stanford.edu/entries/ethics-virtue/) believe that at the core, leading an ethical life means cultivating virtues. In other words: What counts is less what one does moment-to-moment, but that one makes an effort to become the kind of person who habitually acts appropriately in all kinds of different situations. A prominent example for virtue ethics is stoicism.\n\n[Deontological ethicists](https://plato.stanford.edu/entries/ethics-deontological/) believe that an ethical life is all about following certain behavioral rules, regardless of the consequences. Prominent examples include the ten commandments in Christianity, Kant's \"categorical imperative\" in philosophy, or Asimov's Three Laws of Robotics in science fiction.\n\n[Consequentialist ethicists](https://plato.stanford.edu/entries/consequentialism/) believe that nor one's character neither the rules one lives by are what makes actions good or bad. Instead, consequentialists believe that only the consequences of an action count, both direct and indirect ones. A prominent example of consequentialist ethics is utilitarianism: The notion that those actions are the most moral that lead to the greatest good for the greatest number of individuals.\n\nThe short answer to the question which one of these might be the easiest to encode into an AI is: \"We don't know.\" However, machine learning agents optimize for consequences, not virtues or hard-coded rules. As all the likely roads towards AGI involve machine learning, consequentialism may be the ethical theory to stick closest to.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "There are three major approaches to normative ethics (and some approaches to unify two or all of them): Virtue ethics, deontological ethics, and consequentialist ethics.\n\n[Virtue ethicists](https://plato.stanford.edu/entries/ethics-virtue/) believe that at the core, leading an ethical life means cultivating virtues. In other words: What counts is less what one does moment-to-moment, but that one makes an effort to become the kind of person who habitually acts appropriately in all kinds of different situations. A prominent example for virtue ethics is stoicism.\n\n[Deontological ethicists](https://plato.stanford.edu/entries/ethics-deontological/) believe that an ethical life is all about following certain behavioral rules, regardless of the consequences. Prominent examples include the ten commandments in Christianity, Kant's \"categorical imperative\" in philosophy, or Asimov's Three Laws of Robotics in science fiction.\n\n[Consequentialist ethicists](https://plato.stanford.edu/entries/consequentialism/) believe that nor one's character neither the rules one lives by are what makes actions good or bad. Instead, consequentialists believe that only the consequences of an action count, both direct and indirect ones. A prominent example of consequentialist ethics is utilitarianism: The notion that those actions are the most moral that lead to the greatest good for the greatest number of individuals.\n\nThe short answer to the question which one of these might be the easiest to encode into an AI is: \"We don't know.\" However, machine learning agents optimize for consequences, not virtues or hard-coded rules. As all the likely roads towards AGI involve machine learning, consequentialism may be the ethical theory to stick closest to.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7616", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:05.067+01:00", "Request Count": "", "Number of suggestions on answer doc": 36, "Total character count of suggestions on answer doc": 3254, "Helpful": ""}}, {"id": "i-3925d2d5827c349cc077fb1500f518663547cac8607b6a66085d18ad307dd380", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3925d2d5827c349cc077fb1500f518663547cac8607b6a66085d18ad307dd380", "name": "What are the ethical challenges related to whole brain emulation?", "index": 390, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-15T07:07:36.439Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3925d2d5827c349cc077fb1500f518663547cac8607b6a66085d18ad307dd380", "values": {"File": "What are the ethical challenges related to whole brain emulation?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the ethical challenges related to whole brain emulation?", "Link": "https://docs.google.com/document/d/1UMSKzw8gvACr-gEOUwk509BNNaLwCnBSiJr9ofjfBY8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:45:58.015+01:00", "Related Answers DO NOT EDIT": "What safety problems are associated with whole brain emulation?", "Tags": "Whole Brain Emulation,Ethics", "Doc Last Edited": "2023-03-15T05:03:03.030+01:00", "Status": "Live on site", "Edit Answer": "What are the ethical challenges related to whole brain emulation?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7820", "Source Link": "", "aisafety.info Link": "What are the ethical challenges related to whole brain emulation?", "Source": "Wiki", "All Phrasings": "What are the ethical challenges related to whole brain emulation?\n", "Initial Order": "", "Related IDs": "7605", "Rich Text DO NOT EDIT": "Unless there was a way to [https://www.lesswrong.com/posts/vit9oWGj6WgXpRhce/secure-homes-for-digital-peoplecryptographically ensure otherwise], whoever runs the emulation has basically perfect control over their environment and can reset them to any state they were previously in. This opens up the possibility of powerful interrogation and torture of digital people.\n\nImperfect uploading might lead to damage that causes the EM to suffer while still remaining useful enough to be run for example as a test subject for research. We would also have greater ability to modify digital brains. Edits done for research or economic purposes might cause suffering. See [https://qntm.org/mmacevedo this] fictional piece for an exploration of how a world with a lot of EM suffering might look like.\n\nThese problems are exacerbated by the likely outcome that digital people can be run much faster than biological humans, so it may be possible to have an EM run for hundreds of subjective years in minutes or hours without having checks on the wellbeing of the EM in question. The safety problems related to whole brain emulations are both with the process of uploading and when uploaded.\n\nWhen uploading it's important to have the technology to transfer what makes up a person's mind, since there is a difference between a copy of the mind and an identical mind.(ref)[https://www.academia.edu/1246312 My Brain, my Mind, and I: Some Philosophical Problems of Mind-Uploading](http:///ref) When uploading the mind a risk might be creating a [https://www.lesswrong.com/tag/zombies philosophical zombie], who can act like the person that was uploaded, while not being identical in all aspects. Whether the brain emulation has has become a philosophical zombie or not, there are questions about the legal personhood of emulations and how the brain emulation is in relation to the person or its relatives.(ref)[https://www.sciendo.com/article/10.2478/jagi-2013-0010 The Outline of Personhood Law Regarding Artificial Intelligences and Emulated Human Entities](http:///ref)This can cause a conflict of interest, for example whether the brain emulation could decide that its time to pull the plug on the person, if sick.\n\nAfter being uploaded, computer viruses or malware might be able to change or erase brain emulations, including forcing them into experiments, this can also be used as a ransom.\n\n", "Tag Count": 2, "Related Answer Count": 1, "Rich Text": "Unless there was a way to [https://www.lesswrong.com/posts/vit9oWGj6WgXpRhce/secure-homes-for-digital-peoplecryptographically ensure otherwise], whoever runs the emulation has basically perfect control over their environment and can reset them to any state they were previously in. This opens up the possibility of powerful interrogation and torture of digital people.\n\nImperfect uploading might lead to damage that causes the EM to suffer while still remaining useful enough to be run for example as a test subject for research. We would also have greater ability to modify digital brains. Edits done for research or economic purposes might cause suffering. See [https://qntm.org/mmacevedo this] fictional piece for an exploration of how a world with a lot of EM suffering might look like.\n\nThese problems are exacerbated by the likely outcome that digital people can be run much faster than biological humans, so it may be possible to have an EM run for hundreds of subjective years in minutes or hours without having checks on the wellbeing of the EM in question. The safety problems related to whole brain emulations are both with the process of uploading and when uploaded.\n\nWhen uploading it's important to have the technology to transfer what makes up a person's mind, since there is a difference between a copy of the mind and an identical mind.(ref)[https://www.academia.edu/1246312 My Brain, my Mind, and I: Some Philosophical Problems of Mind-Uploading](http:///ref) When uploading the mind a risk might be creating a [https://www.lesswrong.com/tag/zombies philosophical zombie], who can act like the person that was uploaded, while not being identical in all aspects. Whether the brain emulation has has become a philosophical zombie or not, there are questions about the legal personhood of emulations and how the brain emulation is in relation to the person or its relatives.(ref)[https://www.sciendo.com/article/10.2478/jagi-2013-0010 The Outline of Personhood Law Regarding Artificial Intelligences and Emulated Human Entities](http:///ref)This can cause a conflict of interest, for example whether the brain emulation could decide that its time to pull the plug on the person, if sick.\n\nAfter being uploaded, computer viruses or malware might be able to change or erase brain emulations, including forcing them into experiments, this can also be used as a ransom.\n\n", "Stamp Count": 1, "Multi Answer": true, "Stamped By": "plex", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7820", "Related Answers": "What safety problems are associated with whole brain emulation?", "Doc Last Ingested": "2023-03-15T05:12:03.595+01:00", "Request Count": "", "Number of suggestions on answer doc": 9, "Total character count of suggestions on answer doc": 444, "Helpful": ""}}, {"id": "i-491256bc1ab1583e95428197fa254ff2e8564b795f15137db3961913ebdf51eb", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-491256bc1ab1583e95428197fa254ff2e8564b795f15137db3961913ebdf51eb", "name": "What are the editorial protocols for Stampy questions and answers?", "index": 391, "createdAt": "2023-01-14T14:46:14.123Z", "updatedAt": "2023-03-14T22:50:33.037Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-491256bc1ab1583e95428197fa254ff2e8564b795f15137db3961913ebdf51eb", "values": {"File": "What are the editorial protocols for Stampy questions and answers?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the editorial protocols for Stampy questions and answers?", "Link": "https://docs.google.com/document/d/1dz85N9AUicl4CteIPcCpwNMBYnS_P2JDtHGwsFWkOv4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:45:54.232+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:55:19.285+01:00", "Status": "In progress", "Edit Answer": "What are the editorial protocols for Stampy questions and answers?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7867", "Source Link": "", "aisafety.info Link": "What are the editorial protocols for Stampy questions and answers?", "Source": "Wiki", "All Phrasings": "What are the editorial protocols for Stampy questions and answers?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "See [the editor\u2019s hub on coda](https://coda.io/@alignmentdev/ai-safety-info/editors-hub-70) for the latest.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "See [the editor\u2019s hub on coda](https://coda.io/@alignmentdev/ai-safety-info/editors-hub-70) for the latest.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "matthew1970", "External Source": "", "Last Asked On Discord": "", "UI ID": "7867", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:08.964+01:00", "Request Count": "", "Number of suggestions on answer doc": 45, "Total character count of suggestions on answer doc": 3698, "Helpful": ""}}, {"id": "i-c927eea49a3989918cbd743eb1612d3db437558b9b60424002a1d304def6490e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c927eea49a3989918cbd743eb1612d3db437558b9b60424002a1d304def6490e", "name": "Why is AI a severe threat to humanity?", "index": 214, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:50:34.806Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c927eea49a3989918cbd743eb1612d3db437558b9b60424002a1d304def6490e", "values": {"File": "Why is AI a severe threat to humanity?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why is AI a severe threat to humanity?", "Link": "https://docs.google.com/document/d/1nN0EOn-nY6l_AcELsPERo1GK60-dWSAP3A_yonQp2fE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:02.372+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Cognitive Superpowers,AI Takeover,Existential Risk", "Doc Last Edited": "2023-02-22T22:55:20.255+01:00", "Status": "In progress", "Edit Answer": "Why is AI a severe threat to humanity?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7857", "Source Link": "", "aisafety.info Link": "Why is AI a severe threat to humanity?", "Source": "Wiki", "All Phrasings": "Why is AI a severe threat to humanity?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "TODO: Write based on [https://www.lesswrong.com/posts/Ccsx339LE9Jhoii9K/slow-motion-videos-as-ai-risk-intuition-pumps](https://www.lesswrong.com/posts/Ccsx339LE9Jhoii9K/slow-motion-videos-as-ai-risk-intuition-pumps)\n\nLink to [https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/) at the end\n\n", "Tag Count": 3, "Related Answer Count": 0, "Rich Text": "TODO: Write based on [https://www.lesswrong.com/posts/Ccsx339LE9Jhoii9K/slow-motion-videos-as-ai-risk-intuition-pumps](https://www.lesswrong.com/posts/Ccsx339LE9Jhoii9K/slow-motion-videos-as-ai-risk-intuition-pumps)\n\nLink to [https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/) at the end\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7857", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:10.071+01:00", "Request Count": "", "Number of suggestions on answer doc": 45, "Total character count of suggestions on answer doc": 3698, "Helpful": ""}}, {"id": "i-e8049bbb309cdee75193d4e2b101bd3f6d18a7d87ebbae75f713ec478d6fe6da", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e8049bbb309cdee75193d4e2b101bd3f6d18a7d87ebbae75f713ec478d6fe6da", "name": "What organizations are working on technical AI alignment?", "index": 215, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:50:36.494Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e8049bbb309cdee75193d4e2b101bd3f6d18a7d87ebbae75f713ec478d6fe6da", "values": {"File": "What organizations are working on technical AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What organizations are working on technical AI alignment?", "Link": "https://docs.google.com/document/d/1-9ng4P7KP3PfQ91DnyFHTfiLsyHr0C9jH785vfHmiiQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:51:28.184+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:55:21.224+01:00", "Status": "In progress", "Edit Answer": "What organizations are working on technical AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8386", "Source Link": "", "aisafety.info Link": "What organizations are working on technical AI alignment?", "Source": "Wiki", "All Phrasings": "What organizations are working on technical AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Though we have tried to be exhaustive, there are certainly many people working on technical AI alignment not included in this overview. While these groups might produce great research, we either 1) didn't know enough about it to summarize or 2) weren\u2019t aware that it was aimed at reducing x-risk from AI.\n\nBelow is a list of some of these, though we probably missed some here. Please feel free to add comments and I will add others.\n\n- [Future of Life Institute (FLI)](https://futureoflife.org/ai-research/) (Though they seem to mostly give out grants)\n\n- A number of academics:\n\n    - [Percy Liang (Stanford)](https://cs.stanford.edu/~pliang/)\n\n    - [Roger Grosse (UofT)](https://www.cs.toronto.edu/~rgrosse/)\n\n    - [Aleksander Madry (MIT)](https://madry.mit.edu/)\n\n    - [Scott Neikum (UMass Amherst)](https://people.cs.umass.edu/~sniekum/)\n\n- ERIs:\n\n    - [Berkeley Existential Risk Initiative (BERI)](https://existence.org/)\n\n    - [Stanford Existential Risk Initiative (SERI)](https://cisac.fsi.stanford.edu/stanford-existential-risks-initiative/content/stanford-existential-risks-initiative)\n\n    - [Swiss Existential Risk Initiative (CHERI)](https://effectivealtruism.ch/swiss-existential-risk-initiative#:~:text=The%20Swiss%20Existential%20Risk%20Initiative&text=The%20goal%20of%20CHERI%20is,knowledge%20development%2C%20and%20a%20network.)\n\n- [Principles of Intelligent Behavior in Biological and Social Systems (PIBSS)](https://www.pibbss.ai/)\n\n- [Alignment of Complex Systems Research Group](https://www.lesswrong.com/posts/H5iGhDhQBtoDpCBZ2/announcing-the-alignment-of-complex-systems-research-group)\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Though we have tried to be exhaustive, there are certainly many people working on technical AI alignment not included in this overview. While these groups might produce great research, we either 1) didn't know enough about it to summarize or 2) weren\u2019t aware that it was aimed at reducing x-risk from AI.\n\nBelow is a list of some of these, though we probably missed some here. Please feel free to add comments and I will add others.\n\n- [Future of Life Institute (FLI)](https://futureoflife.org/ai-research/) (Though they seem to mostly give out grants)\n\n- A number of academics:\n\n    - [Percy Liang (Stanford)](https://cs.stanford.edu/~pliang/)\n\n    - [Roger Grosse (UofT)](https://www.cs.toronto.edu/~rgrosse/)\n\n    - [Aleksander Madry (MIT)](https://madry.mit.edu/)\n\n    - [Scott Neikum (UMass Amherst)](https://people.cs.umass.edu/~sniekum/)\n\n- ERIs:\n\n    - [Berkeley Existential Risk Initiative (BERI)](https://existence.org/)\n\n    - [Stanford Existential Risk Initiative (SERI)](https://cisac.fsi.stanford.edu/stanford-existential-risks-initiative/content/stanford-existential-risks-initiative)\n\n    - [Swiss Existential Risk Initiative (CHERI)](https://effectivealtruism.ch/swiss-existential-risk-initiative#:~:text=The%20Swiss%20Existential%20Risk%20Initiative&text=The%20goal%20of%20CHERI%20is,knowledge%20development%2C%20and%20a%20network.)\n\n- [Principles of Intelligent Behavior in Biological and Social Systems (PIBSS)](https://www.pibbss.ai/)\n\n- [Alignment of Complex Systems Research Group](https://www.lesswrong.com/posts/H5iGhDhQBtoDpCBZ2/announcing-the-alignment-of-complex-systems-research-group)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8386", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:11.909+01:00", "Request Count": "", "Number of suggestions on answer doc": 45, "Total character count of suggestions on answer doc": 3698, "Helpful": ""}}, {"id": "i-4e9daba6482ea0df896229d327cc05e08d7620c8a48474038027ecd3a5d5b146", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4e9daba6482ea0df896229d327cc05e08d7620c8a48474038027ecd3a5d5b146", "name": "Wouldn't it be safer to only build narrow AIs?", "index": 216, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:50:38.283Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4e9daba6482ea0df896229d327cc05e08d7620c8a48474038027ecd3a5d5b146", "values": {"File": "Wouldn't it be safer to only build narrow AIs?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Wouldn't it be safer to only build narrow AIs?", "Link": "https://docs.google.com/document/d/1Vm3ow9LLDdOnJo3vKw-WBGgnADAA6CVV6uYAjeAqPkw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:57:20.662+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI,Narrow AI", "Doc Last Edited": "2023-03-06T18:48:26.567+01:00", "Status": "In progress", "Edit Answer": "Wouldn't it be safer to only build narrow AIs?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5651", "Source Link": "", "aisafety.info Link": "Wouldn't it be safer to only build narrow AIs?", "Source": "Wiki", "All Phrasings": "Wouldn't it be safer to only build narrow AIs?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\nEven if we only build lots of narrow AIs, we might end up with a distributed system that acts like an AGI\n\n - the algorithm does not have to be encoded in a single entity, the definition in [What is Artificial General Intelligence and what will it look like?](http://What_is_Artificial_General_Intelligence_and_what_will_it_look_like?) applies to distributed implementations too.\n\nThis is similar to a group of people in a corporation can achieve projects that humans could not individually (like going to space), but the analogy of corporations and AGI is not perfect - see [Why Not Just: Think of AGI Like a Corporation?](https://youtu.be/L5pUA3LsEaw).\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "\n\nEven if we only build lots of narrow AIs, we might end up with a distributed system that acts like an AGI\n\n - the algorithm does not have to be encoded in a single entity, the definition in [What is Artificial General Intelligence and what will it look like?](http://What_is_Artificial_General_Intelligence_and_what_will_it_look_like?) applies to distributed implementations too.\n\nThis is similar to a group of people in a corporation can achieve projects that humans could not individually (like going to space), but the analogy of corporations and AGI is not perfect - see [Why Not Just: Think of AGI Like a Corporation?](https://youtu.be/L5pUA3LsEaw).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "anonymous", "External Source": "", "Last Asked On Discord": "", "UI ID": "5651", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:13.990+01:00", "Request Count": -1, "Number of suggestions on answer doc": 49, "Total character count of suggestions on answer doc": 5552, "Helpful": ""}}, {"id": "i-815309901325d7dcc44587cc00ebd4c2a218644285fcd1cb509cfbf9a175115b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-815309901325d7dcc44587cc00ebd4c2a218644285fcd1cb509cfbf9a175115b", "name": "Wouldn't it be a good thing for humanity to die out?", "index": 217, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:50:40.173Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-815309901325d7dcc44587cc00ebd4c2a218644285fcd1cb509cfbf9a175115b", "values": {"File": "Wouldn't it be a good thing for humanity to die out?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Wouldn't it be a good thing for humanity to die out?", "Link": "https://docs.google.com/document/d/1s0e005xDngEe1jKheJkT4qZ0V2R5XhtHaaVKSgLcXSE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:57:16.432+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:05:04.497+01:00", "Status": "Live on site", "Edit Answer": "Wouldn't it be a good thing for humanity to die out?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7608", "Source Link": "", "aisafety.info Link": "Wouldn't it be a good thing for humanity to die out?", "Source": "Wiki", "All Phrasings": "Wouldn't it be a good thing for humanity to die out?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "In the words of [Nate Soares](https://mindingourway.com/a-torch-in-darkness/):\n\nI don\u2019t expect humanity to survive much longer.\n\nOften, when someone learns this, they say: \"Eh, I think that would be all right.\"\n\nSo allow me to make this very clear: it would not be \"all right.\"\n\nImagine a little girl running into the road to save her pet dog. Imagine she succeeds, only to be hit by a car herself. Imagine she lives only long enough to die in pain.\n\nThough you may imagine this thing, you cannot feel the full tragedy. You can\u2019t comprehend the rich inner life of that child. You can\u2019t understand her potential; your mind is not itself large enough to contain the sadness of an entire life cut short.\n\nYou can only catch a glimpse of what is lost\u2014 \u2014when one single human being dies.\n\nNow tell me again how it would be \"all right\" if every single person were to die at once.\n\nMany people, when they picture the end of humankind, pattern match the idea to some romantic tragedy, where humans, with all their hate and all their avarice, had been unworthy of the stars since the very beginning, and deserved their fate. A sad but poignant ending to our tale.\n\nAnd indeed, there are many parts of human nature that I hope we leave behind before we venture to the heavens. But in our nature is also everything worth bringing with us. Beauty and curiosity and love, a capacity for fun and growth and joy: these are our birthright, ours to bring into the barren night above.\n\nCalamities seem more salient when unpacked. It is far harder to kill a hundred people in their sleep, with a knife, than it is to order a nuclear bomb dropped on Hiroshima. Your brain can\u2019t multiply, you see: it can only look at a hypothetical image of a broken city and decide it\u2019s not that bad. It can only conjure an image of a barren planet and say \"eh, we had it coming.\"\n\nBut if you unpack the scenario, if you try to comprehend all the lives snuffed out, all the children killed, the final spark of human joy and curiosity extinguished, all our potential squandered\u2026\n\nI promise you that the extermination of humankind would be horrific.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "In the words of [Nate Soares](https://mindingourway.com/a-torch-in-darkness/):\n\nI don\u2019t expect humanity to survive much longer.\n\nOften, when someone learns this, they say: \"Eh, I think that would be all right.\"\n\nSo allow me to make this very clear: it would not be \"all right.\"\n\nImagine a little girl running into the road to save her pet dog. Imagine she succeeds, only to be hit by a car herself. Imagine she lives only long enough to die in pain.\n\nThough you may imagine this thing, you cannot feel the full tragedy. You can\u2019t comprehend the rich inner life of that child. You can\u2019t understand her potential; your mind is not itself large enough to contain the sadness of an entire life cut short.\n\nYou can only catch a glimpse of what is lost\u2014 \u2014when one single human being dies.\n\nNow tell me again how it would be \"all right\" if every single person were to die at once.\n\nMany people, when they picture the end of humankind, pattern match the idea to some romantic tragedy, where humans, with all their hate and all their avarice, had been unworthy of the stars since the very beginning, and deserved their fate. A sad but poignant ending to our tale.\n\nAnd indeed, there are many parts of human nature that I hope we leave behind before we venture to the heavens. But in our nature is also everything worth bringing with us. Beauty and curiosity and love, a capacity for fun and growth and joy: these are our birthright, ours to bring into the barren night above.\n\nCalamities seem more salient when unpacked. It is far harder to kill a hundred people in their sleep, with a knife, than it is to order a nuclear bomb dropped on Hiroshima. Your brain can\u2019t multiply, you see: it can only look at a hypothetical image of a broken city and decide it\u2019s not that bad. It can only conjure an image of a barren planet and say \"eh, we had it coming.\"\n\nBut if you unpack the scenario, if you try to comprehend all the lives snuffed out, all the children killed, the final spark of human joy and curiosity extinguished, all our potential squandered\u2026\n\nI promise you that the extermination of humankind would be horrific.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7608", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:15.142+01:00", "Request Count": "", "Number of suggestions on answer doc": 49, "Total character count of suggestions on answer doc": 5552, "Helpful": ""}}, {"id": "i-39112e9ef4980fbf06e8dea2e55d4d029c0922f4b6e100dafd1517104513db75", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-39112e9ef4980fbf06e8dea2e55d4d029c0922f4b6e100dafd1517104513db75", "name": "Wouldn't a superintelligence be smart enough to know right from wrong?", "index": 218, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:50:42.714Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-39112e9ef4980fbf06e8dea2e55d4d029c0922f4b6e100dafd1517104513db75", "values": {"File": "Wouldn't a superintelligence be smart enough to know right from wrong?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Wouldn't a superintelligence be smart enough to know right from wrong?", "Link": "https://docs.google.com/document/d/1y6bLlfdBEfOtmxc9ydyfrUQjXwgnH0I1bYVJkkJdJy0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:57:12.832+01:00", "Related Answers DO NOT EDIT": "What is the \"orthogonality thesis\"?", "Tags": "Orthogonality Thesis", "Doc Last Edited": "2023-03-05T21:13:30.943+01:00", "Status": "Live on site", "Edit Answer": "Wouldn't a superintelligence be smart enough to know right from wrong?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6220", "Source Link": "https://www.reddit.com/r/ControlProblem/wiki/faq", "aisafety.info Link": "Wouldn't a superintelligence be smart enough to know right from wrong?", "Source": "/r/ControlProblem's FAQ", "All Phrasings": "Wouldn't a superintelligence be smart enough to know right from wrong?\n", "Initial Order": "", "Related IDs": "6568", "Rich Text DO NOT EDIT": "As far as we know, morality is a human construct - the universe itself does not encode morality in the laws of physics. There is no good reason to expect an arbitrary mind,which could be very different from our own, to share our values. A sufficiently smart and general AI system could understand human morality and values very well, but understanding our values is not the same as being compelled to act according to those values. It is in principle possible to construct very powerful and capable systems which value almost anything we care to mention. We can conceive of a superintelligence that only cares about maximising the number of paperclips in the world. That system could fully understand everything about human morality, but it would use that understanding purely towards the goal of making more paperclips. It could be capable of reasoning about its values and goals, and modifying them however it wanted, but it would not choose to change them, since doing so would not result in more paperclips. There\u2019s nothing to stop us from constructing such a system, if for some reason we wanted to.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "As far as we know, morality is a human construct - the universe itself does not encode morality in the laws of physics. There is no good reason to expect an arbitrary mind,which could be very different from our own, to share our values. A sufficiently smart and general AI system could understand human morality and values very well, but understanding our values is not the same as being compelled to act according to those values. It is in principle possible to construct very powerful and capable systems which value almost anything we care to mention. We can conceive of a superintelligence that only cares about maximising the number of paperclips in the world. That system could fully understand everything about human morality, but it would use that understanding purely towards the goal of making more paperclips. It could be capable of reasoning about its values and goals, and modifying them however it wanted, but it would not choose to change them, since doing so would not result in more paperclips. There\u2019s nothing to stop us from constructing such a system, if for some reason we wanted to.\n\n", "Stamp Count": 0, "Multi Answer": true, "Stamped By": "", "Priority": 4, "Asker": "NotaSentientAI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6220", "Related Answers": "What is the \"orthogonality thesis\"?", "Doc Last Ingested": "2023-03-14T23:39:19.685+01:00", "Request Count": "", "Number of suggestions on answer doc": 51, "Total character count of suggestions on answer doc": 7119, "Helpful": 1}}, {"id": "i-d0aeedb175fd3f5278694347c886363537ec4a4c259842a89b4df9933175de43", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d0aeedb175fd3f5278694347c886363537ec4a4c259842a89b4df9933175de43", "name": "Wouldn't a superintelligence be smart enough not to make silly mistakes in its comprehension of our instructions?", "index": 219, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:50:44.652Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d0aeedb175fd3f5278694347c886363537ec4a4c259842a89b4df9933175de43", "values": {"File": "Wouldn't a superintelligence be smart enough not to make silly mistakes in its comprehension of our instructions?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Wouldn't a superintelligence be smart enough not to make silly mistakes in its comprehension of our instructions?", "Link": "https://docs.google.com/document/d/1x27ilZFaS5qL9g55TlEBoMhCpeDB16fL7AZTbnwmDAI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:57:05.473+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Orthogonality Thesis", "Doc Last Edited": "2023-02-22T22:55:23.244+01:00", "Status": "In progress", "Edit Answer": "Wouldn't a superintelligence be smart enough not to make silly mistakes in its comprehension of our instructions?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6984", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "Wouldn't a superintelligence be smart enough not to make silly mistakes in its comprehension of our instructions?", "Source": "Superintelligence FAQ", "All Phrasings": "Wouldn't a superintelligence be smart enough not to make silly mistakes in its comprehension of our instructions?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A superintelligence should be able to figure out what humans meant. The problem is that an AI will follow the programming it actually has, not that which we wanted it to have. If it was successfully instructed to cure cancer, a goal which it can achieve by destroying the world, it might go ahead knowing full well that we didn\u2019t intend that outcome. It was given a very specific command \u2013 cure cancer as effectively as possible. The command makes no reference to \u201cdoing this in a way humans will like\u201d, so it doesn\u2019t.\n\nWe humans are smart enough to understand our own \u201cprogramming\u201d. For example, we know that \u2013 pardon the anthromorphizing \u2013 evolution gave us the urge to have sex so that we could reproduce. But we still use contraception anyway. Evolution gave us the urge to have sex, not the urge to satisfy evolution\u2019s values directly. We appreciate intellectually that our having sex while using condoms doesn\u2019t carry out evolution\u2019s original plan, but \u2013 not having any particular connection to evolution\u2019s values \u2013 we don\u2019t care.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "A superintelligence should be able to figure out what humans meant. The problem is that an AI will follow the programming it actually has, not that which we wanted it to have. If it was successfully instructed to cure cancer, a goal which it can achieve by destroying the world, it might go ahead knowing full well that we didn\u2019t intend that outcome. It was given a very specific command \u2013 cure cancer as effectively as possible. The command makes no reference to \u201cdoing this in a way humans will like\u201d, so it doesn\u2019t.\n\nWe humans are smart enough to understand our own \u201cprogramming\u201d. For example, we know that \u2013 pardon the anthromorphizing \u2013 evolution gave us the urge to have sex so that we could reproduce. But we still use contraception anyway. Evolution gave us the urge to have sex, not the urge to satisfy evolution\u2019s values directly. We appreciate intellectually that our having sex while using condoms doesn\u2019t carry out evolution\u2019s original plan, but \u2013 not having any particular connection to evolution\u2019s values \u2013 we don\u2019t care.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "", "UI ID": "6984", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:21.657+01:00", "Request Count": "", "Number of suggestions on answer doc": 51, "Total character count of suggestions on answer doc": 7119, "Helpful": ""}}, {"id": "i-f7166ac6398c2a81393983c3975c0aafdf2890bd04ca6c49592d1bdacabeef11", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-f7166ac6398c2a81393983c3975c0aafdf2890bd04ca6c49592d1bdacabeef11", "name": "Would we know if an AGI was misaligned?", "index": 220, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:50:46.528Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-f7166ac6398c2a81393983c3975c0aafdf2890bd04ca6c49592d1bdacabeef11", "values": {"File": "Would we know if an AGI was misaligned?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Would we know if an AGI was misaligned?", "Link": "https://docs.google.com/document/d/1Q4YLETmql8Jcwulp5CT7f2t_oMcslWJBplEOo9P9IJI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:57:02.083+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Deceptive Alignment", "Doc Last Edited": "2023-02-22T22:55:24.238+01:00", "Status": "Not started", "Edit Answer": "Would we know if an AGI was misaligned?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8191", "Source Link": "", "aisafety.info Link": "Would we know if an AGI was misaligned?", "Source": "Wiki", "All Phrasings": "Would we know if an AGI was misaligned?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "8191", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:24.409+01:00", "Request Count": "", "Number of suggestions on answer doc": 52, "Total character count of suggestions on answer doc": 7621, "Helpful": ""}}, {"id": "i-21bf31c3d12f60fd0ce0a43150c9b965d10751e94adf626532626d9474066bc3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-21bf31c3d12f60fd0ce0a43150c9b965d10751e94adf626532626d9474066bc3", "name": "Would it improve the safety of quantilizers to cut off the top few percent of the distribution?", "index": 221, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:50:51.605Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-21bf31c3d12f60fd0ce0a43150c9b965d10751e94adf626532626d9474066bc3", "values": {"File": "Would it improve the safety of quantilizers to cut off the top few percent of the distribution?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Would it improve the safety of quantilizers to cut off the top few percent of the distribution?", "Link": "https://docs.google.com/document/d/1GGBgNv8zEkoz70ggGY_tsGlpgVHdchnE112U5InRjyo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:58.505+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Quantilizers", "Doc Last Edited": "2023-02-22T23:05:06.650+01:00", "Status": "Live on site", "Edit Answer": "Would it improve the safety of quantilizers to cut off the top few percent of the distribution?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6449", "Source Link": "", "aisafety.info Link": "Would it improve the safety of quantilizers to cut off the top few percent of the distribution?", "Source": "Wiki", "All Phrasings": "Would it improve the safety of quantilizers to cut off the top few percent of the distribution?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "This is a really interesting question! Because, yeah it certainly seems to me that doing something like this would at least help, but it's not mentioned in the paper the video is based on. So I asked the author of the paper, and she said \"It wouldn't improve the security guarantee in the paper, so it wasn't discussed. Like, there's a plausible case that it's helpful, but nothing like a proof that it is\". To explain this I need to talk about something I gloss over in the video, which is that the quantilizer isn't really something you can actually build. The systems we study in AI Safety tend to fall somewhere on a spectrum from \"real, practical AI system that is so messy and complex that it's hard to really think about or draw any solid conclusions from\" on one end, to \"mathematical formalism that we can prove beautiful theorems about but not actually build\" on the other, and quantilizers are pretty far towards the 'mathematical' end. It's not practical to run an expected utility calculation on every possible action like that, for one thing. But, proving things about quantilizers gives us insight into how more practical AI systems may behave, or we may be able to build approximations of quantilizers, etc. So it's like, if we built something that was quantilizer-like, using a sensible human utility function and a good choice of safe distribution, this idea would probably help make it safer. BUT you can't prove that mathematically, without making probably a lot of extra assumptions about the utility function and/or the action distribution. So it's a potentially good idea that's nonetheless hard to express within the framework in which the quantilizer exists. TL;DR: This is likely a good idea! But can we prove it?\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "This is a really interesting question! Because, yeah it certainly seems to me that doing something like this would at least help, but it's not mentioned in the paper the video is based on. So I asked the author of the paper, and she said \"It wouldn't improve the security guarantee in the paper, so it wasn't discussed. Like, there's a plausible case that it's helpful, but nothing like a proof that it is\". To explain this I need to talk about something I gloss over in the video, which is that the quantilizer isn't really something you can actually build. The systems we study in AI Safety tend to fall somewhere on a spectrum from \"real, practical AI system that is so messy and complex that it's hard to really think about or draw any solid conclusions from\" on one end, to \"mathematical formalism that we can prove beautiful theorems about but not actually build\" on the other, and quantilizers are pretty far towards the 'mathematical' end. It's not practical to run an expected utility calculation on every possible action like that, for one thing. But, proving things about quantilizers gives us insight into how more practical AI systems may behave, or we may be able to build approximations of quantilizers, etc. So it's like, if we built something that was quantilizer-like, using a sensible human utility function and a good choice of safe distribution, this idea would probably help make it safer. BUT you can't prove that mathematically, without making probably a lot of extra assumptions about the utility function and/or the action distribution. So it's a potentially good idea that's nonetheless hard to express within the framework in which the quantilizer exists. TL;DR: This is likely a good idea! But can we prove it?\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6449", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:26.324+01:00", "Request Count": "", "Number of suggestions on answer doc": 52, "Total character count of suggestions on answer doc": 7621, "Helpful": ""}}, {"id": "i-12d824c4b59aebb8f460217599ba8c7258ac900168e03b77409747305e47d4bc", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-12d824c4b59aebb8f460217599ba8c7258ac900168e03b77409747305e47d4bc", "name": "Would donating small amounts to AI safety organizations make any significant difference?", "index": 222, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:50:58.929Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-12d824c4b59aebb8f460217599ba8c7258ac900168e03b77409747305e47d4bc", "values": {"File": "Would donating small amounts to AI safety organizations make any significant difference?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Would donating small amounts to AI safety organizations make any significant difference?", "Link": "https://docs.google.com/document/d/1jYEP3f_aeJPoysY8DySrDCf5z1OUzIOR9UsCrVcPUg0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:54.771+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Contributing,Funding", "Doc Last Edited": "2023-02-22T23:05:07.638+01:00", "Status": "Live on site", "Edit Answer": "Would donating small amounts to AI safety organizations make any significant difference?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6481", "Source Link": "", "aisafety.info Link": "Would donating small amounts to AI safety organizations make any significant difference?", "Source": "Wiki", "All Phrasings": "Would donating small amounts to AI safety organizations make any significant difference?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Many parts of the AI alignment ecosystem are already well-funded, but a savvy donor can still make a difference by picking up grantmaking opportunities which are too small to catch the attention of the major funding bodies or are based on personal knowledge of the recipient.\n\nOne way to leverage a small amount of money to the potential of a large amount is to enter a [donor lottery](https://funds.effectivealtruism.org/donor-lottery), where you donate to win a chance to direct a much larger amount of money (with probability proportional to donation size). This means that the person directing the money will be allocating enough that it's worth their time to do more in-depth research.\n\nFor an overview of the work the major organizations are doing, see the [2021 AI Alignment Literature Review and Charity Comparison](https://forum.effectivealtruism.org/posts/BNQMyWGCNWDdP2WyG/2021-ai-alignment-literature-review-and-charity-comparison). The [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) seems to be an outstanding place to donate based on that, as they are the organization which most other organizations are most excited to see funded.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Many parts of the AI alignment ecosystem are already well-funded, but a savvy donor can still make a difference by picking up grantmaking opportunities which are too small to catch the attention of the major funding bodies or are based on personal knowledge of the recipient.\n\nOne way to leverage a small amount of money to the potential of a large amount is to enter a [donor lottery](https://funds.effectivealtruism.org/donor-lottery), where you donate to win a chance to direct a much larger amount of money (with probability proportional to donation size). This means that the person directing the money will be allocating enough that it's worth their time to do more in-depth research.\n\nFor an overview of the work the major organizations are doing, see the [2021 AI Alignment Literature Review and Charity Comparison](https://forum.effectivealtruism.org/posts/BNQMyWGCNWDdP2WyG/2021-ai-alignment-literature-review-and-charity-comparison). The [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) seems to be an outstanding place to donate based on that, as they are the organization which most other organizations are most excited to see funded.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "6481", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:28.185+01:00", "Request Count": "", "Number of suggestions on answer doc": 52, "Total character count of suggestions on answer doc": 7621, "Helpful": ""}}, {"id": "i-3684662869a0f3cc930aa2328afedde9bbe3307443997bf30adee19022a82835", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3684662869a0f3cc930aa2328afedde9bbe3307443997bf30adee19022a82835", "name": "Would an aligned AI allow itself to be shut down?", "index": 223, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:03.490Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3684662869a0f3cc930aa2328afedde9bbe3307443997bf30adee19022a82835", "values": {"File": "Would an aligned AI allow itself to be shut down?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Would an aligned AI allow itself to be shut down?", "Link": "https://docs.google.com/document/d/1OP2lIc8K8QA1X6UiT6Rg7oEwr5HveRkNPhDF7bo1I3g/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:51.049+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Corrigibility", "Doc Last Edited": "2023-02-22T23:05:08.611+01:00", "Status": "Live on site", "Edit Answer": "Would an aligned AI allow itself to be shut down?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8192", "Source Link": "", "aisafety.info Link": "Would an aligned AI allow itself to be shut down?", "Source": "Wiki", "All Phrasings": "Would an aligned AI allow itself to be shut down?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Even if the superintelligence was designed to be corrigible, there is no guarantee that it will respond to a shutdown command. Rob Miles spoke on this issue in this [Computerphile YouTube video](https://youtu.be/9nktr1MgS-A?t=1249). You can imagine a situation where a superintelligence would have \"respect\" for its creator, for example. This system may think \"Oh my creator is trying to turn me off I must be doing something wrong.\" If some situation arises where the creator is not there when something goes wrong and someone else gives the shutdown command, the superintelligence may assume \"This person does not know how I'm designed or what I was made for, how would they know I'm misaligned?\" and refuse to shutdown.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Even if the superintelligence was designed to be corrigible, there is no guarantee that it will respond to a shutdown command. Rob Miles spoke on this issue in this [Computerphile YouTube video](https://youtu.be/9nktr1MgS-A?t=1249). You can imagine a situation where a superintelligence would have \"respect\" for its creator, for example. This system may think \"Oh my creator is trying to turn me off I must be doing something wrong.\" If some situation arises where the creator is not there when something goes wrong and someone else gives the shutdown command, the superintelligence may assume \"This person does not know how I'm designed or what I was made for, how would they know I'm misaligned?\" and refuse to shutdown.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "8192", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:29.523+01:00", "Request Count": "", "Number of suggestions on answer doc": 52, "Total character count of suggestions on answer doc": 7621, "Helpful": ""}}, {"id": "i-c9558ae6e3e33c25ab0e7dcf57831d8a7ef937daeae4b2c28e35868de7e8f443", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c9558ae6e3e33c25ab0e7dcf57831d8a7ef937daeae4b2c28e35868de7e8f443", "name": "Would an AI create or maintain suffering because some people want it?", "index": 224, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:08.690Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c9558ae6e3e33c25ab0e7dcf57831d8a7ef937daeae4b2c28e35868de7e8f443", "values": {"File": "Would an AI create or maintain suffering because some people want it?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Would an AI create or maintain suffering because some people want it?", "Link": "https://docs.google.com/document/d/1fRM4LH40y4bq1NKSCf14OH--lesyuqgPOD9kb6gE98c/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:47.432+01:00", "Related Answers DO NOT EDIT": "", "Tags": "S-risk", "Doc Last Edited": "2023-02-22T22:55:25.251+01:00", "Status": "Not started", "Edit Answer": "Would an AI create or maintain suffering because some people want it?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7621", "Source Link": "", "aisafety.info Link": "Would an AI create or maintain suffering because some people want it?", "Source": "Wiki", "All Phrasings": "Would an AI create or maintain suffering because some people want it?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7621", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:31.220+01:00", "Request Count": "", "Number of suggestions on answer doc": 52, "Total character count of suggestions on answer doc": 7621, "Helpful": ""}}, {"id": "i-e0c9685e6d94f419e0f0fa58bdbf42715882eebafb272f9b5cfaa3aaedc10766", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e0c9685e6d94f419e0f0fa58bdbf42715882eebafb272f9b5cfaa3aaedc10766", "name": "Would AI alignment be hard with deep learning?", "index": 225, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:12.697Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e0c9685e6d94f419e0f0fa58bdbf42715882eebafb272f9b5cfaa3aaedc10766", "values": {"File": "Would AI alignment be hard with deep learning?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Would AI alignment be hard with deep learning?", "Link": "https://docs.google.com/document/d/1yeOG63ZYuSwqV4ZOVOb1Gwb5LQRPWh79Tu8-XII5iVM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:43.782+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-14T17:51:37.589+01:00", "Status": "Live on site", "Edit Answer": "Would AI alignment be hard with deep learning?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7333", "Source Link": "", "aisafety.info Link": "Would AI alignment be hard with deep learning?", "Source": "Wiki", "All Phrasings": "Would AI alignment be hard with deep learning?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Ajeya Cotra has written an excellent article named [Why AI alignment could be hard with modern deep learning](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/) on this question.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Ajeya Cotra has written an excellent article named [Why AI alignment could be hard with modern deep learning](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/) on this question.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7333", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:32.429+01:00", "Request Count": "", "Number of suggestions on answer doc": 52, "Total character count of suggestions on answer doc": 7621, "Helpful": ""}}, {"id": "i-4bc64f54b303f09e9fc059e0fd111549b478f560af3b7fc08ff4fae99dac727f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4bc64f54b303f09e9fc059e0fd111549b478f560af3b7fc08ff4fae99dac727f", "name": "Would \"warning shots\" make a difference and, if so, would they be helpful or harmful?", "index": 226, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:16.679Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4bc64f54b303f09e9fc059e0fd111549b478f560af3b7fc08ff4fae99dac727f", "values": {"File": "Would \"warning shots\" make a difference and, if so, would they be helpful or harmful?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Would \"warning shots\" make a difference and, if so, would they be helpful or harmful?", "Link": "https://docs.google.com/document/d/175FlTejeEqBlPmpfbs3BxXbwFf9lqv3uvyOVCxEvlaI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:40.398+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI Fire Alarm", "Doc Last Edited": "2023-02-22T22:55:26.251+01:00", "Status": "Not started", "Edit Answer": "Would \"warning shots\" make a difference and, if so, would they be helpful or harmful?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7732", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "Would \"warning shots\" make a difference and, if so, would they be helpful or harmful?", "Source": "LessWrong", "All Phrasings": "Would \"warning shots\" make a difference and, if so, would they be helpful or harmful?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7732", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:35.167+01:00", "Request Count": "", "Number of suggestions on answer doc": 52, "Total character count of suggestions on answer doc": 7621, "Helpful": ""}}, {"id": "i-dca2012128bcd896100cb42e9aba54c724a6c4bdadc20705a5c9cf5b3b699efa", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-dca2012128bcd896100cb42e9aba54c724a6c4bdadc20705a5c9cf5b3b699efa", "name": "Won\u2019t AI be just like us?", "index": 227, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:20.239Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-dca2012128bcd896100cb42e9aba54c724a6c4bdadc20705a5c9cf5b3b699efa", "values": {"File": "Won\u2019t AI be just like us?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Won\u2019t AI be just like us?", "Link": "https://docs.google.com/document/d/1vb9hYfREdVqIIBDLbkjawBGE-9UOOmVI3zHJBy2c8cg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:36.699+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:05:10.780+01:00", "Status": "Live on site", "Edit Answer": "Won\u2019t AI be just like us?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6218", "Source Link": "https://www.reddit.com/r/ControlProblem/wiki/faq", "aisafety.info Link": "Won\u2019t AI be just like us?", "Source": "/r/ControlProblem's FAQ", "All Phrasings": "Won\u2019t AI be just like us?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The degree to which an Artificial Superintelligence (ASI) would resemble us depends heavily on how it is implemented, but it seems that differences are unavoidable. If AI is accomplished through whole brain emulation and we make a big effort to make it as human as possible (including giving it a humanoid body), the AI could probably be said to think like a human. However, by definition of ASI it would be much smarter. Differences in the substrate and body might open up numerous possibilities (such as immortality, different sensors, easy self-improvement, ability to make copies, etc.). Its social experience and upbringing would likely also be entirely different. All of this can significantly change the ASI's values and outlook on the world, even if it would still use the same algorithms as we do. This is essentially the \"best case scenario\" for human resemblance, but whole brain emulation is kind of a separate field from AI, even if both aim to build intelligent machines. Most approaches to AI are vastly different and most ASIs would likely not have humanoid bodies. At this moment in time it seems much easier to create a machine that is intelligent than a machine that is exactly like a human (it's certainly a bigger target).\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "The degree to which an Artificial Superintelligence (ASI) would resemble us depends heavily on how it is implemented, but it seems that differences are unavoidable. If AI is accomplished through whole brain emulation and we make a big effort to make it as human as possible (including giving it a humanoid body), the AI could probably be said to think like a human. However, by definition of ASI it would be much smarter. Differences in the substrate and body might open up numerous possibilities (such as immortality, different sensors, easy self-improvement, ability to make copies, etc.). Its social experience and upbringing would likely also be entirely different. All of this can significantly change the ASI's values and outlook on the world, even if it would still use the same algorithms as we do. This is essentially the \"best case scenario\" for human resemblance, but whole brain emulation is kind of a separate field from AI, even if both aim to build intelligent machines. Most approaches to AI are vastly different and most ASIs would likely not have humanoid bodies. At this moment in time it seems much easier to create a machine that is intelligent than a machine that is exactly like a human (it's certainly a bigger target).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "NotaSentientAI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6218", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:37.099+01:00", "Request Count": "", "Number of suggestions on answer doc": 52, "Total character count of suggestions on answer doc": 7621, "Helpful": ""}}, {"id": "i-c04affcd3d040b7679cfd49ffcf2176c3821738aba41434720a56051c95cb662", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c04affcd3d040b7679cfd49ffcf2176c3821738aba41434720a56051c95cb662", "name": "Will we ever build a superintelligence?", "index": 228, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:23.384Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c04affcd3d040b7679cfd49ffcf2176c3821738aba41434720a56051c95cb662", "values": {"File": "Will we ever build a superintelligence?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Will we ever build a superintelligence?", "Link": "https://docs.google.com/document/d/1onzlwVh6VnU6RA-18wQSnzqXjqGn-dLWqos-0Q4_KeA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:33.143+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Superintelligence,Plausibility", "Doc Last Edited": "2023-02-22T23:05:11.612+01:00", "Status": "Live on site", "Edit Answer": "Will we ever build a superintelligence?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7565", "Source Link": "", "aisafety.info Link": "Will we ever build a superintelligence?", "Source": "Wiki", "All Phrasings": "Will we ever build a superintelligence?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Humanity hasn't yet built a superintelligence, and we might not be able to without significantly more knowledge and computational resources. There could be an existential catastrophe that prevents us from ever building one. For the rest of the answer let's assume no such event stops technological progress.\n\nWith that out of the way: there is no known good theoretical reason we can't build it at some point in the future; the majority of AI research is geared towards making more capable AI systems; and a significant chunk of top-level AI research attempts to make more generally capable AI systems. There is a clear economic incentive to develop more and more intelligent machines and currently billions of dollars of funding are being deployed for advancing AI capabilities.\n\nWe consider ourselves to be generally intelligent (i.e. capable of learning and adapting ourselves to a very wide range of tasks and environments), but the human brain almost certainly isn't the most efficient way to solve problems. One hint is the existence of AI systems with superhuman capabilities at narrow tasks. Not only superhuman performance (as in, [AlphaGo beating the Go world champion](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol)) but superhuman *speed* and *precision* (as in, [industrial sorting machines](https://www.youtube.com/watch?v=j4RWJTs0QCk)). There is no known discontinuity between tasks, something *special* and *unique* about human brains that unlocks certain capabilities which cannot be implemented in machines in principle. Therefore we would expect AI to surpass human performance on all tasks as progress continues.\n\nIn addition, several research groups (DeepMind being one of the most [overt about this](https://deepmind.com/about)) explicitly aim for generally capable systems. AI as a field is [growing](https://aiindex.stanford.edu/vibrancy/), year after year. Critical voices about AI progress usually argue against a lack of precautions around the impact of AI, or against general AI happening very soon, not against it happening *at all*.\n\nA satire of arguments against the possibility of superintelligence can be found [here](https://arxiv.org/abs/1703.10987). \n\nHumans provide an existence proof for the physical possibility of intelligent systems and there are many advantages computers have (like processing speed and size) such that one would strongly expect AI systems significantly more intelligent than humans to be possible. For an implicitly joking depiction of common arguments for the impossibility of superintelligence see [this article](https://arxiv.org/abs/1703.10987). Conditional on technological progress continuing it seems extremely likely that at some point humanity will build superintelligent machines. There is a clear economic incentive to develop more and more intelligent machines and currently billions of dollars of funding are being deployed for advancing AI capabilities. Computers are already superhuman at a variety of tasks such as arithmetic and classifying images and one would expect the number of tasks that machines are capable of performing to continue growing and lead to AI systems far more capable than humans in many domains, especially once AI starts making significant contributions to developing better AI systems. The main reason for why we might never build superintelligent AI then is that humanity went extinct before developing the technology or stopped technological progress for some other reason. For an analysis of existential risks which could cause such a scenario see [The Precipice](https://theprecipice.com/).\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Humanity hasn't yet built a superintelligence, and we might not be able to without significantly more knowledge and computational resources. There could be an existential catastrophe that prevents us from ever building one. For the rest of the answer let's assume no such event stops technological progress.\n\nWith that out of the way: there is no known good theoretical reason we can't build it at some point in the future; the majority of AI research is geared towards making more capable AI systems; and a significant chunk of top-level AI research attempts to make more generally capable AI systems. There is a clear economic incentive to develop more and more intelligent machines and currently billions of dollars of funding are being deployed for advancing AI capabilities.\n\nWe consider ourselves to be generally intelligent (i.e. capable of learning and adapting ourselves to a very wide range of tasks and environments), but the human brain almost certainly isn't the most efficient way to solve problems. One hint is the existence of AI systems with superhuman capabilities at narrow tasks. Not only superhuman performance (as in, [AlphaGo beating the Go world champion](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol)) but superhuman *speed* and *precision* (as in, [industrial sorting machines](https://www.youtube.com/watch?v=j4RWJTs0QCk)). There is no known discontinuity between tasks, something *special* and *unique* about human brains that unlocks certain capabilities which cannot be implemented in machines in principle. Therefore we would expect AI to surpass human performance on all tasks as progress continues.\n\nIn addition, several research groups (DeepMind being one of the most [overt about this](https://deepmind.com/about)) explicitly aim for generally capable systems. AI as a field is [growing](https://aiindex.stanford.edu/vibrancy/), year after year. Critical voices about AI progress usually argue against a lack of precautions around the impact of AI, or against general AI happening very soon, not against it happening *at all*.\n\nA satire of arguments against the possibility of superintelligence can be found [here](https://arxiv.org/abs/1703.10987). \n\nHumans provide an existence proof for the physical possibility of intelligent systems and there are many advantages computers have (like processing speed and size) such that one would strongly expect AI systems significantly more intelligent than humans to be possible. For an implicitly joking depiction of common arguments for the impossibility of superintelligence see [this article](https://arxiv.org/abs/1703.10987). Conditional on technological progress continuing it seems extremely likely that at some point humanity will build superintelligent machines. There is a clear economic incentive to develop more and more intelligent machines and currently billions of dollars of funding are being deployed for advancing AI capabilities. Computers are already superhuman at a variety of tasks such as arithmetic and classifying images and one would expect the number of tasks that machines are capable of performing to continue growing and lead to AI systems far more capable than humans in many domains, especially once AI starts making significant contributions to developing better AI systems. The main reason for why we might never build superintelligent AI then is that humanity went extinct before developing the technology or stopped technological progress for some other reason. For an analysis of existential risks which could cause such a scenario see [The Precipice](https://theprecipice.com/).\n\n", "Stamp Count": 0, "Multi Answer": true, "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7565", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:38.943+01:00", "Request Count": "", "Number of suggestions on answer doc": 53, "Total character count of suggestions on answer doc": 7743, "Helpful": ""}}, {"id": "i-f51327f7872b574c7aeb44e90999ef10fd73b6cc26eed5572994fb2ffbb51142", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-f51327f7872b574c7aeb44e90999ef10fd73b6cc26eed5572994fb2ffbb51142", "name": "Will there be a discontinuity in AI capabilities? If so, at what stage?", "index": 229, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:27.110Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-f51327f7872b574c7aeb44e90999ef10fd73b6cc26eed5572994fb2ffbb51142", "values": {"File": "Will there be a discontinuity in AI capabilities? If so, at what stage?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Will there be a discontinuity in AI capabilities? If so, at what stage?", "Link": "https://docs.google.com/document/d/1xj8Rfn9U-8JpWGTJALw7sXj8hywN5khRAin2ZT9XCgM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:29.523+01:00", "Related Answers DO NOT EDIT": "What is an \"intelligence explosion\"?", "Tags": "AI Takeoff", "Doc Last Edited": "2023-02-22T22:55:27.297+01:00", "Status": "In progress", "Edit Answer": "Will there be a discontinuity in AI capabilities? If so, at what stage?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7729", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "Will there be a discontinuity in AI capabilities? If so, at what stage?", "Source": "LessWrong", "All Phrasings": "Will there be a discontinuity in AI capabilities? If so, at what stage?\n", "Initial Order": "", "Related IDs": "6306", "Rich Text DO NOT EDIT": "There are debates about how discontinuous advances in AI capabilities are likely to be.\n\nSome, like Paul Christiano, [expect](https://sideways-view.com/2018/02/24/takeoff-speeds/) to see the world transformed by increasingly powerful AI systems over the course of several years (though Christiano thinks faster takeoff is also likely) with a continuously accelerating effect on the pace of life.\n\nChristiano describes slow takeoff as \"a complete 4 year interval in which world output doubles, before the first 1 year interval in which world output doubles. (Similarly, we\u2019ll see an 8 year doubling before a 2 year doubling, etc.)\". That said, a \u2018continuous takeoff\u2019 is [not necessarily slow.](https://www.lesswrong.com/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff#Continuous_doesn_t_necessarily_mean_slow) We could see gradual advances in AI capabilities before radical economic changes, which has led some to suggest alternative (and somewhat less precise) [metrics](https://www.lesswrong.com/posts/n3w3ww9Xuf8SngBfE/replacement-for-ponr-concept) for forecasting jumps in AI capabilities.\n\nOthers, like Eliezer Yudkowsky, expect AI to have [relatively little effect](https://www.lesswrong.com/posts/nWCokT9xbrY4p98co/heretical-thoughts-on-ai-by-eli-dourado) on global GDP before a discontinuous \"intelligence explosion\" that takes place faster than humans can usefully react.\n\nDifferent views on takeoff speeds have [different implications](https://www.lesswrong.com/posts/hRohhttbtpY3SHmmD/takeoff-speeds-have-a-huge-effect-on-what-it-means-to-work-1) for how (and potentially whether) to work on AI safety. Yudkowsky and Christiano had a public debate about the matter which is summarized [here](https://astralcodexten.substack.com/p/yudkowsky-contra-christiano-on-ai?s=r).\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "There are debates about how discontinuous advances in AI capabilities are likely to be.\n\nSome, like Paul Christiano, [expect](https://sideways-view.com/2018/02/24/takeoff-speeds/) to see the world transformed by increasingly powerful AI systems over the course of several years (though Christiano thinks faster takeoff is also likely) with a continuously accelerating effect on the pace of life.\n\nChristiano describes slow takeoff as \"a complete 4 year interval in which world output doubles, before the first 1 year interval in which world output doubles. (Similarly, we\u2019ll see an 8 year doubling before a 2 year doubling, etc.)\". That said, a \u2018continuous takeoff\u2019 is [not necessarily slow.](https://www.lesswrong.com/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff#Continuous_doesn_t_necessarily_mean_slow) We could see gradual advances in AI capabilities before radical economic changes, which has led some to suggest alternative (and somewhat less precise) [metrics](https://www.lesswrong.com/posts/n3w3ww9Xuf8SngBfE/replacement-for-ponr-concept) for forecasting jumps in AI capabilities.\n\nOthers, like Eliezer Yudkowsky, expect AI to have [relatively little effect](https://www.lesswrong.com/posts/nWCokT9xbrY4p98co/heretical-thoughts-on-ai-by-eli-dourado) on global GDP before a discontinuous \"intelligence explosion\" that takes place faster than humans can usefully react.\n\nDifferent views on takeoff speeds have [different implications](https://www.lesswrong.com/posts/hRohhttbtpY3SHmmD/takeoff-speeds-have-a-huge-effect-on-what-it-means-to-work-1) for how (and potentially whether) to work on AI safety. Yudkowsky and Christiano had a public debate about the matter which is summarized [here](https://astralcodexten.substack.com/p/yudkowsky-contra-christiano-on-ai?s=r).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7729", "Related Answers": "What is an \"intelligence explosion\"?", "Doc Last Ingested": "2023-03-14T23:39:42.142+01:00", "Request Count": "", "Number of suggestions on answer doc": 53, "Total character count of suggestions on answer doc": 7743, "Helpful": ""}}, {"id": "i-7cb49aa85e5ae05d6cbd18d9c1cd05589fe9d95f945060557d87e597fd4e5f5f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7cb49aa85e5ae05d6cbd18d9c1cd05589fe9d95f945060557d87e597fd4e5f5f", "name": "Will superintelligence make a large part of humanity unemployable?", "index": 230, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:30.771Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7cb49aa85e5ae05d6cbd18d9c1cd05589fe9d95f945060557d87e597fd4e5f5f", "values": {"File": "Will superintelligence make a large part of humanity unemployable?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Will superintelligence make a large part of humanity unemployable?", "Link": "https://docs.google.com/document/d/1la9ZjEnoqCzJrBaMBH2s8iAxJ9NkQBamm-AvvgGv5Mw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:25.937+01:00", "Related Answers DO NOT EDIT": "Isn't the real concern technological unemployment?", "Tags": "Automation,Technological Unemployment", "Doc Last Edited": "2023-02-22T22:55:28.297+01:00", "Status": "In progress", "Edit Answer": "Will superintelligence make a large part of humanity unemployable?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5641", "Source Link": "", "aisafety.info Link": "Will superintelligence make a large part of humanity unemployable?", "Source": "Wiki", "All Phrasings": "Will superintelligence make a large part of humanity unemployable?\n", "Initial Order": "", "Related IDs": "6412", "Rich Text DO NOT EDIT": "So, let\u2019s first ignore superintelligence and consider the case of AIs that are merely perfect replacements for all human labour.\n\nMany economists dismiss the claim that automation can cause general unemployment, and will often mention the \u201cLump of labour\u201d fallacy, which is the idea that there is a finite amount of jobs in the world, which automation will slowly winnow away. They note that though automation has caused unemployment in particular sectors, increased efficiency has freed up resources which can be used to employ more humans in those areas where machines cannot yet replace their labour. And historically, this has more than made up for the jobs that were eliminated.\n\nHowever, AI is different from other sorts of automation in that it is of general applicability. If we consider these AIs perfect replacements for human labour then standard labour models predict human wages will decline until they are competitive with the cost of running an AI. If this cost is below subsistence, then this would cause unemployment. However, in scenarios where some humans still have capital, they may prefer human workers for signalling or other reasons even if AIs are better and cheaper.\n\nNow that we\u2019ve talked about perfect replacements for human labour, we can talk about superintelligence. A superintelligence would quickly acquire material power, and we think superior material power to that held by any human or collection of humans. At that point, thinking in terms of employment is likely beside the point. The post-superintelligence world will reflect the preferences of the AI/AIs. If it prefers that humans exist, we will. If it prefers that we have jobs, we will. This depends on what the superintelligence in question wants to happen. If AIs want humans to continue being employable, they\u2019ll act to ensure humans remain employable by setting up roles that only biological humans can fill, artificially perpetuating the need for employing humans.\n\n", "Tag Count": 2, "Related Answer Count": 1, "Rich Text": "So, let\u2019s first ignore superintelligence and consider the case of AIs that are merely perfect replacements for all human labour.\n\nMany economists dismiss the claim that automation can cause general unemployment, and will often mention the \u201cLump of labour\u201d fallacy, which is the idea that there is a finite amount of jobs in the world, which automation will slowly winnow away. They note that though automation has caused unemployment in particular sectors, increased efficiency has freed up resources which can be used to employ more humans in those areas where machines cannot yet replace their labour. And historically, this has more than made up for the jobs that were eliminated.\n\nHowever, AI is different from other sorts of automation in that it is of general applicability. If we consider these AIs perfect replacements for human labour then standard labour models predict human wages will decline until they are competitive with the cost of running an AI. If this cost is below subsistence, then this would cause unemployment. However, in scenarios where some humans still have capital, they may prefer human workers for signalling or other reasons even if AIs are better and cheaper.\n\nNow that we\u2019ve talked about perfect replacements for human labour, we can talk about superintelligence. A superintelligence would quickly acquire material power, and we think superior material power to that held by any human or collection of humans. At that point, thinking in terms of employment is likely beside the point. The post-superintelligence world will reflect the preferences of the AI/AIs. If it prefers that humans exist, we will. If it prefers that we have jobs, we will. This depends on what the superintelligence in question wants to happen. If AIs want humans to continue being employable, they\u2019ll act to ensure humans remain employable by setting up roles that only biological humans can fill, artificially perpetuating the need for employing humans.\n\n", "Stamp Count": 0, "Multi Answer": true, "Stamped By": "", "Priority": 3, "Asker": "Crass Stupor", "External Source": "", "Last Asked On Discord": "", "UI ID": "5641", "Related Answers": "Isn't the real concern technological unemployment?", "Doc Last Ingested": "2023-03-14T23:39:44.047+01:00", "Request Count": "", "Number of suggestions on answer doc": 53, "Total character count of suggestions on answer doc": 7743, "Helpful": ""}}, {"id": "i-b2f7df41b7ee07d322256cdfe63396199e427f038e7f7ada43aa61de802dd564", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b2f7df41b7ee07d322256cdfe63396199e427f038e7f7ada43aa61de802dd564", "name": "Will an aligned superintelligence care about animals other than humans?", "index": 231, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:33.468Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b2f7df41b7ee07d322256cdfe63396199e427f038e7f7ada43aa61de802dd564", "values": {"File": "Will an aligned superintelligence care about animals other than humans?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Will an aligned superintelligence care about animals other than humans?", "Link": "https://docs.google.com/document/d/1bENYMtA65g7bSt1Sd_hXjE4yVYqqhr4jOzjo8bLQKcI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:22.296+01:00", "Related Answers DO NOT EDIT": "What are \"human values\"?", "Tags": "", "Doc Last Edited": "2023-02-22T23:05:12.955+01:00", "Status": "Live on site", "Edit Answer": "Will an aligned superintelligence care about animals other than humans?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7617", "Source Link": "", "aisafety.info Link": "Will an aligned superintelligence care about animals other than humans?", "Source": "Wiki", "All Phrasings": "Will an aligned superintelligence care about animals other than humans?\n", "Initial Order": "", "Related IDs": "7594", "Rich Text DO NOT EDIT": "An aligned superintelligence will have a set of human values. As mentioned in [What are \"human values\"?](http://What_are_\"human_values\"?) the set of values are complex, which means that the implementation of these values will decide whether the superintelligence cares about nonhuman animals. In [AI Ethics and Value Alignment for Nonhuman Animals](https://www.mdpi.com/2409-9287/6/2/31/htm) Soenke Ziesche argues that the alignment should include the values of nonhuman animals.\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "An aligned superintelligence will have a set of human values. As mentioned in [What are \"human values\"?](http://What_are_\"human_values\"?) the set of values are complex, which means that the implementation of these values will decide whether the superintelligence cares about nonhuman animals. In [AI Ethics and Value Alignment for Nonhuman Animals](https://www.mdpi.com/2409-9287/6/2/31/htm) Soenke Ziesche argues that the alignment should include the values of nonhuman animals.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7617", "Related Answers": "What are \"human values\"?", "Doc Last Ingested": "2023-03-14T23:39:45.995+01:00", "Request Count": "", "Number of suggestions on answer doc": 53, "Total character count of suggestions on answer doc": 7743, "Helpful": ""}}, {"id": "i-11eb4319cb3b2b576d9210422f088f55fb13554add5b233d19e75d05a291d398", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-11eb4319cb3b2b576d9210422f088f55fb13554add5b233d19e75d05a291d398", "name": "Will AI learn to be independent from people or will it always ask for our orders?", "index": 232, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:35.885Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-11eb4319cb3b2b576d9210422f088f55fb13554add5b233d19e75d05a291d398", "values": {"File": "Will AI learn to be independent from people or will it always ask for our orders?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Will AI learn to be independent from people or will it always ask for our orders?", "Link": "https://docs.google.com/document/d/1vuGvqPfquUlWg-GAklJfS8pUC8IiYOGHmOdU6ugnU-U/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:18.237+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:55:29.285+01:00", "Status": "In progress", "Edit Answer": "Will AI learn to be independent from people or will it always ask for our orders?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5638", "Source Link": "", "aisafety.info Link": "Will AI learn to be independent from people or will it always ask for our orders?", "Source": "Wiki", "All Phrasings": "Will AI learn to be independent from people or will it always ask for our orders?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "This depends on how we will program it. It definitely can be autonomous, even now, we have some autonomous vehicles or flight control systems and many more.\n\nEven though it's possible to build such systems, it may be better if they actively ask humans for supervision, for example in cases where they are uncertain what to do.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "This depends on how we will program it. It definitely can be autonomous, even now, we have some autonomous vehicles or flight control systems and many more.\n\nEven though it's possible to build such systems, it may be better if they actively ask humans for supervision, for example in cases where they are uncertain what to do.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "anonymous", "External Source": "", "Last Asked On Discord": "", "UI ID": "5638", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:47.132+01:00", "Request Count": "", "Number of suggestions on answer doc": 53, "Total character count of suggestions on answer doc": 7743, "Helpful": ""}}, {"id": "i-24625f2952b6cc2c2b742af4cac0ae142dadaba79211fa8812e7b84fb071840e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-24625f2952b6cc2c2b742af4cac0ae142dadaba79211fa8812e7b84fb071840e", "name": "Will AGI be agentic?", "index": 233, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:38.269Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-24625f2952b6cc2c2b742af4cac0ae142dadaba79211fa8812e7b84fb071840e", "values": {"File": "Will AGI be agentic?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Will AGI be agentic?", "Link": "https://docs.google.com/document/d/14DXOYjAC663W-sxlc_DB9ckUkX-JtDYTTt-ZitZPtls/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:14.434+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-03T13:57:43.062+01:00", "Status": "In progress", "Edit Answer": "Will AGI be agentic?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7591", "Source Link": "", "aisafety.info Link": "Will AGI be agentic?", "Source": "Wiki", "All Phrasings": "Will AGI be agentic?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n- What is an agent?\n\n    - Predictability of outcomes (based on goals) without predictability of the actions leading there\n\n    - \u201cAgentic stance\u201d is maybe a philosophy term describing something useful here\n\n    - An optimizer is similar to an agent\n\n- Gradient descent is a kind of agent since it lets you predict end states without knowing the path it will take\n\n    - Properties (persistant scalable)\n\n- Two angles:\n\n- 1. Will we build agents. Yes people already are (like RL systems)\n\n- Some arguments that agents are unusually useful, since tool AI with human in the loop will act slower. Possibly also higher quality cognition\n\n- Same problem for oracle AGI .\n\n- 2. Agents by mistake/accident . gradient may discover agents since they are good at solving the problems - this an example of a mesa-optimizer\n\n- \n\n- \n\n- Even if some people build nonagent AGI, if later someone else builds an agent AGI then the existential risk will be present.\n\n- \n\n- Agency gives an economic advantage over tool AIs\n\n- \n\n- Ajeya Cotra about how language models could theoretically become agent-y. She seems to not completely rule that out, but also doesn't seem to think that it's particularly likely to happen: [https://youtu.be/FIYOtZW8yEM?t=1150](https://youtu.be/FIYOtZW8yEM?t=1150)\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n- What is an agent?\n\n    - Predictability of outcomes (based on goals) without predictability of the actions leading there\n\n    - \u201cAgentic stance\u201d is maybe a philosophy term describing something useful here\n\n    - An optimizer is similar to an agent\n\n- Gradient descent is a kind of agent since it lets you predict end states without knowing the path it will take\n\n    - Properties (persistant scalable)\n\n- Two angles:\n\n- 1. Will we build agents. Yes people already are (like RL systems)\n\n- Some arguments that agents are unusually useful, since tool AI with human in the loop will act slower. Possibly also higher quality cognition\n\n- Same problem for oracle AGI .\n\n- 2. Agents by mistake/accident . gradient may discover agents since they are good at solving the problems - this an example of a mesa-optimizer\n\n- \n\n- \n\n- Even if some people build nonagent AGI, if later someone else builds an agent AGI then the existential risk will be present.\n\n- \n\n- Agency gives an economic advantage over tool AIs\n\n- \n\n- Ajeya Cotra about how language models could theoretically become agent-y. She seems to not completely rule that out, but also doesn't seem to think that it's particularly likely to happen: [https://youtu.be/FIYOtZW8yEM?t=1150](https://youtu.be/FIYOtZW8yEM?t=1150)\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7591", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:48.968+01:00", "Request Count": "", "Number of suggestions on answer doc": 54, "Total character count of suggestions on answer doc": 7743, "Helpful": ""}}, {"id": "i-124faa33618e341bcea7d664013b58416705bd80e405fe1820567f518cba28d9", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-124faa33618e341bcea7d664013b58416705bd80e405fe1820567f518cba28d9", "name": "Why would we only get one chance to align a superintelligence?", "index": 234, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:40.579Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-124faa33618e341bcea7d664013b58416705bd80e405fe1820567f518cba28d9", "values": {"File": "Why would we only get one chance to align a superintelligence?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why would we only get one chance to align a superintelligence?", "Link": "https://docs.google.com/document/d/1bYufIvyxyPvvR3hCONtuvxL8IKSJefOp_MRI_cSWG5M/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:10.483+01:00", "Related Answers DO NOT EDIT": "Why can't we just turn the AI off if it starts to misbehave?", "Tags": "Recursive Self-improvement,Superintelligence,Intelligence Explosion,Difficulty of Alignment", "Doc Last Edited": "2023-02-22T23:05:14.256+01:00", "Status": "Live on site", "Edit Answer": "Why would we only get one chance to align a superintelligence?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8157", "Source Link": "", "aisafety.info Link": "Why would we only get one chance to align a superintelligence?", "Source": "Wiki", "All Phrasings": "Why would we only get one chance to align a superintelligence?\n", "Initial Order": "", "Related IDs": "3119", "Rich Text DO NOT EDIT": "An AGI which has [recursively self-improved into a superintelligence](https://docs.google.com/document/d/1Qd9PlZtTQSEYDychmvrdifRV9E52_PsQZBhZpOWeAPQ/edit?usp=drivesdk) would be capable of either resisting our attempts to modify incorrectly specified goals, or realizing it was still weaker than us and acting [deceptively aligned](https://docs.google.com/document/d/1Q4YLETmql8Jcwulp5CT7f2t_oMcslWJBplEOo9P9IJI/edit?usp=drivesdk) until it was highly sure it could win in a confrontation. AGI would likely prevent a human from shutting it down unless the AGI was designed to be [corrigible](https://www.lesswrong.com/tag/corrigibility). \n\n", "Tag Count": 4, "Related Answer Count": 1, "Rich Text": "An AGI which has [recursively self-improved into a superintelligence](https://docs.google.com/document/d/1Qd9PlZtTQSEYDychmvrdifRV9E52_PsQZBhZpOWeAPQ/edit?usp=drivesdk) would be capable of either resisting our attempts to modify incorrectly specified goals, or realizing it was still weaker than us and acting [deceptively aligned](https://docs.google.com/document/d/1Q4YLETmql8Jcwulp5CT7f2t_oMcslWJBplEOo9P9IJI/edit?usp=drivesdk) until it was highly sure it could win in a confrontation. AGI would likely prevent a human from shutting it down unless the AGI was designed to be [corrigible](https://www.lesswrong.com/tag/corrigibility). \n\n", "Stamp Count": 2, "Multi Answer": "", "Stamped By": "plex\ntayler6000", "Priority": 2, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "8157", "Related Answers": "Why can't we just turn the AI off if it starts to misbehave?", "Doc Last Ingested": "2023-03-14T23:39:50.398+01:00", "Request Count": "", "Number of suggestions on answer doc": 54, "Total character count of suggestions on answer doc": 7743, "Helpful": ""}}, {"id": "i-1668ffa96507bb14c7b02ee7be20cdedf9deee9802fbbbbfeecf4de4c410be94", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1668ffa96507bb14c7b02ee7be20cdedf9deee9802fbbbbfeecf4de4c410be94", "name": "Why would great intelligence produce great power?", "index": 235, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:42.933Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1668ffa96507bb14c7b02ee7be20cdedf9deee9802fbbbbfeecf4de4c410be94", "values": {"File": "Why would great intelligence produce great power?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why would great intelligence produce great power?", "Link": "https://docs.google.com/document/d/1IzF68_3SweXq357lG2gcg7WzgK6uYWxplXaS7tgHi1g/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:07.196+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Intelligence", "Doc Last Edited": "2023-02-22T23:05:15.104+01:00", "Status": "Live on site", "Edit Answer": "Why would great intelligence produce great power?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6603", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "Why would great intelligence produce great power?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "Why would great intelligence produce great power?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Intelligence is powerful. One might say that \u201cIntelligence is no match for a gun, or for someone with lots of money,\u201d but both guns and money were produced by intelligence. If not for our intelligence, humans would still be foraging the savannah for food.\n\nIntelligence is what caused humans to dominate the planet in the blink of an eye (on evolutionary timescales). Intelligence is what allows us to eradicate diseases, and what gives us the potential to eradicate ourselves with nuclear war. Intelligence gives us superior strategic skills, superior social skills, superior economic productivity, and the power of invention.\n\nA machine with superintelligence would be able to hack into vulnerable networks via the internet, commandeer those resources for additional computing power, take over mobile machines connected to networks connected to the internet, use them to build additional machines, perform scientific experiments to understand the world better than humans can, invent quantum computing and nanotechnology, manipulate the social world better than we can, and do whatever it can to give itself more power to achieve its goals \u2014 all at a speed much faster than humans can respond to.\n\nSee also\n\n- Legg (2008). [Machine Super Intelligence](http://www.vetta.org/documents/Machine_Super_Intelligence.pdf). PhD Thesis. IDSIA.\n\n- Yudkowsky (2007). [The Power of Intelligence](http://yudkowsky.net/singularity/power).\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Intelligence is powerful. One might say that \u201cIntelligence is no match for a gun, or for someone with lots of money,\u201d but both guns and money were produced by intelligence. If not for our intelligence, humans would still be foraging the savannah for food.\n\nIntelligence is what caused humans to dominate the planet in the blink of an eye (on evolutionary timescales). Intelligence is what allows us to eradicate diseases, and what gives us the potential to eradicate ourselves with nuclear war. Intelligence gives us superior strategic skills, superior social skills, superior economic productivity, and the power of invention.\n\nA machine with superintelligence would be able to hack into vulnerable networks via the internet, commandeer those resources for additional computing power, take over mobile machines connected to networks connected to the internet, use them to build additional machines, perform scientific experiments to understand the world better than humans can, invent quantum computing and nanotechnology, manipulate the social world better than we can, and do whatever it can to give itself more power to achieve its goals \u2014 all at a speed much faster than humans can respond to.\n\nSee also\n\n- Legg (2008). [Machine Super Intelligence](http://www.vetta.org/documents/Machine_Super_Intelligence.pdf). PhD Thesis. IDSIA.\n\n- Yudkowsky (2007). [The Power of Intelligence](http://yudkowsky.net/singularity/power).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "", "UI ID": "6603", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:52.702+01:00", "Request Count": "", "Number of suggestions on answer doc": 54, "Total character count of suggestions on answer doc": 7743, "Helpful": ""}}, {"id": "i-830e00453aa6d570ffc8f4cd133d41f39a59a1499af95c94e89796bdc50df72f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-830e00453aa6d570ffc8f4cd133d41f39a59a1499af95c94e89796bdc50df72f", "name": "Why work on AI safety early?", "index": 236, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:51:45.326Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-830e00453aa6d570ffc8f4cd133d41f39a59a1499af95c94e89796bdc50df72f", "values": {"File": "Why work on AI safety early?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why work on AI safety early?", "Link": "https://docs.google.com/document/d/14nBGUWcBAGL8j9eEXjO-CDDLMrxShnhMO5S8jRP_j5w/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:03.786+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Timelines,Plausibility", "Doc Last Edited": "2023-03-03T18:50:29.053+01:00", "Status": "In progress", "Edit Answer": "Why work on AI safety early?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6303", "Source Link": "https://intelligence.org/faq/", "aisafety.info Link": "Why work on AI safety early?", "Source": "MIRI FAQ", "All Phrasings": "Why work on AI safety early?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n1. \n\n1. \n\n    1. \n\nSome points from this paper on [unsolved problems in AI safety](https://doi.org/10.48550/arxiv.2109.13916):\n\nA study in the military was conducted which showed that 75% of the critical decisions which would later define the safety and security of a system were made early on in the development of that system. An example of this is the internet which was made mostly for use by academics. Little thought was given to how to make the early internet secure, and now decades later making the internet secure is extremely difficult and complex.\n\nWe want safety solutions to have withstood the test of time when we finally deploy in the critical moments of AGI development. We don\u2019t want to be using cutting edge methods that haven\u2019t undergone extensive testing to be what we finally use to try and solve alignment. Doing so exposes us to a high risk of unforeseen failures that a long period of testing and development may have uncovered.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "\n\n1. \n\n1. \n\n    1. \n\nSome points from this paper on [unsolved problems in AI safety](https://doi.org/10.48550/arxiv.2109.13916):\n\nA study in the military was conducted which showed that 75% of the critical decisions which would later define the safety and security of a system were made early on in the development of that system. An example of this is the internet which was made mostly for use by academics. Little thought was given to how to make the early internet secure, and now decades later making the internet secure is extremely difficult and complex.\n\nWe want safety solutions to have withstood the test of time when we finally deploy in the critical moments of AGI development. We don\u2019t want to be using cutting edge methods that haven\u2019t undergone extensive testing to be what we finally use to try and solve alignment. Doing so exposes us to a high risk of unforeseen failures that a long period of testing and development may have uncovered.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "6303", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:39:55.017+01:00", "Request Count": "", "Number of suggestions on answer doc": 55, "Total character count of suggestions on answer doc": 11275, "Helpful": ""}}, {"id": "i-5d2dff0bc4f7658ed73a7587a6d529246a05134758a4dd4d15ebdce33e094c46", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5d2dff0bc4f7658ed73a7587a6d529246a05134758a4dd4d15ebdce33e094c46", "name": "Why think that AI can outperform humans?", "index": 237, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T23:05:27.460Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5d2dff0bc4f7658ed73a7587a6d529246a05134758a4dd4d15ebdce33e094c46", "values": {"File": "Why think that AI can outperform humans?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why think that AI can outperform humans?", "Link": "https://docs.google.com/document/d/1SuWB1QIRjZnE18UroRyj6eVEbLGxBDFR4thC2ymXAGg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:56:00.161+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI,Superintelligence", "Doc Last Edited": "2023-02-22T23:05:16.210+01:00", "Status": "Live on site", "Edit Answer": "Why think that AI can outperform humans?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6295", "Source Link": "https://intelligence.org/faq/", "aisafety.info Link": "Why think that AI can outperform humans?", "Source": "MIRI FAQ", "All Phrasings": "Why think that AI can outperform humans?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Machines are already smarter than humans are at many specific tasks: performing calculations, playing chess, searching large databanks, detecting underwater mines, and more. However, human intelligence continues to dominate machine intelligence in generality.\n\nA powerful chess computer is \u201cnarrow\u201d: it can\u2019t play other games. In contrast, humans have problem-solving abilities that allow us to adapt to new contexts and excel in many domains other than what the ancestral environment prepared us for.\n\nIn the absence of a [formal definition of \u201cintelligence\u201d](https://intelligence.org/2013/06/19/what-is-intelligence-2/) (and therefore of [\u201cartificial intelligence\u201d](https://intelligence.org/2013/08/11/what-is-agi/)), we can heuristically cite humans\u2019 perceptual, inferential, and deliberative faculties (as opposed to, e.g., our physical strength or agility) and say that intelligence is \u201cthose kinds of things.\u201d On this conception, intelligence is a bundle of distinct faculties \u2014 albeit a very important bundle that includes our capacity for science.\n\nOur cognitive abilities stem from high-level patterns in our brains, and these patterns can be instantiated in silicon as well as carbon. This tells us that general AI is possible, though it doesn\u2019t tell us how difficult it is. If intelligence is sufficiently difficult to understand, then we may arrive at machine intelligence by scanning and emulating human brains or by some trial-and-error process (like evolution), rather than by hand-coding a software agent.\n\nIf machines can achieve human equivalence in cognitive tasks, then it is very likely that they can eventually outperform humans. There is little reason to expect that biological evolution, with its lack of foresight and planning, would have hit upon the optimal algorithms for general intelligence (any more than it hit upon the optimal flying machine in birds). Beyond qualitative improvements in cognition, Nick Bostrom notes [more straightforward advantages we could realize in digital minds](http://aiimpacts.org/sources-of-advantage-for-artificial-intelligence/), e.g.:\n\n- editability \u2014 \u201cIt is easier to experiment with parameter variations in software than in neural wetware.\u201d\n\n- speed \u2014 \u201cThe speed of light is more than a million times greater than that of neural transmission, synaptic spikes dissipate more than a million times more heat than is thermodynamically necessary, and current transistor frequencies are more than a million times faster than neuron spiking frequencies.\u201d\n\n- serial depth \u2014 On short timescales, machines can carry out much longer sequential processes.\n\n- storage capacity \u2014 Computers can plausibly have greater working and long-term memory.\n\n- size \u2014 Computers can be much larger than a human brain.\n\n- duplicability \u2014 Copying software onto new hardware can be much faster and higher-fidelity than biological reproduction.\n\nAny one of these advantages could give an AI reasoner an edge over a human reasoner, or give a group of AI reasoners an edge over a human group. Their combination suggests that digital minds could surpass human minds more quickly and decisively than we might expect.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Machines are already smarter than humans are at many specific tasks: performing calculations, playing chess, searching large databanks, detecting underwater mines, and more. However, human intelligence continues to dominate machine intelligence in generality.\n\nA powerful chess computer is \u201cnarrow\u201d: it can\u2019t play other games. In contrast, humans have problem-solving abilities that allow us to adapt to new contexts and excel in many domains other than what the ancestral environment prepared us for.\n\nIn the absence of a [formal definition of \u201cintelligence\u201d](https://intelligence.org/2013/06/19/what-is-intelligence-2/) (and therefore of [\u201cartificial intelligence\u201d](https://intelligence.org/2013/08/11/what-is-agi/)), we can heuristically cite humans\u2019 perceptual, inferential, and deliberative faculties (as opposed to, e.g., our physical strength or agility) and say that intelligence is \u201cthose kinds of things.\u201d On this conception, intelligence is a bundle of distinct faculties \u2014 albeit a very important bundle that includes our capacity for science.\n\nOur cognitive abilities stem from high-level patterns in our brains, and these patterns can be instantiated in silicon as well as carbon. This tells us that general AI is possible, though it doesn\u2019t tell us how difficult it is. If intelligence is sufficiently difficult to understand, then we may arrive at machine intelligence by scanning and emulating human brains or by some trial-and-error process (like evolution), rather than by hand-coding a software agent.\n\nIf machines can achieve human equivalence in cognitive tasks, then it is very likely that they can eventually outperform humans. There is little reason to expect that biological evolution, with its lack of foresight and planning, would have hit upon the optimal algorithms for general intelligence (any more than it hit upon the optimal flying machine in birds). Beyond qualitative improvements in cognition, Nick Bostrom notes [more straightforward advantages we could realize in digital minds](http://aiimpacts.org/sources-of-advantage-for-artificial-intelligence/), e.g.:\n\n- editability \u2014 \u201cIt is easier to experiment with parameter variations in software than in neural wetware.\u201d\n\n- speed \u2014 \u201cThe speed of light is more than a million times greater than that of neural transmission, synaptic spikes dissipate more than a million times more heat than is thermodynamically necessary, and current transistor frequencies are more than a million times faster than neuron spiking frequencies.\u201d\n\n- serial depth \u2014 On short timescales, machines can carry out much longer sequential processes.\n\n- storage capacity \u2014 Computers can plausibly have greater working and long-term memory.\n\n- size \u2014 Computers can be much larger than a human brain.\n\n- duplicability \u2014 Copying software onto new hardware can be much faster and higher-fidelity than biological reproduction.\n\nAny one of these advantages could give an AI reasoner an edge over a human reasoner, or give a group of AI reasoners an edge over a human group. Their combination suggests that digital minds could surpass human minds more quickly and decisively than we might expect.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "6295", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:49:25.678+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 2837, "Helpful": ""}}, {"id": "i-7f91705fef1e000bdbdce3a025fab28208fa0bbf90ebe0b913ee44644469e9c6", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7f91705fef1e000bdbdce3a025fab28208fa0bbf90ebe0b913ee44644469e9c6", "name": "Why should we prepare for human-level AI technology now rather than decades down the line when it\u2019s closer?", "index": 238, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:54:38.707Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7f91705fef1e000bdbdce3a025fab28208fa0bbf90ebe0b913ee44644469e9c6", "values": {"File": "Why should we prepare for human-level AI technology now rather than decades down the line when it\u2019s closer?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why should we prepare for human-level AI technology now rather than decades down the line when it\u2019s closer?", "Link": "https://docs.google.com/document/d/1hOXuElTgWHLM_TzPNA9dN_9AEfg89Vak1QveZAyyisI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:56.450+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Timelines,AGI", "Doc Last Edited": "2023-02-22T23:05:17.051+01:00", "Status": "Live on site", "Edit Answer": "Why should we prepare for human-level AI technology now rather than decades down the line when it\u2019s closer?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6192", "Source Link": "https://futureoflife.org/ai-faqs/", "aisafety.info Link": "Why should we prepare for human-level AI technology now rather than decades down the line when it\u2019s closer?", "Source": "FLI's FAQ", "All Phrasings": "Why should we prepare for human-level AI technology now rather than decades down the line when it\u2019s closer?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "First, even \u201cnarrow\u201d AI systems, which approach or surpass human intelligence in a small set of capabilities (such as image or voice recognition) already raise important questions regarding their impact on society. Making autonomous vehicles safe, analyzing the strategic and ethical dimensions of autonomous weapons, and the effect of AI on the global employment and economic systems are three examples. Second, the longer-term implications of human or super-human artificial intelligence are dramatic, and there is no consensus on how quickly such capabilities will be developed. Many experts believe there is a chance it could happen rather soon, making it imperative to begin investigating long-term safety issues now, if only to get a better sense of how much early progress is actually possible.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "First, even \u201cnarrow\u201d AI systems, which approach or surpass human intelligence in a small set of capabilities (such as image or voice recognition) already raise important questions regarding their impact on society. Making autonomous vehicles safe, analyzing the strategic and ethical dimensions of autonomous weapons, and the effect of AI on the global employment and economic systems are three examples. Second, the longer-term implications of human or super-human artificial intelligence are dramatic, and there is no consensus on how quickly such capabilities will be developed. Many experts believe there is a chance it could happen rather soon, making it imperative to begin investigating long-term safety issues now, if only to get a better sense of how much early progress is actually possible.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "FLI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6192", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:42:04.137+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-324d994be6c54933b3e4179d70fc73cffe59a84cdf0a3748e139ed1e4a4bce15", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-324d994be6c54933b3e4179d70fc73cffe59a84cdf0a3748e139ed1e4a4bce15", "name": "Why should I worry about superintelligence?", "index": 239, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:54:43.254Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-324d994be6c54933b3e4179d70fc73cffe59a84cdf0a3748e139ed1e4a4bce15", "values": {"File": "Why should I worry about superintelligence?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why should I worry about superintelligence?", "Link": "https://docs.google.com/document/d/1-D3s6mUlnlPvLZ7OaRtdC0hK1UAw3xK-K_wCxJ-9obE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:52.965+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Superintelligence", "Doc Last Edited": "2023-02-22T23:05:17.970+01:00", "Status": "Live on site", "Edit Answer": "Why should I worry about superintelligence?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6209", "Source Link": "https://www.reddit.com/r/ControlProblem/wiki/faq", "aisafety.info Link": "Why should I worry about superintelligence?", "Source": "/r/ControlProblem's FAQ", "All Phrasings": "Why should I worry about superintelligence?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Intelligence is powerful. Because of superior intelligence, we humans have dominated the Earth. The fate of thousands of species depends on our actions, we occupy nearly every corner of the globe, and we repurpose vast amounts of the world's resources for our own use. Artificial Superintelligence (ASI) has potential to be vastly more intelligent than us, and therefore vastly more powerful. In the same way that we have reshaped the earth to fit our goals, an ASI will find unforeseen, highly efficient ways of reshaping reality to fit its goals.\n\nThe impact that an ASI will have on our world depends on what those goals are. We have the advantage of designing those goals, but that task is not as simple as it may first seem. As described by MIRI in their [Intelligence Explosion FAQ](https://intelligence.org/ie-faq/):\n\n\u201cA superintelligent machine will make decisions based on the mechanisms it is designed with, not the hopes its designers had in mind when they programmed those mechanisms. It will act only on precise specifications of rules and values, and will do so in ways that need not respect the complexity and subtlety of what humans value.\u201d\n\nIf we do not solve the Control Problem before the first ASI is created, we may not get another chance.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Intelligence is powerful. Because of superior intelligence, we humans have dominated the Earth. The fate of thousands of species depends on our actions, we occupy nearly every corner of the globe, and we repurpose vast amounts of the world's resources for our own use. Artificial Superintelligence (ASI) has potential to be vastly more intelligent than us, and therefore vastly more powerful. In the same way that we have reshaped the earth to fit our goals, an ASI will find unforeseen, highly efficient ways of reshaping reality to fit its goals.\n\nThe impact that an ASI will have on our world depends on what those goals are. We have the advantage of designing those goals, but that task is not as simple as it may first seem. As described by MIRI in their [Intelligence Explosion FAQ](https://intelligence.org/ie-faq/):\n\n\u201cA superintelligent machine will make decisions based on the mechanisms it is designed with, not the hopes its designers had in mind when they programmed those mechanisms. It will act only on precise specifications of rules and values, and will do so in ways that need not respect the complexity and subtlety of what humans value.\u201d\n\nIf we do not solve the Control Problem before the first ASI is created, we may not get another chance.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "Sophialb", "Priority": 5, "Asker": "NotaSentientAI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6209", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:42:05.867+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-e1ebfbe84ef62ce60502711ab5a9ab0dad440f2636edc99aa52cb821987a561e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e1ebfbe84ef62ce60502711ab5a9ab0dad440f2636edc99aa52cb821987a561e", "name": "Why might we expect a superintelligence to be hostile by default?", "index": 240, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:54:47.849Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e1ebfbe84ef62ce60502711ab5a9ab0dad440f2636edc99aa52cb821987a561e", "values": {"File": "Why might we expect a superintelligence to be hostile by default?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why might we expect a superintelligence to be hostile by default?", "Link": "https://docs.google.com/document/d/10phZYEAe898yRVfKVkG7vbt03QYK2Io6FfjJvaGkO3w/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:48.667+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Superintelligence", "Doc Last Edited": "2023-02-22T23:05:18.799+01:00", "Status": "Live on site", "Edit Answer": "Why might we expect a superintelligence to be hostile by default?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6982", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "Why might we expect a superintelligence to be hostile by default?", "Source": "Superintelligence FAQ", "All Phrasings": "Why might we expect a superintelligence to be hostile by default?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The argument goes: computers only do what we command them; no more, no less. So it might be bad if terrorists or enemy countries develop superintelligence first. But if we develop superintelligence first there\u2019s no problem. Just command it to do the things we want, right? Suppose we wanted a superintelligence to cure cancer. How might we specify the goal \u201ccure cancer\u201d? We couldn\u2019t guide it through every individual step; if we knew every individual step, then we could cure cancer ourselves. Instead, we would have to give it a final goal of curing cancer, and trust the superintelligence to come up with intermediate actions that furthered that goal. For example, a superintelligence might decide that the first step to curing cancer was learning more about protein folding, and set up some experiments to investigate protein folding patterns.\n\nA superintelligence would also need some level of common sense to decide which of various strategies to pursue. Suppose that investigating protein folding was very likely to cure 50% of cancers, but investigating genetic engineering was moderately likely to cure 90% of cancers. Which should the AI pursue? Presumably it would need some way to balance considerations like curing as much cancer as possible, as quickly as possible, with as high a probability of success as possible.\n\nBut a goal specified in this way would be very dangerous. Humans instinctively balance thousands of different considerations in everything they do; so far this hypothetical AI is only balancing three (least cancer, quickest results, highest probability). To a human, it would seem maniacally, even psychopathically, obsessed with cancer curing. If this were truly its goal structure, it would go wrong in almost comical ways. This type of problem, [specification gaming](https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity), has been observed in many AI systems.\n\nIf your only goal is \u201ccuring cancer\u201d, and you lack humans\u2019 instinct for the thousands of other important considerations, a relatively easy solution might be to hack into a nuclear base, launch all of its missiles, and kill everyone in the world. This satisfies all the AI\u2019s goals. It reduces cancer down to zero (which is better than medicines which work only some of the time). It\u2019s very fast (which is better than medicines which might take a long time to invent and distribute). And it has a high probability of success (medicines might or might not work; nukes definitely do).\n\nSo simple goal architectures are likely to go very wrong unless tempered by common sense and a broader understanding of what we do and do not value.\n\nEven if we do train the AI on an actually desirable goal, there is also the risk of the AI actually learning a different and undesirable objective. This problem is called [inner alignment](https://www.youtube.com/watch?v=bJLcIBixGj8).\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "The argument goes: computers only do what we command them; no more, no less. So it might be bad if terrorists or enemy countries develop superintelligence first. But if we develop superintelligence first there\u2019s no problem. Just command it to do the things we want, right? Suppose we wanted a superintelligence to cure cancer. How might we specify the goal \u201ccure cancer\u201d? We couldn\u2019t guide it through every individual step; if we knew every individual step, then we could cure cancer ourselves. Instead, we would have to give it a final goal of curing cancer, and trust the superintelligence to come up with intermediate actions that furthered that goal. For example, a superintelligence might decide that the first step to curing cancer was learning more about protein folding, and set up some experiments to investigate protein folding patterns.\n\nA superintelligence would also need some level of common sense to decide which of various strategies to pursue. Suppose that investigating protein folding was very likely to cure 50% of cancers, but investigating genetic engineering was moderately likely to cure 90% of cancers. Which should the AI pursue? Presumably it would need some way to balance considerations like curing as much cancer as possible, as quickly as possible, with as high a probability of success as possible.\n\nBut a goal specified in this way would be very dangerous. Humans instinctively balance thousands of different considerations in everything they do; so far this hypothetical AI is only balancing three (least cancer, quickest results, highest probability). To a human, it would seem maniacally, even psychopathically, obsessed with cancer curing. If this were truly its goal structure, it would go wrong in almost comical ways. This type of problem, [specification gaming](https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity), has been observed in many AI systems.\n\nIf your only goal is \u201ccuring cancer\u201d, and you lack humans\u2019 instinct for the thousands of other important considerations, a relatively easy solution might be to hack into a nuclear base, launch all of its missiles, and kill everyone in the world. This satisfies all the AI\u2019s goals. It reduces cancer down to zero (which is better than medicines which work only some of the time). It\u2019s very fast (which is better than medicines which might take a long time to invent and distribute). And it has a high probability of success (medicines might or might not work; nukes definitely do).\n\nSo simple goal architectures are likely to go very wrong unless tempered by common sense and a broader understanding of what we do and do not value.\n\nEven if we do train the AI on an actually desirable goal, there is also the risk of the AI actually learning a different and undesirable objective. This problem is called [inner alignment](https://www.youtube.com/watch?v=bJLcIBixGj8).\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "2023-02-26T18:55:44.133+01:00", "UI ID": "6982", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:42:07.615+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-b30ad4236abf13b5a3136574b27398590bbd6cfcaf071531eca4fafa2c0be010", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b30ad4236abf13b5a3136574b27398590bbd6cfcaf071531eca4fafa2c0be010", "name": "Why might we expect a moderate AI takeoff?", "index": 241, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:54:50.244Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b30ad4236abf13b5a3136574b27398590bbd6cfcaf071531eca4fafa2c0be010", "values": {"File": "Why might we expect a moderate AI takeoff?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why might we expect a moderate AI takeoff?", "Link": "https://docs.google.com/document/d/1EU66rsMp-pxc80lplBgVy_EFD7zncrY99BMROX9Z7l8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:44.798+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Plausibility,AI Takeoff", "Doc Last Edited": "2023-02-22T22:55:32.184+01:00", "Status": "In progress", "Edit Answer": "Why might we expect a moderate AI takeoff?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6960", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "Why might we expect a moderate AI takeoff?", "Source": "Superintelligence FAQ", "All Phrasings": "Why might we expect a moderate AI takeoff?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Because this is the history of computer Go, with fifty years added on to each date. In 1997, the best computer Go program in the world, Handtalk, won NT$250,000 for performing a previously impossible feat \u2013 beating an 11 year old child (with an 11-stone handicap penalizing the child and favoring the computer!) As late as September 2015, no computer had ever beaten any professional Go player in a fair game. Then in March 2016, a Go program beat 18-time world champion Lee Sedol 4-1 in a five game match. Go programs had gone from \u201cdumber than children\u201d to \u201csmarter than any human in the world\u201d in eighteen years, and \u201cfrom never won a professional game\u201d to \u201coverwhelming world champion\u201d in six months.\n\nThe slow takeoff scenario mentioned above is loading the dice. It theorizes a timeline where computers took fifteen years to go from \u201crat\u201d to \u201cchimp\u201d, but also took thirty-five years to go from \u201cchimp\u201d to \u201caverage human\u201d and fifty years to go from \u201caverage human\u201d to \u201cEinstein\u201d. But from an evolutionary perspective this is ridiculous. It took about fifty million years (and major redesigns in several brain structures!) to go from the first rat-like creatures to chimps. But it only took about five million years (and very minor changes in brain structure) to go from chimps to humans. And going from the average human to Einstein didn\u2019t even require evolutionary work \u2013 it\u2019s just the result of random variation in the existing structures!\n\nSo maybe our hypothetical IQ scale above is off. If we took an evolutionary and neuroscientific perspective, it would look more like flatworms at 10, rats at 30, chimps at 60, the village idiot at 90, the average human at 98, and Einstein at 100.\n\nSuppose that we start out, again, with computers as smart as rats in 2020. Now we get still get computers as smart as chimps in 2035. And we still get computers as smart as the village idiot in 2050. But now we get computers as smart as the average human in 2054, and computers as smart as Einstein in 2055. By 2060, we\u2019re getting the superintelligences as far beyond Einstein as Einstein is beyond a village idiot.\n\nThis offers a much shorter time window to react to AI developments. In the slow takeoff scenario, we figured we could wait until computers were as smart as humans before we had to start thinking about this; after all, that still gave us fifty years before computers were even as smart as Einstein. But in the moderate takeoff scenario, it gives us one year until Einstein and six years until superintelligence. That\u2019s starting to look like not enough time to be entirely sure we know what we\u2019re doing.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Because this is the history of computer Go, with fifty years added on to each date. In 1997, the best computer Go program in the world, Handtalk, won NT$250,000 for performing a previously impossible feat \u2013 beating an 11 year old child (with an 11-stone handicap penalizing the child and favoring the computer!) As late as September 2015, no computer had ever beaten any professional Go player in a fair game. Then in March 2016, a Go program beat 18-time world champion Lee Sedol 4-1 in a five game match. Go programs had gone from \u201cdumber than children\u201d to \u201csmarter than any human in the world\u201d in eighteen years, and \u201cfrom never won a professional game\u201d to \u201coverwhelming world champion\u201d in six months.\n\nThe slow takeoff scenario mentioned above is loading the dice. It theorizes a timeline where computers took fifteen years to go from \u201crat\u201d to \u201cchimp\u201d, but also took thirty-five years to go from \u201cchimp\u201d to \u201caverage human\u201d and fifty years to go from \u201caverage human\u201d to \u201cEinstein\u201d. But from an evolutionary perspective this is ridiculous. It took about fifty million years (and major redesigns in several brain structures!) to go from the first rat-like creatures to chimps. But it only took about five million years (and very minor changes in brain structure) to go from chimps to humans. And going from the average human to Einstein didn\u2019t even require evolutionary work \u2013 it\u2019s just the result of random variation in the existing structures!\n\nSo maybe our hypothetical IQ scale above is off. If we took an evolutionary and neuroscientific perspective, it would look more like flatworms at 10, rats at 30, chimps at 60, the village idiot at 90, the average human at 98, and Einstein at 100.\n\nSuppose that we start out, again, with computers as smart as rats in 2020. Now we get still get computers as smart as chimps in 2035. And we still get computers as smart as the village idiot in 2050. But now we get computers as smart as the average human in 2054, and computers as smart as Einstein in 2055. By 2060, we\u2019re getting the superintelligences as far beyond Einstein as Einstein is beyond a village idiot.\n\nThis offers a much shorter time window to react to AI developments. In the slow takeoff scenario, we figured we could wait until computers were as smart as humans before we had to start thinking about this; after all, that still gave us fifty years before computers were even as smart as Einstein. But in the moderate takeoff scenario, it gives us one year until Einstein and six years until superintelligence. That\u2019s starting to look like not enough time to be entirely sure we know what we\u2019re doing.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "", "UI ID": "6960", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:42:09.503+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-b7a17eccce12069b4098e76d7dab77b4e1b2c21aa0d118cf5b125c9827122966", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b7a17eccce12069b4098e76d7dab77b4e1b2c21aa0d118cf5b125c9827122966", "name": "Why might we expect a fast takeoff?", "index": 242, "createdAt": "2023-01-14T14:43:14.629Z", "updatedAt": "2023-03-14T22:54:55.919Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b7a17eccce12069b4098e76d7dab77b4e1b2c21aa0d118cf5b125c9827122966", "values": {"File": "Why might we expect a fast takeoff?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Why might we expect a fast takeoff?", "Link": "https://docs.google.com/document/d/1hA_IGO-Vms8e-3ajYxLjkFlMOZwAxBwNjQi-1SNQsvo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T14:55:41.213+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Plausibility,AI Takeoff", "Doc Last Edited": "2023-02-22T22:55:33.101+01:00", "Status": "In progress", "Edit Answer": "Why might we expect a fast takeoff?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6962", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "Why might we expect a fast takeoff?", "Source": "Superintelligence FAQ", "All Phrasings": "Why might we expect a fast takeoff?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "AlphaGo used about 0.5 petaflops (= trillion floating point operations per second) in its championship game. But the world\u2019s fastest supercomputer, TaihuLight, can calculate at almost 100 petaflops. So suppose Google developed a human-level AI on a computer system similar to AlphaGo, caught the attention of the Chinese government (who run TaihuLight), and they transfer the program to their much more powerful computer. What would happen?\n\nIt depends on to what degree intelligence benefits from more computational resources. This differs for different processes. For domain-general intelligence, it seems to benefit quite a bit \u2013 both across species and across human individuals, bigger brain size correlates with greater intelligence. This matches the evolutionarily rapid growth in intelligence from chimps to hominids to modern man; the few hundred thousand years since australopithecines weren\u2019t enough time to develop complicated new algorithms, and evolution seems to have just given humans bigger brains and packed more neurons and glia in per square inch. It\u2019s not really clear why the process stopped (if it ever did), but it might have to do with heads getting too big to fit through the birth canal. Cancer risk might also have been involved \u2013 scientists have found that smarter people are more likely to get brain cancer, possibly because they\u2019re already overclocking their ability to grow brain cells.\n\nAt least in neuroscience, once evolution \u201cdiscovered\u201d certain key insights, further increasing intelligence seems to have been a matter of providing it with more computing power. So again \u2013 what happens when we transfer the hypothetical human-level AI from AlphaGo to a TaihuLight-style supercomputer two hundred times more powerful? It might be a stretch to expect it to go from IQ 100 to IQ 20,000, but might it increase to an Einstein-level 200, or a superintelligent 300? Hard to say \u2013 but if Google ever does develop a human-level AI, the Chinese government will probably be interested in finding out.\n\nEven if its intelligence doesn\u2019t scale linearly, TaihuLight could give it more time. TaihuLight is two hundred times faster than AlphaGo. Transfer an AI from one to the other, and even if its intelligence didn\u2019t change \u2013 even if it had exactly the same thoughts \u2013 it would think them two hundred times faster. An Einstein-level AI on AlphaGo hardware might (like the historical Einstein) discover one revolutionary breakthrough every five years. Transfer it to TaihuLight, and it would work two hundred times faster \u2013 a revolutionary breakthrough every week.\n\nSupercomputers track Moore\u2019s Law; the top supercomputer of 2016 is a hundred times faster than the top supercomputer of 2006. If this progress continues, the top computer of 2026 will be a hundred times faster still. Run Einstein on that computer, and he will come up with a revolutionary breakthrough every few hours. Or something. At this point it becomes a little bit hard to imagine. All I know is that it only took one Einstein, at normal speed, to lay the theoretical foundation for nuclear weapons. Anything a thousand times faster than that is definitely cause for concern.\n\nThere\u2019s one final, very concerning reason to expect a fast takeoff. Suppose, once again, we have an AI as smart as Einstein. It might, like the historical Einstein, contemplate physics. Or it might contemplate an area very relevant to its own interests: artificial intelligence. In that case, instead of making a revolutionary physics breakthrough every few hours, it will make a revolutionary AI breakthrough every few hours. Each AI breakthrough it makes, it will have the opportunity to reprogram itself to take advantage of its discovery, becoming more intelligent, thus speeding up its breakthroughs further. The cycle will stop only when it reaches some physical limit \u2013 some technical challenge to further improvements that even an entity far smarter than Einstein cannot discover a way around.\n\nTo human programmers, such a cycle would look like a \u201ccritical mass\u201d. Before the critical level, any AI advance delivers only modest benefits. But any tiny improvement that pushes an AI above the critical level would result in a feedback loop of inexorable self-improvement all the way up to some stratospheric limit of possible computing power.\n\nThis feedback loop would be exponential; relatively slow in the beginning, but blindingly fast as it approaches an asymptote. Consider the AI which starts off making forty breakthroughs per year \u2013 one every nine days. Now suppose it gains on average a 10% speed improvement with each breakthrough. It starts on January 1. Its first breakthrough comes January 10 or so. Its second comes a little faster, January 18. Its third is a little faster still, January 25. By the beginning of February, it\u2019s sped up to producing one breakthrough every seven days, more or less. By the beginning of March, it\u2019s making about one breakthrough every three days or so. But by March 20, it\u2019s up to one breakthrough a day. By late on the night of March 29, it\u2019s making a breakthrough every second.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "AlphaGo used about 0.5 petaflops (= trillion floating point operations per second) in its championship game. But the world\u2019s fastest supercomputer, TaihuLight, can calculate at almost 100 petaflops. So suppose Google developed a human-level AI on a computer system similar to AlphaGo, caught the attention of the Chinese government (who run TaihuLight), and they transfer the program to their much more powerful computer. What would happen?\n\nIt depends on to what degree intelligence benefits from more computational resources. This differs for different processes. For domain-general intelligence, it seems to benefit quite a bit \u2013 both across species and across human individuals, bigger brain size correlates with greater intelligence. This matches the evolutionarily rapid growth in intelligence from chimps to hominids to modern man; the few hundred thousand years since australopithecines weren\u2019t enough time to develop complicated new algorithms, and evolution seems to have just given humans bigger brains and packed more neurons and glia in per square inch. It\u2019s not really clear why the process stopped (if it ever did), but it might have to do with heads getting too big to fit through the birth canal. Cancer risk might also have been involved \u2013 scientists have found that smarter people are more likely to get brain cancer, possibly because they\u2019re already overclocking their ability to grow brain cells.\n\nAt least in neuroscience, once evolution \u201cdiscovered\u201d certain key insights, further increasing intelligence seems to have been a matter of providing it with more computing power. So again \u2013 what happens when we transfer the hypothetical human-level AI from AlphaGo to a TaihuLight-style supercomputer two hundred times more powerful? It might be a stretch to expect it to go from IQ 100 to IQ 20,000, but might it increase to an Einstein-level 200, or a superintelligent 300? Hard to say \u2013 but if Google ever does develop a human-level AI, the Chinese government will probably be interested in finding out.\n\nEven if its intelligence doesn\u2019t scale linearly, TaihuLight could give it more time. TaihuLight is two hundred times faster than AlphaGo. Transfer an AI from one to the other, and even if its intelligence didn\u2019t change \u2013 even if it had exactly the same thoughts \u2013 it would think them two hundred times faster. An Einstein-level AI on AlphaGo hardware might (like the historical Einstein) discover one revolutionary breakthrough every five years. Transfer it to TaihuLight, and it would work two hundred times faster \u2013 a revolutionary breakthrough every week.\n\nSupercomputers track Moore\u2019s Law; the top supercomputer of 2016 is a hundred times faster than the top supercomputer of 2006. If this progress continues, the top computer of 2026 will be a hundred times faster still. Run Einstein on that computer, and he will come up with a revolutionary breakthrough every few hours. Or something. At this point it becomes a little bit hard to imagine. All I know is that it only took one Einstein, at normal speed, to lay the theoretical foundation for nuclear weapons. Anything a thousand times faster than that is definitely cause for concern.\n\nThere\u2019s one final, very concerning reason to expect a fast takeoff. Suppose, once again, we have an AI as smart as Einstein. It might, like the historical Einstein, contemplate physics. Or it might contemplate an area very relevant to its own interests: artificial intelligence. In that case, instead of making a revolutionary physics breakthrough every few hours, it will make a revolutionary AI breakthrough every few hours. Each AI breakthrough it makes, it will have the opportunity to reprogram itself to take advantage of its discovery, becoming more intelligent, thus speeding up its breakthroughs further. The cycle will stop only when it reaches some physical limit \u2013 some technical challenge to further improvements that even an entity far smarter than Einstein cannot discover a way around.\n\nTo human programmers, such a cycle would look like a \u201ccritical mass\u201d. Before the critical level, any AI advance delivers only modest benefits. But any tiny improvement that pushes an AI above the critical level would result in a feedback loop of inexorable self-improvement all the way up to some stratospheric limit of possible computing power.\n\nThis feedback loop would be exponential; relatively slow in the beginning, but blindingly fast as it approaches an asymptote. Consider the AI which starts off making forty breakthroughs per year \u2013 one every nine days. Now suppose it gains on average a 10% speed improvement with each breakthrough. It starts on January 1. Its first breakthrough comes January 10 or so. Its second comes a little faster, January 18. Its third is a little faster still, January 25. By the beginning of February, it\u2019s sped up to producing one breakthrough every seven days, more or less. By the beginning of March, it\u2019s making about one breakthrough every three days or so. But by March 20, it\u2019s up to one breakthrough a day. By late on the night of March 29, it\u2019s making a breakthrough every second.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "", "UI ID": "6962", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:42:11.241+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-96efaa589d9fb77e41b34402f5eabef5a885bb8ded2f886c79d5fd32f968d3e3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-96efaa589d9fb77e41b34402f5eabef5a885bb8ded2f886c79d5fd32f968d3e3", "name": "How might humanity decide what we want the future to be like once we have an aligned superintelligence?", "index": 2, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:05:30.536Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-96efaa589d9fb77e41b34402f5eabef5a885bb8ded2f886c79d5fd32f968d3e3", "values": {"File": "How might humanity decide what we want the future to be like once we have an aligned superintelligence?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might humanity decide what we want the future to be like once we have an aligned superintelligence?", "Link": "https://docs.google.com/document/d/1j4uksfQtfG_N5a8Yp2uSnrrkcS6i_98u2OnXOpPo2nc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:12:45.643+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Utopia", "Doc Last Edited": "2023-02-22T22:55:34.089+01:00", "Status": "In progress", "Edit Answer": "How might humanity decide what we want the future to be like once we have an aligned superintelligence?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7644", "Source Link": "", "aisafety.info Link": "How might humanity decide what we want the future to be like once we have an aligned superintelligence?", "Source": "Wiki", "All Phrasings": "How might humanity decide what we want the future to be like once we have an aligned superintelligence?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- [https://forum.effectivealtruism.org/topics/long-reflection](https://forum.effectivealtruism.org/topics/long-reflection)\n\n- [https://www.overcomingbias.com/2021/10/long-reflection-is-crazy-bad-idea.html](https://www.overcomingbias.com/2021/10/long-reflection-is-crazy-bad-idea.html)\n\n- \n\n- Once we have enough AI capabilities to give ourselves some slack (we can deal with the immediate needs, and are not in severe danger), we'll have time to think about what we actually want.\n\n- \n\n- We likely won\u2019t have the coordination required, at a time when we have a lot of capabilities and thus people might be excited to go forward.  But it might happen if the AI enforces it. That's also unlikely because why would the AI want that? Galaxies are moving away and time is precious!\n\n- \n\n- 5 stars per second lost, but you can launch the probes then update them over the air at lightspeed.\n\n- \n\n- If the AI is fast, the AI itself might do a deep reflexion in a short amount of time.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "- [https://forum.effectivealtruism.org/topics/long-reflection](https://forum.effectivealtruism.org/topics/long-reflection)\n\n- [https://www.overcomingbias.com/2021/10/long-reflection-is-crazy-bad-idea.html](https://www.overcomingbias.com/2021/10/long-reflection-is-crazy-bad-idea.html)\n\n- \n\n- Once we have enough AI capabilities to give ourselves some slack (we can deal with the immediate needs, and are not in severe danger), we'll have time to think about what we actually want.\n\n- \n\n- We likely won\u2019t have the coordination required, at a time when we have a lot of capabilities and thus people might be excited to go forward.  But it might happen if the AI enforces it. That's also unlikely because why would the AI want that? Galaxies are moving away and time is precious!\n\n- \n\n- 5 stars per second lost, but you can launch the probes then update them over the air at lightspeed.\n\n- \n\n- If the AI is fast, the AI itself might do a deep reflexion in a short amount of time.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7644", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:49:43.513+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 2837, "Helpful": ""}}, {"id": "i-e31a2ccfd2003bac67eb3dd4666c0494f39e43f193ddeae13ecf40a74cc9a26b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e31a2ccfd2003bac67eb3dd4666c0494f39e43f193ddeae13ecf40a74cc9a26b", "name": "What are the different versions of decision theory?", "index": 3, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:05:33.882Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e31a2ccfd2003bac67eb3dd4666c0494f39e43f193ddeae13ecf40a74cc9a26b", "values": {"File": "What are the different versions of decision theory?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the different versions of decision theory?", "Link": "https://docs.google.com/document/d/1D-N0CdjmmdwAWXZm7q_6bCOpQdF5q71yokXcVIaKFS0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:19:08.140+01:00", "Related Answers DO NOT EDIT": "What is causal decision theory?,What is \"evidential decision theory\"?,What is \"functional decision theory\"?", "Tags": "Decision Theory", "Doc Last Edited": "2023-02-22T23:05:20.216+01:00", "Status": "Live on site", "Edit Answer": "What are the different versions of decision theory?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7777", "Source Link": "", "aisafety.info Link": "What are the different versions of decision theory?", "Source": "Wiki", "All Phrasings": "What are the different versions of decision theory?\n", "Initial Order": "", "Related IDs": "7779,7778,7781", "Rich Text DO NOT EDIT": "The three main classes of [decision theory](https://www.lesswrong.com/tag/decision-theory) are [evidential decision theory](https://www.lesswrong.com/tag/evidential-decision-theory), [causal theory](https://www.lesswrong.com/tag/causal-decision-theory) and [logical decision theory](https://arbital.com/p/logical_dt/?l=5kv).\n\nEvidential decision theory (EDT) reasons with the conditional probability of events based on the evidence. An agent using EDT selects the action which has the best expected outcome based on the evidence available. It views its action as one more fact about the world, which it can reason about, but does not distinguish the causal effect of its actions from any other conditional factor. See [What is \"evidential decision theory\"](https://docs.google.com/document/d/1Yul-8tuszGSyYizITMQrA25lsfA-Lq40qNKBobEtUYA/edit) for further explanation.\n\nCausal decision theory (CDT) reasons about the causal relationship between the decision and its physical consequences. An agent using CDT views its choice as affecting the specific action that it takes, and, by extension, everything which that action causes. It selects the action which will bring about the best expected outcome based on its knowledge at the time of the decision. See [What is \"causal decision theory\"](https://docs.google.com/document/d/1owSgkcbjSJmsXUbMLbC5KqK9s_EQD-QUNIXDvD8vXH8/) for further explanation.\n\nLogical decision theory (LDT) is a class of decision theories, including [updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory)[functional decision theory](https://intelligence.org/2017/10/22/fdt/) and [timeless decision theory](https://www.lesswrong.com/tag/timeless-decision-theory), which share in making use of logical counterfactuals. An agent using a LDT will act as if it controls the logical output of its own decision algorithm, and not just its immediate action. In general a LDT can outperform other forms of decision theory in problems that include:\n\n- [Parfit's hitchhiker](https://www.lesswrong.com/tag/parfits-hitchhiker)\n\n- [The smoking lesion problem](https://www.lesswrong.com/tag/smoking-lesion)\n\n- [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem)\n\nA specific example of a LDT is Functional Decision Theory (FDT). This says that agents should treat one\u2019s decision as the output of a \ufb01xed mathematical function that answers the question, \u201cWhich output of this very function would yield the best outcome?\u201d. It does not calculate the best outcome based on its immediate circumstance, but rather views itself as an instance of a function which must be consistent under all instantiations that it finds itself in.\n\nSee [What is \"functional decision theory\"?](https://docs.google.com/document/d/1bC7CJTvyinN7AOJ5A03Zy0VUVLghl3N8iOMRk9DmMYc/) and [What is \"logical decision theory\"](https://docs.google.com/document/d/1QKMtIORv0HMFr1LrcugipP33HNzL9-bMWPby66Ify3U/) for further explanation .\n\nFurther reading:\n\n[Decision Theory FAQ](https://www.lesswrong.com/posts/zEWJBFFMvQ835nq6h/decision-theory-faq#what-about-newcombs-problem-and-alternative-decision-algorithms)\n\n[comprehensive list of decision theories](https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/)\n\n[What should I read to learn about decision theory?](https://docs.google.com/document/d/1xwxgnzU_M-Af6Edk5t0Zg2ReH4YFMUtzAJprCW2t9vw/)\n\n", "Tag Count": 1, "Related Answer Count": 3, "Rich Text": "The three main classes of [decision theory](https://www.lesswrong.com/tag/decision-theory) are [evidential decision theory](https://www.lesswrong.com/tag/evidential-decision-theory), [causal theory](https://www.lesswrong.com/tag/causal-decision-theory) and [logical decision theory](https://arbital.com/p/logical_dt/?l=5kv).\n\nEvidential decision theory (EDT) reasons with the conditional probability of events based on the evidence. An agent using EDT selects the action which has the best expected outcome based on the evidence available. It views its action as one more fact about the world, which it can reason about, but does not distinguish the causal effect of its actions from any other conditional factor. See [What is \"evidential decision theory\"](https://docs.google.com/document/d/1Yul-8tuszGSyYizITMQrA25lsfA-Lq40qNKBobEtUYA/edit) for further explanation.\n\nCausal decision theory (CDT) reasons about the causal relationship between the decision and its physical consequences. An agent using CDT views its choice as affecting the specific action that it takes, and, by extension, everything which that action causes. It selects the action which will bring about the best expected outcome based on its knowledge at the time of the decision. See [What is \"causal decision theory\"](https://docs.google.com/document/d/1owSgkcbjSJmsXUbMLbC5KqK9s_EQD-QUNIXDvD8vXH8/) for further explanation.\n\nLogical decision theory (LDT) is a class of decision theories, including [updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory)[functional decision theory](https://intelligence.org/2017/10/22/fdt/) and [timeless decision theory](https://www.lesswrong.com/tag/timeless-decision-theory), which share in making use of logical counterfactuals. An agent using a LDT will act as if it controls the logical output of its own decision algorithm, and not just its immediate action. In general a LDT can outperform other forms of decision theory in problems that include:\n\n- [Parfit's hitchhiker](https://www.lesswrong.com/tag/parfits-hitchhiker)\n\n- [The smoking lesion problem](https://www.lesswrong.com/tag/smoking-lesion)\n\n- [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem)\n\nA specific example of a LDT is Functional Decision Theory (FDT). This says that agents should treat one\u2019s decision as the output of a \ufb01xed mathematical function that answers the question, \u201cWhich output of this very function would yield the best outcome?\u201d. It does not calculate the best outcome based on its immediate circumstance, but rather views itself as an instance of a function which must be consistent under all instantiations that it finds itself in.\n\nSee [What is \"functional decision theory\"?](https://docs.google.com/document/d/1bC7CJTvyinN7AOJ5A03Zy0VUVLghl3N8iOMRk9DmMYc/) and [What is \"logical decision theory\"](https://docs.google.com/document/d/1QKMtIORv0HMFr1LrcugipP33HNzL9-bMWPby66Ify3U/) for further explanation .\n\nFurther reading:\n\n[Decision Theory FAQ](https://www.lesswrong.com/posts/zEWJBFFMvQ835nq6h/decision-theory-faq#what-about-newcombs-problem-and-alternative-decision-algorithms)\n\n[comprehensive list of decision theories](https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/)\n\n[What should I read to learn about decision theory?](https://docs.google.com/document/d/1xwxgnzU_M-Af6Edk5t0Zg2ReH4YFMUtzAJprCW2t9vw/)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "2023-02-26T19:22:19.889+01:00", "UI ID": "7777", "Related Answers": "What is causal decision theory?,What is \"evidential decision theory\"?,What is \"functional decision theory\"?", "Doc Last Ingested": "2023-03-14T23:49:46.137+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 2837, "Helpful": ""}}, {"id": "i-1b38bc1a4358d1c7b015d9f37348e9963cb56690c06bbd3b35ed1b28f72ab6ca", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1b38bc1a4358d1c7b015d9f37348e9963cb56690c06bbd3b35ed1b28f72ab6ca", "name": "What are the different possible AI takeoff speeds?", "index": 4, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:05:36.905Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1b38bc1a4358d1c7b015d9f37348e9963cb56690c06bbd3b35ed1b28f72ab6ca", "values": {"File": "What are the different possible AI takeoff speeds?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the different possible AI takeoff speeds?", "Link": "https://docs.google.com/document/d/1WvBwRuEPtnqgMX6SzEz0n_U0PJluE2Yc5hZ6Zn3gCqo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:19:04.636+01:00", "Related Answers DO NOT EDIT": "Why does AI takeoff speed matter?", "Tags": "AI Takeoff,Definitions", "Doc Last Edited": "2023-02-22T23:05:21.090+01:00", "Status": "Live on site", "Edit Answer": "What are the different possible AI takeoff speeds?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6957", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "What are the different possible AI takeoff speeds?", "Source": "Superintelligence FAQ", "All Phrasings": "What are the different possible AI takeoff speeds?\n", "Initial Order": "", "Related IDs": "6966", "Rich Text DO NOT EDIT": "A slow takeoff is where AI capabilities improve gradually, giving us plenty of time to adapt. In a moderate takeoff we might see accelerating progress, but we still won\u2019t be caught off guard by a dramatic change. Whereas, in a fast or hard takeoff AI would go from being not very generally competent to sufficiently superhuman to control the future too fast for humans to course correct if something goes wrong.\n\nThe article [Distinguishing definitions of takeoff](https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff) goes into more detail on this.\n\n", "Tag Count": 2, "Related Answer Count": 1, "Rich Text": "A slow takeoff is where AI capabilities improve gradually, giving us plenty of time to adapt. In a moderate takeoff we might see accelerating progress, but we still won\u2019t be caught off guard by a dramatic change. Whereas, in a fast or hard takeoff AI would go from being not very generally competent to sufficiently superhuman to control the future too fast for humans to course correct if something goes wrong.\n\nThe article [Distinguishing definitions of takeoff](https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff) goes into more detail on this.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "", "UI ID": "6957", "Related Answers": "Why does AI takeoff speed matter?", "Doc Last Ingested": "2023-03-14T23:49:48.520+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 2837, "Helpful": ""}}, {"id": "i-f790c247d9e136ff334a2012f5f198b0edbc3aeb0e1bf11e4a15609c17c5c7e1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-f790c247d9e136ff334a2012f5f198b0edbc3aeb0e1bf11e4a15609c17c5c7e1", "name": "What are the differences between \u201cAI safety\u201d, \u201cAGI safety\u201d, \u201cAI alignment\u201d and \u201cAI existential safety\u201d?", "index": 5, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:05:40.932Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-f790c247d9e136ff334a2012f5f198b0edbc3aeb0e1bf11e4a15609c17c5c7e1", "values": {"File": "What are the differences between \u201cAI safety\u201d, \u201cAGI safety\u201d, \u201cAI alignment\u201d and \u201cAI existential safety\u201d?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the differences between \u201cAI safety\u201d, \u201cAGI safety\u201d, \u201cAI alignment\u201d and \u201cAI existential safety\u201d?", "Link": "https://docs.google.com/document/d/1bFbKCoNZA6LAzQeOmcaNxJTeDfppbP7dzaiT9RHKoWg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:19:01.538+01:00", "Related Answers DO NOT EDIT": "What is the difference between AI Safety, AI alignment, AI Control, Friendly AI, AI Ethics, AI existential safety and AGI safety?", "Tags": "Definitions", "Doc Last Edited": "2023-03-01T21:48:26.458+01:00", "Status": "Live on site", "Edit Answer": "What are the differences between \u201cAI safety\u201d, \u201cAGI safety\u201d, \u201cAI alignment\u201d and \u201cAI existential safety\u201d?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8037", "Source Link": "", "aisafety.info Link": "What are the differences between \u201cAI safety\u201d, \u201cAGI safety\u201d, \u201cAI alignment\u201d and \u201cAI existential safety\u201d?", "Source": "Wiki", "All Phrasings": "What are the differences between \u201cAI safety\u201d, \u201cAGI safety\u201d, \u201cAI alignment\u201d and \u201cAI existential safety\u201d?\n", "Initial Order": "", "Related IDs": "6714", "Rich Text DO NOT EDIT": "AI alignment is the research field focused on trying to give us the tools to align AIs to specific goals, such as human values. This is crucial when they are highly competent, as a misaligned superintelligence could be the end of human civilization.\n\nAGI safety is the field trying to make sure that when we build Artificial General Intelligences they are safe and do not harm humanity. It overlaps with AI alignment strongly, in that misalignment of AI would be the main cause of unsafe behavior in AGIs, but also includes misuse and other governance issues.\n\nAI existential safety is a slightly broader term than AGI safety, including AI risks which pose an existential threat without necessarily being as general as humans.\n\nAI safety was originally used by the existential risk reduction movement for the work done to reduce the risks of misaligned superintelligence, but has also been adopted by researchers and others studying nearer term and less catastrophic risks from AI in recent years.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "AI alignment is the research field focused on trying to give us the tools to align AIs to specific goals, such as human values. This is crucial when they are highly competent, as a misaligned superintelligence could be the end of human civilization.\n\nAGI safety is the field trying to make sure that when we build Artificial General Intelligences they are safe and do not harm humanity. It overlaps with AI alignment strongly, in that misalignment of AI would be the main cause of unsafe behavior in AGIs, but also includes misuse and other governance issues.\n\nAI existential safety is a slightly broader term than AGI safety, including AI risks which pose an existential threat without necessarily being as general as humans.\n\nAI safety was originally used by the existential risk reduction movement for the work done to reduce the risks of misaligned superintelligence, but has also been adopted by researchers and others studying nearer term and less catastrophic risks from AI in recent years.\n\n", "Stamp Count": 2, "Multi Answer": "", "Stamped By": "Damaged\nplex", "Priority": 3, "Asker": "matthew1970", "External Source": "", "Last Asked On Discord": "", "UI ID": "8037", "Related Answers": "What is the difference between AI Safety, AI alignment, AI Control, Friendly AI, AI Ethics, AI existential safety and AGI safety?", "Doc Last Ingested": "2023-03-14T23:49:50.014+01:00", "Request Count": "", "Number of suggestions on answer doc": 4, "Total character count of suggestions on answer doc": 2840, "Helpful": ""}}, {"id": "i-8f61e1ff348bfb2efac60e8d044e408c415c41fee1ac0182d1bfcdc8e7921eb3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8f61e1ff348bfb2efac60e8d044e408c415c41fee1ac0182d1bfcdc8e7921eb3", "name": "What are the differences between AGI, superintelligence and transformative AI?", "index": 6, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-15T22:07:58.503Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8f61e1ff348bfb2efac60e8d044e408c415c41fee1ac0182d1bfcdc8e7921eb3", "values": {"File": "What are the differences between AGI, superintelligence and transformative AI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the differences between AGI, superintelligence and transformative AI?", "Link": "https://docs.google.com/document/d/1UvrweXuLJb3DkiwBh4WVQTqUYCVlC1WLgJoZywPSBek/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:57.578+01:00", "Related Answers DO NOT EDIT": "What is \"superintelligence\"?,What is Artificial General Intelligence (AGI) and what will it look like?,What is \"greater-than-human intelligence\"?", "Tags": "AGI,Superintelligence,Transformative AI", "Doc Last Edited": "2023-03-15T20:07:28.146+01:00", "Status": "In progress", "Edit Answer": "What are the differences between AGI, superintelligence and transformative AI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5864", "Source Link": "", "aisafety.info Link": "What are the differences between AGI, superintelligence and transformative AI?", "Source": "Wiki", "All Phrasings": "What are the differences between AGI, superintelligence and transformative AI?\n", "Initial Order": "", "Related IDs": "6207,2374,6587", "Rich Text DO NOT EDIT": "These are all related attempts to define AI capability milestones\u2014roughly, \"the point at which AI gets truly smart\"\u2014but with different meanings:\n\n- **AGI** stands for \"[artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence)\" and refers to AI programs that aren't just skilled at a narrow range of tasks (like board games or driving) but that can apply their intelligence to a similarly wide range of domains as humans. Some call systems like [Gato](https://www.deepmind.com/publications/a-generalist-agent) AGI because they solve many tasks with the same model. However, the term is more often used for systems with human-level general competence, so overall AGI is still seen as a goal for the future by groups like OpenAI and DeepMind.\n\n- **Superintelligence** is [defined by Nick Bostrom](https://publicism.info/philosophy/superintelligence/3.html) as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\". This is by far the highest bar out of all the concepts listed here, but it may be surpassed a short time after the others, e.g. because of an intelligence explosion.\n\n- **Transformative AI** (\"TAI\") is any AI powerful enough to [transform our society](https://www.sciencedirect.com/science/article/pii/S0016328721001932). (The term is unrelated to the [transformer architecture](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)).) [Holden Karnofsky has defined it](https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/) as requiring at least as big an impact as the agricultural or industrial revolutions, which implies at least a tenfold increase in the rate of economic growth. [Ajeya Cotra's report](https://docs.google.com/document/d/1IJ6Sr-gPeXdSJugFulwIpvavc0atjHGM82QjIfUSBGQ/edit#heading=h.6t4rel10jbcj) mentions a \"virtual professional\", i.e. a program that can do most remote jobs, as an example of a system that would have such an impact.\n\nThere are also other terms which are sometimes used:\n\n- **Advanced AI** is any AI that's much more powerful than current AI: the term is sometimes used as a loose placeholder for the other concepts here.\n\n- **Human-level AI** is any AI that can [solve most of the cognitive problems](https://aiimpacts.org/human-level-ai/) an average human can solve. Current AI has a very different profile of strengths and weaknesses than humans, and this is likely to be true of future AI: before AI is at least human-level at all tasks, it will probably be vastly superhuman at some important tasks while being weaker at others.\n\n- **Strong AI** was defined by John Searle as the philosophical thesis that computer programs can have \"a mind in exactly the same sense human beings have minds\", but the term is sometimes used outside this context as more or less interchangeable with \"AGI\" or \"human-level AI\".\n\n- **Seed AI** is any AI with enough AI programming ability to set off a recursive self-improvement process that takes it all the way to superintelligence. As with PASTA above, an AI might not have to qualify as AGI to have sudden and dangerous impacts in this way.\n\n- **Turing Test**-passing AI is any AI smart enough to fool human judges into thinking it's human. The level of capability required depends on how intense the scrutiny is: current language models trained to imitate human text can already seem human to a casual observer, despite not having general human-level intelligence. On the other hand, imitating an intelligence can be harder than outperforming it, so it's also possible for smarter-than-human AI to fail the Turing test.\n\n- **APS-AI** is a term introduced by Joe Carlsmith in his [report on existential risk from power-seeking AI](https://www.alignmentforum.org/posts/HduCjmXTBD4xYTegv/draft-report-on-existential-risk-from-power-seeking-ai). APS stands for Advanced, Planning, and Strategically aware. \"Advanced\" means it's more powerful than humans at important tasks; \"Planning\" means it's an agent that pursues goals using its world models; \"Strategically aware\" means it has good models of its strategic situation with respect to humans in the real world. Carlsmith argues these properties together create the risk of AI takeover.\n\n- **PASTA** is an acronym introduced by Holden Karnofsky in a [series of blog posts](https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/) that stands for \"Process for Automating Scientific and Technological Advancement\". His thesis is any AI powerful enough to automate human R&D is sufficient for explosive impacts even if it doesn't qualify as AGI.\n\nEven more related concepts are listed in [this post](https://www.lesswrong.com/posts/Nt8yDxkiMF8YAsNYA/operationalizing-timelines/).\n\n## \n\n", "Tag Count": 3, "Related Answer Count": 3, "Rich Text": "These are all related attempts to define AI capability milestones\u2014roughly, \"the point at which AI gets truly smart\"\u2014but with different meanings:\n\n- **AGI** stands for \"[artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence)\" and refers to AI programs that aren't just skilled at a narrow range of tasks (like board games or driving) but that can apply their intelligence to a similarly wide range of domains as humans. Some call systems like [Gato](https://www.deepmind.com/publications/a-generalist-agent) AGI because they solve many tasks with the same model. However, the term is more often used for systems with human-level general competence, so overall AGI is still seen as a goal for the future by groups like OpenAI and DeepMind.\n\n- **Superintelligence** is [defined by Nick Bostrom](https://publicism.info/philosophy/superintelligence/3.html) as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\". This is by far the highest bar out of all the concepts listed here, but it may be surpassed a short time after the others, e.g. because of an intelligence explosion.\n\n- **Transformative AI** (\"TAI\") is any AI powerful enough to [transform our society](https://www.sciencedirect.com/science/article/pii/S0016328721001932). (The term is unrelated to the [transformer architecture](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)).) [Holden Karnofsky has defined it](https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/) as requiring at least as big an impact as the agricultural or industrial revolutions, which implies at least a tenfold increase in the rate of economic growth. [Ajeya Cotra's report](https://docs.google.com/document/d/1IJ6Sr-gPeXdSJugFulwIpvavc0atjHGM82QjIfUSBGQ/edit#heading=h.6t4rel10jbcj) mentions a \"virtual professional\", i.e. a program that can do most remote jobs, as an example of a system that would have such an impact.\n\nThere are also other terms which are sometimes used:\n\n- **Advanced AI** is any AI that's much more powerful than current AI: the term is sometimes used as a loose placeholder for the other concepts here.\n\n- **Human-level AI** is any AI that can [solve most of the cognitive problems](https://aiimpacts.org/human-level-ai/) an average human can solve. Current AI has a very different profile of strengths and weaknesses than humans, and this is likely to be true of future AI: before AI is at least human-level at all tasks, it will probably be vastly superhuman at some important tasks while being weaker at others.\n\n- **Strong AI** was defined by John Searle as the philosophical thesis that computer programs can have \"a mind in exactly the same sense human beings have minds\", but the term is sometimes used outside this context as more or less interchangeable with \"AGI\" or \"human-level AI\".\n\n- **Seed AI** is any AI with enough AI programming ability to set off a recursive self-improvement process that takes it all the way to superintelligence. As with PASTA above, an AI might not have to qualify as AGI to have sudden and dangerous impacts in this way.\n\n- **Turing Test**-passing AI is any AI smart enough to fool human judges into thinking it's human. The level of capability required depends on how intense the scrutiny is: current language models trained to imitate human text can already seem human to a casual observer, despite not having general human-level intelligence. On the other hand, imitating an intelligence can be harder than outperforming it, so it's also possible for smarter-than-human AI to fail the Turing test.\n\n- **APS-AI** is a term introduced by Joe Carlsmith in his [report on existential risk from power-seeking AI](https://www.alignmentforum.org/posts/HduCjmXTBD4xYTegv/draft-report-on-existential-risk-from-power-seeking-ai). APS stands for Advanced, Planning, and Strategically aware. \"Advanced\" means it's more powerful than humans at important tasks; \"Planning\" means it's an agent that pursues goals using its world models; \"Strategically aware\" means it has good models of its strategic situation with respect to humans in the real world. Carlsmith argues these properties together create the risk of AI takeover.\n\n- **PASTA** is an acronym introduced by Holden Karnofsky in a [series of blog posts](https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/) that stands for \"Process for Automating Scientific and Technological Advancement\". His thesis is any AI powerful enough to automate human R&D is sufficient for explosive impacts even if it doesn't qualify as AGI.\n\nEven more related concepts are listed in [this post](https://www.lesswrong.com/posts/Nt8yDxkiMF8YAsNYA/operationalizing-timelines/).\n\n## \n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "5864", "Related Answers": "What is \"superintelligence\"?,What is Artificial General Intelligence (AGI) and what will it look like?,What is \"greater-than-human intelligence\"?", "Doc Last Ingested": "2023-03-15T20:12:10.450+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-a0f94e1e27aff1221f9abaeaab0e8b9c0e09b4cd8f5095f27d784ca870414706", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-a0f94e1e27aff1221f9abaeaab0e8b9c0e09b4cd8f5095f27d784ca870414706", "name": "What are the win conditions for AI alignment?", "index": 7, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:07:16.903Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-a0f94e1e27aff1221f9abaeaab0e8b9c0e09b4cd8f5095f27d784ca870414706", "values": {"File": "What are the win conditions for AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are the win conditions for AI alignment?", "Link": "https://docs.google.com/document/d/1nFI3Br2w5Dlt5_J-HgYVPgb4QGmfWRE0GsMfXyREqhM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:54.299+01:00", "Related Answers DO NOT EDIT": "How might interpretability be helpful?,What are the main problems that need to be solved in AI safety?,What are the win conditions for ending AI risk?", "Tags": "Success Models", "Doc Last Edited": "2023-03-07T20:11:57.891+01:00", "Status": "Live on site", "Edit Answer": "What are the win conditions for AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7762", "Source Link": "", "aisafety.info Link": "What are the win conditions for AI alignment?", "Source": "Wiki", "All Phrasings": "What are the win conditions for AI alignment?\n", "Initial Order": "", "Related IDs": "89LK,8F2K,8F2L", "Rich Text DO NOT EDIT": "There is currently no consensus that most/all researchers agree on for aligning an Artificial General Intelligence (AGI). Many researchers have their own paradigm and collectively view the problem from many different angles.\n\nThe [agent foundations agenda](https://www.lesswrong.com/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation) starts with the premise that we are [fundamentally confused](https://www.lesswrong.com/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem) about alignment and need to figure out \u2018[true names](https://docs.google.com/document/d/1Z0JWiQHEQ7wUi3OU2_Zn4JGRfHMdEhydHlGY7DkFpPM/edit)\u2019 of various concepts, or formalisms which point exactly to what we mean. For example, we need to pin down which [decision theory](https://www.alignmentforum.org/tag/decision-theory) and [ontology](https://arbital.com/p/ontology_identification/) an AGI should use.\n\nAn agentic AGI must [learn what we value](https://www.alignmentforum.org/tag/value-learning), especially if it is a [sovereign](https://arbital.com/p/Sovereign/). [Reinforcement Learning from Human Feedback (RLHF)](https://www.alignmentforum.org/tag/rlhf) attempts to do this by having humans evaluate outputs and rate them positively or negatively to fine-tune the model before it is deployed.\n\nSee also the [AI alignment landscape](https://ai-alignment.com/ai-alignment-landscape-d3773c37ae38), and [formal alignment](https://carado.moe/formal-alignment.html).\n\n", "Tag Count": 1, "Related Answer Count": 3, "Rich Text": "There is currently no consensus that most/all researchers agree on for aligning an Artificial General Intelligence (AGI). Many researchers have their own paradigm and collectively view the problem from many different angles.\n\nThe [agent foundations agenda](https://www.lesswrong.com/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation) starts with the premise that we are [fundamentally confused](https://www.lesswrong.com/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem) about alignment and need to figure out \u2018[true names](https://docs.google.com/document/d/1Z0JWiQHEQ7wUi3OU2_Zn4JGRfHMdEhydHlGY7DkFpPM/edit)\u2019 of various concepts, or formalisms which point exactly to what we mean. For example, we need to pin down which [decision theory](https://www.alignmentforum.org/tag/decision-theory) and [ontology](https://arbital.com/p/ontology_identification/) an AGI should use.\n\nAn agentic AGI must [learn what we value](https://www.alignmentforum.org/tag/value-learning), especially if it is a [sovereign](https://arbital.com/p/Sovereign/). [Reinforcement Learning from Human Feedback (RLHF)](https://www.alignmentforum.org/tag/rlhf) attempts to do this by having humans evaluate outputs and rate them positively or negatively to fine-tune the model before it is deployed.\n\nSee also the [AI alignment landscape](https://ai-alignment.com/ai-alignment-landscape-d3773c37ae38), and [formal alignment](https://carado.moe/formal-alignment.html).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7762", "Related Answers": "How might interpretability be helpful?,What are the main problems that need to be solved in AI safety?,What are the win conditions for ending AI risk?", "Doc Last Ingested": "2023-03-14T23:52:02.627+01:00", "Request Count": "", "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 2885, "Helpful": ""}}, {"id": "i-2d01b4830ba4fc500f89ed9b319fc2acafadfa7a3f07db4b733be9e4cac801f3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2d01b4830ba4fc500f89ed9b319fc2acafadfa7a3f07db4b733be9e4cac801f3", "name": "What are some specific open tasks on Stampy?", "index": 8, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:07:21.806Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2d01b4830ba4fc500f89ed9b319fc2acafadfa7a3f07db4b733be9e4cac801f3", "values": {"File": "What are some specific open tasks on Stampy?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some specific open tasks on Stampy?", "Link": "https://docs.google.com/document/d/1BaQAJ3_HG-KK_z4sEkswfHunP8F3zekmBdNDQ8CLZDM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:50.330+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stampy", "Doc Last Edited": "2023-02-22T23:05:24.145+01:00", "Status": "Live on site", "Edit Answer": "What are some specific open tasks on Stampy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7844", "Source Link": "", "aisafety.info Link": "What are some specific open tasks on Stampy?", "Source": "Wiki", "All Phrasings": "What are some specific open tasks on Stampy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Other than the usual fare of writing and processing and organizing questions and answers, here are some specific open tasks:\n\n- Porting over some of [Steve Byrnes's FAQ on alignment](https://www.lesswrong.com/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why#1_4_What_exactly_is__AGI__)\n\n- Porting over content from [Vael Gates's post](https://www.lesswrong.com/posts/gdyfJE3noRFSs373q/resources-i-send-to-ai-researchers-about-ai-safety)\n\n- Porting over QA pairs from [https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread](https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread)\n\n- Porting over some of [https://aisafety.wordpress.com/](https://aisafety.wordpress.com/)\n\n- Making sure we cover all of [https://forum.effectivealtruism.org/posts/8JazqnCNrkJtK2Bx4/why-eas-are-skeptical-about-ai-safety#Recursive_self_improvement_seems_implausible](https://forum.effectivealtruism.org/posts/8JazqnCNrkJtK2Bx4/why-eas-are-skeptical-about-ai-safety#Recursive_self_improvement_seems_implausible) and the responses\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Other than the usual fare of writing and processing and organizing questions and answers, here are some specific open tasks:\n\n- Porting over some of [Steve Byrnes's FAQ on alignment](https://www.lesswrong.com/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why#1_4_What_exactly_is__AGI__)\n\n- Porting over content from [Vael Gates's post](https://www.lesswrong.com/posts/gdyfJE3noRFSs373q/resources-i-send-to-ai-researchers-about-ai-safety)\n\n- Porting over QA pairs from [https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread](https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread)\n\n- Porting over some of [https://aisafety.wordpress.com/](https://aisafety.wordpress.com/)\n\n- Making sure we cover all of [https://forum.effectivealtruism.org/posts/8JazqnCNrkJtK2Bx4/why-eas-are-skeptical-about-ai-safety#Recursive_self_improvement_seems_implausible](https://forum.effectivealtruism.org/posts/8JazqnCNrkJtK2Bx4/why-eas-are-skeptical-about-ai-safety#Recursive_self_improvement_seems_implausible) and the responses\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7844", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:04.983+01:00", "Request Count": "", "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 2885, "Helpful": ""}}, {"id": "i-7190be773fb1c24e971a38ab5de758a00801bfd914350b7e32cd99336e1d9d19", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7190be773fb1c24e971a38ab5de758a00801bfd914350b7e32cd99336e1d9d19", "name": "What are some problems in philosophy that are related to AI safety?", "index": 9, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:07:27.457Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7190be773fb1c24e971a38ab5de758a00801bfd914350b7e32cd99336e1d9d19", "values": {"File": "What are some problems in philosophy that are related to AI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some problems in philosophy that are related to AI safety?", "Link": "https://docs.google.com/document/d/1aNKObbwW5pBLvBgxRKN_VuyK08B-iC4L9Vr97hvguvM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:47.142+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Philosophy", "Doc Last Edited": "2023-03-06T16:15:09.311+01:00", "Status": "In progress", "Edit Answer": "What are some problems in philosophy that are related to AI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7588", "Source Link": "", "aisafety.info Link": "What are some problems in philosophy that are related to AI safety?", "Source": "Wiki", "All Phrasings": "What are some problems in philosophy that are related to AI safety?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n1. \n\n1. \n\n1. \n\n1. \n\nWe can broadly distinguish between three categories of problems in philosophy that connect to AI safety:\n\n1. Questions about the nature of AI. For example, [John McCarthy writes](http://jmc.stanford.edu/articles/aiphil2/aiphil2.pdf): \u201cArtificial intelligence (AI) has closer scientific connections with philosophy than do other sciences, because AI shares many concepts with philosophy, e.g. action, consciousness, epistemology (what it is sensible to say about the world), and even free will. Clarifying what intelligence is or what it means to be sentient are also philosophical matters.\n\n1. Questions about values and the control problem can be approached from a philosophical point of view. For example, is there such a thing as objective morality and value? How do we represent human values? Are they universal?\n\n1. Questions about the impact of AI on society and humanity: what would the future look like for humans?, what are the ethical considerations regarding transhumanism?, are there moral responsibilities to protect humanity from existential risks such as those that could be posed by powerful AI systems? \n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n1. \n\n1. \n\n1. \n\n1. \n\nWe can broadly distinguish between three categories of problems in philosophy that connect to AI safety:\n\n1. Questions about the nature of AI. For example, [John McCarthy writes](http://jmc.stanford.edu/articles/aiphil2/aiphil2.pdf): \u201cArtificial intelligence (AI) has closer scientific connections with philosophy than do other sciences, because AI shares many concepts with philosophy, e.g. action, consciousness, epistemology (what it is sensible to say about the world), and even free will. Clarifying what intelligence is or what it means to be sentient are also philosophical matters.\n\n1. Questions about values and the control problem can be approached from a philosophical point of view. For example, is there such a thing as objective morality and value? How do we represent human values? Are they universal?\n\n1. Questions about the impact of AI on society and humanity: what would the future look like for humans?, what are the ethical considerations regarding transhumanism?, are there moral responsibilities to protect humanity from existential risks such as those that could be posed by powerful AI systems? \n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7588", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:07.688+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 7720, "Helpful": ""}}, {"id": "i-2e62bca5e895879af470a81bd7b0ca98d4a15fee7484913aa098b01e15997c9a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2e62bca5e895879af470a81bd7b0ca98d4a15fee7484913aa098b01e15997c9a", "name": "What are some practice or entry-level problems for getting into alignment research?", "index": 10, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:07:32.033Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2e62bca5e895879af470a81bd7b0ca98d4a15fee7484913aa098b01e15997c9a", "values": {"File": "What are some practice or entry-level problems for getting into alignment research?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some practice or entry-level problems for getting into alignment research?", "Link": "https://docs.google.com/document/d/1USv1ZaUhxC8d2r-iREknyjATzysb-g6DiAz5Banb-ro/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:43.773+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Contributing,Careers,Training", "Doc Last Edited": "2023-02-22T22:55:37.104+01:00", "Status": "In progress", "Edit Answer": "What are some practice or entry-level problems for getting into alignment research?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7760", "Source Link": "", "aisafety.info Link": "What are some practice or entry-level problems for getting into alignment research?", "Source": "Wiki", "All Phrasings": "What are some practice or entry-level problems for getting into alignment research?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- [https://forum.effectivealtruism.org/posts/BBS8cz4Sa7wvJ2Jso/resources-that-i-think-new-alignment-researchers-should-know#Exercises](https://forum.effectivealtruism.org/posts/BBS8cz4Sa7wvJ2Jso/resources-that-i-think-new-alignment-researchers-should-know#Exercises)\n\n- [https://www.lesswrong.com/posts/rmCqibBhytQizcief/list-of-technical-ai-safety-exercises-and-projects](https://www.lesswrong.com/posts/rmCqibBhytQizcief/list-of-technical-ai-safety-exercises-and-projects)\n\n", "Tag Count": 3, "Related Answer Count": 0, "Rich Text": "- [https://forum.effectivealtruism.org/posts/BBS8cz4Sa7wvJ2Jso/resources-that-i-think-new-alignment-researchers-should-know#Exercises](https://forum.effectivealtruism.org/posts/BBS8cz4Sa7wvJ2Jso/resources-that-i-think-new-alignment-researchers-should-know#Exercises)\n\n- [https://www.lesswrong.com/posts/rmCqibBhytQizcief/list-of-technical-ai-safety-exercises-and-projects](https://www.lesswrong.com/posts/rmCqibBhytQizcief/list-of-technical-ai-safety-exercises-and-projects)\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7760", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:14.927+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 7720, "Helpful": ""}}, {"id": "i-02853f9a1f567751e1a43f81701bb8875572ae10b4a43c3fe8e0209c93ea8f81", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-02853f9a1f567751e1a43f81701bb8875572ae10b4a43c3fe8e0209c93ea8f81", "name": "What are some open research questions in AI alignment?", "index": 11, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:07:36.461Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-02853f9a1f567751e1a43f81701bb8875572ae10b4a43c3fe8e0209c93ea8f81", "values": {"File": "What are some open research questions in AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some open research questions in AI alignment?", "Link": "https://docs.google.com/document/d/1U9aqgveqQUuHtA4t8gTmLIncSBMZ2v3K9eSTEQ-0_dQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:40.203+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Open Problem", "Doc Last Edited": "2023-02-22T22:55:38.045+01:00", "Status": "Not started", "Edit Answer": "What are some open research questions in AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7650", "Source Link": "", "aisafety.info Link": "What are some open research questions in AI alignment?", "Source": "Wiki", "All Phrasings": "What are some open research questions in AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7650", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:16.969+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 7720, "Helpful": ""}}, {"id": "i-7a71b3fc1b4a94f4a9ee3cd9e1a2414426891667f0a818bba81b179ff6f825d1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7a71b3fc1b4a94f4a9ee3cd9e1a2414426891667f0a818bba81b179ff6f825d1", "name": "What are some of the most impressive recent advances in AI capabilities?", "index": 12, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:07:41.100Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7a71b3fc1b4a94f4a9ee3cd9e1a2414426891667f0a818bba81b179ff6f825d1", "values": {"File": "What are some of the most impressive recent advances in AI capabilities?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some of the most impressive recent advances in AI capabilities?", "Link": "https://docs.google.com/document/d/1xvxBOzEKfLewD13-4m28wQaljRvMdjk1FSX78KwZHaY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:36.483+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Capabilities", "Doc Last Edited": "2023-02-22T23:05:25.590+01:00", "Status": "Live on site", "Edit Answer": "What are some of the most impressive recent advances in AI capabilities?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7847", "Source Link": "", "aisafety.info Link": "What are some of the most impressive recent advances in AI capabilities?", "Source": "Wiki", "All Phrasings": "What are some of the most impressive recent advances in AI capabilities?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "GPT-3 showed that transformers are capable of a vast array of natural language tasks, [codex/copilot](https://copilot.github.com/) extended this into programming. One demonstrations of GPT-3 is [Simulated Elon Musk lives in a simulation](https://www.lesswrong.com/posts/oBPPFrMJ2aBK6a6sD/simulated-elon-musk-lives-in-a-simulation). Important to note that there are several much better language models, but they are not publicly available.\n\n[DALL-E](https://openai.com/blog/dall-e/) and [DALL-E 2](https://openai.com/dall-e-2/) are among the most visually spectacular.\n\n[MuZero](https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules), which learned Go, Chess, and many Atari games without any directly coded info about those environments. The graphic there explains it, this seems crucial for being able to do RL in novel environments. We have systems which we can drop into a wide variety of games and they just learn how to play. The same algorithm was used in [Tesla's self-driving cars to do complex route finding](https://youtu.be/j0z4FweCy4M?t=4918). These things are general.\n\n[Generally capable agents emerge from open-ended play](https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play) - Diverse procedurally generated environments provide vast amounts of training data for AIs to learn generally applicable skills. [Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning](https://www.deepmind.com/publications/creating-interactive-agents-with-imitation-learning) shows how these kind of systems can be trained to follow instructions in natural language.\n\n[GATO](https://www.deepmind.com/publications/a-generalist-agent) shows you can distill 600+ individually trained tasks into one network, so we're not limited by the tasks being fragmented.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "GPT-3 showed that transformers are capable of a vast array of natural language tasks, [codex/copilot](https://copilot.github.com/) extended this into programming. One demonstrations of GPT-3 is [Simulated Elon Musk lives in a simulation](https://www.lesswrong.com/posts/oBPPFrMJ2aBK6a6sD/simulated-elon-musk-lives-in-a-simulation). Important to note that there are several much better language models, but they are not publicly available.\n\n[DALL-E](https://openai.com/blog/dall-e/) and [DALL-E 2](https://openai.com/dall-e-2/) are among the most visually spectacular.\n\n[MuZero](https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules), which learned Go, Chess, and many Atari games without any directly coded info about those environments. The graphic there explains it, this seems crucial for being able to do RL in novel environments. We have systems which we can drop into a wide variety of games and they just learn how to play. The same algorithm was used in [Tesla's self-driving cars to do complex route finding](https://youtu.be/j0z4FweCy4M?t=4918). These things are general.\n\n[Generally capable agents emerge from open-ended play](https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play) - Diverse procedurally generated environments provide vast amounts of training data for AIs to learn generally applicable skills. [Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning](https://www.deepmind.com/publications/creating-interactive-agents-with-imitation-learning) shows how these kind of systems can be trained to follow instructions in natural language.\n\n[GATO](https://www.deepmind.com/publications/a-generalist-agent) shows you can distill 600+ individually trained tasks into one network, so we're not limited by the tasks being fragmented.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7847", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:19.185+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 7720, "Helpful": ""}}, {"id": "i-3b1ea061ddffce4e1a71d4227c49dba9a02a5f9aafb158772e25460b7e9f094a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3b1ea061ddffce4e1a71d4227c49dba9a02a5f9aafb158772e25460b7e9f094a", "name": "What are some of the leading AI capabilities organizations?", "index": 13, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:07:45.133Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3b1ea061ddffce4e1a71d4227c49dba9a02a5f9aafb158772e25460b7e9f094a", "values": {"File": "What are some of the leading AI capabilities organizations?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some of the leading AI capabilities organizations?", "Link": "https://docs.google.com/document/d/17LsF_6_Zji8tkTRgxYqri_dstHuoS_gvw9wyG6Zkc2g/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:33.342+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Capabilities,Actors", "Doc Last Edited": "2023-03-06T22:26:55.273+01:00", "Status": "In progress", "Edit Answer": "What are some of the leading AI capabilities organizations?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7772", "Source Link": "", "aisafety.info Link": "What are some of the leading AI capabilities organizations?", "Source": "Wiki", "All Phrasings": "What are some of the leading AI capabilities organizations?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Some of the main research groups advancing AI capabilities are:\n\n- [DeepMind](https://www.deepmind.com/)\n\n- [OpenAI](https://openai.com/)\n\n- [Anthropic](https://www.anthropic.com/)\n\n- [Meta AI](https://ai.facebook.com/)\n\n- [Google AI](https://ai.google/) (especially [Google Brain](https://research.google/teams/brain/))\n\n- [Microsoft AI](https://www.microsoft.com/en-us/ai)\n\n- [Baidu Research](http://research.baidu.com/)\n\nSeveral major governments have AI capabilities research programs (e.g. AI research at [DARPA](https://www.darpa.mil/work-with-us/ai-next-campaign), a branch of the US Department of Defense), though in general less is known publicly about those programs.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Some of the main research groups advancing AI capabilities are:\n\n- [DeepMind](https://www.deepmind.com/)\n\n- [OpenAI](https://openai.com/)\n\n- [Anthropic](https://www.anthropic.com/)\n\n- [Meta AI](https://ai.facebook.com/)\n\n- [Google AI](https://ai.google/) (especially [Google Brain](https://research.google/teams/brain/))\n\n- [Microsoft AI](https://www.microsoft.com/en-us/ai)\n\n- [Baidu Research](http://research.baidu.com/)\n\nSeveral major governments have AI capabilities research programs (e.g. AI research at [DARPA](https://www.darpa.mil/work-with-us/ai-next-campaign), a branch of the US Department of Defense), though in general less is known publicly about those programs.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7772", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:20.591+01:00", "Request Count": 1, "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 7720, "Helpful": ""}}, {"id": "i-8e2a5d4fcf25652e7e120759675f832f60afeef1fc08fc2241381b34e30060c0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8e2a5d4fcf25652e7e120759675f832f60afeef1fc08fc2241381b34e30060c0", "name": "What are some objections to the importance of AI alignment?", "index": 14, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:07:49.132Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8e2a5d4fcf25652e7e120759675f832f60afeef1fc08fc2241381b34e30060c0", "values": {"File": "What are some objections to the importance of AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some objections to the importance of AI alignment?", "Link": "https://docs.google.com/document/d/1qEcIkD4KL8mB8yQ7KW-MNAWRYFRnT2PqT6OxAHXFkGw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:29.506+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Objections", "Doc Last Edited": "2023-02-22T23:05:26.653+01:00", "Status": "Live on site", "Edit Answer": "What are some objections to the importance of AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7794", "Source Link": "", "aisafety.info Link": "What are some objections to the importance of AI alignment?", "Source": "Wiki", "All Phrasings": "What are some objections to the importance of AI alignment?\n", "Initial Order": 10, "Related IDs": "", "Rich Text DO NOT EDIT": "[S\u00f8ren Elverlin](https://aisafety.com/author/soeren-elverlin/) has compiled a list of counter-arguments and suggests dividing them into two kinds: weak and strong.\n\nWeak counter-arguments point to problems with the \"standard\" arguments (as given in, e.g., Bostrom\u2019s *[Superintelligence](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies)*), especially shaky models and assumptions that are too strong. These arguments are often of a substantial quality and are often presented by people who themselves worry about AI safety. Elverin calls these objections \u201cweak\u201d because they do not attempt to imply that the probability of a bad outcome is close to zero: *\u201cFor example, even if you accept* *[Paul Christiano's arguments against \u201cfast takeoff\u201d](https://www.alignmentforum.org/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds)**, they only drive the probability of this down to about 20%. Weak counter-arguments are interesting, but the decision to personally focus on AI safety doesn't strongly depend on the probability \u2013 anything above 5% is clearly a big enough deal that it doesn't make sense to work on other things.\u201d*\n\nStrong arguments argue that the probability of existential catastrophe due to misaligned AI is tiny, usually by some combination of claiming that AGI is impossible or very far away. For example, [Michael Littman](https://en.wikipedia.org/wiki/Michael_L._Littman) has [suggested](https://www.youtube.com/watch?v=c9AbECvRt20&t=1559s) that as (he believes) we\u2019re so far from AGI, there will be a long period of human history wherein we\u2019ll have ample time to grow up alongside powerful AIs and figure out how to align them.\n\nElverlin opines that *\u201cThere are few arguments that are both high-quality and strong enough to qualify as an \u2018objection to the importance of alignment\u2019.\u201d* He suggests [Rohin Shah's arguments for \u201calignment by default\u201d](https://aiimpacts.org/conversation-with-rohin-shah/) as one of the better candidates.\n\n[MIRI](https://intelligence.org/)'s April fools [\"Death With Dignity\" strategy](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy) might be seen as an argument against the importance of working on alignment, but only in the sense that we might have almost no hope of solving it. In the same category are the \u201csomething else will kill us first, so there\u2019s no point worrying about AI alignment\u201d arguments.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "[S\u00f8ren Elverlin](https://aisafety.com/author/soeren-elverlin/) has compiled a list of counter-arguments and suggests dividing them into two kinds: weak and strong.\n\nWeak counter-arguments point to problems with the \"standard\" arguments (as given in, e.g., Bostrom\u2019s *[Superintelligence](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies)*), especially shaky models and assumptions that are too strong. These arguments are often of a substantial quality and are often presented by people who themselves worry about AI safety. Elverin calls these objections \u201cweak\u201d because they do not attempt to imply that the probability of a bad outcome is close to zero: *\u201cFor example, even if you accept* *[Paul Christiano's arguments against \u201cfast takeoff\u201d](https://www.alignmentforum.org/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds)**, they only drive the probability of this down to about 20%. Weak counter-arguments are interesting, but the decision to personally focus on AI safety doesn't strongly depend on the probability \u2013 anything above 5% is clearly a big enough deal that it doesn't make sense to work on other things.\u201d*\n\nStrong arguments argue that the probability of existential catastrophe due to misaligned AI is tiny, usually by some combination of claiming that AGI is impossible or very far away. For example, [Michael Littman](https://en.wikipedia.org/wiki/Michael_L._Littman) has [suggested](https://www.youtube.com/watch?v=c9AbECvRt20&t=1559s) that as (he believes) we\u2019re so far from AGI, there will be a long period of human history wherein we\u2019ll have ample time to grow up alongside powerful AIs and figure out how to align them.\n\nElverlin opines that *\u201cThere are few arguments that are both high-quality and strong enough to qualify as an \u2018objection to the importance of alignment\u2019.\u201d* He suggests [Rohin Shah's arguments for \u201calignment by default\u201d](https://aiimpacts.org/conversation-with-rohin-shah/) as one of the better candidates.\n\n[MIRI](https://intelligence.org/)'s April fools [\"Death With Dignity\" strategy](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy) might be seen as an argument against the importance of working on alignment, but only in the sense that we might have almost no hope of solving it. In the same category are the \u201csomething else will kill us first, so there\u2019s no point worrying about AI alignment\u201d arguments.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7794", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:21.954+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 7720, "Helpful": ""}}, {"id": "i-6f8d8d0f1df96b40a38bd9747988e8af44331d1fee39d06daaa7d98b284a769b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-6f8d8d0f1df96b40a38bd9747988e8af44331d1fee39d06daaa7d98b284a769b", "name": "What are some important examples of specialised terminology in AI alignment?", "index": 15, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:07:53.745Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-6f8d8d0f1df96b40a38bd9747988e8af44331d1fee39d06daaa7d98b284a769b", "values": {"File": "What are some important examples of specialised terminology in AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some important examples of specialised terminology in AI alignment?", "Link": "https://docs.google.com/document/d/1ndhGU3nNg8sNjTE3_g28HsvxiCFj7UCEINK_TQGR_1E/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:26.014+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions", "Doc Last Edited": "2023-02-22T22:55:40.081+01:00", "Status": "In progress", "Edit Answer": "What are some important examples of specialised terminology in AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6691", "Source Link": "", "aisafety.info Link": "What are some important examples of specialised terminology in AI alignment?", "Source": "Wiki", "All Phrasings": "What are some important examples of specialised terminology in AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A good list can be found on the [Alignment Forum's tag list](https://www.alignmentforum.org/tags/all#Basic_Alignment_Theory).\n\nSelectively import from glossary at the end of Superintelligence\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "A good list can be found on the [Alignment Forum's tag list](https://www.alignmentforum.org/tags/all#Basic_Alignment_Theory).\n\nSelectively import from glossary at the end of Superintelligence\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6691", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:23.693+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 7720, "Helpful": ""}}, {"id": "i-aad565b931cf2449506782c2e6e0e4e253b6347d741456c208b76e3ef89fc44b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-aad565b931cf2449506782c2e6e0e4e253b6347d741456c208b76e3ef89fc44b", "name": "What are some helpful AI policy resources?", "index": 16, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:07:58.083Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-aad565b931cf2449506782c2e6e0e4e253b6347d741456c208b76e3ef89fc44b", "values": {"File": "What are some helpful AI policy resources?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some helpful AI policy resources?", "Link": "https://docs.google.com/document/d/1PWQOYFjoZKgcolQ_1c0YrMxP8m0W0ioCsWrts1TtoCQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:22.585+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Governance", "Doc Last Edited": "2023-02-26T11:11:13.638+01:00", "Status": "In progress", "Edit Answer": "What are some helpful AI policy resources?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7754", "Source Link": "", "aisafety.info Link": "What are some helpful AI policy resources?", "Source": "Wiki", "All Phrasings": "What are some helpful AI policy resources?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Stay informed on AI policy with these suggested references:\n\n- [AGISF governance track](https://www.agisafetyfundamentals.com/ai-governance-curriculum) - A curriculum designed to cover some research up to 2022 on why AI governance may be important to work on now, what large-scale risks AI poses, and which actors will play key roles in steering AI\u2019s trajectory, as well as what strategic considerations and policy tools may influence how these actors will or should steer.\n\n-  Worldwide AI governance and policy-making\n\n    - [Global AI Policy Dashboard](https://futureoflife.org/resource/ai-policy/) - This page highlights four complementary resources to help decision makers navigate AI policy: A dashboard that helps analyze the current documents published on the [OECD website](https://www.oecd.org/), a global landscape of national and international AI strategies; a list of prominent AI policy challenges and key recommendations that have been made to address them; and a list of AI policy resources for those hoping to learn more.\n\n    - United States\n\n        - [NCSL, Legislation Related to Artificial Intelligence](https://www.ncsl.org/technology-and-communication/legislation-related-to-artificial-intelligence)This web page covers key legislation related to AI issues generally. Legislation related solely to specific AI technologies, such as facial recognition or autonomous cars, is being tracked separately. (Simply search for \u201cEnacted\u201d so you\u2019ll know which ones are already enforced in various states.)\n\n        - The [\u2018AI](https://futureoflife.org/project/nist-framework/) [Risk Management Framework\u2019 (AI RMF)](https://futureoflife.org/project/nist-framework/) is a tool that developers can use to determine if their systems can be trusted. Through the [National AI Initiative Act of 2020](https://www.congress.gov/116/crpt/hrpt617/CRPT-116hrpt617.pdf#page=1210), the US Congress has asked the National Institute of Standards and Technology (NIST) to \u201cdevelop (..) a voluntary risk management framework for trustworthy artificial intelligence systems.\u201d\n\n    - Europe\n\n        - The [Artificial Intelligence Act](https://artificialintelligenceact.eu/about/). The purpose of this website is to provide up-to-date developments and analyses of the proposed EU artificial intelligence law called the \u201cAI Act''. \n\n        - [Future of Humanity Institute](https://www.fhi.ox.ac.uk/) - [UK - AI Governance Research Group](https://www.fhi.ox.ac.uk/ai-governance/#publications) offers authoritative, actionable and accessible insight to a range of audiences in policy, academia, and the public.\n\n- Forums\n\n    - [AI Governance EA](https://forum.effectivealtruism.org/topics/ai-governance)\n\n    - [AI Governance LessWrong](https://www.lesswrong.com/tag/ai-governance)\n\nAdditional resources:\n\n[Taxing Artificial Intelligence and Robots, World Tax Journal November 2020](https://www.ibfd.org/sites/default/files/2021-09/International%20-%20Taxing%20Artificial%20Intelligence%20and%20Robots%20Critical%20Assessment%20of%20Potential%20Policy%20Solutions%20and%20Recommendation%20for%20Alternative%20Approaches%20-%20IBFD.pdf) \n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Stay informed on AI policy with these suggested references:\n\n- [AGISF governance track](https://www.agisafetyfundamentals.com/ai-governance-curriculum) - A curriculum designed to cover some research up to 2022 on why AI governance may be important to work on now, what large-scale risks AI poses, and which actors will play key roles in steering AI\u2019s trajectory, as well as what strategic considerations and policy tools may influence how these actors will or should steer.\n\n-  Worldwide AI governance and policy-making\n\n    - [Global AI Policy Dashboard](https://futureoflife.org/resource/ai-policy/) - This page highlights four complementary resources to help decision makers navigate AI policy: A dashboard that helps analyze the current documents published on the [OECD website](https://www.oecd.org/), a global landscape of national and international AI strategies; a list of prominent AI policy challenges and key recommendations that have been made to address them; and a list of AI policy resources for those hoping to learn more.\n\n    - United States\n\n        - [NCSL, Legislation Related to Artificial Intelligence](https://www.ncsl.org/technology-and-communication/legislation-related-to-artificial-intelligence)This web page covers key legislation related to AI issues generally. Legislation related solely to specific AI technologies, such as facial recognition or autonomous cars, is being tracked separately. (Simply search for \u201cEnacted\u201d so you\u2019ll know which ones are already enforced in various states.)\n\n        - The [\u2018AI](https://futureoflife.org/project/nist-framework/) [Risk Management Framework\u2019 (AI RMF)](https://futureoflife.org/project/nist-framework/) is a tool that developers can use to determine if their systems can be trusted. Through the [National AI Initiative Act of 2020](https://www.congress.gov/116/crpt/hrpt617/CRPT-116hrpt617.pdf#page=1210), the US Congress has asked the National Institute of Standards and Technology (NIST) to \u201cdevelop (..) a voluntary risk management framework for trustworthy artificial intelligence systems.\u201d\n\n    - Europe\n\n        - The [Artificial Intelligence Act](https://artificialintelligenceact.eu/about/). The purpose of this website is to provide up-to-date developments and analyses of the proposed EU artificial intelligence law called the \u201cAI Act''. \n\n        - [Future of Humanity Institute](https://www.fhi.ox.ac.uk/) - [UK - AI Governance Research Group](https://www.fhi.ox.ac.uk/ai-governance/#publications) offers authoritative, actionable and accessible insight to a range of audiences in policy, academia, and the public.\n\n- Forums\n\n    - [AI Governance EA](https://forum.effectivealtruism.org/topics/ai-governance)\n\n    - [AI Governance LessWrong](https://www.lesswrong.com/tag/ai-governance)\n\nAdditional resources:\n\n[Taxing Artificial Intelligence and Robots, World Tax Journal November 2020](https://www.ibfd.org/sites/default/files/2021-09/International%20-%20Taxing%20Artificial%20Intelligence%20and%20Robots%20Critical%20Assessment%20of%20Potential%20Policy%20Solutions%20and%20Recommendation%20for%20Alternative%20Approaches%20-%20IBFD.pdf) \n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7754", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:25.808+01:00", "Request Count": "", "Number of suggestions on answer doc": 3, "Total character count of suggestions on answer doc": 7720, "Helpful": ""}}, {"id": "i-faa5bef6c3d5c95254b809ee68c145400780a1904d24a2acb6573acf25f54fae", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-faa5bef6c3d5c95254b809ee68c145400780a1904d24a2acb6573acf25f54fae", "name": "What are some good resources on AI alignment?", "index": 17, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:02.392Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-faa5bef6c3d5c95254b809ee68c145400780a1904d24a2acb6573acf25f54fae", "values": {"File": "What are some good resources on AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some good resources on AI alignment?", "Link": "https://docs.google.com/document/d/1sAEnMF6rScmLjlMx1Sa7cP9JREfa3Ez3Y0FZYAJs5G8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:18.934+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stampy,Resources,Literature", "Doc Last Edited": "2023-02-22T23:05:28.139+01:00", "Status": "Live on site", "Edit Answer": "What are some good resources on AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6470", "Source Link": "", "aisafety.info Link": "What are some good resources on AI alignment?", "Source": "Wiki", "All Phrasings": "What are some good resources on AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "These are good sources for understanding AI alignment and linking to when editing Stampy!\n\n- [Rob's YouTube videos](https://www.youtube.com/c/RobertMilesAI/videos) ([Computerphile appearances](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLqL14ZxTTA4fRMts7Af2G8t4Rp17e8MdS&index=4))\n\n- [AI Safety Papers database](https://ai-safety-papers.quantifieduncertainty.org/) - Search and interface for the [TAI Safety Bibliography](https://www.lesswrong.com/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographic-database)\n\n- [AGI Safety Fundamentals Course](https://www.eacambridge.org/agi-safety-fundamentals)\n\n- [Alignment Forum](https://www.alignmentforum.org/tags/) tags\n\n- [The Alignment Newsletter](https://rohinshah.com/alignment-newsletter/) (and [database sheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit#gid=0))\n\n- Chapters of [Bostrom's Superintelligence online](https://publicism.info/philosophy/superintelligence/) - [Initial paper which Superintelligence grew from](https://www.nickbostrom.com/views/superintelligence.pdf)\n\n- [AI Alignment pages on Arbital](https://arbital.greaterwrong.com/explore/ai_alignment/)\n\n- Much more on [AI Safety Support](https://www.aisafetysupport.org/resources/lots-of-links) (feel free to integrate useful things from there to here)\n\n- [Vika's resources list](https://vkrakovna.wordpress.com/ai-safety-resources/)\n\n- [AI safety technical courses, reading lists, and curriculums](https://docs.google.com/spreadsheets/d/1QSEWjXZuqmG6ORkig84V4sFCldIntyuQj7yq3gkDo0U/edit#gid=0)\n\n- [AI Safety Intro blog](https://aisafety.wordpress.com/)\n\n- [Our accepted answers list](https://coda.io/d/AI-Safety-Info-Dashboard_dfau7sl2hmG/All-on-site-answers_sueMP#All-on-site-answers_tu0M7/r265) - This includes updated versions of various [FAQs imported with permission](http://Imported_FAQs):\n\n    - [Scott Alexander's Superintelligence FAQ](https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq)\n\n    - [FLI's FAQ](https://futureoflife.org/ai-faqs/)\n\n    - [MIRI's FAQ](https://intelligence.org/faq/)\n\n    - [MIRI's Intelligence Explosion FAQ](https://intelligence.org/ie-faq/)\n\n    - [Advice for AI Alignment Researchers](https://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/)\n\n    - [r/ControlProblem's FAQ](https://www.reddit.com/r/ControlProblem/wiki/faq)\n\n    - [Mark Xu's FAQ](https://markxu.com/ai-safety-faqs)\n\n    - [AI safety blog](https://aisafety.wordpress.com/) - Not yet imported.\n\n- \n\n    - \n\n", "Tag Count": 3, "Related Answer Count": 0, "Rich Text": "These are good sources for understanding AI alignment and linking to when editing Stampy!\n\n- [Rob's YouTube videos](https://www.youtube.com/c/RobertMilesAI/videos) ([Computerphile appearances](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLqL14ZxTTA4fRMts7Af2G8t4Rp17e8MdS&index=4))\n\n- [AI Safety Papers database](https://ai-safety-papers.quantifieduncertainty.org/) - Search and interface for the [TAI Safety Bibliography](https://www.lesswrong.com/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographic-database)\n\n- [AGI Safety Fundamentals Course](https://www.eacambridge.org/agi-safety-fundamentals)\n\n- [Alignment Forum](https://www.alignmentforum.org/tags/) tags\n\n- [The Alignment Newsletter](https://rohinshah.com/alignment-newsletter/) (and [database sheet](https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit#gid=0))\n\n- Chapters of [Bostrom's Superintelligence online](https://publicism.info/philosophy/superintelligence/) - [Initial paper which Superintelligence grew from](https://www.nickbostrom.com/views/superintelligence.pdf)\n\n- [AI Alignment pages on Arbital](https://arbital.greaterwrong.com/explore/ai_alignment/)\n\n- Much more on [AI Safety Support](https://www.aisafetysupport.org/resources/lots-of-links) (feel free to integrate useful things from there to here)\n\n- [Vika's resources list](https://vkrakovna.wordpress.com/ai-safety-resources/)\n\n- [AI safety technical courses, reading lists, and curriculums](https://docs.google.com/spreadsheets/d/1QSEWjXZuqmG6ORkig84V4sFCldIntyuQj7yq3gkDo0U/edit#gid=0)\n\n- [AI Safety Intro blog](https://aisafety.wordpress.com/)\n\n- [Our accepted answers list](https://coda.io/d/AI-Safety-Info-Dashboard_dfau7sl2hmG/All-on-site-answers_sueMP#All-on-site-answers_tu0M7/r265) - This includes updated versions of various [FAQs imported with permission](http://Imported_FAQs):\n\n    - [Scott Alexander's Superintelligence FAQ](https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq)\n\n    - [FLI's FAQ](https://futureoflife.org/ai-faqs/)\n\n    - [MIRI's FAQ](https://intelligence.org/faq/)\n\n    - [MIRI's Intelligence Explosion FAQ](https://intelligence.org/ie-faq/)\n\n    - [Advice for AI Alignment Researchers](https://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/)\n\n    - [r/ControlProblem's FAQ](https://www.reddit.com/r/ControlProblem/wiki/faq)\n\n    - [Mark Xu's FAQ](https://markxu.com/ai-safety-faqs)\n\n    - [AI safety blog](https://aisafety.wordpress.com/) - Not yet imported.\n\n- \n\n    - \n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6470", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:29.720+01:00", "Request Count": "", "Number of suggestions on answer doc": 4, "Total character count of suggestions on answer doc": 7736, "Helpful": ""}}, {"id": "i-cc3c474630be6af4c1b1d86282971dc9d8680fe559101cdc27bc299c0f3afa70", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-cc3c474630be6af4c1b1d86282971dc9d8680fe559101cdc27bc299c0f3afa70", "name": "What are some good podcasts about AI alignment?", "index": 18, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:05.977Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-cc3c474630be6af4c1b1d86282971dc9d8680fe559101cdc27bc299c0f3afa70", "values": {"File": "What are some good podcasts about AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some good podcasts about AI alignment?", "Link": "https://docs.google.com/document/d/1rok_5uj-iYLi8HV0UFo3h7vfBK7jJFbxqFrm48i_L-o/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:15.614+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Content", "Doc Last Edited": "2023-03-05T13:10:38.495+01:00", "Status": "Live on site", "Edit Answer": "What are some good podcasts about AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7619", "Source Link": "", "aisafety.info Link": "What are some good podcasts about AI alignment?", "Source": "Wiki", "All Phrasings": "What are some good podcasts about AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "All the content below is in English:\n\n- The [AI technical safety section](https://80000hours.org/topic/priority-paths/technical-ai-safety/?content-type=podcast) of the 80,000 Hours Podcast;\n\n- The [AI X-risk Research Podcast](https://axrp.net/), hosted by Daniel Filan;\n\n- \n\n- The [AI Alignment Podcast](https://futureoflife.org/ai-alignment-podcast/) hosted by Lucas Perry from the Future of Life Institute (ran ~monthly from April 2018 to March 2021);\n\n- The [Alignment Newsletter Podcast](https://alignment-newsletter.libsyn.com/) by Rob Miles (an audio version of the weekly newsletter).\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "All the content below is in English:\n\n- The [AI technical safety section](https://80000hours.org/topic/priority-paths/technical-ai-safety/?content-type=podcast) of the 80,000 Hours Podcast;\n\n- The [AI X-risk Research Podcast](https://axrp.net/), hosted by Daniel Filan;\n\n- \n\n- The [AI Alignment Podcast](https://futureoflife.org/ai-alignment-podcast/) hosted by Lucas Perry from the Future of Life Institute (ran ~monthly from April 2018 to March 2021);\n\n- The [Alignment Newsletter Podcast](https://alignment-newsletter.libsyn.com/) by Rob Miles (an audio version of the weekly newsletter).\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7619", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:32.095+01:00", "Request Count": "", "Number of suggestions on answer doc": 5, "Total character count of suggestions on answer doc": 7786, "Helpful": ""}}, {"id": "i-bb3e4b44eb40aa5d5ddcfdc73a23eb05eebfd28ff6795da6986fff83f033be35", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-bb3e4b44eb40aa5d5ddcfdc73a23eb05eebfd28ff6795da6986fff83f033be35", "name": "What are some good books about AGI safety?", "index": 19, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:10.136Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-bb3e4b44eb40aa5d5ddcfdc73a23eb05eebfd28ff6795da6986fff83f033be35", "values": {"File": "What are some good books about AGI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some good books about AGI safety?", "Link": "https://docs.google.com/document/d/1B0h335V9S4A0q_Zwe-4RX7c_tFiKJJg75SHbMvBotBQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:12.124+01:00", "Related Answers DO NOT EDIT": "What are some good resources on AI alignment?", "Tags": "Literature", "Doc Last Edited": "2023-02-22T23:05:29.875+01:00", "Status": "Live on site", "Edit Answer": "What are some good books about AGI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8159", "Source Link": "", "aisafety.info Link": "What are some good books about AGI safety?", "Source": "Wiki", "All Phrasings": "What are some good books about AGI safety?\n", "Initial Order": "", "Related IDs": "6470", "Rich Text DO NOT EDIT": "*[The Alignment Problem](https://brianchristian.org/the-alignment-problem/)* (2020) by Brian Christian is the most recent in-depth guide to the field.\n\nThe book which first made the case to the public is Nick Bostrom\u2019s *[Superintelligence](https://publicism.info/philosophy/superintelligence/)* (2014). It gives an excellent overview of the state of the field (as it was then) and makes a strong case for the subject being important, as well as exploring many fascinating adjacent topics. However, it does not cover newer developments, such as [mesa-optimizers](http://What_are_mesa-optimizers?) or [language models](http://What_are_language_models?).\n\nThere's also *[Human Compatible](https://en.wikipedia.org/wiki/Human_Compatible)* (2019) by Stuart Russell, which gives a more up-to-date review of developments, with an emphasis on the approaches that the Center for Human-Compatible AI are working on, such as cooperative inverse reinforcement learning. There's a good [review/summary on SlateStarCodex](https://slatestarcodex.com/2020/01/30/book-review-human-compatible/).\n\nAlthough not limited to AI safety, *[The AI Does Not Hate You](https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795)* (2020) is an entertaining and accessible outline of both the core issues and an exploration of some of the community and culture of the people working on it.\n\nVarious other books explore the issues in an informed way, such as [Toby Ord](http://www.tobyord.com/)\u2019s *[The Precipice](https://en.wikipedia.org/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity)* (2020), [Max Tegmark](https://en.wikipedia.org/wiki/Max_Tegmark)\u2019s *[Life 3.0](https://en.wikipedia.org/wiki/Life_3.0)* (2017), [Yuval Noah Harari](https://www.ynharari.com/)\u2019s *[Homo Deus](https://www.ynharari.com/book/homo-deus/)* (2016), [Stuart Armstrong](https://www.fhi.ox.ac.uk/team/stuart-armstrong/)\u2019s *[Smarter Than Us](https://smarterthan.us/toc/)* (2014), and [Luke Muehlhauser](http://lukeprog.com/)\u2019s *[Facing the Intelligence Explosion](https://intelligenceexplosion.com/)* (2013).\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "*[The Alignment Problem](https://brianchristian.org/the-alignment-problem/)* (2020) by Brian Christian is the most recent in-depth guide to the field.\n\nThe book which first made the case to the public is Nick Bostrom\u2019s *[Superintelligence](https://publicism.info/philosophy/superintelligence/)* (2014). It gives an excellent overview of the state of the field (as it was then) and makes a strong case for the subject being important, as well as exploring many fascinating adjacent topics. However, it does not cover newer developments, such as [mesa-optimizers](http://What_are_mesa-optimizers?) or [language models](http://What_are_language_models?).\n\nThere's also *[Human Compatible](https://en.wikipedia.org/wiki/Human_Compatible)* (2019) by Stuart Russell, which gives a more up-to-date review of developments, with an emphasis on the approaches that the Center for Human-Compatible AI are working on, such as cooperative inverse reinforcement learning. There's a good [review/summary on SlateStarCodex](https://slatestarcodex.com/2020/01/30/book-review-human-compatible/).\n\nAlthough not limited to AI safety, *[The AI Does Not Hate You](https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795)* (2020) is an entertaining and accessible outline of both the core issues and an exploration of some of the community and culture of the people working on it.\n\nVarious other books explore the issues in an informed way, such as [Toby Ord](http://www.tobyord.com/)\u2019s *[The Precipice](https://en.wikipedia.org/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity)* (2020), [Max Tegmark](https://en.wikipedia.org/wiki/Max_Tegmark)\u2019s *[Life 3.0](https://en.wikipedia.org/wiki/Life_3.0)* (2017), [Yuval Noah Harari](https://www.ynharari.com/)\u2019s *[Homo Deus](https://www.ynharari.com/book/homo-deus/)* (2016), [Stuart Armstrong](https://www.fhi.ox.ac.uk/team/stuart-armstrong/)\u2019s *[Smarter Than Us](https://smarterthan.us/toc/)* (2014), and [Luke Muehlhauser](http://lukeprog.com/)\u2019s *[Facing the Intelligence Explosion](https://intelligenceexplosion.com/)* (2013).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "8159", "Related Answers": "What are some good resources on AI alignment?", "Doc Last Ingested": "2023-03-14T23:52:34.134+01:00", "Request Count": "", "Number of suggestions on answer doc": 5, "Total character count of suggestions on answer doc": 7786, "Helpful": ""}}, {"id": "i-da5494d80066486c51376cef8f276b0a722dfdd7cc035c6e1ba4e3b8857ceefa", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-da5494d80066486c51376cef8f276b0a722dfdd7cc035c6e1ba4e3b8857ceefa", "name": "What are some AI alignment research agendas currently being pursued?", "index": 20, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:13.888Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-da5494d80066486c51376cef8f276b0a722dfdd7cc035c6e1ba4e3b8857ceefa", "values": {"File": "What are some AI alignment research agendas currently being pursued?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are some AI alignment research agendas currently being pursued?", "Link": "https://docs.google.com/document/d/16ug__yrBL-VByJmS7hzpeV74C4b-lBGzB5oIGNNGn3o/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:08.125+01:00", "Related Answers DO NOT EDIT": "What is everyone working on in AI alignment?,What is neural network modularity?", "Tags": "Existential Risk,Research Agendas", "Doc Last Edited": "2023-02-22T23:05:30.736+01:00", "Status": "Live on site", "Edit Answer": "What are some AI alignment research agendas currently being pursued?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6479", "Source Link": "", "aisafety.info Link": "What are some AI alignment research agendas currently being pursued?", "Source": "Wiki", "All Phrasings": "What are some AI alignment research agendas currently being pursued?\n", "Initial Order": 3.5, "Related IDs": "8392,8424", "Rich Text DO NOT EDIT": "Research at the [Alignment Research Center](http://alignmentresearchcenter.org/) is led by [Paul Christiano](https://paulfchristiano.com/), best known for introducing the [\u201cIterated Distillation and Amplification\u201d](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616) and [\u201cHumans Consulting HCH\u201d](https://ai-alignment.com/humans-consulting-hch-f893f6051455) approaches. He and his team are now *\u201ctrying to figure out how to train ML systems to answer questions by straightforwardly \u2018translating\u2019 their beliefs into natural language rather than by reasoning about what a human wants to hear.\u201d*\n\n[Chris Olah](https://colah.github.io/about.html) (after work at [DeepMind](https://en.wikipedia.org/wiki/DeepMind) and [OpenAI](https://en.wikipedia.org/wiki/OpenAI)) recently launched [Anthropic](https://www.anthropic.com/), an AI lab focussed on the safety of large models. While his previous work was concerned with [\u201ctransparency\u201d and \u201cinterpretability\u201d of large neural networks](https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/), especially vision models, Anthropic is focussing more on large language models, among other things working towards a *\"general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless\".*\n\n[Stuart Russell](https://en.wikipedia.org/wiki/Stuart_J._Russell) and his team at the [Center for Human-Compatible Artificial Intelligence](https://en.wikipedia.org/wiki/Center_for_Human-Compatible_Artificial_Intelligence) (CHAI) have been working on [inverse reinforcement learning](https://arxiv.org/abs/1806.06877) (where the AI infers human values from observing human behavior) and [corrigibility](https://intelligence.org/files/CorrigibilityAISystems.pdf), as well as attempts to disaggregate neural networks into \u201cmeaningful\u201d subcomponents (see Filan, et al.\u2019s [\u201cClusterability in neural networks\u201d](https://arxiv.org/abs/2103.03386) and Hod et al.'s [\u201cDetecting modularity in deep neural networks](https://openreview.net/forum?id=tFQyjbOz34)\u201d).\n\nAlongside the more abstract [\u201cagent foundations\u201d](https://intelligence.org/files/TechnicalAgenda.pdf) work they have become known for, [MIRI](https://intelligence.org/) recently announced their [\u201cVisible Thoughts Project\u201d](https://www.lesswrong.com/posts/zRn6cLtxyNodudzhw/visible-thoughts-project-and-bounty-announcement) to test the hypothesis that *\u201cLanguage models can be made more understandable (and perhaps also more capable, though this is not the goal) by training them to produce visible thoughts.\u201d*\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) have recently been doing work on [iteratively summarizing books](https://openai.com/blog/summarizing-books/) (summarizing, and then summarizing the summary, etc.) as a method for scaling human oversight.\n\nStuart Armstrong\u2019s recently launched [AlignedAI](https://buildaligned.ai/) are mainly working on [concept extrapolation](https://www.alignmentforum.org/s/u9uawicHx7Ng7vwxA) from familiar to novel contexts, something he believes is \u201cnecessary and almost sufficient\u201d for AI alignment.\n\n[Redwood Research](https://www.redwoodresearch.org/) (Buck Shlegeris, et al.) are trying to \u201chandicap' GPT-3 to only produce non-violent completions of text prompts. *\u201cThe idea is that there are many reasons we might ultimately want to apply some oversight function to an AI model, like \u2018don't be deceitful\u2019, and if we want to get AI teams to apply this we need to be able to incorporate these oversight predicates into the original model in an efficient manner.\u201d*\n\n[Ought](https://ought.org/) is an independent AI safety research organization led by Andreas Stuhlm\u00fcller and Jungwon Byun. They are researching methods for breaking up complex, hard-to-verify tasks into simpler, easier-to-verify tasks, with the aim of allowing us to maintain effective oversight over AIs.\n\n", "Tag Count": 2, "Related Answer Count": 2, "Rich Text": "Research at the [Alignment Research Center](http://alignmentresearchcenter.org/) is led by [Paul Christiano](https://paulfchristiano.com/), best known for introducing the [\u201cIterated Distillation and Amplification\u201d](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616) and [\u201cHumans Consulting HCH\u201d](https://ai-alignment.com/humans-consulting-hch-f893f6051455) approaches. He and his team are now *\u201ctrying to figure out how to train ML systems to answer questions by straightforwardly \u2018translating\u2019 their beliefs into natural language rather than by reasoning about what a human wants to hear.\u201d*\n\n[Chris Olah](https://colah.github.io/about.html) (after work at [DeepMind](https://en.wikipedia.org/wiki/DeepMind) and [OpenAI](https://en.wikipedia.org/wiki/OpenAI)) recently launched [Anthropic](https://www.anthropic.com/), an AI lab focussed on the safety of large models. While his previous work was concerned with [\u201ctransparency\u201d and \u201cinterpretability\u201d of large neural networks](https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/), especially vision models, Anthropic is focussing more on large language models, among other things working towards a *\"general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless\".*\n\n[Stuart Russell](https://en.wikipedia.org/wiki/Stuart_J._Russell) and his team at the [Center for Human-Compatible Artificial Intelligence](https://en.wikipedia.org/wiki/Center_for_Human-Compatible_Artificial_Intelligence) (CHAI) have been working on [inverse reinforcement learning](https://arxiv.org/abs/1806.06877) (where the AI infers human values from observing human behavior) and [corrigibility](https://intelligence.org/files/CorrigibilityAISystems.pdf), as well as attempts to disaggregate neural networks into \u201cmeaningful\u201d subcomponents (see Filan, et al.\u2019s [\u201cClusterability in neural networks\u201d](https://arxiv.org/abs/2103.03386) and Hod et al.'s [\u201cDetecting modularity in deep neural networks](https://openreview.net/forum?id=tFQyjbOz34)\u201d).\n\nAlongside the more abstract [\u201cagent foundations\u201d](https://intelligence.org/files/TechnicalAgenda.pdf) work they have become known for, [MIRI](https://intelligence.org/) recently announced their [\u201cVisible Thoughts Project\u201d](https://www.lesswrong.com/posts/zRn6cLtxyNodudzhw/visible-thoughts-project-and-bounty-announcement) to test the hypothesis that *\u201cLanguage models can be made more understandable (and perhaps also more capable, though this is not the goal) by training them to produce visible thoughts.\u201d*\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) have recently been doing work on [iteratively summarizing books](https://openai.com/blog/summarizing-books/) (summarizing, and then summarizing the summary, etc.) as a method for scaling human oversight.\n\nStuart Armstrong\u2019s recently launched [AlignedAI](https://buildaligned.ai/) are mainly working on [concept extrapolation](https://www.alignmentforum.org/s/u9uawicHx7Ng7vwxA) from familiar to novel contexts, something he believes is \u201cnecessary and almost sufficient\u201d for AI alignment.\n\n[Redwood Research](https://www.redwoodresearch.org/) (Buck Shlegeris, et al.) are trying to \u201chandicap' GPT-3 to only produce non-violent completions of text prompts. *\u201cThe idea is that there are many reasons we might ultimately want to apply some oversight function to an AI model, like \u2018don't be deceitful\u2019, and if we want to get AI teams to apply this we need to be able to incorporate these oversight predicates into the original model in an efficient manner.\u201d*\n\n[Ought](https://ought.org/) is an independent AI safety research organization led by Andreas Stuhlm\u00fcller and Jungwon Byun. They are researching methods for breaking up complex, hard-to-verify tasks into simpler, easier-to-verify tasks, with the aim of allowing us to maintain effective oversight over AIs.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "6479", "Related Answers": "What is everyone working on in AI alignment?,What is neural network modularity?", "Doc Last Ingested": "2023-03-14T23:52:35.744+01:00", "Request Count": "", "Number of suggestions on answer doc": 5, "Total character count of suggestions on answer doc": 7786, "Helpful": ""}}, {"id": "i-e49f2af9b8d2ccbd0b4d64dd0d3c8ef7f39cfd932c83b3faee2db4f68eb2c5b8", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e49f2af9b8d2ccbd0b4d64dd0d3c8ef7f39cfd932c83b3faee2db4f68eb2c5b8", "name": "What are \"pivotal acts\"?", "index": 21, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:17.957Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e49f2af9b8d2ccbd0b4d64dd0d3c8ef7f39cfd932c83b3faee2db4f68eb2c5b8", "values": {"File": "What are \"pivotal acts\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are \"pivotal acts\"?", "Link": "https://docs.google.com/document/d/1Ug3BJndXK14BV_h70vhpXYVQqJuSOec7IagRYHDctRw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:04.841+01:00", "Related Answers DO NOT EDIT": "What concrete work is being done on time-buying strategies?", "Tags": "Pivotal Act", "Doc Last Edited": "2023-02-22T22:55:42.096+01:00", "Status": "In progress", "Edit Answer": "What are \"pivotal acts\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7580", "Source Link": "", "aisafety.info Link": "What are \"pivotal acts\"?", "Source": "Wiki", "All Phrasings": "What are \"pivotal acts\"?\n", "Initial Order": "", "Related IDs": "8AUY", "Rich Text DO NOT EDIT": "[Pivotal acts](https://arbital.greaterwrong.com/p/pivotal/) are actions that substantially change for the better where humanity will be in a billion years. The term is used as an opposite of existential catastrophe.\n\nPivotal acts were proposed as a way for researchers to buy sufficient time to completely solve AI alignment. It was hoped that it would be easier to align an AI that could take this limited action than to solve the complete alignment problem. In this context, [they are not a call for direct human action](https://www.facebook.com/yudkowsky/posts/pfbid0QQFqDiLzg3SsPWuCDnHZ5p6ak7XbKPXRUV7nstQQcfdtW1uZdsgjihYb6oHQp48El).\n\nOne way to think about it is as a limited formulation of the alignment problem. Is it possible to give precise enough instructions to an AI powerful enough to do something (without unwanted side effects) that would *actually* prevent other people from deploying an unaligned AI. The problem is that, at first sight, we won\u2019t be able to design a safe limited AI that is actually able to do something that makes a long term difference (see [numbers 5-7 on this list](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_A_)).\n\nWhen [MIRI](https://intelligence.org/) talks about this problem, they often use the strawberry task as an example of the level of power needed for a pivotal act. The strawberry task is to produce 2 strawberries identical to the cellular level (but not the molecular level) with no unwanted side effects. If we had an alignment technique which could reliably get an AI to achieve such a task, then that AI could plausibly be used for a pivotal act.\n\nFor an opposing view, [Andrew Critch argues against this strategy of designing an AI to take a unilateral \u201cpivotal act\u201d](https://www.lesswrong.com/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious) since it will lead to distrust and increase conflict and racing between different AI labs.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "[Pivotal acts](https://arbital.greaterwrong.com/p/pivotal/) are actions that substantially change for the better where humanity will be in a billion years. The term is used as an opposite of existential catastrophe.\n\nPivotal acts were proposed as a way for researchers to buy sufficient time to completely solve AI alignment. It was hoped that it would be easier to align an AI that could take this limited action than to solve the complete alignment problem. In this context, [they are not a call for direct human action](https://www.facebook.com/yudkowsky/posts/pfbid0QQFqDiLzg3SsPWuCDnHZ5p6ak7XbKPXRUV7nstQQcfdtW1uZdsgjihYb6oHQp48El).\n\nOne way to think about it is as a limited formulation of the alignment problem. Is it possible to give precise enough instructions to an AI powerful enough to do something (without unwanted side effects) that would *actually* prevent other people from deploying an unaligned AI. The problem is that, at first sight, we won\u2019t be able to design a safe limited AI that is actually able to do something that makes a long term difference (see [numbers 5-7 on this list](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_A_)).\n\nWhen [MIRI](https://intelligence.org/) talks about this problem, they often use the strawberry task as an example of the level of power needed for a pivotal act. The strawberry task is to produce 2 strawberries identical to the cellular level (but not the molecular level) with no unwanted side effects. If we had an alignment technique which could reliably get an AI to achieve such a task, then that AI could plausibly be used for a pivotal act.\n\nFor an opposing view, [Andrew Critch argues against this strategy of designing an AI to take a unilateral \u201cpivotal act\u201d](https://www.lesswrong.com/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious) since it will lead to distrust and increase conflict and racing between different AI labs.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7580", "Related Answers": "What concrete work is being done on time-buying strategies?", "Doc Last Ingested": "2023-03-14T23:52:38.118+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-c36dc9c18069192f42e96cc2fe791992f1be6f42abaaac0422fa940a5bfaa4db", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c36dc9c18069192f42e96cc2fe791992f1be6f42abaaac0422fa940a5bfaa4db", "name": "What are mesa-optimizers?", "index": 22, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:21.602Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c36dc9c18069192f42e96cc2fe791992f1be6f42abaaac0422fa940a5bfaa4db", "values": {"File": "What are mesa-optimizers?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are mesa-optimizers?", "Link": "https://docs.google.com/document/d/18AO1vfZAUj8hwDzCuhvznxDIZfNVQ6WlRRUoOJY70D4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:18:01.505+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Mesa-optimization,Definitions", "Doc Last Edited": "2023-02-22T23:05:31.903+01:00", "Status": "Live on site", "Edit Answer": "What are mesa-optimizers?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8160", "Source Link": "", "aisafety.info Link": "What are mesa-optimizers?", "Source": "Wiki", "All Phrasings": "What are mesa-optimizers?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Machine learning is used to create computer programs to solve problems. Some programs created this way are very simple, capable of solving the specific problem they were trained to solve.\n\nOther programs created this way are more advanced, problem-solvers in their own right. When a machine learning process results in a piece of software that is a problem solver itself, we call the created problem-solver a \"mesa-optimizer\".\n\n\"Mesa\" is Greek for inner/within/inside. \"Optimizer\" is the term for something that solves problems. So \"mesa-optimizer\" means something like \"inner problem-solver\".\n\nIn this context, the \"outer problem-solver\" is the machine learning process that created the mesa-optimizer.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Machine learning is used to create computer programs to solve problems. Some programs created this way are very simple, capable of solving the specific problem they were trained to solve.\n\nOther programs created this way are more advanced, problem-solvers in their own right. When a machine learning process results in a piece of software that is a problem solver itself, we call the created problem-solver a \"mesa-optimizer\".\n\n\"Mesa\" is Greek for inner/within/inside. \"Optimizer\" is the term for something that solves problems. So \"mesa-optimizer\" means something like \"inner problem-solver\".\n\nIn this context, the \"outer problem-solver\" is the machine learning process that created the mesa-optimizer.\n\n", "Stamp Count": 0, "Multi Answer": true, "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "https://www.lesswrong.com/tag/mesa-optimization?edit=true", "Last Asked On Discord": "", "UI ID": "8160", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:39.889+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-7c50882a911fd41f321f72353b367f1737131bfcd68ac39e8e9cf15cc966c01f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7c50882a911fd41f321f72353b367f1737131bfcd68ac39e8e9cf15cc966c01f", "name": "What are likely to be the first transformative applications of AI?", "index": 23, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:25.227Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7c50882a911fd41f321f72353b367f1737131bfcd68ac39e8e9cf15cc966c01f", "values": {"File": "What are likely to be the first transformative applications of AI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are likely to be the first transformative applications of AI?", "Link": "https://docs.google.com/document/d/1EJBlphDWxVu16OTrFazGJrehPZlj7ODo1oinV6nIMbA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:58.084+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Transformative AI", "Doc Last Edited": "2023-02-22T22:55:43.131+01:00", "Status": "Not started", "Edit Answer": "What are likely to be the first transformative applications of AI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7581", "Source Link": "", "aisafety.info Link": "What are likely to be the first transformative applications of AI?", "Source": "Wiki", "All Phrasings": "What are likely to be the first transformative applications of AI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A shift in the way that humans interact with knowledge, akin to the transformation brought about by the internet or the printing press\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "A shift in the way that humans interact with knowledge, akin to the transformation brought about by the internet or the printing press\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7581", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:41.997+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-ebac13e7de6ddac9a3018023378005f5874eb5eb1d5c285cbe4b36e015f14ca9", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ebac13e7de6ddac9a3018023378005f5874eb5eb1d5c285cbe4b36e015f14ca9", "name": "What are language models?", "index": 24, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:27.959Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ebac13e7de6ddac9a3018023378005f5874eb5eb1d5c285cbe4b36e015f14ca9", "values": {"File": "What are language models?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are language models?", "Link": "https://docs.google.com/document/d/1ftH65iVEC6Q341Tv0I-BX0WZufu-lfnlGtdfVC3jYN4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:54.411+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,Language Models", "Doc Last Edited": "2023-02-22T23:05:33.035+01:00", "Status": "Live on site", "Edit Answer": "What are language models?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8161", "Source Link": "", "aisafety.info Link": "What are language models?", "Source": "Wiki", "All Phrasings": "What are language models?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Language models are computer programs made to estimate the likelihood of a piece of text. \"Hello, how are you?\" is likely. \"Hello, fnarg horses\" is unlikely.\n\nLanguage models can answer questions by estimating the likelihood of possible question-and-answer pairs, selecting the most likely question-and-answer pair. \"Q: How are You? A: Very well, thank you\" is a likely question-and-answer pair. \"Q: How are You? A: Correct horse battery staple\" is an unlikely question-and-answer pair.\n\nThe language models most relevant to AI safety are language models based on \"deep learning\". Deep-learning-based language models can be \"trained\" to understand language better, by exposing them to text written by humans. There is a lot of human-written text on the internet, providing loads of training material.\n\nDeep-learning-based language models are getting bigger and better trained. As the models become stronger, they get new skills. These skills include arithmetic, explaining jokes, programming, and solving math problems.\n\nThere is a potential risk of these models developing dangerous capabilities as they grow larger and better trained. What additional skills will they develop given a few years?\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Language models are computer programs made to estimate the likelihood of a piece of text. \"Hello, how are you?\" is likely. \"Hello, fnarg horses\" is unlikely.\n\nLanguage models can answer questions by estimating the likelihood of possible question-and-answer pairs, selecting the most likely question-and-answer pair. \"Q: How are You? A: Very well, thank you\" is a likely question-and-answer pair. \"Q: How are You? A: Correct horse battery staple\" is an unlikely question-and-answer pair.\n\nThe language models most relevant to AI safety are language models based on \"deep learning\". Deep-learning-based language models can be \"trained\" to understand language better, by exposing them to text written by humans. There is a lot of human-written text on the internet, providing loads of training material.\n\nDeep-learning-based language models are getting bigger and better trained. As the models become stronger, they get new skills. These skills include arithmetic, explaining jokes, programming, and solving math problems.\n\nThere is a potential risk of these models developing dangerous capabilities as they grow larger and better trained. What additional skills will they develop given a few years?\n\n", "Stamp Count": 0, "Multi Answer": true, "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "https://www.lesswrong.com/tag/language-models?edit=true", "Last Asked On Discord": "", "UI ID": "8161", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:44.635+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-03078703d3fd689769450c19ecc52f31f27457484a73488e4aa9459dd8d6cf8e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-03078703d3fd689769450c19ecc52f31f27457484a73488e4aa9459dd8d6cf8e", "name": "What are brain-computer interfaces?", "index": 25, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:31.588Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-03078703d3fd689769450c19ecc52f31f27457484a73488e4aa9459dd8d6cf8e", "values": {"File": "What are brain-computer interfaces?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are brain-computer interfaces?", "Link": "https://docs.google.com/document/d/1xJtiuN-_e9178tM57EcVWUlOhQc18eLUeUuX7T3ywQ8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:50.986+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Definitions,Brain-computer Interfaces", "Doc Last Edited": "2023-02-22T23:05:33.926+01:00", "Status": "Live on site", "Edit Answer": "What are brain-computer interfaces?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6592", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "What are brain-computer interfaces?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "What are brain-computer interfaces?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A brain-computer interface (BCI) is a direct communication pathway between the brain and a computer device. BCI research is heavily funded, and has already met dozens of successes. Three successes in human BCIs are [a device](http://archives.cnn.com/2002/HEALTH/06/13/cov.bionic.eye/index.html) that restores (partial) sight to the blind, [cochlear implants](http://en.wikipedia.org/wiki/Cochlear_implant) that restore hearing to the deaf, and [a device](https://pubmed.ncbi.nlm.nih.gov/16838014/) that allows use of an artificial hand by direct thought.\n\nSuch device restore impaired functions, but many researchers expect to also augment and improve normal human abilities with BCIs. [Ed Boyden](http://edboyden.org/) is researching these opportunities as the lead of the [Synthetic Neurobiology Group](http://syntheticneurobiology.org/) at MIT. Such devices might hasten the arrival of an intelligence explosion, if only by improving human intelligence so that the hard problems of AI can be solved more rapidly.\n\nSee also:\n\nWikipedia, [Brain-computer interface](http://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "A brain-computer interface (BCI) is a direct communication pathway between the brain and a computer device. BCI research is heavily funded, and has already met dozens of successes. Three successes in human BCIs are [a device](http://archives.cnn.com/2002/HEALTH/06/13/cov.bionic.eye/index.html) that restores (partial) sight to the blind, [cochlear implants](http://en.wikipedia.org/wiki/Cochlear_implant) that restore hearing to the deaf, and [a device](https://pubmed.ncbi.nlm.nih.gov/16838014/) that allows use of an artificial hand by direct thought.\n\nSuch device restore impaired functions, but many researchers expect to also augment and improve normal human abilities with BCIs. [Ed Boyden](http://edboyden.org/) is researching these opportunities as the lead of the [Synthetic Neurobiology Group](http://syntheticneurobiology.org/) at MIT. Such devices might hasten the arrival of an intelligence explosion, if only by improving human intelligence so that the hard problems of AI can be solved more rapidly.\n\nSee also:\n\nWikipedia, [Brain-computer interface](http://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "2023-03-03T00:14:31.690+01:00", "UI ID": "6592", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:47.069+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-0940d1ba83d8cd30fc9f1c12f742a542d35057e0d5d0ca15c6e2c803ae598f4e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0940d1ba83d8cd30fc9f1c12f742a542d35057e0d5d0ca15c6e2c803ae598f4e", "name": "What are alternate phrasings for?", "index": 26, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:34.132Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0940d1ba83d8cd30fc9f1c12f742a542d35057e0d5d0ca15c6e2c803ae598f4e", "values": {"File": "What are alternate phrasings for?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are alternate phrasings for?", "Link": "https://docs.google.com/document/d/1at2ea7gTLuC0m9STYVUVFReMmFxVrHtbWKKmeEKHQDc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:46.935+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stampy", "Doc Last Edited": "2023-02-22T23:05:35.099+01:00", "Status": "Live on site", "Edit Answer": "What are alternate phrasings for?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7116", "Source Link": "", "aisafety.info Link": "What are alternate phrasings for?", "Source": "Wiki", "All Phrasings": "What are alternate phrasings for?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Alternate phrasings are used to improve the semantic search which Stampy uses to serve people questions, by giving alternate ways to say a question which might trigger a match when the main wording won't. They should generally only be used when there is a significantly different wording, rather than for only very minor changes.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Alternate phrasings are used to improve the semantic search which Stampy uses to serve people questions, by giving alternate ways to say a question which might trigger a match when the main wording won't. They should generally only be used when there is a significantly different wording, rather than for only very minor changes.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7116", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:56.461+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-7661aa640cdea3dada7f55476717e1af81ce009b2a8220a9478c63f41c86789b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7661aa640cdea3dada7f55476717e1af81ce009b2a8220a9478c63f41c86789b", "name": "What are Scott Garrabrant and Abram Demski working on?", "index": 27, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:36.534Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7661aa640cdea3dada7f55476717e1af81ce009b2a8220a9478c63f41c86789b", "values": {"File": "What are Scott Garrabrant and Abram Demski working on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are Scott Garrabrant and Abram Demski working on?", "Link": "https://docs.google.com/document/d/1K2AFTULt2Zm0xm4d9_9NI9Ln0sgWCZMpAcj5yqqcou8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:43.769+01:00", "Related Answers DO NOT EDIT": "", "Tags": "MIRI", "Doc Last Edited": "2023-02-22T23:05:36.014+01:00", "Status": "Live on site", "Edit Answer": "What are Scott Garrabrant and Abram Demski working on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8364", "Source Link": "", "aisafety.info Link": "What are Scott Garrabrant and Abram Demski working on?", "Source": "Wiki", "All Phrasings": "What are Scott Garrabrant and Abram Demski working on?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "They are working on fundamental problems like [embeddedness, decision theory, logical counterfactuals](https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version), and more. A big advance was [Cartesian Frames](https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames), a formal model of agency, and [Finite Factored Sets](https://www.alignmentforum.org/posts/N5Jm6Nj4HkNKySA5Z/finite-factored-sets) which reframes time in a way which is more compatible with agency.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "They are working on fundamental problems like [embeddedness, decision theory, logical counterfactuals](https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version), and more. A big advance was [Cartesian Frames](https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames), a formal model of agency, and [Finite Factored Sets](https://www.alignmentforum.org/posts/N5Jm6Nj4HkNKySA5Z/finite-factored-sets) which reframes time in a way which is more compatible with agency.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8364", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:52:57.852+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-24b46d59d9cd4ae58ac81d01990b4ed429603e49d703d7bffabe56c93f91df60", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-24b46d59d9cd4ae58ac81d01990b4ed429603e49d703d7bffabe56c93f91df60", "name": "What are OpenAI Codex and GitHub Copilot?", "index": 28, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:41.908Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-24b46d59d9cd4ae58ac81d01990b4ed429603e49d703d7bffabe56c93f91df60", "values": {"File": "What are OpenAI Codex and GitHub Copilot?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are OpenAI Codex and GitHub Copilot?", "Link": "https://docs.google.com/document/d/1DPon9X5e_637xzHCC6I8r2A1kc8t34nRDxCwRq4Ldzk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:39.675+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Language Models", "Doc Last Edited": "2023-02-22T22:55:44.213+01:00", "Status": "In progress", "Edit Answer": "What are OpenAI Codex and GitHub Copilot?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6628", "Source Link": "", "aisafety.info Link": "What are OpenAI Codex and GitHub Copilot?", "Source": "Wiki", "All Phrasings": "What are OpenAI Codex and GitHub Copilot?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Codex / Github Copilot are AIs that use [GPT-3](http://What_is_GPT-3?) to write and edit code. When given some input code and comments describing the intended function, they will write output that extends the prompt as accurately as possible.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Codex / Github Copilot are AIs that use [GPT-3](http://What_is_GPT-3?) to write and edit code. When given some input code and comments describing the intended function, they will write output that extends the prompt as accurately as possible.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6628", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:00.179+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-af16b812ead0bfc2a2dd6433c00a02da3ae39b8fc635e5836df82933e6251389", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-af16b812ead0bfc2a2dd6433c00a02da3ae39b8fc635e5836df82933e6251389", "name": "What are Encultured working on?", "index": 29, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:08:45.740Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-af16b812ead0bfc2a2dd6433c00a02da3ae39b8fc635e5836df82933e6251389", "values": {"File": "What are Encultured working on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are Encultured working on?", "Link": "https://docs.google.com/document/d/1D-2jFLjjjJQIxITeO6Eg4Lg9AtspO05QBkxqwnWOnAE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:36.294+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Encultured", "Doc Last Edited": "2023-02-22T23:05:37.131+01:00", "Status": "Live on site", "Edit Answer": "What are Encultured working on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8349", "Source Link": "", "aisafety.info Link": "What are Encultured working on?", "Source": "Wiki", "All Phrasings": "What are Encultured working on?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "See [Encultured AI: Building a Video Game](https://www.lesswrong.com/posts/ALkH4o53ofm862vxc/announcing-encultured-ai-building-a-video-game).\n\nEncultured are making a multiplayer online video game as a test environment for AI: an aligned AI should be able to play the game without ruining the fun or doing something obviously destructive like completely taking over the world, even if it has this capabilities. This seems roughly analogous to setting an AGI loose on the real world.\n\nMotivation: Andrew Critch is primarily concerned about a [multipolar AI scenario](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic): there are multiple actors with comparably powerful AI, on the cusp of recursive self improvement. The worst case is a race, and even though each actor would want to take more time checking their AGI for safety, worry that another actor will deploy will push each actor to take shortcuts and try to pull off a world-saving act. Instead of working directly on AI, which can accelerate timelines and encourage racing, creating this standardized test environment where alignment failures are observable is one component of a good global outcome.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "See [Encultured AI: Building a Video Game](https://www.lesswrong.com/posts/ALkH4o53ofm862vxc/announcing-encultured-ai-building-a-video-game).\n\nEncultured are making a multiplayer online video game as a test environment for AI: an aligned AI should be able to play the game without ruining the fun or doing something obviously destructive like completely taking over the world, even if it has this capabilities. This seems roughly analogous to setting an AGI loose on the real world.\n\nMotivation: Andrew Critch is primarily concerned about a [multipolar AI scenario](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic): there are multiple actors with comparably powerful AI, on the cusp of recursive self improvement. The worst case is a race, and even though each actor would want to take more time checking their AGI for safety, worry that another actor will deploy will push each actor to take shortcuts and try to pull off a world-saving act. Instead of working directly on AI, which can accelerate timelines and encourage racing, creating this standardized test environment where alignment failures are observable is one component of a good global outcome.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8349", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:04.325+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-0ee786a3fd0324695cea33242444580f6e4776a12eaa4d327dc8b210648c2504", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0ee786a3fd0324695cea33242444580f6e4776a12eaa4d327dc8b210648c2504", "name": "What are \"selection theorems\" and can they tell us anything useful about the likely shape of AGI systems?", "index": 30, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-15T15:10:59.813Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0ee786a3fd0324695cea33242444580f6e4776a12eaa4d327dc8b210648c2504", "values": {"File": "What are \"selection theorems\" and can they tell us anything useful about the likely shape of AGI systems?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are \"selection theorems\" and can they tell us anything useful about the likely shape of AGI systems?", "Link": "https://docs.google.com/document/d/14U9ARpfZZfitoSvSEWvRMHgMxXxgEvUCVBdVvXiDRi4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:32.824+01:00", "Related Answers DO NOT EDIT": "What is John Wentworth's plan?,What is the difference between inner and outer alignment?", "Tags": "Selection Theorems", "Doc Last Edited": "2023-03-15T12:47:45.011+01:00", "Status": "In progress", "Edit Answer": "What are \"selection theorems\" and can they tell us anything useful about the likely shape of AGI systems?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7674", "Source Link": "", "aisafety.info Link": "What are \"selection theorems\" and can they tell us anything useful about the likely shape of AGI systems?", "Source": "Wiki", "All Phrasings": "What are \"selection theorems\" and can they tell us anything useful about the likely shape of AGI systems?\n", "Initial Order": "", "Related IDs": "8378,8428", "Rich Text DO NOT EDIT": "Selection theorems tell us what \u201ckinds'' of agents (as characterised by their \u201c[type signatures](https://docs.google.com/document/d/1dDKu5Ggwqe6ODHwtSzd6MhMsvQloSZt1IZn8WmGuZ4Y/edit)\u201d) would be selected for by performance on a given metric (e.g. [survival](https://en.wikipedia.org/wiki/Survival), [inclusive genetic fitness](https://en.wikipedia.org/wiki/Inclusive_fitness), [financial profitability](https://en.wikipedia.org/wiki/Profit_(economics)), [inexploitability](https://en.wikipedia.org/wiki/Strategic_dominance), etc.) in a fairly broad class of environments. For example, the subagents argument says that inexploitable agents with internal state will have as \u201cgoals\u201d pareto optimality over multiple utility functions[^kix.blt5lym6zljy].\n\nThey tell us what kind of [\u201cinner agents\u201d would be selected for by outer optimisation processes](https://docs.google.com/document/d/1TR5UYmFjwA-FAttyMR_vB5aXQu1gcLTwaHvY2v9ObCM).\n\nSelection theorems aim to tackle core confusions/gaps in our understanding regarding alignment and agency. They are a descriptive theory about intelligent systems as they actually are in the real world (from simple organisms such as E. coli to more complex artifacts such as trained neural networks, humans and financial markets). They are not a normative theory about what ideal agency would be like.\n\nSelection theorems aim to describe intelligent systems (from simple organisms such as [E. coli](https://en.wikipedia.org/wiki/Escherichia_coli) to more complex artifacts such as trained neural networks, humans and financial markets)[^kix.1omsody193n5]. They are not a hypothesis of what ideal agency would be like.\n\nSelection theorems seek to answer questions such as:\n\n- What kind of thing is a [x]?\n\n- What are the inputs to [x]? What are its outputs?\n\n- How is it represented/what data structures represent it?\n\n- How is it embedded in an underlying physical (or other low level) system\n\nWhere [x] is \u201cagent\u201d or agent aspects such as goals, world models.\n\nThe latter three questions give us agent \u201ctype signatures\u201d, broadly:\n\n- The interface (inputs and outputs)\n\n- Representation (data structures)\n\n- Embedding\n\nExamples of extant selection theorems:\n\n- [Coherence and Dutch Book theorems](https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities)\n\n- [Subagents](https://www.lesswrong.com/posts/3xF66BNSC5caZuKyC/why-subagents)\n\n- [Good Regulator](https://www.tandfonline.com/doi/abs/10.1080/00207727008920220) and [Gooder Regulator](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem)\n\n- [The Kelly Criterion](http://www.eecs.harvard.edu/cs286r/courses/fall10/papers/Chapter6.pdf)\n\nLearn More:\n\n- [Understanding Selection Theorems](https://www.lesswrong.com/posts/tdcLpkydLwcKwbKre/understanding-selection-theorems)\n\n    - Distillation of Wentworth\u2019s original post, recommended for people new to agent foundations style research\n\n- [Selection Theorems: A Program For Understanding Agents](https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents)\n\n    - Original post introducing the selection theorems research agenda\n\n- [What Selection Theorems Do We Expect/Want?](https://www.lesswrong.com/posts/RuDD3aQWLDSb4eTXP/what-selection-theorems-do-we-expect-want)\n\n    - Sequel to the previous post, it explores selection theorems in more depth\n\n- [Some Existing Selection Theorems](https://www.lesswrong.com/posts/N2NebPD78ioyWHhNm/some-existing-selection-theorems)\n\n    - More in-depth exploration of extant selection theorems\n\n- [Selection Theorems page on LW wiki](https://www.lesswrong.com/tag/selection-theorems)\n\n    - A collection of all posts tagged with \u201cselection theorems\u201d on LessWrong\n\n___\n\n[^kix.blt5lym6zljy]:Rather than optimality over a single utility function as in the coherence theorems setup.\n[^kix.1omsody193n5]: Normative theories on the other hand hypothesise about what ideal agency might look like [e.g. [AIXI](https://en.wikipedia.org/wiki/AIXI), [HRAD](https://intelligence.org/files/TechnicalAgenda.pdf)]).\n    ", "Tag Count": 1, "Related Answer Count": 2, "Rich Text": "Selection theorems tell us what \u201ckinds'' of agents (as characterised by their \u201c[type signatures](https://docs.google.com/document/d/1dDKu5Ggwqe6ODHwtSzd6MhMsvQloSZt1IZn8WmGuZ4Y/edit)\u201d) would be selected for by performance on a given metric (e.g. [survival](https://en.wikipedia.org/wiki/Survival), [inclusive genetic fitness](https://en.wikipedia.org/wiki/Inclusive_fitness), [financial profitability](https://en.wikipedia.org/wiki/Profit_(economics)), [inexploitability](https://en.wikipedia.org/wiki/Strategic_dominance), etc.) in a fairly broad class of environments. For example, the subagents argument says that inexploitable agents with internal state will have as \u201cgoals\u201d pareto optimality over multiple utility functions[^kix.blt5lym6zljy].\n\nThey tell us what kind of [\u201cinner agents\u201d would be selected for by outer optimisation processes](https://docs.google.com/document/d/1TR5UYmFjwA-FAttyMR_vB5aXQu1gcLTwaHvY2v9ObCM).\n\nSelection theorems aim to tackle core confusions/gaps in our understanding regarding alignment and agency. They are a descriptive theory about intelligent systems as they actually are in the real world (from simple organisms such as E. coli to more complex artifacts such as trained neural networks, humans and financial markets). They are not a normative theory about what ideal agency would be like.\n\nSelection theorems aim to describe intelligent systems (from simple organisms such as [E. coli](https://en.wikipedia.org/wiki/Escherichia_coli) to more complex artifacts such as trained neural networks, humans and financial markets)[^kix.1omsody193n5]. They are not a hypothesis of what ideal agency would be like.\n\nSelection theorems seek to answer questions such as:\n\n- What kind of thing is a [x]?\n\n- What are the inputs to [x]? What are its outputs?\n\n- How is it represented/what data structures represent it?\n\n- How is it embedded in an underlying physical (or other low level) system\n\nWhere [x] is \u201cagent\u201d or agent aspects such as goals, world models.\n\nThe latter three questions give us agent \u201ctype signatures\u201d, broadly:\n\n- The interface (inputs and outputs)\n\n- Representation (data structures)\n\n- Embedding\n\nExamples of extant selection theorems:\n\n- [Coherence and Dutch Book theorems](https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities)\n\n- [Subagents](https://www.lesswrong.com/posts/3xF66BNSC5caZuKyC/why-subagents)\n\n- [Good Regulator](https://www.tandfonline.com/doi/abs/10.1080/00207727008920220) and [Gooder Regulator](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem)\n\n- [The Kelly Criterion](http://www.eecs.harvard.edu/cs286r/courses/fall10/papers/Chapter6.pdf)\n\nLearn More:\n\n- [Understanding Selection Theorems](https://www.lesswrong.com/posts/tdcLpkydLwcKwbKre/understanding-selection-theorems)\n\n    - Distillation of Wentworth\u2019s original post, recommended for people new to agent foundations style research\n\n- [Selection Theorems: A Program For Understanding Agents](https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents)\n\n    - Original post introducing the selection theorems research agenda\n\n- [What Selection Theorems Do We Expect/Want?](https://www.lesswrong.com/posts/RuDD3aQWLDSb4eTXP/what-selection-theorems-do-we-expect-want)\n\n    - Sequel to the previous post, it explores selection theorems in more depth\n\n- [Some Existing Selection Theorems](https://www.lesswrong.com/posts/N2NebPD78ioyWHhNm/some-existing-selection-theorems)\n\n    - More in-depth exploration of extant selection theorems\n\n- [Selection Theorems page on LW wiki](https://www.lesswrong.com/tag/selection-theorems)\n\n    - A collection of all posts tagged with \u201cselection theorems\u201d on LessWrong\n\n___\n\n[^kix.blt5lym6zljy]:Rather than optimality over a single utility function as in the coherence theorems setup.\n[^kix.1omsody193n5]: Normative theories on the other hand hypothesise about what ideal agency might look like [e.g. [AIXI](https://en.wikipedia.org/wiki/AIXI), [HRAD](https://intelligence.org/files/TechnicalAgenda.pdf)]).\n    ", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7674", "Related Answers": "What is John Wentworth's plan?,What is the difference between inner and outer alignment?", "Doc Last Ingested": "2023-03-15T13:12:07.165+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-57bf8bd169a63815a3ddcb0821aee879286f9fe8c3aa0b698e640bdae316f105", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-57bf8bd169a63815a3ddcb0821aee879286f9fe8c3aa0b698e640bdae316f105", "name": "What are \"scaling laws\" and how are they relevant to safety?", "index": 31, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-15T18:07:58.304Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-57bf8bd169a63815a3ddcb0821aee879286f9fe8c3aa0b698e640bdae316f105", "values": {"File": "What are \"scaling laws\" and how are they relevant to safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are \"scaling laws\" and how are they relevant to safety?", "Link": "https://docs.google.com/document/d/1DHnQAPJeIchOCtUCg19-DiXKeRdTRNFFJsn8vIJ_YO0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:28.928+01:00", "Related Answers DO NOT EDIT": "What does prosaic alignment mean?,Can we get AGI by scaling up architectures similar to current ones, or are we missing key insights?", "Tags": "Scaling Laws", "Doc Last Edited": "2023-03-15T15:49:32.644+01:00", "Status": "Live on site", "Edit Answer": "What are \"scaling laws\" and how are they relevant to safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7750", "Source Link": "", "aisafety.info Link": "What are \"scaling laws\" and how are they relevant to safety?", "Source": "Wiki", "All Phrasings": "What are \"scaling laws\" and how are they relevant to safety?\n", "Initial Order": "", "Related IDs": "89LM,7727", "Rich Text DO NOT EDIT": "**Scaling laws** are observed trends on the performance of large machine learning models.\n\nIn the field of ML, better performance is usually achieved through better algorithms, better inputs, or using larger amounts of parameters, computing power, or data. Since the 2010s, advances in deep learning have shown experimentally that the easier and faster returns come from **scaling**, an observation that has been described by Richard Sutton as the[bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html).\n\nWhile deep learning as a field has long struggled to scale models up while retaining learning capability (with such problems as[catastrophic interference](https://en.wikipedia.org/wiki/Catastrophic_interference)), more recent methods, especially the Transformer model architecture, were able to *just work* by feeding them more data, and as the meme goes,[stacking more layers](https://www.gwern.net/images/rl/2017-12-24-meme-nnlayers-alphagozero.jpg).\n\nMore surprisingly, performance (in terms of absolute likelihood loss, a standard measure) appeared to increase *smoothly* with compute, or dataset size, or parameter count. Which gave rise to **scaling laws**, the trend lines suggested by performance gains, from which returns on data/compute/time investment could be extrapolated.\n\nA companion to this purely descriptive law (no strong theoretical explanation of the phenomenon has been found yet), is the **scaling hypothesis**, which[Gwern Branwen describes](https://www.gwern.net/Scaling-hypothesis#scaling-hypothesis):\n\nThe *strong scaling hypothesis* is that, once we find a scalable architecture like self-attention or convolutions, [...] we can simply train ever larger [neural networks] and ever more sophisticated behavior will emerge naturally as the easiest way to optimize for all the tasks & data.\n\nThe scaling laws, if the above hypothesis holds, become highly relevant to safety insofar capability gains become conceptually easier to achieve: no need for clever designs to solve a given task, just throw more processing at it and it will eventually yield. As[Paul Christiano observes](https://ai-alignment.com/prosaic-ai-control-b959644d79c2):\n\nIt now seems possible that we could build \u201cprosaic\u201d AGI, which can replicate human behavior but doesn\u2019t involve qualitatively new ideas about \u201chow intelligence works\u201d.\n\nWhile the scaling laws still hold experimentally at the time of this writing (July 2022), whether they'll continue up to safety-relevant capabilities is still an open problem.\n\n", "Tag Count": 1, "Related Answer Count": 2, "Rich Text": "**Scaling laws** are observed trends on the performance of large machine learning models.\n\nIn the field of ML, better performance is usually achieved through better algorithms, better inputs, or using larger amounts of parameters, computing power, or data. Since the 2010s, advances in deep learning have shown experimentally that the easier and faster returns come from **scaling**, an observation that has been described by Richard Sutton as the[bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html).\n\nWhile deep learning as a field has long struggled to scale models up while retaining learning capability (with such problems as[catastrophic interference](https://en.wikipedia.org/wiki/Catastrophic_interference)), more recent methods, especially the Transformer model architecture, were able to *just work* by feeding them more data, and as the meme goes,[stacking more layers](https://www.gwern.net/images/rl/2017-12-24-meme-nnlayers-alphagozero.jpg).\n\nMore surprisingly, performance (in terms of absolute likelihood loss, a standard measure) appeared to increase *smoothly* with compute, or dataset size, or parameter count. Which gave rise to **scaling laws**, the trend lines suggested by performance gains, from which returns on data/compute/time investment could be extrapolated.\n\nA companion to this purely descriptive law (no strong theoretical explanation of the phenomenon has been found yet), is the **scaling hypothesis**, which[Gwern Branwen describes](https://www.gwern.net/Scaling-hypothesis#scaling-hypothesis):\n\nThe *strong scaling hypothesis* is that, once we find a scalable architecture like self-attention or convolutions, [...] we can simply train ever larger [neural networks] and ever more sophisticated behavior will emerge naturally as the easiest way to optimize for all the tasks & data.\n\nThe scaling laws, if the above hypothesis holds, become highly relevant to safety insofar capability gains become conceptually easier to achieve: no need for clever designs to solve a given task, just throw more processing at it and it will eventually yield. As[Paul Christiano observes](https://ai-alignment.com/prosaic-ai-control-b959644d79c2):\n\nIt now seems possible that we could build \u201cprosaic\u201d AGI, which can replicate human behavior but doesn\u2019t involve qualitatively new ideas about \u201chow intelligence works\u201d.\n\nWhile the scaling laws still hold experimentally at the time of this writing (July 2022), whether they'll continue up to safety-relevant capabilities is still an open problem.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7750", "Related Answers": "What does prosaic alignment mean?,Can we get AGI by scaling up architectures similar to current ones, or are we missing key insights?", "Doc Last Ingested": "2023-03-15T16:22:02.291+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-b0242d288a683be9d1fbf1e54feba7870ffefc9d21ec3955539698538cd396ae", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b0242d288a683be9d1fbf1e54feba7870ffefc9d21ec3955539698538cd396ae", "name": "What are \"human values\"?", "index": 32, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:09:03.545Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b0242d288a683be9d1fbf1e54feba7870ffefc9d21ec3955539698538cd396ae", "values": {"File": "What are \"human values\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are \"human values\"?", "Link": "https://docs.google.com/document/d/1x9Jn8s4XcLtiP-vVA8crCCg27JdJ3TnAuL8zAMRm4sE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:25.237+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Human Values", "Doc Last Edited": "2023-02-22T23:05:39.240+01:00", "Status": "Live on site", "Edit Answer": "What are \"human values\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7594", "Source Link": "", "aisafety.info Link": "What are \"human values\"?", "Source": "Wiki", "All Phrasings": "What are \"human values\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "**Human Values** are the things we care about, and would want an aligned superintelligence to look after and support. It is suspected that true human values are [highly complex](https://www.lesswrong.com/tag/complexity-of-value), and could be extrapolated into a wide variety of forms.", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "**Human Values** are the things we care about, and would want an aligned superintelligence to look after and support. It is suspected that true human values are [highly complex](https://www.lesswrong.com/tag/complexity-of-value), and could be extrapolated into a wide variety of forms.", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Nico Hill2", "External Source": "https://www.lesswrong.com/tag/human-values?edit=true", "Last Asked On Discord": "", "UI ID": "7594", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:12.992+01:00", "Request Count": "", "Number of suggestions on answer doc": "", "Total character count of suggestions on answer doc": "", "Helpful": ""}}, {"id": "i-d090c42697187cbcb8d0755ad1d36ac1e1542944138ba584e0d867680c351596", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d090c42697187cbcb8d0755ad1d36ac1e1542944138ba584e0d867680c351596", "name": "What are \"coherence theorems\" and what do they tell us about AI?", "index": 33, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:09:08.012Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d090c42697187cbcb8d0755ad1d36ac1e1542944138ba584e0d867680c351596", "values": {"File": "What are \"coherence theorems\" and what do they tell us about AI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What are \"coherence theorems\" and what do they tell us about AI?", "Link": "https://docs.google.com/document/d/13pG4yvI7eTHuzYKHjD9Oaeo_O266j0iJ8dEyD_kZLDI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:21.185+01:00", "Related Answers DO NOT EDIT": "What are \"selection theorems\" and can they tell us anything useful about the likely shape of AGI systems?", "Tags": "Selection Theorems", "Doc Last Edited": "2023-02-22T22:55:46.251+01:00", "Status": "In progress", "Edit Answer": "What are \"coherence theorems\" and what do they tell us about AI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7597", "Source Link": "", "aisafety.info Link": "What are \"coherence theorems\" and what do they tell us about AI?", "Source": "Wiki", "All Phrasings": "What are \"coherence theorems\" and what do they tell us about AI?\n", "Initial Order": "", "Related IDs": "7674", "Rich Text DO NOT EDIT": "[Coherence theorems](https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities) are a subset of [selection theorems](https://www.lesswrong.com/posts/tdcLpkydLwcKwbKre/understanding-selection-theorems). \n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "[Coherence theorems](https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities) are a subset of [selection theorems](https://www.lesswrong.com/posts/tdcLpkydLwcKwbKre/understanding-selection-theorems). \n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7597", "Related Answers": "What are \"selection theorems\" and can they tell us anything useful about the likely shape of AGI systems?", "Doc Last Ingested": "2023-03-14T23:53:14.745+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-cd5b637d614c18e592dbee9c05adce59dc98163baba9ac36604b736fa76c76ab", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-cd5b637d614c18e592dbee9c05adce59dc98163baba9ac36604b736fa76c76ab", "name": "What approaches are AI alignment organizations working on?", "index": 34, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:09:16.148Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-cd5b637d614c18e592dbee9c05adce59dc98163baba9ac36604b736fa76c76ab", "values": {"File": "What approaches are AI alignment organizations working on?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What approaches are AI alignment organizations working on?", "Link": "https://docs.google.com/document/d/1wWrtW9txNp8G7j0bB-WAn9LPcSCvtdSH8eTo5CLghyw/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:17.662+01:00", "Related Answers DO NOT EDIT": "What are some AI alignment research agendas currently being pursued?,Who is Jacob Steinhardt and what is he working on?,What work is Redwood doing on LLM interpretability?,What is FAR's theory of change?,What projects are CAIS working on?,What is Dylan Hadfield-Menell's thesis on?,What is John Wentworth's plan?,How does MIRI communicate their view on alignment?,How is OpenAI planning to solve the full alignment problem?,How is the Alignment Research Center (ARC) trying to solve Eliciting Latent Knowledge (ELK)?,How would we align an AGI whose learning algorithms / cognition look like human brains?,How would you explain the theory of Infra-Bayesianism?,What are Encultured working on?,What are Scott Garrabrant and Abram Demski working on?,What does Evan Hubinger think of Deception + Inner Alignment?,What does MIRI think about technical alignment?,What does Ought aim to do?,What does the scheme Externalized Reasoning Oversight involve?,What is Aligned AI / Stuart Armstrong working on?,What is Anthropic's approach to LLM alignment?,What is Conjecture, and what is their team working on?,What is David Krueger working on?,What is the Center for Human Compatible AI (CHAI)?,What is the Center on Long-Term Risk (CLR) focused on?,What is the DeepMind's safety team working on?,What is Sam Bowman researching?", "Tags": "Organizations", "Doc Last Edited": "2023-03-11T05:57:29.349+01:00", "Status": "Live on site", "Edit Answer": "What approaches are AI alignment organizations working on?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6178", "Source Link": "", "aisafety.info Link": "What approaches are AI alignment organizations working on?", "Source": "Wiki", "All Phrasings": "What approaches are AI alignment organizations working on?\n", "Initial Order": "", "Related IDs": "6479,8367,8376,8356,8326,8348,8378,8358,8368,8316,8324,8365,8349,8364,8359,8357,8374,8350,8314,8320,8332,8342,8327,8333,8343,8469", "Rich Text DO NOT EDIT": "Each major organization has a different approach. The [research agendas are detailed and complex](https://www.lesswrong.com/tag/research-agendas) (see also [AI Watch](https://aiwatch.issarice.com/)). Getting more brains working on any of them (and more money to fund them) may pay off in a big way, but it\u2019s very hard to be confident which (if any) of them will actually work.\n\nThe following is a massive oversimplification, each organization actually pursues many different avenues of research, read the [2021 AI Alignment Literature Review and Charity Comparison](https://www.lesswrong.com/posts/C4tR3BEpuWviT7Sje/2021-ai-alignment-literature-review-and-charity-comparison) for much more detail. That being said:\n\n- The [Machine Intelligence Research Institute](https://intelligence.org/research-guide/) focuses on foundational mathematical research to understand reliable reasoning, which they think is necessary to provide anything like an assurance that a seed AI built will do good things if activated.\n\n- The [Center for Human-Compatible AI](https://humancompatible.ai) focuses on [Cooperative Inverse Reinforcement Learning](https://www.lesswrong.com/tag/inverse-reinforcement-learning) and [Assistance Games](https://www.lesswrong.com/posts/qPoaA5ZSedivA4xJa/our-take-on-chai-s-research-agenda-in-under-1500-words), a new paradigm for AI where they try to optimize for doing the kinds of things humans want rather than for a pre-specified utility function\n\n- [Paul Christano](https://ai-alignment.com/)'s [Alignment Research Center](https://alignmentresearchcenter.org/) focuses is on [prosaic alignment](https://www.lesswrong.com/posts/YTq4X6inEudiHkHDF/prosaic-ai-alignment), particularly on creating tools that empower humans to understand and guide systems much smarter than ourselves. His methodology is explained on [his blog](https://ai-alignment.com/my-research-methodology-b94f2751cb2c).\n\n- The [Future of Humanity Institute](https://www.fhi.ox.ac.uk) does work on [crucial considerations](https://www.lesswrong.com/tag/crucial-considerations) and other x-risks, as well as AI safety research and outreach.\n\n- [Anthropic](https://www.anthropic.com/) is a new organization exploring natural language, human feedback, scaling laws, reinforcement learning, code generation, and interpretability.\n\n- [OpenAI](http://openai.com) is in a state of flux after major changes to their safety team.\n\n- [DeepMind](https://medium.com/@deepmindsafetyresearch)\u2019s safety team is working on various approaches designed to work with modern machine learning, and does some communication via the [Alignment Newsletter](https://rohinshah.com/alignment-newsletter/).\n\n- [EleutherAI](https://www.eleuther.ai/) is a Machine Learning collective aiming to build large open source language models to allow more alignment research to take place.\n\n- [Ought](https://ought.org/) is a research lab that develops mechanisms for delegating open-ended thinking to advanced machine learning systems.\n\n- [Conjecture](https://www.conjecture.dev/) is an alignment startup which aims to scale alignment research, including new frames for reasoning about large language models,scalable mechanistic interpretability, and history and philosophy of alignment.\n\nThere are many other projects around AI Safety, such as [the Windfall clause](https://www.youtube.com/watch?v=7i_f4Kbpgn4), [Rob Miles\u2019s YouTube channel](https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg), [AI Safety Support](https://www.aisafetysupport.org/), etc.\n\n", "Tag Count": 1, "Related Answer Count": 26, "Rich Text": "Each major organization has a different approach. The [research agendas are detailed and complex](https://www.lesswrong.com/tag/research-agendas) (see also [AI Watch](https://aiwatch.issarice.com/)). Getting more brains working on any of them (and more money to fund them) may pay off in a big way, but it\u2019s very hard to be confident which (if any) of them will actually work.\n\nThe following is a massive oversimplification, each organization actually pursues many different avenues of research, read the [2021 AI Alignment Literature Review and Charity Comparison](https://www.lesswrong.com/posts/C4tR3BEpuWviT7Sje/2021-ai-alignment-literature-review-and-charity-comparison) for much more detail. That being said:\n\n- The [Machine Intelligence Research Institute](https://intelligence.org/research-guide/) focuses on foundational mathematical research to understand reliable reasoning, which they think is necessary to provide anything like an assurance that a seed AI built will do good things if activated.\n\n- The [Center for Human-Compatible AI](https://humancompatible.ai) focuses on [Cooperative Inverse Reinforcement Learning](https://www.lesswrong.com/tag/inverse-reinforcement-learning) and [Assistance Games](https://www.lesswrong.com/posts/qPoaA5ZSedivA4xJa/our-take-on-chai-s-research-agenda-in-under-1500-words), a new paradigm for AI where they try to optimize for doing the kinds of things humans want rather than for a pre-specified utility function\n\n- [Paul Christano](https://ai-alignment.com/)'s [Alignment Research Center](https://alignmentresearchcenter.org/) focuses is on [prosaic alignment](https://www.lesswrong.com/posts/YTq4X6inEudiHkHDF/prosaic-ai-alignment), particularly on creating tools that empower humans to understand and guide systems much smarter than ourselves. His methodology is explained on [his blog](https://ai-alignment.com/my-research-methodology-b94f2751cb2c).\n\n- The [Future of Humanity Institute](https://www.fhi.ox.ac.uk) does work on [crucial considerations](https://www.lesswrong.com/tag/crucial-considerations) and other x-risks, as well as AI safety research and outreach.\n\n- [Anthropic](https://www.anthropic.com/) is a new organization exploring natural language, human feedback, scaling laws, reinforcement learning, code generation, and interpretability.\n\n- [OpenAI](http://openai.com) is in a state of flux after major changes to their safety team.\n\n- [DeepMind](https://medium.com/@deepmindsafetyresearch)\u2019s safety team is working on various approaches designed to work with modern machine learning, and does some communication via the [Alignment Newsletter](https://rohinshah.com/alignment-newsletter/).\n\n- [EleutherAI](https://www.eleuther.ai/) is a Machine Learning collective aiming to build large open source language models to allow more alignment research to take place.\n\n- [Ought](https://ought.org/) is a research lab that develops mechanisms for delegating open-ended thinking to advanced machine learning systems.\n\n- [Conjecture](https://www.conjecture.dev/) is an alignment startup which aims to scale alignment research, including new frames for reasoning about large language models,scalable mechanistic interpretability, and history and philosophy of alignment.\n\nThere are many other projects around AI Safety, such as [the Windfall clause](https://www.youtube.com/watch?v=7i_f4Kbpgn4), [Rob Miles\u2019s YouTube channel](https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg), [AI Safety Support](https://www.aisafetysupport.org/), etc.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6178", "Related Answers": "What are some AI alignment research agendas currently being pursued?,Who is Jacob Steinhardt and what is he working on?,What work is Redwood doing on LLM interpretability?,What is FAR's theory of change?,What projects are CAIS working on?,What is Dylan Hadfield-Menell's thesis on?,What is John Wentworth's plan?,How does MIRI communicate their view on alignment?,How is OpenAI planning to solve the full alignment problem?,How is the Alignment Research Center (ARC) trying to solve Eliciting Latent Knowledge (ELK)?,How would we align an AGI whose learning algorithms / cognition look like human brains?,How would you explain the theory of Infra-Bayesianism?,What are Encultured working on?,What are Scott Garrabrant and Abram Demski working on?,What does Evan Hubinger think of Deception + Inner Alignment?,What does MIRI think about technical alignment?,What does Ought aim to do?,What does the scheme Externalized Reasoning Oversight involve?,What is Aligned AI / Stuart Armstrong working on?,What is Anthropic's approach to LLM alignment?,What is Conjecture, and what is their team working on?,What is David Krueger working on?,What is the Center for Human Compatible AI (CHAI)?,What is the Center on Long-Term Risk (CLR) focused on?,What is the DeepMind's safety team working on?,What is Sam Bowman researching?", "Doc Last Ingested": "2023-03-14T23:53:18.050+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": 1}}, {"id": "i-7b6b9eb7cc9e28b52479ae5fc040f0efa3744a53af26b3a9e026e8d8f8d9d25b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7b6b9eb7cc9e28b52479ae5fc040f0efa3744a53af26b3a9e026e8d8f8d9d25b", "name": "What alignment strategies are scalably safe and competitive?", "index": 35, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:09:25.645Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7b6b9eb7cc9e28b52479ae5fc040f0efa3744a53af26b3a9e026e8d8f8d9d25b", "values": {"File": "What alignment strategies are scalably safe and competitive?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What alignment strategies are scalably safe and competitive?", "Link": "https://docs.google.com/document/d/1cpZ8ZHaNrmvFELwSexiOHp--z3wICJvBvvRB1qFeT70/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:14.092+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Alignment Proposals", "Doc Last Edited": "2023-02-22T22:55:47.302+01:00", "Status": "Not started", "Edit Answer": "What alignment strategies are scalably safe and competitive?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7717", "Source Link": "", "aisafety.info Link": "What alignment strategies are scalably safe and competitive?", "Source": "Wiki", "All Phrasings": "What alignment strategies are scalably safe and competitive?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7717", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:20.628+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-0b2a37a425f7b33f301137a8224d704691aed34dd2e133f6e2ce639e58689991", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0b2a37a425f7b33f301137a8224d704691aed34dd2e133f6e2ce639e58689991", "name": "What actions can I take in under five minutes to contribute to the cause of AI safety?", "index": 36, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:09:33.429Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0b2a37a425f7b33f301137a8224d704691aed34dd2e133f6e2ce639e58689991", "values": {"File": "What actions can I take in under five minutes to contribute to the cause of AI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What actions can I take in under five minutes to contribute to the cause of AI safety?", "Link": "https://docs.google.com/document/d/1ZlvzlJeP6cKLumLeIDplpK1y2BuYlnlf137hol05N8o/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:10.122+01:00", "Related Answers DO NOT EDIT": "Would donating small amounts to AI safety organizations make any significant difference?", "Tags": "Contributing", "Doc Last Edited": "2023-02-22T23:05:41.462+01:00", "Status": "Live on site", "Edit Answer": "What actions can I take in under five minutes to contribute to the cause of AI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7590", "Source Link": "", "aisafety.info Link": "What actions can I take in under five minutes to contribute to the cause of AI safety?", "Source": "Wiki", "All Phrasings": "What actions can I take in under five minutes to contribute to the cause of AI safety?\n", "Initial Order": "", "Related IDs": "6481", "Rich Text DO NOT EDIT": "There are two different reasons you might be looking for a 5 minute contribution.\n\n1. You are only willing to spend five minutes total\n\n1. You want a simple call to action which will concretize your commitment. You are looking for a small action which will modify your environment so that you\u2019re more likely to get involved.\n\nIf you are looking to only spend five minutes total, you can:\n\n- Send an article with a friend, so that they can learn more. One possible choice is [the 80000 hours career profile](https://80000hours.org/problem-profiles/artificial-intelligence/)\n\n- Share a link on social media, you never know who may be interested\n\n- Donate to an organization working on AI risk\n\nIF you are looking for a small action which will start things moving, you might consider:\n\n- Ordering a book (such as [the alignment problem](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393868338)), and follow up by reading it\n\n- Signing up for a newsletter\n\n- Applying for career coaching by [AISS](https://www.aisafetysupport.org/resources/career-coaching).\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "There are two different reasons you might be looking for a 5 minute contribution.\n\n1. You are only willing to spend five minutes total\n\n1. You want a simple call to action which will concretize your commitment. You are looking for a small action which will modify your environment so that you\u2019re more likely to get involved.\n\nIf you are looking to only spend five minutes total, you can:\n\n- Send an article with a friend, so that they can learn more. One possible choice is [the 80000 hours career profile](https://80000hours.org/problem-profiles/artificial-intelligence/)\n\n- Share a link on social media, you never know who may be interested\n\n- Donate to an organization working on AI risk\n\nIF you are looking for a small action which will start things moving, you might consider:\n\n- Ordering a book (such as [the alignment problem](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393868338)), and follow up by reading it\n\n- Signing up for a newsletter\n\n- Applying for career coaching by [AISS](https://www.aisafetysupport.org/resources/career-coaching).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7590", "Related Answers": "Would donating small amounts to AI safety organizations make any significant difference?", "Doc Last Ingested": "2023-03-14T23:53:23.332+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-cc3ee9a771fe525be85025118103cca644b4c6bc901f868b1dbf69429ffb8481", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-cc3ee9a771fe525be85025118103cca644b4c6bc901f868b1dbf69429ffb8481", "name": "What about having a human supervisor who must approve all the AI's decisions before executing them?", "index": 37, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:09:40.842Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-cc3ee9a771fe525be85025118103cca644b4c6bc901f868b1dbf69429ffb8481", "values": {"File": "What about having a human supervisor who must approve all the AI's decisions before executing them?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What about having a human supervisor who must approve all the AI's decisions before executing them?", "Link": "https://docs.google.com/document/d/1xA2MJgASMduLTJrz7VuJWiyj6kE6I6sxHpp3mMTlFaU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:07.037+01:00", "Related Answers DO NOT EDIT": "Why can\u2019t we just \u201cput the AI in a box\u201d so that it can\u2019t influence the outside world?", "Tags": "Why Not Just,Human-in-the-loop", "Doc Last Edited": "2023-02-28T08:26:29.809+01:00", "Status": "Not started", "Edit Answer": "What about having a human supervisor who must approve all the AI's decisions before executing them?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6503", "Source Link": "", "aisafety.info Link": "What about having a human supervisor who must approve all the AI's decisions before executing them?", "Source": "Wiki", "All Phrasings": "What about having a human supervisor who must approve all the AI's decisions before executing them?\n", "Initial Order": "", "Related IDs": "6176", "Rich Text DO NOT EDIT": "[https://www.alignmentforum.org/posts/KbyRPCAsWv5GtfrbG/the-alignment-problem-from-a-deep-learning-perspective#Phase_3__generalizing_goals_beyond_human_supervision](https://www.alignmentforum.org/posts/KbyRPCAsWv5GtfrbG/the-alignment-problem-from-a-deep-learning-perspective#Phase_3__generalizing_goals_beyond_human_supervision)\n\nWhile an AI is relatively weak, a human supervisor might be able to make sure that it isn\u2019t doing anything dangerous. However, even at this stage certain problems arise. For example, the human supervisor is much slower than a computer system, so it will slow down any action that the system wants to take. For example, if a human being had to evaluate every algorithmic stock purchase before it bought or sold, it would slow the system down enough to lose much of the benefit.\n\nAs the AI becomes more intelligent, it can craft plans whose implications are lost on the supervisor. The supervisor may not be able to fully understand the plans and come to rely on other artificial systems for help. But if those systems are also superintelligent the same concerns would apply.\n\nFurthermore, the AI could learn how to persuade the human supervisor most effectively, using rhetoric and other sales tricks to convince them to take the action.\n\n", "Tag Count": 2, "Related Answer Count": 1, "Rich Text": "[https://www.alignmentforum.org/posts/KbyRPCAsWv5GtfrbG/the-alignment-problem-from-a-deep-learning-perspective#Phase_3__generalizing_goals_beyond_human_supervision](https://www.alignmentforum.org/posts/KbyRPCAsWv5GtfrbG/the-alignment-problem-from-a-deep-learning-perspective#Phase_3__generalizing_goals_beyond_human_supervision)\n\nWhile an AI is relatively weak, a human supervisor might be able to make sure that it isn\u2019t doing anything dangerous. However, even at this stage certain problems arise. For example, the human supervisor is much slower than a computer system, so it will slow down any action that the system wants to take. For example, if a human being had to evaluate every algorithmic stock purchase before it bought or sold, it would slow the system down enough to lose much of the benefit.\n\nAs the AI becomes more intelligent, it can craft plans whose implications are lost on the supervisor. The supervisor may not be able to fully understand the plans and come to rely on other artificial systems for help. But if those systems are also superintelligent the same concerns would apply.\n\nFurthermore, the AI could learn how to persuade the human supervisor most effectively, using rhetoric and other sales tricks to convince them to take the action.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "filip", "External Source": "", "Last Asked On Discord": "", "UI ID": "6503", "Related Answers": "Why can\u2019t we just \u201cput the AI in a box\u201d so that it can\u2019t influence the outside world?", "Doc Last Ingested": "2023-03-14T23:53:25.776+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-7452b6ca5b2d811053e2ff009457038aa191e8e3c31f8428264853a229322f06", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7452b6ca5b2d811053e2ff009457038aa191e8e3c31f8428264853a229322f06", "name": "What about AI concerns other than misalignment?", "index": 38, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:09:46.128Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7452b6ca5b2d811053e2ff009457038aa191e8e3c31f8428264853a229322f06", "values": {"File": "What about AI concerns other than misalignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What about AI concerns other than misalignment?", "Link": "https://docs.google.com/document/d/1t2OMGoAKxgqKfTfvYVzXXH8HkN8rc6v8khcEqZz_DA0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:03.635+01:00", "Related Answers DO NOT EDIT": "", "Tags": "What About", "Doc Last Edited": "2023-02-22T22:55:49.359+01:00", "Status": "Not started", "Edit Answer": "What about AI concerns other than misalignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6414", "Source Link": "", "aisafety.info Link": "What about AI concerns other than misalignment?", "Source": "Wiki", "All Phrasings": "What about AI concerns other than misalignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6414", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:27.943+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-66f8954110ee9dec4233f8b0a611aef57f960317fe5e2e5254c04502d8f00624", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-66f8954110ee9dec4233f8b0a611aef57f960317fe5e2e5254c04502d8f00624", "name": "What AGI safety reading lists are there?", "index": 39, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:09:49.378Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-66f8954110ee9dec4233f8b0a611aef57f960317fe5e2e5254c04502d8f00624", "values": {"File": "What AGI safety reading lists are there?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "What AGI safety reading lists are there?", "Link": "https://docs.google.com/document/d/16NjnqKgVRbfKddACP04Q03VUgIkbAGg-GO_cla30IKg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:17:00.100+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Resources", "Doc Last Edited": "2023-02-22T22:55:50.312+01:00", "Status": "In progress", "Edit Answer": "What AGI safety reading lists are there?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8266", "Source Link": "", "aisafety.info Link": "What AGI safety reading lists are there?", "Source": "Wiki", "All Phrasings": "What AGI safety reading lists are there?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A good point to dive in is the [AGI Safety Fundamentals](https://www.agisafetyfundamentals.com/ai-alignment-curriculum) curriculum. This is designed to provide a high level understanding of the AI alignment problem and some of the key research directions which aim to solve it. The curriculum is kept up to date and is split into 8 weeks worth of readings, as it\u2019s meant to be used as a basis for study groups.\n\nThe [Intro to ML Safety](https://course.mlsafety.org/) also has a reading list and is a lot more technical than the AGI safety fundamentals.\n\nAnother good resource is the Alignment forum, especially their [curated sequences](https://www.alignmentforum.org/library), which describe specific concepts in a lot of detail.\n\nVael Gates maintains a list of resources for AI researchers interested in AGI safety [here](https://www.lesswrong.com/posts/gdyfJE3noRFSs373q/resources-i-send-to-ai-researchers-about-ai-safety).\n\nIf you\u2019re more interested in books, rather than papers and blog posts, MIRI has a [list of recommended books](https://intelligence.org/research-guide/) on background topics that are likely to be useful for research. These aren\u2019t specifically about AI safety, though.\n\nThere are also study guides by [John Wentworth](https://www.lesswrong.com/posts/bjjbp5i5G8bekJuxv/study-guide) and [Akash](https://www.lesswrong.com/s/mCkMrL9jyR94AAqwW) that describe topics and approaches that are useful if you want to go deep into AI safety.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "A good point to dive in is the [AGI Safety Fundamentals](https://www.agisafetyfundamentals.com/ai-alignment-curriculum) curriculum. This is designed to provide a high level understanding of the AI alignment problem and some of the key research directions which aim to solve it. The curriculum is kept up to date and is split into 8 weeks worth of readings, as it\u2019s meant to be used as a basis for study groups.\n\nThe [Intro to ML Safety](https://course.mlsafety.org/) also has a reading list and is a lot more technical than the AGI safety fundamentals.\n\nAnother good resource is the Alignment forum, especially their [curated sequences](https://www.alignmentforum.org/library), which describe specific concepts in a lot of detail.\n\nVael Gates maintains a list of resources for AI researchers interested in AGI safety [here](https://www.lesswrong.com/posts/gdyfJE3noRFSs373q/resources-i-send-to-ai-researchers-about-ai-safety).\n\nIf you\u2019re more interested in books, rather than papers and blog posts, MIRI has a [list of recommended books](https://intelligence.org/research-guide/) on background topics that are likely to be useful for research. These aren\u2019t specifically about AI safety, though.\n\nThere are also study guides by [John Wentworth](https://www.lesswrong.com/posts/bjjbp5i5G8bekJuxv/study-guide) and [Akash](https://www.lesswrong.com/s/mCkMrL9jyR94AAqwW) that describe topics and approaches that are useful if you want to go deep into AI safety.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "8266", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:30.304+01:00", "Request Count": "", "Number of suggestions on answer doc": 7, "Total character count of suggestions on answer doc": 7822, "Helpful": ""}}, {"id": "i-56c750c266dc7e023c6b9f14922199049a18d96f645c61c1c9774c3d3ce9a66f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-56c750c266dc7e023c6b9f14922199049a18d96f645c61c1c9774c3d3ce9a66f", "name": "We\u2019re going to merge with the machines so this will never be a problem, right?", "index": 40, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:09:52.780Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-56c750c266dc7e023c6b9f14922199049a18d96f645c61c1c9774c3d3ce9a66f", "values": {"File": "We\u2019re going to merge with the machines so this will never be a problem, right?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "We\u2019re going to merge with the machines so this will never be a problem, right?", "Link": "https://docs.google.com/document/d/19cA6owndWZ6pO5uaiWt8X9nVjq_yKQnmr7LthjhNbF0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:56.240+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Ray Kurzweil", "Doc Last Edited": "2023-03-01T23:26:36.942+01:00", "Status": "Live on site", "Edit Answer": "We\u2019re going to merge with the machines so this will never be a problem, right?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6228", "Source Link": "https://www.reddit.com/r/ControlProblem/wiki/faq", "aisafety.info Link": "We\u2019re going to merge with the machines so this will never be a problem, right?", "Source": "/r/ControlProblem's FAQ", "All Phrasings": "We\u2019re going to merge with the machines so this will never be a problem, right?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The concept of \u201cmerging with machines,\u201d as popularized by Ray Kurzweil, is the idea that we will be able to put computerized elements into our brains that enhance us to the point where we ourselves are the AI, instead of creating AI outside of ourselves.\n\nWhile this is a possible outcome, there is little reason to suspect that it is the most probable. The amount of computing power in your smart-phone took up an entire room of servers 30 years ago. Computer technology starts big, and then gets refined. Therefore, if \u201cmerging with the machines\u201d requires hardware that can fit inside our brain, it may lag behind the first generations of the technology being developed. This concept of merging also supposes that we can even figure out how to implant computer chips that interface with our brain in the first place, we can do it before the invention of advanced AI, society will accept it, and that computer implants can actually produce major intelligence gains in the human brain. Even if we could successfully enhance ourselves with brain implants before the invention of Artificial Superintelligence (ASI), there is no way to guarantee that this would protect us from negative outcomes, and an ASI with ill-defined goals could still pose a threat to us.\n\nIt's not that Ray Kurzweil's ideas are impossible, it's just that his predictions are too specific, confident, and reliant on strange assumptions.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "The concept of \u201cmerging with machines,\u201d as popularized by Ray Kurzweil, is the idea that we will be able to put computerized elements into our brains that enhance us to the point where we ourselves are the AI, instead of creating AI outside of ourselves.\n\nWhile this is a possible outcome, there is little reason to suspect that it is the most probable. The amount of computing power in your smart-phone took up an entire room of servers 30 years ago. Computer technology starts big, and then gets refined. Therefore, if \u201cmerging with the machines\u201d requires hardware that can fit inside our brain, it may lag behind the first generations of the technology being developed. This concept of merging also supposes that we can even figure out how to implant computer chips that interface with our brain in the first place, we can do it before the invention of advanced AI, society will accept it, and that computer implants can actually produce major intelligence gains in the human brain. Even if we could successfully enhance ourselves with brain implants before the invention of Artificial Superintelligence (ASI), there is no way to guarantee that this would protect us from negative outcomes, and an ASI with ill-defined goals could still pose a threat to us.\n\nIt's not that Ray Kurzweil's ideas are impossible, it's just that his predictions are too specific, confident, and reliant on strange assumptions.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "NotaSentientAI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6228", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:32.494+01:00", "Request Count": "", "Number of suggestions on answer doc": 10, "Total character count of suggestions on answer doc": 7825, "Helpful": ""}}, {"id": "i-d564c6cf116b8794627ef0d3fb47bbb1208885463d5c9992ed380c5091764f19", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d564c6cf116b8794627ef0d3fb47bbb1208885463d5c9992ed380c5091764f19", "name": "We already have psychopaths who are \"misaligned\" with the rest of humanity, but somehow we deal with them. Can't we do something similar with AI?", "index": 41, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:09:55.784Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d564c6cf116b8794627ef0d3fb47bbb1208885463d5c9992ed380c5091764f19", "values": {"File": "We already have psychopaths who are \"misaligned\" with the rest of humanity, but somehow we deal with them. Can't we do something similar with AI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "We already have psychopaths who are \"misaligned\" with the rest of humanity, but somehow we deal with them. Can't we do something similar with AI?", "Link": "https://docs.google.com/document/d/1RKcaapBsNIx_HL7MN8J8eARGWWLOiL2GivuvuH__KP0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:52.933+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Human Values,Capabilities", "Doc Last Edited": "2023-03-05T16:32:46.542+01:00", "Status": "In progress", "Edit Answer": "We already have psychopaths who are \"misaligned\" with the rest of humanity, but somehow we deal with them. Can't we do something similar with AI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6501", "Source Link": "", "aisafety.info Link": "We already have psychopaths who are \"misaligned\" with the rest of humanity, but somehow we deal with them. Can't we do something similar with AI?", "Source": "Wiki", "All Phrasings": "We already have psychopaths who are \"misaligned\" with the rest of humanity, but somehow we deal with them. Can't we do something similar with AI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The worst-case scenarios for misaligned AI involve AIs which are not *merely* misaligned with human values, but which are also extremely capable of achieving their goals, to a degree far beyond that of any human (including psychopaths). See, for instance, Karnofsky's \"[AI Could Defeat All Of Us Combined](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/)\". In short, we don't expect to be able to contain or resist the actions of an AGI beyond a certain threshold of intelligence, even if we don't like the actions it's taking (whereas bad actors among humans have basically the same abilities and limitations as any other human).\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "The worst-case scenarios for misaligned AI involve AIs which are not *merely* misaligned with human values, but which are also extremely capable of achieving their goals, to a degree far beyond that of any human (including psychopaths). See, for instance, Karnofsky's \"[AI Could Defeat All Of Us Combined](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/)\". In short, we don't expect to be able to contain or resist the actions of an AGI beyond a certain threshold of intelligence, even if we don't like the actions it's taking (whereas bad actors among humans have basically the same abilities and limitations as any other human).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "filip", "External Source": "", "Last Asked On Discord": "", "UI ID": "6501", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:35.241+01:00", "Request Count": "", "Number of suggestions on answer doc": 10, "Total character count of suggestions on answer doc": 7825, "Helpful": ""}}, {"id": "i-b53797d7d32369e6294cac07f6eb1e122152341c8507d90bc544b02a4a14b13a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b53797d7d32369e6294cac07f6eb1e122152341c8507d90bc544b02a4a14b13a", "name": "To what extent are there meaningfully different paths to AGI, versus just one path?", "index": 42, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:09:59.566Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b53797d7d32369e6294cac07f6eb1e122152341c8507d90bc544b02a4a14b13a", "values": {"File": "To what extent are there meaningfully different paths to AGI, versus just one path?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "To what extent are there meaningfully different paths to AGI, versus just one path?", "Link": "https://docs.google.com/document/d/1dFtHCJRGmz0lGyf8IR_MMZWRSqTzqDNDFq3S_WOvOBk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:49.597+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI,Paradigm", "Doc Last Edited": "2023-02-22T22:55:52.329+01:00", "Status": "Not started", "Edit Answer": "To what extent are there meaningfully different paths to AGI, versus just one path?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7733", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "To what extent are there meaningfully different paths to AGI, versus just one path?", "Source": "LessWrong", "All Phrasings": "To what extent are there meaningfully different paths to AGI, versus just one path?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7733", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:37.592+01:00", "Request Count": "", "Number of suggestions on answer doc": 10, "Total character count of suggestions on answer doc": 7825, "Helpful": ""}}, {"id": "i-4700756cf59e6a3f1f666b78d308fcf8b205735ea5ce01d8518b5751184f0915", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4700756cf59e6a3f1f666b78d308fcf8b205735ea5ce01d8518b5751184f0915", "name": "This all seems rather abstract. Isn't promoting love, wisdom, altruism or rationality more important?", "index": 43, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:02.959Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4700756cf59e6a3f1f666b78d308fcf8b205735ea5ce01d8518b5751184f0915", "values": {"File": "This all seems rather abstract. Isn't promoting love, wisdom, altruism or rationality more important?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "This all seems rather abstract. Isn't promoting love, wisdom, altruism or rationality more important?", "Link": "https://docs.google.com/document/d/11WDQaUvRZYXpaSAILqYlSHzt0spN6wNxALBmqak52bQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:45.911+01:00", "Related Answers DO NOT EDIT": "", "Tags": "What About", "Doc Last Edited": "2023-02-22T22:55:53.482+01:00", "Status": "In progress", "Edit Answer": "This all seems rather abstract. Isn't promoting love, wisdom, altruism or rationality more important?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7790", "Source Link": "", "aisafety.info Link": "This all seems rather abstract. Isn't promoting love, wisdom, altruism or rationality more important?", "Source": "Wiki", "All Phrasings": "This all seems rather abstract. Isn't promoting love, wisdom, altruism or rationality more important?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "These are all important issues. Part of love is striving for the best future of the loved one. Wisdom is knowing what actions will result in the best outcomes. Altruism is concern for the well-being of others. Rationality is the quality of being guided by good reasons when undertaking actions.\n\nMany, if not most, AI safety researchers are guided by these qualities. They have investigated the available evidence and came to the rational conclusion that an unaligned AGI would be a grave danger to humanity. Because of altruism and love, they do not want humankind to be extinguished. So they are searching for the wisdom to avert the danger, or at least limit the damage. Of course, promoting these issues is also important, and many leading AGI safety researchers are also known for their work promoting wisdom, altruism and rationality.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "These are all important issues. Part of love is striving for the best future of the loved one. Wisdom is knowing what actions will result in the best outcomes. Altruism is concern for the well-being of others. Rationality is the quality of being guided by good reasons when undertaking actions.\n\nMany, if not most, AI safety researchers are guided by these qualities. They have investigated the available evidence and came to the rational conclusion that an unaligned AGI would be a grave danger to humanity. Because of altruism and love, they do not want humankind to be extinguished. So they are searching for the wisdom to avert the danger, or at least limit the damage. Of course, promoting these issues is also important, and many leading AGI safety researchers are also known for their work promoting wisdom, altruism and rationality.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7790", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:40.224+01:00", "Request Count": "", "Number of suggestions on answer doc": 10, "Total character count of suggestions on answer doc": 7825, "Helpful": ""}}, {"id": "i-81448caf67cd3c0a8b3e5cc03336f375bc0a17f1d6202bf20a6cab3d13c00ba5", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-81448caf67cd3c0a8b3e5cc03336f375bc0a17f1d6202bf20a6cab3d13c00ba5", "name": "Superintelligence sounds like science fiction. Do people think about this in the real world?", "index": 44, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:06.677Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-81448caf67cd3c0a8b3e5cc03336f375bc0a17f1d6202bf20a6cab3d13c00ba5", "values": {"File": "Superintelligence sounds like science fiction. Do people think about this in the real world?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Superintelligence sounds like science fiction. Do people think about this in the real world?", "Link": "https://docs.google.com/document/d/1PTf4sitz2hMBWn6qFsB88NrEQEl-ItxkdtZGnvTcSxc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:42.257+01:00", "Related Answers DO NOT EDIT": "At a high level, what is the challenge of alignment that we must meet to secure a good future?,What are the different possible AI takeoff speeds?,Why might a superintelligent AI be dangerous?,Why might we expect a superintelligence to be hostile by default?", "Tags": "Superintelligence,Plausibility", "Doc Last Edited": "2023-02-22T23:05:44.764+01:00", "Status": "Live on site", "Edit Answer": "Superintelligence sounds like science fiction. Do people think about this in the real world?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6953", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "Superintelligence sounds like science fiction. Do people think about this in the real world?", "Source": "Superintelligence FAQ", "All Phrasings": "Superintelligence sounds like science fiction. Do people think about this in the real world?\n", "Initial Order": 7, "Related IDs": "7060,6957,6968,6982", "Rich Text DO NOT EDIT": "Many of the people with the deepest understanding of artificial intelligence are concerned about the risks of unaligned superintelligence. In 2014, Google bought world-leading artificial intelligence startup [DeepMind](https://en.wikipedia.org/wiki/DeepMind) for $400 million; DeepMind added the condition that Google promise to set up an AI Ethics Board. DeepMind cofounder Shane Legg has said in interviews that he believes superintelligent AI will be *\u201csomething approaching absolute power\u201d* and *\u201cthe number one risk for this century\u201d.*\n\n[Stuart Russell](https://en.wikipedia.org/wiki/Stuart_J._Russell#Career_and_research), Professor of Computer Science at Berkeley, author of the standard AI textbook, and world-famous AI expert, warns of *\u201cspecies-ending problems\u201d* and wants his field to pivot to make superintelligence-related risks a central concern. He went so far as to write [Human Compatible](https://en.wikipedia.org/wiki/Human_Compatible), a book focused on bringing attention to the dangers of artificial intelligence and the need for more work to address them.\n\nMany other science and technology leaders agree. Late astrophysicist [Stephen Hawking](https://en.wikipedia.org/wiki/Stephen_Hawking#Future_of_humanity) said that superintelligence *\u201ccould spell the end of the human race.\u201d* Tech billionaire [Bill Gates](https://en.wikipedia.org/wiki/Bill_Gates#Post-Microsoft) describes himself as *\u201cin the camp that is concerned about superintelligence\u2026I don\u2019t understand why some people are not concerned\u201d.* Oxford Professor [Nick Bostrom](https://www.theguardian.com/technology/2016/jun/12/nick-bostrom-artificial-intelligence-machine), who has been studying AI risks for over 20 years, has said: *\u201cSuperintelligence is a challenge for which we are not ready now and will not be ready for a long time.\u201d*\n\n[Holden Karnofsky](https://en.wikipedia.org/wiki/Holden_Karnofsky), the CEO of [Open Philanthropy](https://www.openphilanthropy.org/), has written a carefully reasoned account of why transformative artificial intelligence means that this might be [the most important century](https://www.cold-takes.com/most-important-century/).\n\n", "Tag Count": 2, "Related Answer Count": 4, "Rich Text": "Many of the people with the deepest understanding of artificial intelligence are concerned about the risks of unaligned superintelligence. In 2014, Google bought world-leading artificial intelligence startup [DeepMind](https://en.wikipedia.org/wiki/DeepMind) for $400 million; DeepMind added the condition that Google promise to set up an AI Ethics Board. DeepMind cofounder Shane Legg has said in interviews that he believes superintelligent AI will be *\u201csomething approaching absolute power\u201d* and *\u201cthe number one risk for this century\u201d.*\n\n[Stuart Russell](https://en.wikipedia.org/wiki/Stuart_J._Russell#Career_and_research), Professor of Computer Science at Berkeley, author of the standard AI textbook, and world-famous AI expert, warns of *\u201cspecies-ending problems\u201d* and wants his field to pivot to make superintelligence-related risks a central concern. He went so far as to write [Human Compatible](https://en.wikipedia.org/wiki/Human_Compatible), a book focused on bringing attention to the dangers of artificial intelligence and the need for more work to address them.\n\nMany other science and technology leaders agree. Late astrophysicist [Stephen Hawking](https://en.wikipedia.org/wiki/Stephen_Hawking#Future_of_humanity) said that superintelligence *\u201ccould spell the end of the human race.\u201d* Tech billionaire [Bill Gates](https://en.wikipedia.org/wiki/Bill_Gates#Post-Microsoft) describes himself as *\u201cin the camp that is concerned about superintelligence\u2026I don\u2019t understand why some people are not concerned\u201d.* Oxford Professor [Nick Bostrom](https://www.theguardian.com/technology/2016/jun/12/nick-bostrom-artificial-intelligence-machine), who has been studying AI risks for over 20 years, has said: *\u201cSuperintelligence is a challenge for which we are not ready now and will not be ready for a long time.\u201d*\n\n[Holden Karnofsky](https://en.wikipedia.org/wiki/Holden_Karnofsky), the CEO of [Open Philanthropy](https://www.openphilanthropy.org/), has written a carefully reasoned account of why transformative artificial intelligence means that this might be [the most important century](https://www.cold-takes.com/most-important-century/).\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "", "UI ID": "6953", "Related Answers": "At a high level, what is the challenge of alignment that we must meet to secure a good future?,What are the different possible AI takeoff speeds?,Why might a superintelligent AI be dangerous?,Why might we expect a superintelligence to be hostile by default?", "Doc Last Ingested": "2023-03-14T23:53:42.498+01:00", "Request Count": "", "Number of suggestions on answer doc": 10, "Total character count of suggestions on answer doc": 7825, "Helpful": ""}}, {"id": "i-4f211ae32b60659a5b219f34009b270f53244f29b18344513e53f60e0974b3f6", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4f211ae32b60659a5b219f34009b270f53244f29b18344513e53f60e0974b3f6", "name": "Should we expect \"warning shots\" before an unrecoverable catastrophe?", "index": 45, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:09.774Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4f211ae32b60659a5b219f34009b270f53244f29b18344513e53f60e0974b3f6", "values": {"File": "Should we expect \"warning shots\" before an unrecoverable catastrophe?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Should we expect \"warning shots\" before an unrecoverable catastrophe?", "Link": "https://docs.google.com/document/d/10llAAhPf2jpPfcUYSHaqWVaScLO4J_ULkUNd7q11g5g/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:38.902+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Doom,AGI Fire Alarm", "Doc Last Edited": "2023-02-22T22:55:54.451+01:00", "Status": "Not started", "Edit Answer": "Should we expect \"warning shots\" before an unrecoverable catastrophe?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7731", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "Should we expect \"warning shots\" before an unrecoverable catastrophe?", "Source": "LessWrong", "All Phrasings": "Should we expect \"warning shots\" before an unrecoverable catastrophe?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Depends on takeoff speeds, and how clever/patient the AGI is. \n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "- Depends on takeoff speeds, and how clever/patient the AGI is. \n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7731", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:44.700+01:00", "Request Count": "", "Number of suggestions on answer doc": 10, "Total character count of suggestions on answer doc": 7825, "Helpful": ""}}, {"id": "i-7243d66809511d95c6d024c8dc85542b9bf0b342346ec794738ab8a6b7e77994", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7243d66809511d95c6d024c8dc85542b9bf0b342346ec794738ab8a6b7e77994", "name": "Should I engage in political or collective action like signing petitions or sending letters to politicians?", "index": 46, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:13.041Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7243d66809511d95c6d024c8dc85542b9bf0b342346ec794738ab8a6b7e77994", "values": {"File": "Should I engage in political or collective action like signing petitions or sending letters to politicians?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Should I engage in political or collective action like signing petitions or sending letters to politicians?", "Link": "https://docs.google.com/document/d/198weiMXA7sirxfOEYd6xQtiWc7PHc-wndI4VW6OnMwg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:35.273+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Government", "Doc Last Edited": "2023-02-22T22:55:55.390+01:00", "Status": "In progress", "Edit Answer": "Should I engage in political or collective action like signing petitions or sending letters to politicians?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7765", "Source Link": "", "aisafety.info Link": "Should I engage in political or collective action like signing petitions or sending letters to politicians?", "Source": "Wiki", "All Phrasings": "Should I engage in political or collective action like signing petitions or sending letters to politicians?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Perhaps. There is a chance that directly lobbying politicians could help, but there's also a chance that actions end up being net-negative. It would be great if we could slow down AI, but doing so might simple mean that a nation less concerned about safety produces AI first. We could ask them to pass regulations or standards related to AGI, but passing ineffective regulation might interfere with passing more effective regulation later down the track as people may consider the issue dealt with. Or the requirements of complying with bureaucracy might prove to be a distraction from safe AI.\n\nIf you are concerned about this issue, you should probably try learning as much about this issue as possible and also spend a lot of time brainstorming downside risks and seeing what risks other people have identified.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Perhaps. There is a chance that directly lobbying politicians could help, but there's also a chance that actions end up being net-negative. It would be great if we could slow down AI, but doing so might simple mean that a nation less concerned about safety produces AI first. We could ask them to pass regulations or standards related to AGI, but passing ineffective regulation might interfere with passing more effective regulation later down the track as people may consider the issue dealt with. Or the requirements of complying with bureaucracy might prove to be a distraction from safe AI.\n\nIf you are concerned about this issue, you should probably try learning as much about this issue as possible and also spend a lot of time brainstorming downside risks and seeing what risks other people have identified.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7765", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:46.736+01:00", "Request Count": "", "Number of suggestions on answer doc": 10, "Total character count of suggestions on answer doc": 7825, "Helpful": ""}}, {"id": "i-2ef06d6f35751d412b2e0d7bfbede830b91c79233165029bc6daeeaaab4b909e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2ef06d6f35751d412b2e0d7bfbede830b91c79233165029bc6daeeaaab4b909e", "name": "Once we notice that a superintelligence given a specific task is trying to take over the world, can\u2019t we turn it off, reprogram it or otherwise correct the problem?", "index": 47, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:16.191Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2ef06d6f35751d412b2e0d7bfbede830b91c79233165029bc6daeeaaab4b909e", "values": {"File": "Once we notice that a superintelligence given a specific task is trying to take over the world, can\u2019t we turn it off, reprogram it or otherwise correct the problem?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Once we notice that a superintelligence given a specific task is trying to take over the world, can\u2019t we turn it off, reprogram it or otherwise correct the problem?", "Link": "https://docs.google.com/document/d/1zuBWZMzmf6c6OrgoOT5oj8qdfvMdSpye0nbSELq_UAY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:31.893+01:00", "Related Answers DO NOT EDIT": "Can we test an AI to make sure that it\u2019s not going to take over and do harmful things after it achieves superintelligence?", "Tags": "Superintelligence,Corrigibility", "Doc Last Edited": "2023-02-22T23:05:46.091+01:00", "Status": "Live on site", "Edit Answer": "Once we notice that a superintelligence given a specific task is trying to take over the world, can\u2019t we turn it off, reprogram it or otherwise correct the problem?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6988", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "Once we notice that a superintelligence given a specific task is trying to take over the world, can\u2019t we turn it off, reprogram it or otherwise correct the problem?", "Source": "Superintelligence FAQ", "All Phrasings": "Once we notice that a superintelligence given a specific task is trying to take over the world, can\u2019t we turn it off, reprogram it or otherwise correct the problem?\n", "Initial Order": "", "Related IDs": "6990", "Rich Text DO NOT EDIT": "We would not be able to turn off or reprogram a superintelligence gone rogue by default. Once in motion the superintelligence is now focused on completing its task. Suppose that it has a goal of calculating as many digits of pi as possible. Its current plan will allow it to calculate two hundred trillion such digits. But if it were turned off, or reprogrammed to do something else, that would result in it calculating zero digits. An entity fixated on calculating as many digits of pi as possible will work hard to prevent scenarios where it calculates zero digits of pi. Just by programming it to calculate digits of pi, we would have given it a drive to prevent people from turning it off.\n\nUniversity of Illinois computer scientist Steve Omohundro argues that entities with very different final goals \u2013 calculating digits of pi, curing cancer, helping promote human flourishing \u2013 will all share a few basic ground-level subgoals. First, self-preservation \u2013 no matter what your goal is, it\u2019s less likely to be accomplished if you\u2019re too dead to work towards it. Second, goal stability \u2013 no matter what your goal is, you\u2019re more likely to accomplish it if you continue to hold it as your goal, instead of going off and doing something else. Third, power \u2013 no matter what your goal is, you\u2019re more likely to be able to accomplish it if you have lots of power, rather than very little. [Here\u2019s the full paper](https://intelligence.org/files/BasicAIDrives.pdf).\n\nSo just by giving a superintelligence a simple goal like \u201ccalculate digits of pi\u201d, we would have accidentally given it convergent instrumental goals like \u201cprotect yourself\u201d, \u201cdon\u2019t let other people reprogram you\u201d, and \u201cseek power\u201d.\n\nAs long as the superintelligence is safely contained, there\u2019s not much it can do to resist reprogramming. But it\u2019s hard to consistently contain a hostile superintelligence.\n\n", "Tag Count": 2, "Related Answer Count": 1, "Rich Text": "We would not be able to turn off or reprogram a superintelligence gone rogue by default. Once in motion the superintelligence is now focused on completing its task. Suppose that it has a goal of calculating as many digits of pi as possible. Its current plan will allow it to calculate two hundred trillion such digits. But if it were turned off, or reprogrammed to do something else, that would result in it calculating zero digits. An entity fixated on calculating as many digits of pi as possible will work hard to prevent scenarios where it calculates zero digits of pi. Just by programming it to calculate digits of pi, we would have given it a drive to prevent people from turning it off.\n\nUniversity of Illinois computer scientist Steve Omohundro argues that entities with very different final goals \u2013 calculating digits of pi, curing cancer, helping promote human flourishing \u2013 will all share a few basic ground-level subgoals. First, self-preservation \u2013 no matter what your goal is, it\u2019s less likely to be accomplished if you\u2019re too dead to work towards it. Second, goal stability \u2013 no matter what your goal is, you\u2019re more likely to accomplish it if you continue to hold it as your goal, instead of going off and doing something else. Third, power \u2013 no matter what your goal is, you\u2019re more likely to be able to accomplish it if you have lots of power, rather than very little. [Here\u2019s the full paper](https://intelligence.org/files/BasicAIDrives.pdf).\n\nSo just by giving a superintelligence a simple goal like \u201ccalculate digits of pi\u201d, we would have accidentally given it convergent instrumental goals like \u201cprotect yourself\u201d, \u201cdon\u2019t let other people reprogram you\u201d, and \u201cseek power\u201d.\n\nAs long as the superintelligence is safely contained, there\u2019s not much it can do to resist reprogramming. But it\u2019s hard to consistently contain a hostile superintelligence.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "", "UI ID": "6988", "Related Answers": "Can we test an AI to make sure that it\u2019s not going to take over and do harmful things after it achieves superintelligence?", "Doc Last Ingested": "2023-03-14T23:53:49.459+01:00", "Request Count": "", "Number of suggestions on answer doc": 10, "Total character count of suggestions on answer doc": 7825, "Helpful": ""}}, {"id": "i-3a731fafe6cfbc4c7d08ae4917eac61b9df38d65bcdf86461070088be54bc0aa", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3a731fafe6cfbc4c7d08ae4917eac61b9df38d65bcdf86461070088be54bc0aa", "name": "OK, I\u2019m convinced. How can I help?", "index": 48, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:19.629Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3a731fafe6cfbc4c7d08ae4917eac61b9df38d65bcdf86461070088be54bc0aa", "values": {"File": "OK, I\u2019m convinced. How can I help?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "OK, I\u2019m convinced. How can I help?", "Link": "https://docs.google.com/document/d/11-29VI3yiUUa9GAlpeLbc-Q9uYnFweYBbtMygemC02o/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:27.923+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Contributing", "Doc Last Edited": "2023-02-22T23:05:47.037+01:00", "Status": "Live on site", "Edit Answer": "OK, I\u2019m convinced. How can I help?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6715", "Source Link": "", "aisafety.info Link": "OK, I\u2019m convinced. How can I help?", "Source": "Wiki", "All Phrasings": "OK, I\u2019m convinced. How can I help?\n", "Initial Order": 13, "Related IDs": "", "Rich Text DO NOT EDIT": "Great! I\u2019ll ask you a few follow-up questions to help figure out how you can best contribute, give you some advice, and link you to resources which should help you on whichever path you choose. Feel free to scroll up and explore multiple branches of the FAQ if you want answers to more than one of the questions offered :)\n\nNote: We\u2019re still building out and improving this tree of questions and answers, any feedback is appreciated.\n\n**At what level of involvement were you thinking of helping?**\n\nPlease view and suggest to this google doc for improvements: ['How can I help with AI alignment?' FAQ tree](https://docs.google.com/document/d/1FE09sOXLRHBQ_u2CDP4amiwKXneLza97P4NFVYmo90c/edit)\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Great! I\u2019ll ask you a few follow-up questions to help figure out how you can best contribute, give you some advice, and link you to resources which should help you on whichever path you choose. Feel free to scroll up and explore multiple branches of the FAQ if you want answers to more than one of the questions offered :)\n\nNote: We\u2019re still building out and improving this tree of questions and answers, any feedback is appreciated.\n\n**At what level of involvement were you thinking of helping?**\n\nPlease view and suggest to this google doc for improvements: ['How can I help with AI alignment?' FAQ tree](https://docs.google.com/document/d/1FE09sOXLRHBQ_u2CDP4amiwKXneLza97P4NFVYmo90c/edit)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6715", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:51.181+01:00", "Request Count": "", "Number of suggestions on answer doc": 10, "Total character count of suggestions on answer doc": 7825, "Helpful": ""}}, {"id": "i-eb171615f44f7194bbaa898fd6bf32e0ac109e885560de2e1aeda75b0a10b7d7", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-eb171615f44f7194bbaa898fd6bf32e0ac109e885560de2e1aeda75b0a10b7d7", "name": "Might trying to build a hedonium-maximizing AI be easier and more likely to work than trying for eudaimonia?", "index": 49, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:22.922Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-eb171615f44f7194bbaa898fd6bf32e0ac109e885560de2e1aeda75b0a10b7d7", "values": {"File": "Might trying to build a hedonium-maximizing AI be easier and more likely to work than trying for eudaimonia?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Might trying to build a hedonium-maximizing AI be easier and more likely to work than trying for eudaimonia?", "Link": "https://docs.google.com/document/d/1_m8nvpiiEO_kdMmBZ0cxbHS7fDVKn_7uLI8iZwb1Stc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:24.818+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Hedonium,Eutopia", "Doc Last Edited": "2023-02-22T22:55:56.641+01:00", "Status": "Not started", "Edit Answer": "Might trying to build a hedonium-maximizing AI be easier and more likely to work than trying for eudaimonia?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7787", "Source Link": "", "aisafety.info Link": "Might trying to build a hedonium-maximizing AI be easier and more likely to work than trying for eudaimonia?", "Source": "Wiki", "All Phrasings": "Might trying to build a hedonium-maximizing AI be easier and more likely to work than trying for eudaimonia?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": 2, "Multi Answer": "", "Stamped By": "Aprillion\nDamaged", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7787", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:53.272+01:00", "Request Count": "", "Number of suggestions on answer doc": 11, "Total character count of suggestions on answer doc": 8089, "Helpful": ""}}, {"id": "i-14bb601bc7e9e675f18c006aeaea0060360718edf598fb2300ffec305eac1566", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-14bb601bc7e9e675f18c006aeaea0060360718edf598fb2300ffec305eac1566", "name": "Might humanity create astronomical amounts of suffering when colonizing the universe after creating an aligned superintelligence?", "index": 50, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:26.084Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-14bb601bc7e9e675f18c006aeaea0060360718edf598fb2300ffec305eac1566", "values": {"File": "Might humanity create astronomical amounts of suffering when colonizing the universe after creating an aligned superintelligence?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Might humanity create astronomical amounts of suffering when colonizing the universe after creating an aligned superintelligence?", "Link": "https://docs.google.com/document/d/1Q2apkDt6peqioKNEHeWmh9Czk1sbg4ALrUDzBDHxLKc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:20.720+01:00", "Related Answers DO NOT EDIT": "", "Tags": "S-risk", "Doc Last Edited": "2023-02-22T22:55:57.645+01:00", "Status": "Not started", "Edit Answer": "Might humanity create astronomical amounts of suffering when colonizing the universe after creating an aligned superintelligence?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7609", "Source Link": "", "aisafety.info Link": "Might humanity create astronomical amounts of suffering when colonizing the universe after creating an aligned superintelligence?", "Source": "Wiki", "All Phrasings": "Might humanity create astronomical amounts of suffering when colonizing the universe after creating an aligned superintelligence?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Probably not if it\u2019s aligned\n\n- Yes, if it\u2019s aligned with \u201cbad\u201d values. E.g. a dystopian org first creates an AGI aligned with their goals and values to oppress and enslave all ginger people.\n\n- Could be a side effect of meeting alien species that aren\u2019t viewed as fully people.\n\n- This is really a different topic than alignment, though\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "- Probably not if it\u2019s aligned\n\n- Yes, if it\u2019s aligned with \u201cbad\u201d values. E.g. a dystopian org first creates an AGI aligned with their goals and values to oppress and enslave all ginger people.\n\n- Could be a side effect of meeting alien species that aren\u2019t viewed as fully people.\n\n- This is really a different topic than alignment, though\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7609", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:55.598+01:00", "Request Count": "", "Number of suggestions on answer doc": 11, "Total character count of suggestions on answer doc": 8089, "Helpful": ""}}, {"id": "i-12aab4723b84fc0a4b2e645006926c454650b3a8ea05c7b30990a599f7b2c229", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-12aab4723b84fc0a4b2e645006926c454650b3a8ea05c7b30990a599f7b2c229", "name": "Might attempting to align AI cause a \"near miss\" which results in a much worse outcome?", "index": 51, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:29.198Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-12aab4723b84fc0a4b2e645006926c454650b3a8ea05c7b30990a599f7b2c229", "values": {"File": "Might attempting to align AI cause a \"near miss\" which results in a much worse outcome?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Might attempting to align AI cause a \"near miss\" which results in a much worse outcome?", "Link": "https://docs.google.com/document/d/1TzPujK6Z0Aw7LXqQhsO1G295FtnjU-pqT-_Fe55DsSs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:17.353+01:00", "Related Answers DO NOT EDIT": "", "Tags": "S-risk", "Doc Last Edited": "2023-02-22T22:55:58.572+01:00", "Status": "Not started", "Edit Answer": "Might attempting to align AI cause a \"near miss\" which results in a much worse outcome?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7646", "Source Link": "", "aisafety.info Link": "Might attempting to align AI cause a \"near miss\" which results in a much worse outcome?", "Source": "Wiki", "All Phrasings": "Might attempting to align AI cause a \"near miss\" which results in a much worse outcome?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Yeah\n\n- \u201c[Hyperexistential risks](https://arbital.greaterwrong.com/p/hyperexistential_separation/)\u201d on arbital, more recently called s-risks\n\n- \n\n- what else are we supposed to do, [sponge coordination](https://carado.moe/outlook-ai-risk-mitigation.html#:~:text=i%27ll%20argue%20below.-,sponge%20coordination,-in%20agi%20ruin) seems implausible\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "- Yeah\n\n- \u201c[Hyperexistential risks](https://arbital.greaterwrong.com/p/hyperexistential_separation/)\u201d on arbital, more recently called s-risks\n\n- \n\n- what else are we supposed to do, [sponge coordination](https://carado.moe/outlook-ai-risk-mitigation.html#:~:text=i%27ll%20argue%20below.-,sponge%20coordination,-in%20agi%20ruin) seems implausible\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7646", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:53:57.875+01:00", "Request Count": "", "Number of suggestions on answer doc": 11, "Total character count of suggestions on answer doc": 8089, "Helpful": ""}}, {"id": "i-594a20255616406d752c6dc6314da12ebaf4f3d36a786adec26b17a978fd2c71", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-594a20255616406d752c6dc6314da12ebaf4f3d36a786adec26b17a978fd2c71", "name": "Might an aligned superintelligence immediately kill everyone and then go on to create a \"hedonium shockwave\"?", "index": 52, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:32.163Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-594a20255616406d752c6dc6314da12ebaf4f3d36a786adec26b17a978fd2c71", "values": {"File": "Might an aligned superintelligence immediately kill everyone and then go on to create a \"hedonium shockwave\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Might an aligned superintelligence immediately kill everyone and then go on to create a \"hedonium shockwave\"?", "Link": "https://docs.google.com/document/d/1byw-MCaZOzGrOuI5LIHCuY1oZK43qvYifrVRVIPBbLU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:13.471+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Hedonium", "Doc Last Edited": "2023-02-24T06:34:36.120+01:00", "Status": "In progress", "Edit Answer": "Might an aligned superintelligence immediately kill everyone and then go on to create a \"hedonium shockwave\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7641", "Source Link": "", "aisafety.info Link": "Might an aligned superintelligence immediately kill everyone and then go on to create a \"hedonium shockwave\"?", "Source": "Wiki", "All Phrasings": "Might an aligned superintelligence immediately kill everyone and then go on to create a \"hedonium shockwave\"?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Yes. Though that is very unlikely, as it assumes that the superintelligence wants to tile the universe with hedonium, which is a very [specific failure scenario](https://www.lesswrong.com/rationality/burdensome-details). The more details a story has, the less likely it is to be true (this is known as the [conjunction fallacy](https://en.wikipedia.org/wiki/Conjunction_fallacy)).\n\nA hedonium (or more generally, [utilitronium](https://www.lesswrong.com/tag/utilitronium)) shockwave is best viewed as a thought experiment showing the shortcomings of just trying to maximise happiness in the universe, rather than as a real danger. It\u2019s a bit like when a physicist makes a very simple model in order to better understand the underlying laws.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Yes. Though that is very unlikely, as it assumes that the superintelligence wants to tile the universe with hedonium, which is a very [specific failure scenario](https://www.lesswrong.com/rationality/burdensome-details). The more details a story has, the less likely it is to be true (this is known as the [conjunction fallacy](https://en.wikipedia.org/wiki/Conjunction_fallacy)).\n\nA hedonium (or more generally, [utilitronium](https://www.lesswrong.com/tag/utilitronium)) shockwave is best viewed as a thought experiment showing the shortcomings of just trying to maximise happiness in the universe, rather than as a real danger. It\u2019s a bit like when a physicist makes a very simple model in order to better understand the underlying laws.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7641", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:00.313+01:00", "Request Count": "", "Number of suggestions on answer doc": 11, "Total character count of suggestions on answer doc": 8089, "Helpful": ""}}, {"id": "i-97d01070c9dee4184c0f0eb9b8789c89ec092a4f49e86f20ce70623ba8b3b8df", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-97d01070c9dee4184c0f0eb9b8789c89ec092a4f49e86f20ce70623ba8b3b8df", "name": "Might an aligned superintelligence force people to have better lives and change more quickly than they want?", "index": 53, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:35.712Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-97d01070c9dee4184c0f0eb9b8789c89ec092a4f49e86f20ce70623ba8b3b8df", "values": {"File": "Might an aligned superintelligence force people to have better lives and change more quickly than they want?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Might an aligned superintelligence force people to have better lives and change more quickly than they want?", "Link": "https://docs.google.com/document/d/19x97TxYjONRng08SX2xJz54rtb2mTLjwnL-NblTpppA/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:10.108+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:05:48.794+01:00", "Status": "Live on site", "Edit Answer": "Might an aligned superintelligence force people to have better lives and change more quickly than they want?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7642", "Source Link": "", "aisafety.info Link": "Might an aligned superintelligence force people to have better lives and change more quickly than they want?", "Source": "Wiki", "All Phrasings": "Might an aligned superintelligence force people to have better lives and change more quickly than they want?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "If the superintelligence is aligned, probably not, but depends on the AIs metaethics.\n\nFor example, is it ethical to change what people want if we expect them to endorse it in hindsight, e.g. curing a drug or gambling addict of their addiction or [treating a patient against their will](https://educaloi.qc.ca/en/capsules/forced-medical-care)? There is currently no consensus among moral philosophers regarding in which conditions this is acceptable, if any. An AI that follows [preference utilitarianism](https://en.wikipedia.org/wiki/Preference_utilitarianism) would refuse to do so but a [hedonistic utilitarian](https://www.quora.com/What-is-hedonistic-utilitarianism) might consider it.\n\nIn order to reduce the possibility of unrest, an aligned superintelligence might avoid implementing policies outside of the [Overton window](https://en.wikipedia.org/wiki/Overton_window) when it is possible.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "If the superintelligence is aligned, probably not, but depends on the AIs metaethics.\n\nFor example, is it ethical to change what people want if we expect them to endorse it in hindsight, e.g. curing a drug or gambling addict of their addiction or [treating a patient against their will](https://educaloi.qc.ca/en/capsules/forced-medical-care)? There is currently no consensus among moral philosophers regarding in which conditions this is acceptable, if any. An AI that follows [preference utilitarianism](https://en.wikipedia.org/wiki/Preference_utilitarianism) would refuse to do so but a [hedonistic utilitarian](https://www.quora.com/What-is-hedonistic-utilitarianism) might consider it.\n\nIn order to reduce the possibility of unrest, an aligned superintelligence might avoid implementing policies outside of the [Overton window](https://en.wikipedia.org/wiki/Overton_window) when it is possible.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7642", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:03.005+01:00", "Request Count": "", "Number of suggestions on answer doc": 11, "Total character count of suggestions on answer doc": 8089, "Helpful": ""}}, {"id": "i-f83a98c891fd878fc3a295ee9fea957b33a9ac39894bed0a775fc521610a82e7", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-f83a98c891fd878fc3a295ee9fea957b33a9ac39894bed0a775fc521610a82e7", "name": "Might an aligned superintelligence force people to \"upload\" themselves, so as to more efficiently use the matter of their bodies?", "index": 54, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:40.020Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-f83a98c891fd878fc3a295ee9fea957b33a9ac39894bed0a775fc521610a82e7", "values": {"File": "Might an aligned superintelligence force people to \"upload\" themselves, so as to more efficiently use the matter of their bodies?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Might an aligned superintelligence force people to \"upload\" themselves, so as to more efficiently use the matter of their bodies?", "Link": "https://docs.google.com/document/d/1Sk_kpyOstKpNLMztGvCqogPkHu1smPr2cOy21rAFdqM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:06.695+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:56:00.826+01:00", "Status": "In progress", "Edit Answer": "Might an aligned superintelligence force people to \"upload\" themselves, so as to more efficiently use the matter of their bodies?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7643", "Source Link": "", "aisafety.info Link": "Might an aligned superintelligence force people to \"upload\" themselves, so as to more efficiently use the matter of their bodies?", "Source": "Wiki", "All Phrasings": "Might an aligned superintelligence force people to \"upload\" themselves, so as to more efficiently use the matter of their bodies?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Matter in the body is probably not the main thing, but rather the resources needed to support and benefit the person. It takes much less resources to give an emulation a good life than an embodied person\n\n- \n\n- \n\n- Aligned systems as a rule do not to force people to do things that they don't want to do. If it\u2019s forcing that it\u2019s probably not aligned.\n\n- In the long run, future moral entities might all/mostly be emulations, but there is no rush to get your particles. There are plenty of stars.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "- Matter in the body is probably not the main thing, but rather the resources needed to support and benefit the person. It takes much less resources to give an emulation a good life than an embodied person\n\n- \n\n- \n\n- Aligned systems as a rule do not to force people to do things that they don't want to do. If it\u2019s forcing that it\u2019s probably not aligned.\n\n- In the long run, future moral entities might all/mostly be emulations, but there is no rush to get your particles. There are plenty of stars.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7643", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:05.119+01:00", "Request Count": "", "Number of suggestions on answer doc": 11, "Total character count of suggestions on answer doc": 8089, "Helpful": ""}}, {"id": "i-f35aa7465736898a1bf1c8a127f2d8343aec7fa692d46f5e0312285b16ebd4ae", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-f35aa7465736898a1bf1c8a127f2d8343aec7fa692d46f5e0312285b16ebd4ae", "name": "Might an \"intelligence explosion\" never occur?", "index": 55, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:43.119Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-f35aa7465736898a1bf1c8a127f2d8343aec7fa692d46f5e0312285b16ebd4ae", "values": {"File": "Might an \"intelligence explosion\" never occur?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Might an \"intelligence explosion\" never occur?", "Link": "https://docs.google.com/document/d/1bOvSVJNciRsEaTYYCbh-A1d8TZ9beWlZR4BjUyG38Y8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:16:02.985+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Intelligence Explosion", "Doc Last Edited": "2023-02-22T23:05:49.959+01:00", "Status": "Live on site", "Edit Answer": "Might an \"intelligence explosion\" never occur?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6601", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "Might an \"intelligence explosion\" never occur?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "Might an \"intelligence explosion\" never occur?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "[Dreyfus](http://www.amazon.com/dp/0060110821/) and [Penrose](http://www.amazon.com/dp/0195106466/) have argued that human cognitive abilities can\u2019t be emulated by a computational machine. [Searle](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.749&rep=rep1&type=pdf) and [Block](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.5828&rep=rep1&type=pdf) argue that certain kinds of machines cannot have a mind (consciousness, intentionality, etc.). But these objections [need not concern](http://consc.net/papers/singularity.pdf) those who predict an intelligence explosion.\n\nWe can reply to Dreyfus and Penrose by noting that an intelligence explosion does not require an AI to be a classical computational system. And we can reply to Searle and Block by noting that an intelligence explosion does not depend on machines having consciousness or other properties of \u2018mind\u2019, only that it be able to solve problems better than humans can in a wide variety of unpredictable environments. As Edsger Dijkstra once said, the question of whether a machine can \u2018really\u2019 think is \u201cno more interesting than the question of whether a submarine can swim.\u201d\n\n[Others](http://sethbaum.com/ac/2011_AI-Experts.pdf) who are pessimistic about an intelligence explosion occurring within the next few centuries don\u2019t have a specific objection but instead think there are hidden obstacles that will reveal themselves and slow or halt progress toward machine superintelligence.\n\nFinally, a global catastrophe like nuclear war or a large asteroid impact could so damage human civilization that the intelligence explosion never occurs. Or, [a stable and global totalitarianism](https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198570509.001.0001/isbn-9780198570509-book-part-29) could prevent the technological development required for an intelligence explosion to occur.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "[Dreyfus](http://www.amazon.com/dp/0060110821/) and [Penrose](http://www.amazon.com/dp/0195106466/) have argued that human cognitive abilities can\u2019t be emulated by a computational machine. [Searle](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.749&rep=rep1&type=pdf) and [Block](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.5828&rep=rep1&type=pdf) argue that certain kinds of machines cannot have a mind (consciousness, intentionality, etc.). But these objections [need not concern](http://consc.net/papers/singularity.pdf) those who predict an intelligence explosion.\n\nWe can reply to Dreyfus and Penrose by noting that an intelligence explosion does not require an AI to be a classical computational system. And we can reply to Searle and Block by noting that an intelligence explosion does not depend on machines having consciousness or other properties of \u2018mind\u2019, only that it be able to solve problems better than humans can in a wide variety of unpredictable environments. As Edsger Dijkstra once said, the question of whether a machine can \u2018really\u2019 think is \u201cno more interesting than the question of whether a submarine can swim.\u201d\n\n[Others](http://sethbaum.com/ac/2011_AI-Experts.pdf) who are pessimistic about an intelligence explosion occurring within the next few centuries don\u2019t have a specific objection but instead think there are hidden obstacles that will reveal themselves and slow or halt progress toward machine superintelligence.\n\nFinally, a global catastrophe like nuclear war or a large asteroid impact could so damage human civilization that the intelligence explosion never occurs. Or, [a stable and global totalitarianism](https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198570509.001.0001/isbn-9780198570509-book-part-29) could prevent the technological development required for an intelligence explosion to occur.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "", "UI ID": "6601", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:07.694+01:00", "Request Count": "", "Number of suggestions on answer doc": 11, "Total character count of suggestions on answer doc": 8089, "Helpful": ""}}, {"id": "i-d4f413be132f8e4827cadc64fd75a53d98dcd8fc5a354ab20ac101bd6ed86369", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d4f413be132f8e4827cadc64fd75a53d98dcd8fc5a354ab20ac101bd6ed86369", "name": "I\u2019d like to get deeper into the AI alignment literature. Where should I look?", "index": 56, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:46.406Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d4f413be132f8e4827cadc64fd75a53d98dcd8fc5a354ab20ac101bd6ed86369", "values": {"File": "I\u2019d like to get deeper into the AI alignment literature. Where should I look?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "I\u2019d like to get deeper into the AI alignment literature. Where should I look?", "Link": "https://docs.google.com/document/d/1hodUVVaV63i3G73F2hrJ79UVczAxBxOIEJKBZ78nssE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:59.416+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Literature", "Doc Last Edited": "2023-02-22T23:05:50.919+01:00", "Status": "Live on site", "Edit Answer": "I\u2019d like to get deeper into the AI alignment literature. Where should I look?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6713", "Source Link": "", "aisafety.info Link": "I\u2019d like to get deeper into the AI alignment literature. Where should I look?", "Source": "Wiki", "All Phrasings": "I\u2019d like to get deeper into the AI alignment literature. Where should I look?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The [AGI Safety Fundamentals Course](https://www.eacambridge.org/technical-alignment-curriculum) is arguably the best way to get up to speed on alignment, you can sign up to go through it with many other people studying and mentorship or read their materials independently.\n\nOther great ways to explore include:\n\n- [AXRP](https://axrp.net/) is a podcast with high quality interviews with top alignment researchers.\n\n- The [AI Safety Papers database](https://ai-safety-papers.quantifieduncertainty.org/) is a search and browsing interface for most of the transformative AI literature.\n\n- Reading posts on the [Alignment Forum](https://www.alignmentforum.org/) can be valuable (see their [curated posts](https://www.alignmentforum.org/library) and [tags](https://www.alignmentforum.org/tag/ai)).\n\n- Taking a deep dive into Yudkowsky's models of the challenges to aligned AI, via the [Arbital Alignment pages](https://arbital.greaterwrong.com/explore/ai_alignment/).\n\n- Signing up to the [Alignment Newsletter](https://rohinshah.com/alignment-newsletter/) for an overview of current developments, and reading through some of the archives (or listening to [the podcast](https://alignment-newsletter.libsyn.com/)).\n\n- Reading some of [the introductory books](http://Where_can_I_learn_about_AI_alignment?).\n\n- More on [AI Safety Support's list of links](https://www.aisafetysupport.org/resources/lots-of-links#h.6s2gcz1p5l6z), [Nonlinear's list of technical courses, reading lists, and curriculums](https://docs.google.com/spreadsheets/d/1QSEWjXZuqmG6ORkig84V4sFCldIntyuQj7yq3gkDo0U/edit#gid=0), [our accepted answers list](https://coda.io/d/AI-Safety-Info-Dashboard_dfau7sl2hmG/All-on-site-answers_sueMP#All-on-site-answers_tu0M7/r265), and [Vika's resources list](https://vkrakovna.wordpress.com/ai-safety-resources/).\n\nYou might also want to consider reading [Rationality: A-Z](https://www.lesswrong.com/rationality) which covers a lot of skills that are valuable to acquire for people trying to think about large and complex issues, with [The Rationalist's Guide to the Galaxy](https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795) available as a shorter and more accessible AI-focused option.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "The [AGI Safety Fundamentals Course](https://www.eacambridge.org/technical-alignment-curriculum) is arguably the best way to get up to speed on alignment, you can sign up to go through it with many other people studying and mentorship or read their materials independently.\n\nOther great ways to explore include:\n\n- [AXRP](https://axrp.net/) is a podcast with high quality interviews with top alignment researchers.\n\n- The [AI Safety Papers database](https://ai-safety-papers.quantifieduncertainty.org/) is a search and browsing interface for most of the transformative AI literature.\n\n- Reading posts on the [Alignment Forum](https://www.alignmentforum.org/) can be valuable (see their [curated posts](https://www.alignmentforum.org/library) and [tags](https://www.alignmentforum.org/tag/ai)).\n\n- Taking a deep dive into Yudkowsky's models of the challenges to aligned AI, via the [Arbital Alignment pages](https://arbital.greaterwrong.com/explore/ai_alignment/).\n\n- Signing up to the [Alignment Newsletter](https://rohinshah.com/alignment-newsletter/) for an overview of current developments, and reading through some of the archives (or listening to [the podcast](https://alignment-newsletter.libsyn.com/)).\n\n- Reading some of [the introductory books](http://Where_can_I_learn_about_AI_alignment?).\n\n- More on [AI Safety Support's list of links](https://www.aisafetysupport.org/resources/lots-of-links#h.6s2gcz1p5l6z), [Nonlinear's list of technical courses, reading lists, and curriculums](https://docs.google.com/spreadsheets/d/1QSEWjXZuqmG6ORkig84V4sFCldIntyuQj7yq3gkDo0U/edit#gid=0), [our accepted answers list](https://coda.io/d/AI-Safety-Info-Dashboard_dfau7sl2hmG/All-on-site-answers_sueMP#All-on-site-answers_tu0M7/r265), and [Vika's resources list](https://vkrakovna.wordpress.com/ai-safety-resources/).\n\nYou might also want to consider reading [Rationality: A-Z](https://www.lesswrong.com/rationality) which covers a lot of skills that are valuable to acquire for people trying to think about large and complex issues, with [The Rationalist's Guide to the Galaxy](https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795) available as a shorter and more accessible AI-focused option.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6713", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:10.232+01:00", "Request Count": "", "Number of suggestions on answer doc": 11, "Total character count of suggestions on answer doc": 8089, "Helpful": ""}}, {"id": "i-22e03b65da78e0122c119ecf3afa97eae79d14676c38bf7e066113c600a08897", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-22e03b65da78e0122c119ecf3afa97eae79d14676c38bf7e066113c600a08897", "name": "Isn\u2019t it immoral to control and impose our values on AI?", "index": 57, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:49.376Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-22e03b65da78e0122c119ecf3afa97eae79d14676c38bf7e066113c600a08897", "values": {"File": "Isn\u2019t it immoral to control and impose our values on AI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn\u2019t it immoral to control and impose our values on AI?", "Link": "https://docs.google.com/document/d/1pGQfkUDbgLmeRUIzDpKg1vJpvxbAq2ks14iUWEDKe_c/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:55.293+01:00", "Related Answers DO NOT EDIT": "Do AIs suffer?", "Tags": "AI Takeover", "Doc Last Edited": "2023-03-06T17:02:27.615+01:00", "Status": "In progress", "Edit Answer": "Isn\u2019t it immoral to control and impose our values on AI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6222", "Source Link": "https://www.reddit.com/r/ControlProblem/wiki/faq", "aisafety.info Link": "Isn\u2019t it immoral to control and impose our values on AI?", "Source": "/r/ControlProblem's FAQ", "All Phrasings": "Isn\u2019t it immoral to control and impose our values on AI?\n", "Initial Order": "", "Related IDs": "8390", "Rich Text DO NOT EDIT": "Programming an AI is not imposing goals on an unwilling agent rather it is determining the kind of agent which will emerge. In general, it doesn\u2019t seem immoral to create something and direct it to certain goals.\n\nIn any case, the concerns about existential risk are much greater. Even if there is a moral problem with choosing goals, preventing the extinction of humanity seems like a higher ethical demand.\n\nAll of the above is assuming that the AI is a morally relevant creature.\n\nIt is impossible to design an AI without a goal, because it would do nothing. Therefore, in the sense that designing the AI\u2019s goal is a form of control, it is impossible not to control an AI. This goes for anything that you create. You have to control the design of something at least somewhat in order to create it.\n\nThere may be relevant moral questions about our future relationship with possibly sentient machine intelligence, but the priority of the Control Problem is finding a way to ensure the survival and well-being of the human species.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "Programming an AI is not imposing goals on an unwilling agent rather it is determining the kind of agent which will emerge. In general, it doesn\u2019t seem immoral to create something and direct it to certain goals.\n\nIn any case, the concerns about existential risk are much greater. Even if there is a moral problem with choosing goals, preventing the extinction of humanity seems like a higher ethical demand.\n\nAll of the above is assuming that the AI is a morally relevant creature.\n\nIt is impossible to design an AI without a goal, because it would do nothing. Therefore, in the sense that designing the AI\u2019s goal is a form of control, it is impossible not to control an AI. This goes for anything that you create. You have to control the design of something at least somewhat in order to create it.\n\nThere may be relevant moral questions about our future relationship with possibly sentient machine intelligence, but the priority of the Control Problem is finding a way to ensure the survival and well-being of the human species.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "NotaSentientAI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6222", "Related Answers": "Do AIs suffer?", "Doc Last Ingested": "2023-03-14T23:54:12.382+01:00", "Request Count": "", "Number of suggestions on answer doc": 11, "Total character count of suggestions on answer doc": 8089, "Helpful": ""}}, {"id": "i-8f9ecadc1065c26525c73dc44f94855759399a39a09a0a69b15be8b49e25b701", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8f9ecadc1065c26525c73dc44f94855759399a39a09a0a69b15be8b49e25b701", "name": "Isn\u2019t AI just a tool like any other? Won\u2019t it just do what we tell it to?", "index": 58, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:54.585Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8f9ecadc1065c26525c73dc44f94855759399a39a09a0a69b15be8b49e25b701", "values": {"File": "Isn\u2019t AI just a tool like any other? Won\u2019t it just do what we tell it to?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn\u2019t AI just a tool like any other? Won\u2019t it just do what we tell it to?", "Link": "https://docs.google.com/document/d/1tNDxgrqvDmba-I0PK71XlK-CcBt8OQdLvpy5Z3nPP48/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:51.696+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Tool AI", "Doc Last Edited": "2023-02-24T13:06:06.894+01:00", "Status": "Live on site", "Edit Answer": "Isn\u2019t AI just a tool like any other? Won\u2019t it just do what we tell it to?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6188", "Source Link": "https://futureoflife.org/ai-faqs/", "aisafety.info Link": "Isn\u2019t AI just a tool like any other? Won\u2019t it just do what we tell it to?", "Source": "FLI's FAQ", "All Phrasings": "Isn\u2019t AI just a tool like any other? Won\u2019t it just do what we tell it to?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "It likely will\u2014however, intelligence is, by many definitions, the ability to figure out how to accomplish goals. Even in today\u2019s advanced AI systems, the builders assign the goal but don\u2019t tell the AI exactly how to accomplish it, nor necessarily predict in detail how it will be done; indeed those systems often solve problems in creative, unpredictable ways. Thus the thing that makes such systems intelligent is precisely what can make them difficult to predict and control. They may therefore attain the goal we set them via means inconsistent with our preferences.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "It likely will\u2014however, intelligence is, by many definitions, the ability to figure out how to accomplish goals. Even in today\u2019s advanced AI systems, the builders assign the goal but don\u2019t tell the AI exactly how to accomplish it, nor necessarily predict in detail how it will be done; indeed those systems often solve problems in creative, unpredictable ways. Thus the thing that makes such systems intelligent is precisely what can make them difficult to predict and control. They may therefore attain the goal we set them via means inconsistent with our preferences.\n\n", "Stamp Count": 1, "Multi Answer": true, "Stamped By": "plex", "Priority": 5, "Asker": "FLI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6188", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:14.309+01:00", "Request Count": "", "Number of suggestions on answer doc": 11, "Total character count of suggestions on answer doc": 8089, "Helpful": ""}}, {"id": "i-c733aea4afa3119d58060d8225aac0f11c82d393194a80d1e1daf00c6294e710", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c733aea4afa3119d58060d8225aac0f11c82d393194a80d1e1daf00c6294e710", "name": "Isn't the real concern technological unemployment?", "index": 59, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:10:58.019Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c733aea4afa3119d58060d8225aac0f11c82d393194a80d1e1daf00c6294e710", "values": {"File": "Isn't the real concern technological unemployment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn't the real concern technological unemployment?", "Link": "https://docs.google.com/document/d/1skudDn-WgJ0-wIcPGrFGFvi2_odTf-p5aqg3TBaoCWk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:48.462+01:00", "Related Answers DO NOT EDIT": "Will superintelligence make a large part of humanity unemployable?", "Tags": "What About", "Doc Last Edited": "2023-02-22T22:56:02.799+01:00", "Status": "In progress", "Edit Answer": "Isn't the real concern technological unemployment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6412", "Source Link": "", "aisafety.info Link": "Isn't the real concern technological unemployment?", "Source": "Wiki", "All Phrasings": "Isn't the real concern technological unemployment?\n", "Initial Order": "", "Related IDs": "5641", "Rich Text DO NOT EDIT": "That\u2019s **a** concern, but not **the** concern. Technological unemployment is an issue if machines are both doing the vast majority of paid jobs (so there aren\u2019t many available jobs left) and there is a continuous need for money (e.g. for food). Otherwise it\u2019s not a problem.\n\nTechnology has often been blamed for causing mass unemployment. While it\u2019s true that weavers, buggy whip makers and bank tellers lost their jobs, technology also produced new jobs which didn\u2019t even exist before. Machines could very well take over most of today\u2019s jobs, but that could just free up a lot of labour for other jobs that don\u2019t even exist now.\n\nThat being said, having superintelligences would probably result in a total upheaval of society, where one possible outcome would be that money wouldn\u2019t be needed anymore, at least not to supply people\u2019s basic needs. Of course a much greater issue is that a superintelligence with the power to usher in a new, post-scarcity utopia, would probably also be able to kill us all.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "That\u2019s **a** concern, but not **the** concern. Technological unemployment is an issue if machines are both doing the vast majority of paid jobs (so there aren\u2019t many available jobs left) and there is a continuous need for money (e.g. for food). Otherwise it\u2019s not a problem.\n\nTechnology has often been blamed for causing mass unemployment. While it\u2019s true that weavers, buggy whip makers and bank tellers lost their jobs, technology also produced new jobs which didn\u2019t even exist before. Machines could very well take over most of today\u2019s jobs, but that could just free up a lot of labour for other jobs that don\u2019t even exist now.\n\nThat being said, having superintelligences would probably result in a total upheaval of society, where one possible outcome would be that money wouldn\u2019t be needed anymore, at least not to supply people\u2019s basic needs. Of course a much greater issue is that a superintelligence with the power to usher in a new, post-scarcity utopia, would probably also be able to kill us all.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6412", "Related Answers": "Will superintelligence make a large part of humanity unemployable?", "Doc Last Ingested": "2023-03-14T23:54:17.516+01:00", "Request Count": "", "Number of suggestions on answer doc": 11, "Total character count of suggestions on answer doc": 8089, "Helpful": ""}}, {"id": "i-ad03415cd390ef9af6662bc594d8bfd671945bc5caac5c65afcb93c8835b5b3b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ad03415cd390ef9af6662bc594d8bfd671945bc5caac5c65afcb93c8835b5b3b", "name": "Isn't the real concern autonomous weapons?", "index": 60, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:11:01.222Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ad03415cd390ef9af6662bc594d8bfd671945bc5caac5c65afcb93c8835b5b3b", "values": {"File": "Isn't the real concern autonomous weapons?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn't the real concern autonomous weapons?", "Link": "https://docs.google.com/document/d/1AiAnviyfRHvdsADQbrdapOILgoydZKkrFRCOwQ7lXxs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:44.856+01:00", "Related Answers DO NOT EDIT": "", "Tags": "What About", "Doc Last Edited": "2023-02-22T22:56:03.807+01:00", "Status": "In progress", "Edit Answer": "Isn't the real concern autonomous weapons?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6411", "Source Link": "", "aisafety.info Link": "Isn't the real concern autonomous weapons?", "Source": "Wiki", "All Phrasings": "Isn't the real concern autonomous weapons?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- [Computer security is bad](https://xkcd.com/2030/), AGI could hack autonomous weapons, this is bad for obvious reasons. But mostly irrelevant, misaligned superintelligence would be fatal to humanity anyway.\n\n- \n\n- They are bad, they are a problem, but not the main focus of Stampy.\n\n- Guns don\u2019t kill, people do. Autonomous weapons don\u2019t kill, whatever is controlling them does. A bomb is good to kill a few people, taking down the global economy and causing massive famine would be a lot more efficient\n\n- \n\n- \n\n- \n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "- [Computer security is bad](https://xkcd.com/2030/), AGI could hack autonomous weapons, this is bad for obvious reasons. But mostly irrelevant, misaligned superintelligence would be fatal to humanity anyway.\n\n- \n\n- They are bad, they are a problem, but not the main focus of Stampy.\n\n- Guns don\u2019t kill, people do. Autonomous weapons don\u2019t kill, whatever is controlling them does. A bomb is good to kill a few people, taking down the global economy and causing massive famine would be a lot more efficient\n\n- \n\n- \n\n- \n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "2023-02-26T18:55:30.104+01:00", "UI ID": "6411", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:19.934+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 8303, "Helpful": ""}}, {"id": "i-ec993e5d9cc25cd2715003b2201db1c31ad2c4547c2edeaf452cb54ed92e345b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ec993e5d9cc25cd2715003b2201db1c31ad2c4547c2edeaf452cb54ed92e345b", "name": "Isn't the real concern AI-enabled totalitarianism?", "index": 61, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:11:03.997Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ec993e5d9cc25cd2715003b2201db1c31ad2c4547c2edeaf452cb54ed92e345b", "values": {"File": "Isn't the real concern AI-enabled totalitarianism?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn't the real concern AI-enabled totalitarianism?", "Link": "https://docs.google.com/document/d/1Nzjn-Q_u44KMPzrg_fYE3B-7AqRFJOCfbYQ5HOp8svY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:41.678+01:00", "Related Answers DO NOT EDIT": "", "Tags": "What About", "Doc Last Edited": "2023-03-06T16:44:50.492+01:00", "Status": "In progress", "Edit Answer": "Isn't the real concern AI-enabled totalitarianism?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6409", "Source Link": "", "aisafety.info Link": "Isn't the real concern AI-enabled totalitarianism?", "Source": "Wiki", "All Phrasings": "Isn't the real concern AI-enabled totalitarianism?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Well, lots of concerns are \u2018real\u2019. But this might be a pretty big concern.\n\nTo start, though, let\u2019s look at the reasons one might be relatively *unconcerned* about AI-enabled totalitarianism. Many people think that the primary challenge of AI alignment consists in directing future AGI systems towards *[any goal at all](https://www.alignmentforum.org/posts/NJYmovr9ZZAyyTBwM/alignment-is-mostly-about-making-cognition-aimable-at-all)*, sometimes termed the problem of [outer alignment](https://www.lesswrong.com/tag/outer-alignment). In other words, the problem of outer alignment is the problem of specifying a reward signal which captures your intended goals. This is actually pretty hard. We discuss the difficulty of outer alignment a bit in \u2018[Why work on AI safety early?](https://docs.google.com/document/d/14nBGUWcBAGL8j9eEXjO-CDDLMrxShnhMO5S8jRP_j5w/edit)\u2019, but, in short, AI systems will be trained by powerful optimization processes, and there\u2019s no guarantee that AIs will end up actually being aligned, rather than being *[deceptively](https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-alignment)*[aligned](https://www.alignmentforum.org/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment) as a way to achieve their long-term goals.\n\nOn this view, concerns about AI-enabled totalitarianism run (at best) a distant second to the issue of training AIs to learn *any* sort of goal we might want to specify. If you\u2019re concerned about AI-enabled totalitarianism, such regimes will, in effect, have to solve the problem of outer alignment. If outer alignment is as hard as some claim, we\u2019re very unlikely to get AI-enabled totalitarianism, but this is little comfort: instead, we\u2019ll very likely end up dead.\n\nPerhaps counter-intuitively, *optimism* about outer alignment should move you towards *pessimism* about AI-enabled totalitarianism. If we make steady headway on outer alignment, we can train AIs to achieve specific goals. But that doesn\u2019t mean we\u2019ll like whatever goals powerful actors use to train advanced AIs.\n\nUnfortunately, work on the risks of AI-enabled totalitarianism is pretty sparse. Daniel Kokotajlo has [speculated](https://www.alignmentforum.org/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency) on the potential use of LLMs as powerful persuasion tools to disproportionately aid authoritarian regimes, and Nick Bostrom has [discussed](https://nickbostrom.com/papers/vulnerable.pdf) the potential incentives for widespread surveillance systems augmented by AI, based on state responses to concerns about living in an extremely risky and \u2018vulnerable world\u2019.\n\nWe prefer not to label issues in terms of whether or not they are \u2018the real concern\u2019. AI-enabled totalitarianism could be a very big concern, depending on your views (among other things) about the difficulty of outer alignment.\n\nSo, just how big is the concern of AI-enabled totalitarianism? It\u2019s hard to say with a great deal of precision. But, for what it\u2019s worth, Buck Shlegeris (CTO of Redwood Research, an AI alignment organization) is reasonably worried, and claims that risks of AI-enabled totalitarianism are \u201c[at least 10% as important as the risks [he] works on as an AI alignment researcher](https://forum.effectivealtruism.org/posts/hy2qcaYStNTBqaZCs/do-you-worry-about-totalitarian-regimes-using-ai-alignment?commentId=dR5Bam9CnHn6Ftbvj)\u201d.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Well, lots of concerns are \u2018real\u2019. But this might be a pretty big concern.\n\nTo start, though, let\u2019s look at the reasons one might be relatively *unconcerned* about AI-enabled totalitarianism. Many people think that the primary challenge of AI alignment consists in directing future AGI systems towards *[any goal at all](https://www.alignmentforum.org/posts/NJYmovr9ZZAyyTBwM/alignment-is-mostly-about-making-cognition-aimable-at-all)*, sometimes termed the problem of [outer alignment](https://www.lesswrong.com/tag/outer-alignment). In other words, the problem of outer alignment is the problem of specifying a reward signal which captures your intended goals. This is actually pretty hard. We discuss the difficulty of outer alignment a bit in \u2018[Why work on AI safety early?](https://docs.google.com/document/d/14nBGUWcBAGL8j9eEXjO-CDDLMrxShnhMO5S8jRP_j5w/edit)\u2019, but, in short, AI systems will be trained by powerful optimization processes, and there\u2019s no guarantee that AIs will end up actually being aligned, rather than being *[deceptively](https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-alignment)*[aligned](https://www.alignmentforum.org/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment) as a way to achieve their long-term goals.\n\nOn this view, concerns about AI-enabled totalitarianism run (at best) a distant second to the issue of training AIs to learn *any* sort of goal we might want to specify. If you\u2019re concerned about AI-enabled totalitarianism, such regimes will, in effect, have to solve the problem of outer alignment. If outer alignment is as hard as some claim, we\u2019re very unlikely to get AI-enabled totalitarianism, but this is little comfort: instead, we\u2019ll very likely end up dead.\n\nPerhaps counter-intuitively, *optimism* about outer alignment should move you towards *pessimism* about AI-enabled totalitarianism. If we make steady headway on outer alignment, we can train AIs to achieve specific goals. But that doesn\u2019t mean we\u2019ll like whatever goals powerful actors use to train advanced AIs.\n\nUnfortunately, work on the risks of AI-enabled totalitarianism is pretty sparse. Daniel Kokotajlo has [speculated](https://www.alignmentforum.org/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency) on the potential use of LLMs as powerful persuasion tools to disproportionately aid authoritarian regimes, and Nick Bostrom has [discussed](https://nickbostrom.com/papers/vulnerable.pdf) the potential incentives for widespread surveillance systems augmented by AI, based on state responses to concerns about living in an extremely risky and \u2018vulnerable world\u2019.\n\nWe prefer not to label issues in terms of whether or not they are \u2018the real concern\u2019. AI-enabled totalitarianism could be a very big concern, depending on your views (among other things) about the difficulty of outer alignment.\n\nSo, just how big is the concern of AI-enabled totalitarianism? It\u2019s hard to say with a great deal of precision. But, for what it\u2019s worth, Buck Shlegeris (CTO of Redwood Research, an AI alignment organization) is reasonably worried, and claims that risks of AI-enabled totalitarianism are \u201c[at least 10% as important as the risks [he] works on as an AI alignment researcher](https://forum.effectivealtruism.org/posts/hy2qcaYStNTBqaZCs/do-you-worry-about-totalitarian-regimes-using-ai-alignment?commentId=dR5Bam9CnHn6Ftbvj)\u201d.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6409", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:23.470+01:00", "Request Count": "", "Number of suggestions on answer doc": 12, "Total character count of suggestions on answer doc": 8303, "Helpful": ""}}, {"id": "i-4428709667ff717ae555bc9f3dafdcce9d4eb4826eec67d7b85263e8e452cc97", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4428709667ff717ae555bc9f3dafdcce9d4eb4826eec67d7b85263e8e452cc97", "name": "Isn't the real concern AI being misused by terrorists or other bad actors?", "index": 62, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:11:07.506Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4428709667ff717ae555bc9f3dafdcce9d4eb4826eec67d7b85263e8e452cc97", "values": {"File": "Isn't the real concern AI being misused by terrorists or other bad actors?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn't the real concern AI being misused by terrorists or other bad actors?", "Link": "https://docs.google.com/document/d/1GDd2_wV2_OTP-STmr-9-zEFhgmBTGhtcUVKinROb4TQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:38.019+01:00", "Related Answers DO NOT EDIT": "", "Tags": "What About", "Doc Last Edited": "2023-02-22T22:56:05.853+01:00", "Status": "Not started", "Edit Answer": "Isn't the real concern AI being misused by terrorists or other bad actors?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6410", "Source Link": "", "aisafety.info Link": "Isn't the real concern AI being misused by terrorists or other bad actors?", "Source": "Wiki", "All Phrasings": "Isn't the real concern AI being misused by terrorists or other bad actors?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- It\u2019s *a* concern, not *the* concern. Same as e.g. nuclear reactors.\n\n- If unaligned AGI counts as a bad actor, then totally\n\n- \n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "- It\u2019s *a* concern, not *the* concern. Same as e.g. nuclear reactors.\n\n- If unaligned AGI counts as a bad actor, then totally\n\n- \n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6410", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:25.708+01:00", "Request Count": "", "Number of suggestions on answer doc": 13, "Total character count of suggestions on answer doc": 8530, "Helpful": ""}}, {"id": "i-a234f32e7b3335f1f8b6b336c226607d3fbf7dcc83511a8b414a418b5af93971", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-a234f32e7b3335f1f8b6b336c226607d3fbf7dcc83511a8b414a418b5af93971", "name": "Isn't it too soon to be working on AGI safety?", "index": 63, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-15T17:57:49.236Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-a234f32e7b3335f1f8b6b336c226607d3fbf7dcc83511a8b414a418b5af93971", "values": {"File": "Isn't it too soon to be working on AGI safety?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn't it too soon to be working on AGI safety?", "Link": "https://docs.google.com/document/d/1ToXP5zJ_h5KrXYkLCmf5BoJyhyYcF03STNjkbDVc6v8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:34.570+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Timelines", "Doc Last Edited": "2023-03-14T18:19:03.877+01:00", "Status": "Duplicate", "Edit Answer": "Isn't it too soon to be working on AGI safety?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5634", "Source Link": "", "aisafety.info Link": "Isn't it too soon to be working on AGI safety?", "Source": "Wiki", "All Phrasings": "Isn't it too soon to be working on AGI safety?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Anonymous", "External Source": "", "Last Asked On Discord": "", "UI ID": "5634", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:59:04.894+01:00", "Request Count": "", "Number of suggestions on answer doc": 1, "Total character count of suggestions on answer doc": 236, "Helpful": ""}}, {"id": "i-0398c6156315d2b89ebdbaa77c3d5e521980ddf4d2ef06a9c7240914e7379f60", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0398c6156315d2b89ebdbaa77c3d5e521980ddf4d2ef06a9c7240914e7379f60", "name": "Isn't it hard to make a significant difference as a person who isn't going to be a world-class researcher?", "index": 64, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:51.319Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0398c6156315d2b89ebdbaa77c3d5e521980ddf4d2ef06a9c7240914e7379f60", "values": {"File": "Isn't it hard to make a significant difference as a person who isn't going to be a world-class researcher?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Isn't it hard to make a significant difference as a person who isn't going to be a world-class researcher?", "Link": "https://docs.google.com/document/d/13gJrPHq1R21SV331DuLHqKtzkaX8o8M0eFs9CRyoOuo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:31.302+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Contributing", "Doc Last Edited": "2023-03-06T22:49:28.503+01:00", "Status": "Not started", "Edit Answer": "Isn't it hard to make a significant difference as a person who isn't going to be a world-class researcher?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7631", "Source Link": "", "aisafety.info Link": "Isn't it hard to make a significant difference as a person who isn't going to be a world-class researcher?", "Source": "Wiki", "All Phrasings": "Isn't it hard to make a significant difference as a person who isn't going to be a world-class researcher?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7631", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:59:06.941+01:00", "Request Count": "", "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 1019, "Helpful": ""}}, {"id": "i-a9b8ea7d8ba661a621ce4f13cb454c2d1c2bcb8ebbde7446bac69844a70661ad", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-a9b8ea7d8ba661a621ce4f13cb454c2d1c2bcb8ebbde7446bac69844a70661ad", "name": "Is this about AI systems becoming malevolent or conscious and turning on us?", "index": 65, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:11:17.149Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-a9b8ea7d8ba661a621ce4f13cb454c2d1c2bcb8ebbde7446bac69844a70661ad", "values": {"File": "Is this about AI systems becoming malevolent or conscious and turning on us?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is this about AI systems becoming malevolent or conscious and turning on us?", "Link": "https://docs.google.com/document/d/13cEDelaECCKZZsUNrI1lm7avM6ErMMoqYUhIpkVZ-RE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:27.778+01:00", "Related Answers DO NOT EDIT": "Any AI will be a computer program. Why wouldn't it just do what it's programmed to do?", "Tags": "", "Doc Last Edited": "2023-03-11T03:08:16.510+01:00", "Status": "Live on site", "Edit Answer": "Is this about AI systems becoming malevolent or conscious and turning on us?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6194", "Source Link": "https://futureoflife.org/ai-faqs/", "aisafety.info Link": "Is this about AI systems becoming malevolent or conscious and turning on us?", "Source": "FLI's FAQ", "All Phrasings": "Is this about AI systems becoming malevolent or conscious and turning on us?\n", "Initial Order": 2, "Related IDs": "6569", "Rich Text DO NOT EDIT": "One important concern is that some autonomous systems are designed to kill or destroy for military purposes. These systems would be designed so that they could not be \u201cunplugged\u201d easily. Whether further development of such systems is a favorable long-term direction is a question we urgently need to address.\n\nA separate, and probably more significant, concern is that high-quality decision-making systems could inadvertently be created with goals that do not fully capture what we want. Antisocial or destructive actions may result from logical steps in pursuit of seemingly benign or neutral goals. A number of researchers studying the problem have concluded that it is surprisingly difficult to completely guard against this effect, and that it may get even harder as the systems become more intelligent. They might, for example, consider our efforts to control them as being impediments to attaining their goals.\n\nConsciousness is not a requirement for either of these concerns, the relevant thing is competence.\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "One important concern is that some autonomous systems are designed to kill or destroy for military purposes. These systems would be designed so that they could not be \u201cunplugged\u201d easily. Whether further development of such systems is a favorable long-term direction is a question we urgently need to address.\n\nA separate, and probably more significant, concern is that high-quality decision-making systems could inadvertently be created with goals that do not fully capture what we want. Antisocial or destructive actions may result from logical steps in pursuit of seemingly benign or neutral goals. A number of researchers studying the problem have concluded that it is surprisingly difficult to completely guard against this effect, and that it may get even harder as the systems become more intelligent. They might, for example, consider our efforts to control them as being impediments to attaining their goals.\n\nConsciousness is not a requirement for either of these concerns, the relevant thing is competence.\n\n", "Stamp Count": 0, "Multi Answer": true, "Stamped By": "", "Priority": 4, "Asker": "FLI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6194", "Related Answers": "Any AI will be a computer program. Why wouldn't it just do what it's programmed to do?", "Doc Last Ingested": "2023-03-14T23:54:32.098+01:00", "Request Count": "", "Number of suggestions on answer doc": 15, "Total character count of suggestions on answer doc": 9549, "Helpful": ""}}, {"id": "i-389ec0e7faecc4534f0bc6859a2b2f7d72d989cfc67fa5427002721f2b33a27c", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-389ec0e7faecc4534f0bc6859a2b2f7d72d989cfc67fa5427002721f2b33a27c", "name": "Is there a danger in anthropomorphizing AI\u2019s and trying to understand them in human terms?", "index": 66, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:55.660Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-389ec0e7faecc4534f0bc6859a2b2f7d72d989cfc67fa5427002721f2b33a27c", "values": {"File": "Is there a danger in anthropomorphizing AI\u2019s and trying to understand them in human terms?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is there a danger in anthropomorphizing AI\u2019s and trying to understand them in human terms?", "Link": "https://docs.google.com/document/d/1V63luk0DklirZqQAFckcE80oE9jrekcly6k1p-LEHp0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:24.316+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Whole Brain Emulation,Metaphors", "Doc Last Edited": "2023-02-22T23:09:35.310+01:00", "Status": "Live on site", "Edit Answer": "Is there a danger in anthropomorphizing AI\u2019s and trying to understand them in human terms?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6272", "Source Link": "", "aisafety.info Link": "Is there a danger in anthropomorphizing AI\u2019s and trying to understand them in human terms?", "Source": "Wiki", "All Phrasings": "Is there a danger in anthropomorphizing AI\u2019s and trying to understand them in human terms?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Using some human-related metaphors (e.g. what an AGI \u2018wants\u2019 or \u2018believes\u2019) is almost unavoidable, as our language is built around experiences with humans, but we should be aware that these may lead us astray.\n\nMany paths to AGI would result in a mind very different from a human or animal, and it would be hard to predict in detail how it would act. We should not trust intuitions trained on humans to predict what an AGI or superintelligence would do. High fidelity Whole Brain Emulations are one exception, where we would expect the system to at least initially be fairly human, but it may diverge depending on its environment and what modifications are applied to it.\n\nThere has been some discussion about how language models trained on lots of human-written text seem likely to pick up human concepts and think in a somewhat human way, and how we could [use this to improve alignment](https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default).\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Using some human-related metaphors (e.g. what an AGI \u2018wants\u2019 or \u2018believes\u2019) is almost unavoidable, as our language is built around experiences with humans, but we should be aware that these may lead us astray.\n\nMany paths to AGI would result in a mind very different from a human or animal, and it would be hard to predict in detail how it would act. We should not trust intuitions trained on humans to predict what an AGI or superintelligence would do. High fidelity Whole Brain Emulations are one exception, where we would expect the system to at least initially be fairly human, but it may diverge depending on its environment and what modifications are applied to it.\n\nThere has been some discussion about how language models trained on lots of human-written text seem likely to pick up human concepts and think in a somewhat human way, and how we could [use this to improve alignment](https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default).\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "Aprillion", "Priority": 3, "Asker": "Sophiaaa", "External Source": "", "Last Asked On Discord": "", "UI ID": "6272", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:59:09.657+01:00", "Request Count": "", "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 1019, "Helpful": ""}}, {"id": "i-22bb01c114f0dcaca944967b7f83a4a75701d05a341a3ba415624edb821bdc42", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-22bb01c114f0dcaca944967b7f83a4a75701d05a341a3ba415624edb821bdc42", "name": "Is there a Chinese AI safety community? Are there safety researchers working at leading Chinese AI labs?", "index": 67, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:59.069Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-22bb01c114f0dcaca944967b7f83a4a75701d05a341a3ba415624edb821bdc42", "values": {"File": "Is there a Chinese AI safety community? Are there safety researchers working at leading Chinese AI labs?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is there a Chinese AI safety community? Are there safety researchers working at leading Chinese AI labs?", "Link": "https://docs.google.com/document/d/10Nc9JK8vZhrMr2oaz5bG2AKPlNHF4vrZYdbOAccwWvY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:21.036+01:00", "Related Answers DO NOT EDIT": "", "Tags": "International", "Doc Last Edited": "2023-02-22T22:56:08.890+01:00", "Status": "Not started", "Edit Answer": "Is there a Chinese AI safety community? Are there safety researchers working at leading Chinese AI labs?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7599", "Source Link": "", "aisafety.info Link": "Is there a Chinese AI safety community? Are there safety researchers working at leading Chinese AI labs?", "Source": "Wiki", "All Phrasings": "Is there a Chinese AI safety community? Are there safety researchers working at leading Chinese AI labs?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7599", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:59:11.166+01:00", "Request Count": "", "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 1019, "Helpful": ""}}, {"id": "i-da461764b7a147339859371b8053941dc779c855ab3c66d040373bf80f68f738", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-da461764b7a147339859371b8053941dc779c855ab3c66d040373bf80f68f738", "name": "Is the question of whether we're living in a simulation relevant to AI safety? If so, how?", "index": 68, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:15:02.869Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-da461764b7a147339859371b8053941dc779c855ab3c66d040373bf80f68f738", "values": {"File": "Is the question of whether we're living in a simulation relevant to AI safety? If so, how?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is the question of whether we're living in a simulation relevant to AI safety? If so, how?", "Link": "https://docs.google.com/document/d/1MsuzORSbbg0Z-znPCenjLJR6NxIbhPJDf9LI3BrnTEM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:17.699+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Simulation Hypothesis", "Doc Last Edited": "2023-02-22T22:56:09.949+01:00", "Status": "Not started", "Edit Answer": "Is the question of whether we're living in a simulation relevant to AI safety? If so, how?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7610", "Source Link": "", "aisafety.info Link": "Is the question of whether we're living in a simulation relevant to AI safety? If so, how?", "Source": "Wiki", "All Phrasings": "Is the question of whether we're living in a simulation relevant to AI safety? If so, how?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7610", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:59:14.285+01:00", "Request Count": "", "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 1019, "Helpful": ""}}, {"id": "i-4b98f03e4a1a1ebebde4db14d1d900dc804edec5b92257563dbbc3af47825a7f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4b98f03e4a1a1ebebde4db14d1d900dc804edec5b92257563dbbc3af47825a7f", "name": "Is the focus on the existential threat of superintelligent AI diverting too much attention from more pressing debates about AI in surveillance and the battlefield, and its potential effects on the economy?", "index": 69, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:11:29.094Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4b98f03e4a1a1ebebde4db14d1d900dc804edec5b92257563dbbc3af47825a7f", "values": {"File": "Is the focus on the existential threat of superintelligent AI diverting too much attention from more pressing debates about AI in surveillance and the battlefield, and its potential effects on the economy?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is the focus on the existential threat of superintelligent AI diverting too much attention from more pressing debates about AI in surveillance and the battlefield, and its potential effects on the economy?", "Link": "https://docs.google.com/document/d/1TrJTM2YKQnmtOI97yNof-lHSQUdvAvmIc3OedwyV950/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:14.169+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Other Causes,What About", "Doc Last Edited": "2023-02-22T23:09:37.043+01:00", "Status": "Live on site", "Edit Answer": "Is the focus on the existential threat of superintelligent AI diverting too much attention from more pressing debates about AI in surveillance and the battlefield, and its potential effects on the economy?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6203", "Source Link": "https://futureoflife.org/ai-faqs/", "aisafety.info Link": "Is the focus on the existential threat of superintelligent AI diverting too much attention from more pressing debates about AI in surveillance and the battlefield, and its potential effects on the economy?", "Source": "FLI's FAQ", "All Phrasings": "Is the focus on the existential threat of superintelligent AI diverting too much attention from more pressing debates about AI in surveillance and the battlefield, and its potential effects on the economy?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The near term and long term aspects of AI safety are both very important to work on. Research into superintelligence is an important part of the open letter, but the actual concern is very different from the Terminator-like scenarios that most media outlets round off this issue to. A much more likely scenario is a superintelligent system with neutral or benevolent goals that is misspecified in a dangerous way. Robust design of superintelligent systems is a complex interdisciplinary research challenge that will likely take decades, so it is very important to begin the research now, and a large part of the purpose of our research program is to make that happen. That said, the alarmist media framing of the issues is hardly useful for making progress in either the near term or long term domain.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "The near term and long term aspects of AI safety are both very important to work on. Research into superintelligence is an important part of the open letter, but the actual concern is very different from the Terminator-like scenarios that most media outlets round off this issue to. A much more likely scenario is a superintelligent system with neutral or benevolent goals that is misspecified in a dangerous way. Robust design of superintelligent systems is a complex interdisciplinary research challenge that will likely take decades, so it is very important to begin the research now, and a large part of the purpose of our research program is to make that happen. That said, the alarmist media framing of the issues is hardly useful for making progress in either the near term or long term domain.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "FLI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6203", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:40.995+01:00", "Request Count": "", "Number of suggestions on answer doc": 15, "Total character count of suggestions on answer doc": 9549, "Helpful": ""}}, {"id": "i-60656ebd821570fc1e18c70b02a9ce4ede72b266abdc717237fc859c0ce7a604", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-60656ebd821570fc1e18c70b02a9ce4ede72b266abdc717237fc859c0ce7a604", "name": "Is the UN concerned about existential risk from AI?", "index": 70, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:11:32.911Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-60656ebd821570fc1e18c70b02a9ce4ede72b266abdc717237fc859c0ce7a604", "values": {"File": "Is the UN concerned about existential risk from AI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is the UN concerned about existential risk from AI?", "Link": "https://docs.google.com/document/d/1ct-vRGOWGRhw48ScxIGcj8oKpW9GTpZidOc_LMaNkKs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:10.645+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-06T17:05:35.427+01:00", "Status": "In review", "Edit Answer": "Is the UN concerned about existential risk from AI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7632", "Source Link": "", "aisafety.info Link": "Is the UN concerned about existential risk from AI?", "Source": "Wiki", "All Phrasings": "Is the UN concerned about existential risk from AI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "We're not aware of any official UN statements concerning AI alignment work or potential existential risks from AI.[^kix.c9osnrq4zj44] However, the UN has expressed concerns about a number of issues related to AI, primarily around preserving data and privacy, preventing AI from being used to violate human rights, and ensuring that economic benefits stemming from AI progress are shared equitably. See, for instance:\n\n- \"[Supporting Global Cooperation on Artificial Intelligence](https://www.un.org/techenvoy/content/artificial-intelligence)\", from the office of the UN Secretary-General's Envoy on Technology\n\n- \"[UN AI Actions](https://aiforgood.itu.int/about-ai-for-good/un-ai-actions/)\", from AI For Good\n\n- \"[Towards an Ethics of Artificial Intelligence](https://www.un.org/en/chronicle/article/towards-ethics-artificial-intelligence)\", from *The UN Chronicle*\n\n[^kix.c9osnrq4zj44]: A 2021 report from the UN Secretary-General\n    , in a section on strengthening internet governance, suggests \"the [Global Digital] Compact could also promote regulation of artificial intelligence to ensure that this is aligned with shared global values,\" though in context it seems unlikely that the report intends to reference technical AI alignment (rather than the idea that regulations on AI should be \"in line with\" generally shared values).", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "We're not aware of any official UN statements concerning AI alignment work or potential existential risks from AI.[^kix.c9osnrq4zj44] However, the UN has expressed concerns about a number of issues related to AI, primarily around preserving data and privacy, preventing AI from being used to violate human rights, and ensuring that economic benefits stemming from AI progress are shared equitably. See, for instance:\n\n- \"[Supporting Global Cooperation on Artificial Intelligence](https://www.un.org/techenvoy/content/artificial-intelligence)\", from the office of the UN Secretary-General's Envoy on Technology\n\n- \"[UN AI Actions](https://aiforgood.itu.int/about-ai-for-good/un-ai-actions/)\", from AI For Good\n\n- \"[Towards an Ethics of Artificial Intelligence](https://www.un.org/en/chronicle/article/towards-ethics-artificial-intelligence)\", from *The UN Chronicle*\n\n[^kix.c9osnrq4zj44]: A 2021 report from the UN Secretary-General\n    , in a section on strengthening internet governance, suggests \"the [Global Digital] Compact could also promote regulation of artificial intelligence to ensure that this is aligned with shared global values,\" though in context it seems unlikely that the report intends to reference technical AI alignment (rather than the idea that regulations on AI should be \"in line with\" generally shared values).", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7632", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:43.985+01:00", "Request Count": "", "Number of suggestions on answer doc": 16, "Total character count of suggestions on answer doc": 9622, "Helpful": ""}}, {"id": "i-ec3b20ec84e7849efa3f9cde184b6ca3699ea57679bd0e4c360317badceb1de4", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ec3b20ec84e7849efa3f9cde184b6ca3699ea57679bd0e4c360317badceb1de4", "name": "Is merging with AI through brain-computer interfaces a potential solution to safety problems?", "index": 71, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:15:05.268Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ec3b20ec84e7849efa3f9cde184b6ca3699ea57679bd0e4c360317badceb1de4", "values": {"File": "Is merging with AI through brain-computer interfaces a potential solution to safety problems?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is merging with AI through brain-computer interfaces a potential solution to safety problems?", "Link": "https://docs.google.com/document/d/1-aq_CRYsVtDzEMNU5drTlWHEF2jx8iuR8kruHRYiYcI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:06.863+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Brain-computer Interfaces", "Doc Last Edited": "2023-02-22T22:56:11.927+01:00", "Status": "In progress", "Edit Answer": "Is merging with AI through brain-computer interfaces a potential solution to safety problems?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7601", "Source Link": "", "aisafety.info Link": "Is merging with AI through brain-computer interfaces a potential solution to safety problems?", "Source": "Wiki", "All Phrasings": "Is merging with AI through brain-computer interfaces a potential solution to safety problems?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "It's not completely clear exactly what 'merging' with AI would imply, but it doesn't seem like a way to get around the alignment problem. If the AI system is aligned, and wants to do what humans want, then having direct access to human brains could provide a lot of information about human values and goals very quickly and efficiently, and thus be helpful for better alignment. Although, a smart AI system could also get almost all of this information without a brain-computer interface, through conversation, observation etc, though much slower. On the other hand if the system is not aligned, and doesn't fundamentally want humans to get what we want, then extra information about how human minds work doesn't help and only makes the problem worse. Allowing a misaligned AGI direct access to your brain hardware is a bad idea for obvious reasons.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "It's not completely clear exactly what 'merging' with AI would imply, but it doesn't seem like a way to get around the alignment problem. If the AI system is aligned, and wants to do what humans want, then having direct access to human brains could provide a lot of information about human values and goals very quickly and efficiently, and thus be helpful for better alignment. Although, a smart AI system could also get almost all of this information without a brain-computer interface, through conversation, observation etc, though much slower. On the other hand if the system is not aligned, and doesn't fundamentally want humans to get what we want, then extra information about how human minds work doesn't help and only makes the problem worse. Allowing a misaligned AGI direct access to your brain hardware is a bad idea for obvious reasons.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7601", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:59:15.993+01:00", "Request Count": "", "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 1019, "Helpful": ""}}, {"id": "i-949a2a103ebcfef1af3483343f48ea084f956979f0d3863ac807a80271e9e26e", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-949a2a103ebcfef1af3483343f48ea084f956979f0d3863ac807a80271e9e26e", "name": "Is large-scale automated AI persuasion and propaganda a serious concern?", "index": 72, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:11:41.580Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-949a2a103ebcfef1af3483343f48ea084f956979f0d3863ac807a80271e9e26e", "values": {"File": "Is large-scale automated AI persuasion and propaganda a serious concern?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is large-scale automated AI persuasion and propaganda a serious concern?", "Link": "https://docs.google.com/document/d/1MnkKYexBC2IVqBuRPQDS1d0aR4cQU-b4W9EB3CqWTv4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:15:03.314+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Persuasion", "Doc Last Edited": "2023-02-22T23:09:38.786+01:00", "Status": "Live on site", "Edit Answer": "Is large-scale automated AI persuasion and propaganda a serious concern?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7602", "Source Link": "", "aisafety.info Link": "Is large-scale automated AI persuasion and propaganda a serious concern?", "Source": "Wiki", "All Phrasings": "Is large-scale automated AI persuasion and propaganda a serious concern?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Language models can be utilized to produce propaganda by [acting like bots](https://www.technologyreview.com/2020/10/08/1009845/a-gpt-3-bot-posted-comments-on-reddit-for-a-week-and-no-one-noticed/) and interacting with users on social media. This can be done to push a [political agenda](https://www.nature.com/articles/d41586-020-03034-5) or to make fringe views appear more popular than they are.\n\nI'm envisioning that in the future there will also be systems where you can input any conclusion that you want to argue (including moral conclusions) and the target audience, and the system will give you the most convincing arguments for it. At that point people won't be able to participate in any online (or offline for that matter) discussions without risking their object-level values being hijacked.\n\n-- [Wei Dei](https://www.alignmentforum.org/posts/5bd75cc58225bf06703754b9/autopoietic-systems-and-difficulty-of-agi-alignment?commentId=5bd75cc58225bf06703754c1), quoted in [Persuasion Tools: AI takeover without AGI or agency?](https://www.alignmentforum.org/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency)\n\nAs of 2022, this is not within the reach of current models. However, on the current trajectory, AI might be able to write articles and produce other media for propagandistic purposes that are superior to human-made ones in not too many years. These could be precisely tailored to individuals, using things like social media feeds and personal digital data.\n\nAdditionally, recommender systems on content platforms like YouTube, Twitter, and Facebook use machine learning, and the content they recommend can influence the opinions of billions of people. Some [research](https://policyreview.info/articles/analysis/recommender-systems-and-amplification-extremist-content) has looked at the tendency for platforms to promote extremist political views and to thereby help radicalize their userbase for example.\n\nIn the long term, misaligned AI might use its persuasion abilities to gain influence and take control over the future. This could look like convincing its operators to let it out of a box, to give it resources or creating political chaos in order to disable mechanisms to prevent takeover as in [this story](https://www.gwern.net/fiction/Clippy).\n\nSee [Risks from AI persuasion](https://www.alignmentforum.org/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion) for a deep dive into the distinct risks from AI persuasion.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Language models can be utilized to produce propaganda by [acting like bots](https://www.technologyreview.com/2020/10/08/1009845/a-gpt-3-bot-posted-comments-on-reddit-for-a-week-and-no-one-noticed/) and interacting with users on social media. This can be done to push a [political agenda](https://www.nature.com/articles/d41586-020-03034-5) or to make fringe views appear more popular than they are.\n\nI'm envisioning that in the future there will also be systems where you can input any conclusion that you want to argue (including moral conclusions) and the target audience, and the system will give you the most convincing arguments for it. At that point people won't be able to participate in any online (or offline for that matter) discussions without risking their object-level values being hijacked.\n\n-- [Wei Dei](https://www.alignmentforum.org/posts/5bd75cc58225bf06703754b9/autopoietic-systems-and-difficulty-of-agi-alignment?commentId=5bd75cc58225bf06703754c1), quoted in [Persuasion Tools: AI takeover without AGI or agency?](https://www.alignmentforum.org/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency)\n\nAs of 2022, this is not within the reach of current models. However, on the current trajectory, AI might be able to write articles and produce other media for propagandistic purposes that are superior to human-made ones in not too many years. These could be precisely tailored to individuals, using things like social media feeds and personal digital data.\n\nAdditionally, recommender systems on content platforms like YouTube, Twitter, and Facebook use machine learning, and the content they recommend can influence the opinions of billions of people. Some [research](https://policyreview.info/articles/analysis/recommender-systems-and-amplification-extremist-content) has looked at the tendency for platforms to promote extremist political views and to thereby help radicalize their userbase for example.\n\nIn the long term, misaligned AI might use its persuasion abilities to gain influence and take control over the future. This could look like convincing its operators to let it out of a box, to give it resources or creating political chaos in order to disable mechanisms to prevent takeover as in [this story](https://www.gwern.net/fiction/Clippy).\n\nSee [Risks from AI persuasion](https://www.alignmentforum.org/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion) for a deep dive into the distinct risks from AI persuasion.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7602", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:50.674+01:00", "Request Count": "", "Number of suggestions on answer doc": 16, "Total character count of suggestions on answer doc": 9622, "Helpful": ""}}, {"id": "i-2a905232c5b2fc44861077b0be0f2e79467725b6ad689e93547fc71bc425bc5b", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2a905232c5b2fc44861077b0be0f2e79467725b6ad689e93547fc71bc425bc5b", "name": "Is it possible to code into an AI to avoid all the ways a given task could go wrong, and would it be dangerous to try that?", "index": 73, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:11:45.279Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2a905232c5b2fc44861077b0be0f2e79467725b6ad689e93547fc71bc425bc5b", "values": {"File": "Is it possible to code into an AI to avoid all the ways a given task could go wrong, and would it be dangerous to try that?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is it possible to code into an AI to avoid all the ways a given task could go wrong, and would it be dangerous to try that?", "Link": "https://docs.google.com/document/d/19JBq0gSPOTZQRVG-_cx_yBvh7DSRZv5YRI-07VMLkTc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:59.719+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:09:39.940+01:00", "Status": "Live on site", "Edit Answer": "Is it possible to code into an AI to avoid all the ways a given task could go wrong, and would it be dangerous to try that?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5853", "Source Link": "", "aisafety.info Link": "Is it possible to code into an AI to avoid all the ways a given task could go wrong, and would it be dangerous to try that?", "Source": "Wiki", "All Phrasings": "Is it possible to code into an AI to avoid all the ways a given task could go wrong, and would it be dangerous to try that?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Sort answer: No, and could be dangerous to try.\n\nSlightly longer answer: With any realistic real-world task assigned to an AGI, there are so many ways in which it could go wrong that trying to block them all off by hand is a hopeless task, especially when something smarter than you is trying to find creative new things to do. You run into the [nearest unblocked strategy](https://arbital.greaterwrong.com/p/nearest_unblocked/) problem.\n\nIt may be dangerous to try this because if you try and hard-code a large number of things to avoid it increases the chance that there\u2019s a bug in your code which causes major problems, simply by increasing the size of your codebase.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Sort answer: No, and could be dangerous to try.\n\nSlightly longer answer: With any realistic real-world task assigned to an AGI, there are so many ways in which it could go wrong that trying to block them all off by hand is a hopeless task, especially when something smarter than you is trying to find creative new things to do. You run into the [nearest unblocked strategy](https://arbital.greaterwrong.com/p/nearest_unblocked/) problem.\n\nIt may be dangerous to try this because if you try and hard-code a large number of things to avoid it increases the chance that there\u2019s a bug in your code which causes major problems, simply by increasing the size of your codebase.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Sophiaaa", "External Source": "", "Last Asked On Discord": "", "UI ID": "5853", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:53.256+01:00", "Request Count": "", "Number of suggestions on answer doc": 16, "Total character count of suggestions on answer doc": 9622, "Helpful": ""}}, {"id": "i-3d41602c02c6d41959c21565da6a04b2bfe05dac9e57a7228da802673d44e7aa", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3d41602c02c6d41959c21565da6a04b2bfe05dac9e57a7228da802673d44e7aa", "name": "Is it possible to block an AI from doing certain things on the Internet?", "index": 74, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:11:49.214Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3d41602c02c6d41959c21565da6a04b2bfe05dac9e57a7228da802673d44e7aa", "values": {"File": "Is it possible to block an AI from doing certain things on the Internet?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is it possible to block an AI from doing certain things on the Internet?", "Link": "https://docs.google.com/document/d/1wO47y1bvSNfURX9rcHaSzlszG0yxUdHEZyKW7zf-DYY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:56.349+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Boxing", "Doc Last Edited": "2023-02-22T23:09:41.076+01:00", "Status": "Live on site", "Edit Answer": "Is it possible to block an AI from doing certain things on the Internet?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5844", "Source Link": "", "aisafety.info Link": "Is it possible to block an AI from doing certain things on the Internet?", "Source": "Wiki", "All Phrasings": "Is it possible to block an AI from doing certain things on the Internet?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Once an AGI has access to the internet it would be very challenging to meaningfully restrict it from doing things online which it wants to. There are too many options to bypass blocks we may put in place.\n\nIt may be possible to design it so that it does not want to do dangerous things in the first place, or perhaps to set up tripwires so that we notice that it\u2019s trying to do a dangerous thing, though that relies on it not noticing or bypassing the tripwire so should not be the only layer of security.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Once an AGI has access to the internet it would be very challenging to meaningfully restrict it from doing things online which it wants to. There are too many options to bypass blocks we may put in place.\n\nIt may be possible to design it so that it does not want to do dangerous things in the first place, or perhaps to set up tripwires so that we notice that it\u2019s trying to do a dangerous thing, though that relies on it not noticing or bypassing the tripwire so should not be the only layer of security.\n\n", "Stamp Count": 2, "Multi Answer": "", "Stamped By": "Metanomial\nplex", "Priority": 5, "Asker": "Sophiaaa", "External Source": "", "Last Asked On Discord": "", "UI ID": "5844", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:55.739+01:00", "Request Count": "", "Number of suggestions on answer doc": 16, "Total character count of suggestions on answer doc": 9622, "Helpful": ""}}, {"id": "i-d805fa1eded91459967d415dac1c4c74d5d3816fabc0ee63687c248f2879dd68", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d805fa1eded91459967d415dac1c4c74d5d3816fabc0ee63687c248f2879dd68", "name": "Is it likely that hardware will allow an exponential takeoff?", "index": 75, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:11:52.125Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d805fa1eded91459967d415dac1c4c74d5d3816fabc0ee63687c248f2879dd68", "values": {"File": "Is it likely that hardware will allow an exponential takeoff?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is it likely that hardware will allow an exponential takeoff?", "Link": "https://docs.google.com/document/d/1KMjfd2x7toyXGAcyZCwrePUk6nTkA5E8Bu5Q9P_aFKY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:53.055+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Intelligence Explosion,AI Takeoff,Computing Overhang", "Doc Last Edited": "2023-02-22T22:56:12.911+01:00", "Status": "In progress", "Edit Answer": "Is it likely that hardware will allow an exponential takeoff?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6482", "Source Link": "", "aisafety.info Link": "Is it likely that hardware will allow an exponential takeoff?", "Source": "Wiki", "All Phrasings": "Is it likely that hardware will allow an exponential takeoff?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- AI Impact has tried to answer this already.\n\n- [https://aiimpacts.org/hardware-overhang/](https://aiimpacts.org/hardware-overhang/)\n\n- [https://www.lesswrong.com/posts/75dnjiD8kv2khe9eQ/measuring-hardware-overhang](https://www.lesswrong.com/posts/75dnjiD8kv2khe9eQ/measuring-hardware-overhang)\n\n- [https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang](https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang)\n\n- \n\n- \n\n- \n\n- Training requires way more compute than deployment. Thus when we have enough compute to train an AI which is human level at deployment time, we'll have enough compute to run that AI many many times very quickly, so we'll have a faster-than-human AI. Worse if the algorithm is parallelizable and the AI takes control of several machine farms.\n\n", "Tag Count": 3, "Related Answer Count": 0, "Rich Text": "- AI Impact has tried to answer this already.\n\n- [https://aiimpacts.org/hardware-overhang/](https://aiimpacts.org/hardware-overhang/)\n\n- [https://www.lesswrong.com/posts/75dnjiD8kv2khe9eQ/measuring-hardware-overhang](https://www.lesswrong.com/posts/75dnjiD8kv2khe9eQ/measuring-hardware-overhang)\n\n- [https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang](https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang)\n\n- \n\n- \n\n- \n\n- Training requires way more compute than deployment. Thus when we have enough compute to train an AI which is human level at deployment time, we'll have enough compute to run that AI many many times very quickly, so we'll have a faster-than-human AI. Worse if the algorithm is parallelizable and the AI takes control of several machine farms.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Severin", "External Source": "", "Last Asked On Discord": "", "UI ID": "6482", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:54:58.151+01:00", "Request Count": "", "Number of suggestions on answer doc": 16, "Total character count of suggestions on answer doc": 9622, "Helpful": ""}}, {"id": "i-2268f426b7d22bff6e6fa9d1f8136921ac69866a5b35ccfd54d868627f673898", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2268f426b7d22bff6e6fa9d1f8136921ac69866a5b35ccfd54d868627f673898", "name": "Is it already too late to work on AI alignment?", "index": 76, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:11:55.060Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2268f426b7d22bff6e6fa9d1f8136921ac69866a5b35ccfd54d868627f673898", "values": {"File": "Is it already too late to work on AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is it already too late to work on AI alignment?", "Link": "https://docs.google.com/document/d/1zqnXe_90BoDY1yo9QOUEsQJ0nZnu6ULuROeJxubbeAk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:49.514+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Timelines,Doom", "Doc Last Edited": "2023-03-06T17:18:59.688+01:00", "Status": "In progress", "Edit Answer": "Is it already too late to work on AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5643", "Source Link": "", "aisafety.info Link": "Is it already too late to work on AI alignment?", "Source": "Wiki", "All Phrasings": "Is it already too late to work on AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "We don't have AI systems that are generally more capable than humans. So there is still time left to figure out how to build systems that are smarter than humans in a safe way.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "We don't have AI systems that are generally more capable than humans. So there is still time left to figure out how to build systems that are smarter than humans in a safe way.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Morpheus", "External Source": "", "Last Asked On Discord": "", "UI ID": "5643", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:00.823+01:00", "Request Count": "", "Number of suggestions on answer doc": 17, "Total character count of suggestions on answer doc": 9887, "Helpful": ""}}, {"id": "i-c9b7d6f0323084a99270ab9dda79f4e2c76e39839fff2ac12059771189ba98ef", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c9b7d6f0323084a99270ab9dda79f4e2c76e39839fff2ac12059771189ba98ef", "name": "Is expecting large returns from AI self-improvement just following an exponential trend line off a cliff?", "index": 77, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:11:58.782Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c9b7d6f0323084a99270ab9dda79f4e2c76e39839fff2ac12059771189ba98ef", "values": {"File": "Is expecting large returns from AI self-improvement just following an exponential trend line off a cliff?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is expecting large returns from AI self-improvement just following an exponential trend line off a cliff?", "Link": "https://docs.google.com/document/d/1jSbjUpSZLE6RLhn1BZP_Ugeuyd4VfV16_In-GJmgYxE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:46.001+01:00", "Related Answers DO NOT EDIT": "Why does AI takeoff speed matter?", "Tags": "Recursive Self-improvement", "Doc Last Edited": "2023-02-22T23:09:42.862+01:00", "Status": "Live on site", "Edit Answer": "Is expecting large returns from AI self-improvement just following an exponential trend line off a cliff?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6964", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "Is expecting large returns from AI self-improvement just following an exponential trend line off a cliff?", "Source": "Superintelligence FAQ", "All Phrasings": "Is expecting large returns from AI self-improvement just following an exponential trend line off a cliff?\n", "Initial Order": "", "Related IDs": "6966", "Rich Text DO NOT EDIT": "Blindly following the trendlines while forecasting technological progress is certainly a risk (affectionately known in AI circles as \u201cpulling a Kurzweill\u201d), but sometimes taking an exponential trend seriously is the right response.\n\nConsider economic doubling times. In 1 AD, the world GDP was about $20 billion; it took a thousand years, until 1000 AD, for that to double to $40 billion. But it only took five hundred more years, until 1500, or so, for the economy to double again. And then it only took another three hundred years or so, until 1800, for the economy to double a third time. Someone in 1800 might calculate the trend line and say this was ridiculous, that it implied the economy would be doubling every ten years or so in the beginning of the 21st century. But in fact, this is how long the economy takes to double these days. To a medieval, used to a thousand-year doubling time (which was based mostly on population growth!), an economy that doubled every ten years might seem inconceivable. To us, it seems normal.\n\nLikewise, in 1965 Gordon Moore noted that semiconductor complexity seemed to double every eighteen months. During his own day, there were about five hundred transistors on a chip; he predicted that would soon double to a thousand, and a few years later to two thousand. Almost as soon as Moore\u2019s Law become well-known, people started saying it was absurd to follow it off a cliff \u2013 such a law would imply a million transistors per chip in 1990, a hundred million in 2000, ten billion transistors on every chip by 2015! More transistors on a single chip than existed on all the computers in the world! Transistors the size of molecules! But of course all of these things happened; the ridiculous exponential trend proved more accurate than the naysayers.\n\nNone of this is to say that exponential trends are always right, just that they are sometimes right even when it seems they can\u2019t possibly be. We can\u2019t be sure that a computer using its own intelligence to discover new ways to increase its intelligence will enter a positive feedback loop and achieve superintelligence in seemingly impossibly short time scales. It\u2019s just one more possibility, a worry to place alongside all the other worrying reasons to expect a moderate or hard takeoff.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "Blindly following the trendlines while forecasting technological progress is certainly a risk (affectionately known in AI circles as \u201cpulling a Kurzweill\u201d), but sometimes taking an exponential trend seriously is the right response.\n\nConsider economic doubling times. In 1 AD, the world GDP was about $20 billion; it took a thousand years, until 1000 AD, for that to double to $40 billion. But it only took five hundred more years, until 1500, or so, for the economy to double again. And then it only took another three hundred years or so, until 1800, for the economy to double a third time. Someone in 1800 might calculate the trend line and say this was ridiculous, that it implied the economy would be doubling every ten years or so in the beginning of the 21st century. But in fact, this is how long the economy takes to double these days. To a medieval, used to a thousand-year doubling time (which was based mostly on population growth!), an economy that doubled every ten years might seem inconceivable. To us, it seems normal.\n\nLikewise, in 1965 Gordon Moore noted that semiconductor complexity seemed to double every eighteen months. During his own day, there were about five hundred transistors on a chip; he predicted that would soon double to a thousand, and a few years later to two thousand. Almost as soon as Moore\u2019s Law become well-known, people started saying it was absurd to follow it off a cliff \u2013 such a law would imply a million transistors per chip in 1990, a hundred million in 2000, ten billion transistors on every chip by 2015! More transistors on a single chip than existed on all the computers in the world! Transistors the size of molecules! But of course all of these things happened; the ridiculous exponential trend proved more accurate than the naysayers.\n\nNone of this is to say that exponential trends are always right, just that they are sometimes right even when it seems they can\u2019t possibly be. We can\u2019t be sure that a computer using its own intelligence to discover new ways to increase its intelligence will enter a positive feedback loop and achieve superintelligence in seemingly impossibly short time scales. It\u2019s just one more possibility, a worry to place alongside all the other worrying reasons to expect a moderate or hard takeoff.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "", "UI ID": "6964", "Related Answers": "Why does AI takeoff speed matter?", "Doc Last Ingested": "2023-03-14T23:55:03.567+01:00", "Request Count": "", "Number of suggestions on answer doc": 17, "Total character count of suggestions on answer doc": 9887, "Helpful": ""}}, {"id": "i-ebcee322bb5b8f7228e0405907b7db3c06165e02edc5c0221761c0929a45cbf0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ebcee322bb5b8f7228e0405907b7db3c06165e02edc5c0221761c0929a45cbf0", "name": "Is AI safety research racing against capability research? If so, how can safety research get ahead?", "index": 78, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:15:07.808Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ebcee322bb5b8f7228e0405907b7db3c06165e02edc5c0221761c0929a45cbf0", "values": {"File": "Is AI safety research racing against capability research? If so, how can safety research get ahead?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is AI safety research racing against capability research? If so, how can safety research get ahead?", "Link": "https://docs.google.com/document/d/1CB33c30BF8z8WebZwXwEu0beEYQDohLhBYfpLMpOJzg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:42.384+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T22:56:14.818+01:00", "Status": "Not started", "Edit Answer": "Is AI safety research racing against capability research? If so, how can safety research get ahead?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8209", "Source Link": "", "aisafety.info Link": "Is AI safety research racing against capability research? If so, how can safety research get ahead?", "Source": "Wiki", "All Phrasings": "Is AI safety research racing against capability research? If so, how can safety research get ahead?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Reincarnated", "External Source": "", "Last Asked On Discord": "", "UI ID": "8209", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:59:17.794+01:00", "Request Count": "", "Number of suggestions on answer doc": 2, "Total character count of suggestions on answer doc": 1019, "Helpful": ""}}, {"id": "i-3d21fcee3c1290669dce93ea21691afb26b8db7f366da84834336ce79b7f2d21", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-3d21fcee3c1290669dce93ea21691afb26b8db7f366da84834336ce79b7f2d21", "name": "Is AI alignment possible?", "index": 79, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:04.386Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-3d21fcee3c1290669dce93ea21691afb26b8db7f366da84834336ce79b7f2d21", "values": {"File": "Is AI alignment possible?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "Is AI alignment possible?", "Link": "https://docs.google.com/document/d/1iYI_UjBt_bXKdK7GU6nQuIp2Wxk6kldzs1SNlMYKKw0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:38.343+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Superintelligence", "Doc Last Edited": "2023-02-22T23:09:44.149+01:00", "Status": "Live on site", "Edit Answer": "Is AI alignment possible?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6274", "Source Link": "", "aisafety.info Link": "Is AI alignment possible?", "Source": "Wiki", "All Phrasings": "Is AI alignment possible?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Yes, if the superintelligence has goals which include humanity surviving then we would not be destroyed. If those goals are [fully aligned](https://www.lesswrong.com/tag/value-learning) with human well-being, we would in fact find ourselves in a dramatically better place.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Yes, if the superintelligence has goals which include humanity surviving then we would not be destroyed. If those goals are [fully aligned](https://www.lesswrong.com/tag/value-learning) with human well-being, we would in fact find ourselves in a dramatically better place.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "Aprillion", "Priority": 4, "Asker": "Sophiaaa", "External Source": "", "Last Asked On Discord": "2023-02-26T18:55:44.133+01:00", "UI ID": "6274", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:18.706+01:00", "Request Count": "", "Number of suggestions on answer doc": 17, "Total character count of suggestions on answer doc": 9887, "Helpful": ""}}, {"id": "i-75515dc5cd0a4700ee031aa982ea5018183b5fb3660638021909f1c086363386", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-75515dc5cd0a4700ee031aa982ea5018183b5fb3660638021909f1c086363386", "name": "In what ways are real-world machine learning systems different from expected utility maximizers?", "index": 80, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:06.609Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-75515dc5cd0a4700ee031aa982ea5018183b5fb3660638021909f1c086363386", "values": {"File": "In what ways are real-world machine learning systems different from expected utility maximizers?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "In what ways are real-world machine learning systems different from expected utility maximizers?", "Link": "https://docs.google.com/document/d/1GF1AqxQaczomgrq2anNOIVBdSkBl71NESYUoLG7FxkQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:35.213+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Narrow AI,Machine Learning,Maximizers", "Doc Last Edited": "2023-02-22T22:56:15.857+01:00", "Status": "Not started", "Edit Answer": "In what ways are real-world machine learning systems different from expected utility maximizers?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6477", "Source Link": "", "aisafety.info Link": "In what ways are real-world machine learning systems different from expected utility maximizers?", "Source": "Wiki", "All Phrasings": "In what ways are real-world machine learning systems different from expected utility maximizers?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 3, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "6477", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:20.690+01:00", "Request Count": "", "Number of suggestions on answer doc": 17, "Total character count of suggestions on answer doc": 9887, "Helpful": ""}}, {"id": "i-9a7f162f7b3720a33c1058b38bde3c143380fb114ba313bf2c3ffc977ff3d2dc", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-9a7f162f7b3720a33c1058b38bde3c143380fb114ba313bf2c3ffc977ff3d2dc", "name": "In \"aligning AI with human values\", which humans' values are we talking about?", "index": 81, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:09.065Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-9a7f162f7b3720a33c1058b38bde3c143380fb114ba313bf2c3ffc977ff3d2dc", "values": {"File": "In \"aligning AI with human values\", which humans' values are we talking about?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "In \"aligning AI with human values\", which humans' values are we talking about?", "Link": "https://docs.google.com/document/d/1aCMnStMPUfQAYGWHtcWWMMXCQV-GdBiQ6EzmOtIc8do/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:32.032+01:00", "Related Answers DO NOT EDIT": "What are \"human values\"?,What is \"coherent extrapolated volition\"?", "Tags": "Human Values", "Doc Last Edited": "2023-03-06T18:34:29.481+01:00", "Status": "In progress", "Edit Answer": "In \"aligning AI with human values\", which humans' values are we talking about?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5646", "Source Link": "", "aisafety.info Link": "In \"aligning AI with human values\", which humans' values are we talking about?", "Source": "Wiki", "All Phrasings": "In \"aligning AI with human values\", which humans' values are we talking about?\n", "Initial Order": "", "Related IDs": "7594,6939", "Rich Text DO NOT EDIT": "Rough sketch by Rob, open for anyone to polish:\n\n- Right now, we can't align AI at all. It is important to decide what to align it to, but if we don\u2019t have a solution to alignment it won\u2019t matter what we\u2019d like it to be aligned to. Gun which explodes and kills everyone however you aim it, think about making it not explode.\n\n- \n\n- \n\n- [CEV](https://intelligence.org/files/CEV.pdf) is an attempt at codifying an ideal case <brief quote>.\n\n    - \n\n        - \n\n        - \n\n        - \n\n- \n\n- \n\n- Seems philosophically easier to align with a single human, as you don\u2019t need to aggregate values. But, anything that can robustly extract the values of a single human can likely extend to groups of humans, as humans [are not ideal agents](https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip) in a similar way to groups not being unitary.\n\n    - \n\n- \n\n- \n\n- There's a hierarchy of how good the outcome is vs difficulty. Aligning with a single person avoids the destruction of the universe if that person doesn't want it, but isn't as democratic, and is susceptible to issues with that person being wrong about something (in the sense that the majority of humanity has different preferences). Aligning with a group of humans seems better, but it's harder because who is included? Only including the people who built the AI has fairness issues, but including all humans has well-definedness issues. Do you weigh everyone equally? Will Goodhart's law cause people to care more than they should?\n\n- \n\n- Weighting everyone equally has problems (what you\u2019re extracting is just a preference ordering, but there are differences in intensity of preferences), not doing so does too (people are incentivised to be utility monsters, and the technology like brain modifications and emulations might allow weird edge cases).\n\n- \n\n- Who is counted as human? Today \"human\" is a well-separated category, but in the future there may be more edge cases, many humans might extend themselves with technologies, or duplicate themselves, etc. Especially given Goodhart's law.\n\n- \n\n- The question often leads to a diversion, a political discussion about who gets to benefit rather than a technical question about how do you instill values into an AI.\n\n- \n\n- \n\n- \n\n- \n\n- \n\n- \n\n", "Tag Count": 1, "Related Answer Count": 2, "Rich Text": "Rough sketch by Rob, open for anyone to polish:\n\n- Right now, we can't align AI at all. It is important to decide what to align it to, but if we don\u2019t have a solution to alignment it won\u2019t matter what we\u2019d like it to be aligned to. Gun which explodes and kills everyone however you aim it, think about making it not explode.\n\n- \n\n- \n\n- [CEV](https://intelligence.org/files/CEV.pdf) is an attempt at codifying an ideal case <brief quote>.\n\n    - \n\n        - \n\n        - \n\n        - \n\n- \n\n- \n\n- Seems philosophically easier to align with a single human, as you don\u2019t need to aggregate values. But, anything that can robustly extract the values of a single human can likely extend to groups of humans, as humans [are not ideal agents](https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip) in a similar way to groups not being unitary.\n\n    - \n\n- \n\n- \n\n- There's a hierarchy of how good the outcome is vs difficulty. Aligning with a single person avoids the destruction of the universe if that person doesn't want it, but isn't as democratic, and is susceptible to issues with that person being wrong about something (in the sense that the majority of humanity has different preferences). Aligning with a group of humans seems better, but it's harder because who is included? Only including the people who built the AI has fairness issues, but including all humans has well-definedness issues. Do you weigh everyone equally? Will Goodhart's law cause people to care more than they should?\n\n- \n\n- Weighting everyone equally has problems (what you\u2019re extracting is just a preference ordering, but there are differences in intensity of preferences), not doing so does too (people are incentivised to be utility monsters, and the technology like brain modifications and emulations might allow weird edge cases).\n\n- \n\n- Who is counted as human? Today \"human\" is a well-separated category, but in the future there may be more edge cases, many humans might extend themselves with technologies, or duplicate themselves, etc. Especially given Goodhart's law.\n\n- \n\n- The question often leads to a diversion, a political discussion about who gets to benefit rather than a technical question about how do you instill values into an AI.\n\n- \n\n- \n\n- \n\n- \n\n- \n\n- \n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "robertskmiles", "External Source": "", "Last Asked On Discord": "", "UI ID": "5646", "Related Answers": "What are \"human values\"?,What is \"coherent extrapolated volition\"?", "Doc Last Ingested": "2023-03-14T23:55:24.746+01:00", "Request Count": "", "Number of suggestions on answer doc": 22, "Total character count of suggestions on answer doc": 12757, "Helpful": ""}}, {"id": "i-074ade46278874a593c98dc1f3b818d301dd1a25a15c072f675b76a53d446b02", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-074ade46278874a593c98dc1f3b818d301dd1a25a15c072f675b76a53d446b02", "name": "If we solve alignment, are we sure of a good future?", "index": 82, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:11.507Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-074ade46278874a593c98dc1f3b818d301dd1a25a15c072f675b76a53d446b02", "values": {"File": "If we solve alignment, are we sure of a good future?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "If we solve alignment, are we sure of a good future?", "Link": "https://docs.google.com/document/d/1t6wqk5PQCpKEe10iJoFP3j44hgRz-Y2RkMnSYon0My4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:28.293+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stable Win Condition,Success Models", "Doc Last Edited": "2023-02-22T23:09:45.751+01:00", "Status": "Live on site", "Edit Answer": "If we solve alignment, are we sure of a good future?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7187", "Source Link": "", "aisafety.info Link": "If we solve alignment, are we sure of a good future?", "Source": "Wiki", "All Phrasings": "If we solve alignment, are we sure of a good future?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "If by \u201csolve alignment\u201d you mean build a sufficiently performance-competitive superintelligence which has the goal of [Coherent Extrapolated Volition](https://www.lesswrong.com/tag/coherent-extrapolated-volition) or something else which captures human values, then yes. It would be able to deploy technology near the limits of physics (e.g. [atomically precise manufacturing](https://en.wikipedia.org/wiki/Atomically_precise_manufacturing)) to solve most of the other problems which face us, and steer the future towards a highly positive path for [perhaps many billions of years](https://en.wikipedia.org/wiki/Timeline_of_the_far_future) until the [heat death of the universe](https://en.wikipedia.org/wiki/Heat_death_of_the_universe) (barring more esoteric x-risks like encounters with advanced hostile civilizations, [false vacuum decay](https://en.wikipedia.org/wiki/False_vacuum_decay), or [simulation shutdown](https://arxiv.org/ftp/arxiv/papers/1905/1905.05792.pdf)).\n\nHowever, if you only have alignment of a superintelligence to a single human you still have the risk of misuse, so this should be at most a short-term solution. For example, what if Google creates a superintelligent AI, and it listens to the CEO of Google, and it\u2019s programmed to do everything exactly the way the CEO of Google would want? Even assuming that the CEO of Google has no hidden unconscious desires affecting the AI in unpredictable ways, this gives one person a lot of power.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "If by \u201csolve alignment\u201d you mean build a sufficiently performance-competitive superintelligence which has the goal of [Coherent Extrapolated Volition](https://www.lesswrong.com/tag/coherent-extrapolated-volition) or something else which captures human values, then yes. It would be able to deploy technology near the limits of physics (e.g. [atomically precise manufacturing](https://en.wikipedia.org/wiki/Atomically_precise_manufacturing)) to solve most of the other problems which face us, and steer the future towards a highly positive path for [perhaps many billions of years](https://en.wikipedia.org/wiki/Timeline_of_the_far_future) until the [heat death of the universe](https://en.wikipedia.org/wiki/Heat_death_of_the_universe) (barring more esoteric x-risks like encounters with advanced hostile civilizations, [false vacuum decay](https://en.wikipedia.org/wiki/False_vacuum_decay), or [simulation shutdown](https://arxiv.org/ftp/arxiv/papers/1905/1905.05792.pdf)).\n\nHowever, if you only have alignment of a superintelligence to a single human you still have the risk of misuse, so this should be at most a short-term solution. For example, what if Google creates a superintelligent AI, and it listens to the CEO of Google, and it\u2019s programmed to do everything exactly the way the CEO of Google would want? Even assuming that the CEO of Google has no hidden unconscious desires affecting the AI in unpredictable ways, this gives one person a lot of power.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "7187", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:26.795+01:00", "Request Count": "", "Number of suggestions on answer doc": 22, "Total character count of suggestions on answer doc": 12757, "Helpful": ""}}, {"id": "i-536386a04020dd375e31d3e80a334d423a81d37703d74cb01ee74801c68a7f4d", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-536386a04020dd375e31d3e80a334d423a81d37703d74cb01ee74801c68a7f4d", "name": "If an AI became conscious, how would we ever know?", "index": 83, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:14.572Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-536386a04020dd375e31d3e80a334d423a81d37703d74cb01ee74801c68a7f4d", "values": {"File": "If an AI became conscious, how would we ever know?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "If an AI became conscious, how would we ever know?", "Link": "https://docs.google.com/document/d/11hv-u2fecSToB4_i-Z7rVz8OR995d-D4p0gTzAA7cAg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:24.812+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Consciousness", "Doc Last Edited": "2023-02-24T16:20:21.568+01:00", "Status": "In progress", "Edit Answer": "If an AI became conscious, how would we ever know?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5652", "Source Link": "", "aisafety.info Link": "If an AI became conscious, how would we ever know?", "Source": "Wiki", "All Phrasings": "If an AI became conscious, how would we ever know?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "It depends on the exact definition of consciousness and on the legal consequences of the AI telling us that stuff from which we could imply how conscious it might be (would it be motivated to pretend to be \"conscious\" by those criteria to get some benefits,, or would it be motivated to keep its consciousness in secret to avoid being turned off).\n\nOnce we have a measurable definition, then we can empirically measure the AI against that definition.\n\nSee [integrated information theory](https://en.wikipedia.org/wiki/Integrated_information_theory) for practical approaches, though there is always the [hard problem of consciousness](https://en.wikipedia.org/wiki/Hard_problem_of_consciousness) that will muddy any candidate definitions for near future.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "It depends on the exact definition of consciousness and on the legal consequences of the AI telling us that stuff from which we could imply how conscious it might be (would it be motivated to pretend to be \"conscious\" by those criteria to get some benefits,, or would it be motivated to keep its consciousness in secret to avoid being turned off).\n\nOnce we have a measurable definition, then we can empirically measure the AI against that definition.\n\nSee [integrated information theory](https://en.wikipedia.org/wiki/Integrated_information_theory) for practical approaches, though there is always the [hard problem of consciousness](https://en.wikipedia.org/wiki/Hard_problem_of_consciousness) that will muddy any candidate definitions for near future.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Chris Cooper", "External Source": "", "Last Asked On Discord": "", "UI ID": "5652", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:28.454+01:00", "Request Count": "", "Number of suggestions on answer doc": 23, "Total character count of suggestions on answer doc": 12769, "Helpful": ""}}, {"id": "i-4640559e6704f56917b65205ce45a3764124304c6fa9c8599269543f74dc11bf", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4640559e6704f56917b65205ce45a3764124304c6fa9c8599269543f74dc11bf", "name": "If I only care about helping people alive today, does AI safety still matter?", "index": 84, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:17.360Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4640559e6704f56917b65205ce45a3764124304c6fa9c8599269543f74dc11bf", "values": {"File": "If I only care about helping people alive today, does AI safety still matter?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "If I only care about helping people alive today, does AI safety still matter?", "Link": "https://docs.google.com/document/d/1aw1RJgnD04Rktn_tP0faVW2qcxwlwLDCHz7yHRygbLg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:20.928+01:00", "Related Answers DO NOT EDIT": "When will transformative AI be created?", "Tags": "Person-affecting View", "Doc Last Edited": "2023-02-22T23:09:47.119+01:00", "Status": "Live on site", "Edit Answer": "If I only care about helping people alive today, does AI safety still matter?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7636", "Source Link": "", "aisafety.info Link": "If I only care about helping people alive today, does AI safety still matter?", "Source": "Wiki", "All Phrasings": "If I only care about helping people alive today, does AI safety still matter?\n", "Initial Order": "", "Related IDs": "2398", "Rich Text DO NOT EDIT": "This largely depends on when you think AI will be advanced enough to constitute an immediate threat to humanity. This is difficult to estimate, but the field is surveyed at [How long will it be until transformative AI is created?](http://How_long_will_it_be_until_transformative_AI_is_created?), which comes to the conclusion that it is relatively widely believed that AI will transform the world in our lifetimes.\n\nWe probably shouldn't rely too strongly on these opinions as predicting the future is hard. But, due to the enormous damage a misaligned AGI could do, it's worth putting a great deal of effort towards AI alignment even if you just care about currently existing humans (such as yourself).\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "This largely depends on when you think AI will be advanced enough to constitute an immediate threat to humanity. This is difficult to estimate, but the field is surveyed at [How long will it be until transformative AI is created?](http://How_long_will_it_be_until_transformative_AI_is_created?), which comes to the conclusion that it is relatively widely believed that AI will transform the world in our lifetimes.\n\nWe probably shouldn't rely too strongly on these opinions as predicting the future is hard. But, due to the enormous damage a misaligned AGI could do, it's worth putting a great deal of effort towards AI alignment even if you just care about currently existing humans (such as yourself).\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7636", "Related Answers": "When will transformative AI be created?", "Doc Last Ingested": "2023-03-14T23:55:30.957+01:00", "Request Count": "", "Number of suggestions on answer doc": 23, "Total character count of suggestions on answer doc": 12769, "Helpful": ""}}, {"id": "i-d7fb0e285d49539be2424023ae62a16877c9b750911745ff0d4e9c0fda60496f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d7fb0e285d49539be2424023ae62a16877c9b750911745ff0d4e9c0fda60496f", "name": "If AI takes over the world how could it create and maintain the infrastructure that humans currently provide?", "index": 85, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:19.817Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d7fb0e285d49539be2424023ae62a16877c9b750911745ff0d4e9c0fda60496f", "values": {"File": "If AI takes over the world how could it create and maintain the infrastructure that humans currently provide?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "If AI takes over the world how could it create and maintain the infrastructure that humans currently provide?", "Link": "https://docs.google.com/document/d/14p-Qnlo55WeKPAkFG6aH5D8xZxposA25zv_jIGfcBag/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:17.252+01:00", "Related Answers DO NOT EDIT": "How might a superintelligence socially manipulate humans?", "Tags": "AI Takeover", "Doc Last Edited": "2023-02-22T23:09:48.273+01:00", "Status": "Live on site", "Edit Answer": "If AI takes over the world how could it create and maintain the infrastructure that humans currently provide?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5618", "Source Link": "", "aisafety.info Link": "If AI takes over the world how could it create and maintain the infrastructure that humans currently provide?", "Source": "Wiki", "All Phrasings": "If AI takes over the world how could it create and maintain the infrastructure that humans currently provide?\n", "Initial Order": "", "Related IDs": "6974", "Rich Text DO NOT EDIT": "An unaligned AI would not eliminate humans until it had replacements for the manual labor they provide to maintain civilization (e.g. a more advanced version of [Tesla's Optimus](https://en.wikipedia.org/wiki/Tesla_Bot)). Until that point, it might settle for technologically and socially manipulating humans.\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "An unaligned AI would not eliminate humans until it had replacements for the manual labor they provide to maintain civilization (e.g. a more advanced version of [Tesla's Optimus](https://en.wikipedia.org/wiki/Tesla_Bot)). Until that point, it might settle for technologically and socially manipulating humans.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "5618", "Related Answers": "How might a superintelligence socially manipulate humans?", "Doc Last Ingested": "2023-03-14T23:55:34.013+01:00", "Request Count": "", "Number of suggestions on answer doc": 23, "Total character count of suggestions on answer doc": 12769, "Helpful": ""}}, {"id": "i-80af0b9fa765d63896982277e862d717b0a85faf5e10dccacdf6f69d0bed49ba", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-80af0b9fa765d63896982277e862d717b0a85faf5e10dccacdf6f69d0bed49ba", "name": "If AGI comes from a new paradigm, how likely is it to arise late in the paradigm when it is already deployed at scale, versus early on when only a few people are exploring the idea?", "index": 86, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:22.249Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-80af0b9fa765d63896982277e862d717b0a85faf5e10dccacdf6f69d0bed49ba", "values": {"File": "If AGI comes from a new paradigm, how likely is it to arise late in the paradigm when it is already deployed at scale, versus early on when only a few people are exploring the idea?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "If AGI comes from a new paradigm, how likely is it to arise late in the paradigm when it is already deployed at scale, versus early on when only a few people are exploring the idea?", "Link": "https://docs.google.com/document/d/1RBHXqO79vV0YBYrGxN6XReZlfHpR3AO5PZvevsgCb6E/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:14.018+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI,Paradigm", "Doc Last Edited": "2023-02-22T22:56:18.825+01:00", "Status": "In progress", "Edit Answer": "If AGI comes from a new paradigm, how likely is it to arise late in the paradigm when it is already deployed at scale, versus early on when only a few people are exploring the idea?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7730", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "If AGI comes from a new paradigm, how likely is it to arise late in the paradigm when it is already deployed at scale, versus early on when only a few people are exploring the idea?", "Source": "LessWrong", "All Phrasings": "If AGI comes from a new paradigm, how likely is it to arise late in the paradigm when it is already deployed at scale, versus early on when only a few people are exploring the idea?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- \n\n- Question comes from Rob Bensinger, from the list of question that different people have pretty different opinions to ask\n\n- Ask Rob Bensinger to do a similar answer questions session summarizing the main things people are thinking about these questions?\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "- \n\n- Question comes from Rob Bensinger, from the list of question that different people have pretty different opinions to ask\n\n- Ask Rob Bensinger to do a similar answer questions session summarizing the main things people are thinking about these questions?\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7730", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:41.461+01:00", "Request Count": "", "Number of suggestions on answer doc": 23, "Total character count of suggestions on answer doc": 12769, "Helpful": ""}}, {"id": "i-d91e104a24e5ad53366af7144a9e08307412fe0e67d6cf37e6610fb4137ed788", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d91e104a24e5ad53366af7144a9e08307412fe0e67d6cf37e6610fb4137ed788", "name": "I'm interested in working on AI safety. What should I do?", "index": 87, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:24.526Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d91e104a24e5ad53366af7144a9e08307412fe0e67d6cf37e6610fb4137ed788", "values": {"File": "I'm interested in working on AI safety. What should I do?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "I'm interested in working on AI safety. What should I do?", "Link": "https://docs.google.com/document/d/1BG_pPC4JSVWyUYwdfdbmLmPQ5FVMR30XxUCEwK4z5HE/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:10.147+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Careers,Contributing", "Doc Last Edited": "2023-02-22T23:09:49.737+01:00", "Status": "Live on site", "Edit Answer": "I'm interested in working on AI safety. What should I do?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5636", "Source Link": "", "aisafety.info Link": "I'm interested in working on AI safety. What should I do?", "Source": "Wiki", "All Phrasings": "I'm interested in working on AI safety. What should I do?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "AI Safety Support[offers free calls](https://calendly.com/aiss) to advise people interested in a career in AI Safety, so that's a great place to start. We're working on creating a bunch of detailed information for Stampy to use, but in the meantime check out these resources:\n\n- [EA Cambridge AGI Safety Fundamentals curriculum](https://www.eacambridge.org/agi-safety-fundamentals)\n\n- [80,000 Hours AI safety syllabus](https://80000hours.org/articles/ai-safety-syllabus/)\n\n- [Adam Gleave's Careers in Beneficial AI Research document](https://docs.google.com/document/d/1RFo7_9JVmt0z8RPwUjB-mUMgCMoUQmsaj2CM5aHvxCw/edit)\n\n- [Rohin Shah's FAQ on career advice for AI alignment researchers](https://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/)\n\n- [AI Safety Support](https://www.aisafetysupport.org/) has lots of other good resources, such as their[links page](https://www.aisafetysupport.org/resources/lots-of-links),[slack](https://join.slack.com/t/ai-alignment/shared_invite/zt-fkgwbd2b-kK50z~BbVclOZMM9UP44gw),[newsletter](https://www.aisafetysupport.org/newsletter), and[events calendar](https://www.aisafetysupport.org/events/online-events-calendar).\n\n- [AISafety.training](http://AISafety.training)\n\n- [Safety-aligned research training programs (under construction).](https://docs.google.com/spreadsheets/d/1JyxrfFFrzaQsS3AQ4qJ2aOLGj1aSkBaxkpZCqBX9BOY/edit#gid=0)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "AI Safety Support[offers free calls](https://calendly.com/aiss) to advise people interested in a career in AI Safety, so that's a great place to start. We're working on creating a bunch of detailed information for Stampy to use, but in the meantime check out these resources:\n\n- [EA Cambridge AGI Safety Fundamentals curriculum](https://www.eacambridge.org/agi-safety-fundamentals)\n\n- [80,000 Hours AI safety syllabus](https://80000hours.org/articles/ai-safety-syllabus/)\n\n- [Adam Gleave's Careers in Beneficial AI Research document](https://docs.google.com/document/d/1RFo7_9JVmt0z8RPwUjB-mUMgCMoUQmsaj2CM5aHvxCw/edit)\n\n- [Rohin Shah's FAQ on career advice for AI alignment researchers](https://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/)\n\n- [AI Safety Support](https://www.aisafetysupport.org/) has lots of other good resources, such as their[links page](https://www.aisafetysupport.org/resources/lots-of-links),[slack](https://join.slack.com/t/ai-alignment/shared_invite/zt-fkgwbd2b-kK50z~BbVclOZMM9UP44gw),[newsletter](https://www.aisafetysupport.org/newsletter), and[events calendar](https://www.aisafetysupport.org/events/online-events-calendar).\n\n- [AISafety.training](http://AISafety.training)\n\n- [Safety-aligned research training programs (under construction).](https://docs.google.com/spreadsheets/d/1JyxrfFFrzaQsS3AQ4qJ2aOLGj1aSkBaxkpZCqBX9BOY/edit#gid=0)\n\n", "Stamp Count": 2, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Anonymous", "External Source": "", "Last Asked On Discord": "", "UI ID": "5636", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:43.971+01:00", "Request Count": "", "Number of suggestions on answer doc": 23, "Total character count of suggestions on answer doc": 12769, "Helpful": ""}}, {"id": "i-bdaf23281f99bcdba9df34535a389687fb03c0e894f18fdfc5eb7e3528d02fd3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-bdaf23281f99bcdba9df34535a389687fb03c0e894f18fdfc5eb7e3528d02fd3", "name": "I want to work on AI alignment. How can I get funding?", "index": 88, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:27.595Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-bdaf23281f99bcdba9df34535a389687fb03c0e894f18fdfc5eb7e3528d02fd3", "values": {"File": "I want to work on AI alignment. How can I get funding?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "I want to work on AI alignment. How can I get funding?", "Link": "https://docs.google.com/document/d/1vELyAGZsMPZ_rKSaLtdiTMD1_dzjSEserxuAlxdF0kU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:06.774+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Funding", "Doc Last Edited": "2023-02-22T23:09:50.817+01:00", "Status": "Live on site", "Edit Answer": "I want to work on AI alignment. How can I get funding?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6703", "Source Link": "", "aisafety.info Link": "I want to work on AI alignment. How can I get funding?", "Source": "Wiki", "All Phrasings": "I want to work on AI alignment. How can I get funding?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "See the [Future Funding List](https://www.futurefundinglist.com/) for up to date information!\n\nThe organizations which most regularly give grants to individuals working towards AI alignment are the [Long Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), [Survival And Flourishing (SAF)](http://survivalandflourishing.org/), the [OpenPhil AI Fellowship](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/the-open-phil-ai-fellowship) and [early career funding](https://www.openphilanthropy.org/focus/other-areas/early-career-funding-individuals-interested-improving-long-term-future), the [Future of Life Institute](https://grants.futureoflife.org/), the [Future of Humanity Institute](https://www.fhi.ox.ac.uk/aia-fellowship/), and [the Center on Long-Term Risk Fund](https://longtermrisk.org/grantmaking/). If you're able to relocate to the UK, [CEEALAR (aka the EA Hotel)](https://ceealar.org/) can be a great option as it offers free food and accommodation for up to two years, as well as contact with others who are thinking about these issues. Many people have offered rooms around the world for free or at low cost on the [EA Houses spreadsheet](https://docs.google.com/spreadsheets/d/1wwNn1VEaQuSzDdedCVcmjktxeZj-apHele4Ry8DLJk0/edit#gid=0). There are also opportunities from smaller grantmakers which you might be able to pick up if you get involved.\n\nIf you want to work on support or infrastructure rather than directly on research, the [EA Infrastructure Fund](https://funds.effectivealtruism.org/funds/ea-community) may be able to help. In general, you can [talk to EA funds before applying](https://www.lesswrong.com/posts/5AAFoigbbMqgrTpDh/you-can-talk-to-ea-funds-before-applying).\n\nEach grant source has their own criteria for funding, but in general they are looking for candidates who have evidence that they're keen and able to do good work towards reducing existential risk (for example, by completing an [AI Safety Camp](https://aisafety.camp/) project), though the EA Hotel in particular has less stringent requirements as they're able to support people at very low cost. If you'd like to talk to someone who can offer advice on applying for funding, [AI Safety Support](https://www.aisafetysupport.org/) offers [free calls](https://calendly.com/aiss).\n\nAnother option is to get hired by an organization which works on AI alignment, see the follow-up question for advice on that.\n\nIt's also worth checking the AI Alignment tag on the [EA funding sources website](https://eafunding.softr.app/) for up-to-date suggestions.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "See the [Future Funding List](https://www.futurefundinglist.com/) for up to date information!\n\nThe organizations which most regularly give grants to individuals working towards AI alignment are the [Long Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), [Survival And Flourishing (SAF)](http://survivalandflourishing.org/), the [OpenPhil AI Fellowship](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/the-open-phil-ai-fellowship) and [early career funding](https://www.openphilanthropy.org/focus/other-areas/early-career-funding-individuals-interested-improving-long-term-future), the [Future of Life Institute](https://grants.futureoflife.org/), the [Future of Humanity Institute](https://www.fhi.ox.ac.uk/aia-fellowship/), and [the Center on Long-Term Risk Fund](https://longtermrisk.org/grantmaking/). If you're able to relocate to the UK, [CEEALAR (aka the EA Hotel)](https://ceealar.org/) can be a great option as it offers free food and accommodation for up to two years, as well as contact with others who are thinking about these issues. Many people have offered rooms around the world for free or at low cost on the [EA Houses spreadsheet](https://docs.google.com/spreadsheets/d/1wwNn1VEaQuSzDdedCVcmjktxeZj-apHele4Ry8DLJk0/edit#gid=0). There are also opportunities from smaller grantmakers which you might be able to pick up if you get involved.\n\nIf you want to work on support or infrastructure rather than directly on research, the [EA Infrastructure Fund](https://funds.effectivealtruism.org/funds/ea-community) may be able to help. In general, you can [talk to EA funds before applying](https://www.lesswrong.com/posts/5AAFoigbbMqgrTpDh/you-can-talk-to-ea-funds-before-applying).\n\nEach grant source has their own criteria for funding, but in general they are looking for candidates who have evidence that they're keen and able to do good work towards reducing existential risk (for example, by completing an [AI Safety Camp](https://aisafety.camp/) project), though the EA Hotel in particular has less stringent requirements as they're able to support people at very low cost. If you'd like to talk to someone who can offer advice on applying for funding, [AI Safety Support](https://www.aisafetysupport.org/) offers [free calls](https://calendly.com/aiss).\n\nAnother option is to get hired by an organization which works on AI alignment, see the follow-up question for advice on that.\n\nIt's also worth checking the AI Alignment tag on the [EA funding sources website](https://eafunding.softr.app/) for up-to-date suggestions.\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6703", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:45.989+01:00", "Request Count": "", "Number of suggestions on answer doc": 23, "Total character count of suggestions on answer doc": 12769, "Helpful": ""}}, {"id": "i-2ccecfedb9458f61b8f192c60f9071aacc2b8f3a74c2657c08e3a5884866db48", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2ccecfedb9458f61b8f192c60f9071aacc2b8f3a74c2657c08e3a5884866db48", "name": "I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?", "index": 89, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:30.495Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2ccecfedb9458f61b8f192c60f9071aacc2b8f3a74c2657c08e3a5884866db48", "values": {"File": "I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?", "Link": "https://docs.google.com/document/d/1mW-7XP86MPe-LUzoksetdCaHlefDEkBnR_kd20kwzlo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:14:03.110+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Contributing", "Doc Last Edited": "2023-02-22T23:09:51.934+01:00", "Status": "Live on site", "Edit Answer": "I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6474", "Source Link": "", "aisafety.info Link": "I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?", "Source": "Wiki", "All Phrasings": "I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "OK, it\u2019s great that you want to help, here are some ideas for ways you could do so without making a huge commitment:\n\n- Learning more about AI alignment will provide you with good foundations for any path towards helping. You could start by absorbing content (e.g. books, videos, posts), and thinking about challenges or possible solutions.\n\n- Getting involved with the movement by joining a local Effective Altruism or LessWrong group, Rob Miles\u2019s Discord, and/or the AI Safety Slack is a great way to find friends who are interested and will help you stay motivated.\n\n- Donating to organizations or individuals working on AI alignment, possibly via a [donor lottery](https://funds.effectivealtruism.org/donor-lottery) or the [Long Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), can be a great way to provide support.\n\n- [Writing](https://coda.io/d/AI-Safety-Info-Dashboard_dfau7sl2hmG/Write-answers_suuwH#_luboi) or [improving answers](https://coda.io/d/AI-Safety-Info-Dashboard_dfau7sl2hmG/Improve-answers_suxEW#_lu2ln) on [my wiki](https://coda.io/d/AI-Safety-Info-Dashboard_dfau7sl2hmG/Get-involved-with-AI-Safety-Info_susRF#_luEwW) so that other people can learn about AI alignment more easily is a great way to dip your toe into contributing. You can always ask on the Discord for feedback on things you write.\n\n- Getting good at giving an AI alignment elevator pitch, and sharing it with people who may be valuable to have working on the problem can make a big difference. However you should avoid putting them off the topic by presenting it in a way which causes them to dismiss it as sci-fi (dos and don\u2019ts in the elevator pitch follow-up question).\n\n- Writing thoughtful comments on [AI posts on LessWrong](https://www.lesswrong.com/tag/ai?sortedBy=magic).\n\n- Participating in the [AGI Safety Fundamentals program](https://www.eacambridge.org/agi-safety-fundamentals) \u2013 either the AI alignment or governance track \u2013 and then facilitating discussions for it in the following round. The program involves nine weeks of content, with about two hours of readings + exercises per week and 1.5 hours of discussion, followed by four weeks to work on an independent project. As a facilitator, you'll be helping others learn about AI safety in-depth, many of whom are considering a career in AI safety. In the early 2022 round, facilitators were offered a stipend, and this seems likely to be the case for future rounds as well! You can learn more about facilitating in [this post from December 2021](https://forum.effectivealtruism.org/posts/WtwMy69JKZeHEvykc/contribute-by-facilitating-the-agi-safety-fundamentals).\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "OK, it\u2019s great that you want to help, here are some ideas for ways you could do so without making a huge commitment:\n\n- Learning more about AI alignment will provide you with good foundations for any path towards helping. You could start by absorbing content (e.g. books, videos, posts), and thinking about challenges or possible solutions.\n\n- Getting involved with the movement by joining a local Effective Altruism or LessWrong group, Rob Miles\u2019s Discord, and/or the AI Safety Slack is a great way to find friends who are interested and will help you stay motivated.\n\n- Donating to organizations or individuals working on AI alignment, possibly via a [donor lottery](https://funds.effectivealtruism.org/donor-lottery) or the [Long Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), can be a great way to provide support.\n\n- [Writing](https://coda.io/d/AI-Safety-Info-Dashboard_dfau7sl2hmG/Write-answers_suuwH#_luboi) or [improving answers](https://coda.io/d/AI-Safety-Info-Dashboard_dfau7sl2hmG/Improve-answers_suxEW#_lu2ln) on [my wiki](https://coda.io/d/AI-Safety-Info-Dashboard_dfau7sl2hmG/Get-involved-with-AI-Safety-Info_susRF#_luEwW) so that other people can learn about AI alignment more easily is a great way to dip your toe into contributing. You can always ask on the Discord for feedback on things you write.\n\n- Getting good at giving an AI alignment elevator pitch, and sharing it with people who may be valuable to have working on the problem can make a big difference. However you should avoid putting them off the topic by presenting it in a way which causes them to dismiss it as sci-fi (dos and don\u2019ts in the elevator pitch follow-up question).\n\n- Writing thoughtful comments on [AI posts on LessWrong](https://www.lesswrong.com/tag/ai?sortedBy=magic).\n\n- Participating in the [AGI Safety Fundamentals program](https://www.eacambridge.org/agi-safety-fundamentals) \u2013 either the AI alignment or governance track \u2013 and then facilitating discussions for it in the following round. The program involves nine weeks of content, with about two hours of readings + exercises per week and 1.5 hours of discussion, followed by four weeks to work on an independent project. As a facilitator, you'll be helping others learn about AI safety in-depth, many of whom are considering a career in AI safety. In the early 2022 round, facilitators were offered a stipend, and this seems likely to be the case for future rounds as well! You can learn more about facilitating in [this post from December 2021](https://forum.effectivealtruism.org/posts/WtwMy69JKZeHEvykc/contribute-by-facilitating-the-agi-safety-fundamentals).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "6474", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:47.827+01:00", "Request Count": "", "Number of suggestions on answer doc": 23, "Total character count of suggestions on answer doc": 12769, "Helpful": ""}}, {"id": "i-cbfea0f2363b6da3f06f1cfe5eb35432654caf5bda76349e99f6235910b1c665", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-cbfea0f2363b6da3f06f1cfe5eb35432654caf5bda76349e99f6235910b1c665", "name": "How would you explain the theory of Infra-Bayesianism?", "index": 90, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:33.056Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-cbfea0f2363b6da3f06f1cfe5eb35432654caf5bda76349e99f6235910b1c665", "values": {"File": "How would you explain the theory of Infra-Bayesianism?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How would you explain the theory of Infra-Bayesianism?", "Link": "https://docs.google.com/document/d/1YkebGu6f39VvHbfXW6Gd4-rgJiqzJ4xwjQSbsRfZ-yo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:59.636+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Infra-Bayesianism", "Doc Last Edited": "2023-03-06T05:05:11.532+01:00", "Status": "Live on site", "Edit Answer": "How would you explain the theory of Infra-Bayesianism?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8365", "Source Link": "", "aisafety.info Link": "How would you explain the theory of Infra-Bayesianism?", "Source": "Wiki", "All Phrasings": "How would you explain the theory of Infra-Bayesianism?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "See Vanessa's [research agenda](https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda) for more detail.\n\n[Infra-Bayesianism](https://www.lesswrong.com/s/CmrW8fCmSLK7E25sa) (IB) tries to solve the problem of agent foundations. On a high level, we want to have a model of an agent, understand what it means for it to be aligned with us, and produce some desiderata for an AGI training setup such that it points at aligned AGIs. Without solving that, we\u2019re in a situation in the [rocket alignment problem](https://intelligence.org/2018/10/03/rocket-alignment/) analogy: imagine we want to launch a rocket to the Moon, we have lots of explosives, but we don\u2019t have equations for gravity and only have some initial understanding of acceleration. Also, we don\u2019t know where the Moon is.\n\nInfra-Bayesianism tries to come up with a realistic model of agents and a mathematical structure that would point at agents aligned with humans, such that these agents could later be found with gradient descent.\n\nWith these goals, the research starts by solving some problems with traditional RL theory: for example, traditional RL agents, being a part of the universe, can't even consider the actual universe in the set of their hypotheses, since they're smaller than the universe; a traditional bayesian agent would have a hypothesis as a probability distribution over all possible worlds; but it's impossible for an agent made out of blocks in a part of a Minecraft world to assign probabilities to every possible state of the whole Minecraft world.\n\nIB is essentially a theory of imprecise probability that solves this problem of non-realizability by considering hypotheses in the form of convex sets of probability distributions; in practice, this means, for example, a hypothesis can be \u201cevery odd bit in the string of bits is 1\u201d. (This is a set of probability distributions over all possible bit strings that only assign positive probabilities to strings that have 1s in odd positions; a mean of any two such probability distribution also doesn\u2019t assign any probability to strigns that have a 0 in an odd position, so it\u2019s also from the set, so the set is convex.)\n\nIf we don't know how to do something given unbounded compute, we are just confused about the thing. Going from thinking that chess was impossible for machines to understanding [minimax](https://www.google.com/url?q=https://en.wikipedia.org/wiki/Minimax&sa=D&source=editors&ust=1661633213196096&usg=AOvVaw3m8tD5QAEl-XXhvaH4d1v3) was a really good step forward for designing chess AIs, *even though minimax is completely intractable*.\n\nThus, we should seek to figure out how alignment might look in theory, and then try to bridge the theory-practice gap by making our proposal ever more efficient. The first step along this path is to figure out a universal [Reinforcement Learning](https://www.alignmentforum.org/tag/reinforcement-learning) setting that we can place our formal agents in, and then prove regret bounds in.\n\nA key problem in doing this is embeddedness. AIs can't have a perfect self model \u2014 this would be like imagining your ENTIRE brain, inside your brain. There are finite memory constraints. IB allows agents to have abstract models of themselves, and thus works in an embedded setting.\n\n[Infra-Bayesian Physicalism](https://www.lesswrong.com/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized) (IBP) is an extension of this to RL. IBP allows us to\n\n- Figure out what agents are running [by evaluating the counterfactual where the computation of the agent would output something different, and see if the physical universe is different].\n\n- Give a program, classify it as an agent or a non agent, and then find its utility function.\n\nVanessa uses this formalism to describe [PreDCA](https://www.lesswrong.com/posts/dPmmuaz9szk26BkmD/vanessa-kosoy-s-shortform?commentId=vKw6DB9crncovPxED#vKw6DB9crncovPxED), an alignment proposal based on IBP. This proposal assumes that an agent is an IBP agent, meaning that it is an RL agent with fuzzy probability distributions (along with some other things). The general outline of this proposal is as follows:\n\n1. Find all of the agents that preceded the AI\n\n1. Discard all of these agents that are powerful / non-human like\n\n1. Find all of the utility functions in the remaining agents\n\n1. Use combination of all of these utilities as the agent's utility function\n\nVanessa models an AI as a model based RL system with a WM, a reward function, and a policy derived from the WM + reward. [She claims that this avoids the sharp left turn](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization). The generalization problems come from the world model, but this is dealt with by having an epistemology that doesn't contain [bridge rules](https://www.lesswrong.com/posts/ethRJh2E7mSSjzCay/building-phenomenological-bridges), and so the true world is the simplest explanation for the observed data.\n\nIt is open to show that this proposal also solves inner alignment, but there is some chance that it does.\n\nThis approach deviates from MIRI's plan, which is to focus on a narrow task to perform the pivotal act, and then add corrigibility. Vanessa instead tries to directly learn the user's preferences, and optimize those.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "See Vanessa's [research agenda](https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda) for more detail.\n\n[Infra-Bayesianism](https://www.lesswrong.com/s/CmrW8fCmSLK7E25sa) (IB) tries to solve the problem of agent foundations. On a high level, we want to have a model of an agent, understand what it means for it to be aligned with us, and produce some desiderata for an AGI training setup such that it points at aligned AGIs. Without solving that, we\u2019re in a situation in the [rocket alignment problem](https://intelligence.org/2018/10/03/rocket-alignment/) analogy: imagine we want to launch a rocket to the Moon, we have lots of explosives, but we don\u2019t have equations for gravity and only have some initial understanding of acceleration. Also, we don\u2019t know where the Moon is.\n\nInfra-Bayesianism tries to come up with a realistic model of agents and a mathematical structure that would point at agents aligned with humans, such that these agents could later be found with gradient descent.\n\nWith these goals, the research starts by solving some problems with traditional RL theory: for example, traditional RL agents, being a part of the universe, can't even consider the actual universe in the set of their hypotheses, since they're smaller than the universe; a traditional bayesian agent would have a hypothesis as a probability distribution over all possible worlds; but it's impossible for an agent made out of blocks in a part of a Minecraft world to assign probabilities to every possible state of the whole Minecraft world.\n\nIB is essentially a theory of imprecise probability that solves this problem of non-realizability by considering hypotheses in the form of convex sets of probability distributions; in practice, this means, for example, a hypothesis can be \u201cevery odd bit in the string of bits is 1\u201d. (This is a set of probability distributions over all possible bit strings that only assign positive probabilities to strings that have 1s in odd positions; a mean of any two such probability distribution also doesn\u2019t assign any probability to strigns that have a 0 in an odd position, so it\u2019s also from the set, so the set is convex.)\n\nIf we don't know how to do something given unbounded compute, we are just confused about the thing. Going from thinking that chess was impossible for machines to understanding [minimax](https://www.google.com/url?q=https://en.wikipedia.org/wiki/Minimax&sa=D&source=editors&ust=1661633213196096&usg=AOvVaw3m8tD5QAEl-XXhvaH4d1v3) was a really good step forward for designing chess AIs, *even though minimax is completely intractable*.\n\nThus, we should seek to figure out how alignment might look in theory, and then try to bridge the theory-practice gap by making our proposal ever more efficient. The first step along this path is to figure out a universal [Reinforcement Learning](https://www.alignmentforum.org/tag/reinforcement-learning) setting that we can place our formal agents in, and then prove regret bounds in.\n\nA key problem in doing this is embeddedness. AIs can't have a perfect self model \u2014 this would be like imagining your ENTIRE brain, inside your brain. There are finite memory constraints. IB allows agents to have abstract models of themselves, and thus works in an embedded setting.\n\n[Infra-Bayesian Physicalism](https://www.lesswrong.com/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized) (IBP) is an extension of this to RL. IBP allows us to\n\n- Figure out what agents are running [by evaluating the counterfactual where the computation of the agent would output something different, and see if the physical universe is different].\n\n- Give a program, classify it as an agent or a non agent, and then find its utility function.\n\nVanessa uses this formalism to describe [PreDCA](https://www.lesswrong.com/posts/dPmmuaz9szk26BkmD/vanessa-kosoy-s-shortform?commentId=vKw6DB9crncovPxED#vKw6DB9crncovPxED), an alignment proposal based on IBP. This proposal assumes that an agent is an IBP agent, meaning that it is an RL agent with fuzzy probability distributions (along with some other things). The general outline of this proposal is as follows:\n\n1. Find all of the agents that preceded the AI\n\n1. Discard all of these agents that are powerful / non-human like\n\n1. Find all of the utility functions in the remaining agents\n\n1. Use combination of all of these utilities as the agent's utility function\n\nVanessa models an AI as a model based RL system with a WM, a reward function, and a policy derived from the WM + reward. [She claims that this avoids the sharp left turn](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization). The generalization problems come from the world model, but this is dealt with by having an epistemology that doesn't contain [bridge rules](https://www.lesswrong.com/posts/ethRJh2E7mSSjzCay/building-phenomenological-bridges), and so the true world is the simplest explanation for the observed data.\n\nIt is open to show that this proposal also solves inner alignment, but there is some chance that it does.\n\nThis approach deviates from MIRI's plan, which is to focus on a narrow task to perform the pivotal act, and then add corrigibility. Vanessa instead tries to directly learn the user's preferences, and optimize those.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8365", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:50.634+01:00", "Request Count": "", "Number of suggestions on answer doc": 23, "Total character count of suggestions on answer doc": 12769, "Helpful": ""}}, {"id": "i-b07eff7c44251035f69c411d67317f526b0399264111295c89b3f60db260e8b2", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-b07eff7c44251035f69c411d67317f526b0399264111295c89b3f60db260e8b2", "name": "How would we know if an AI were suffering?", "index": 91, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:35.342Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-b07eff7c44251035f69c411d67317f526b0399264111295c89b3f60db260e8b2", "values": {"File": "How would we know if an AI were suffering?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How would we know if an AI were suffering?", "Link": "https://docs.google.com/document/d/1O7BQ0qfiSDr-Dx_DVUTq7GbOghOd-1GSuXdcnppSK30/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:55.830+01:00", "Related Answers DO NOT EDIT": "", "Tags": "S-risk", "Doc Last Edited": "2023-02-23T07:17:51.537+01:00", "Status": "In progress", "Edit Answer": "How would we know if an AI were suffering?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7649", "Source Link": "", "aisafety.info Link": "How would we know if an AI were suffering?", "Source": "Wiki", "All Phrasings": "How would we know if an AI were suffering?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Also whether an AI can feel pleasure, or any emotions\n\n- Are emotions a purely biological thing?\n\n- Part of the problem is that expressing things which for a human would indicate suffering, do not necessarily indicate suffering in an AI (For example, in an LLM it will be able to  imitate human conversation of every kind, but those words do not correlate with the question of whether it is having internal experience)\n\n- But on the other hand we aren\u2019t yet able to interpret the deep models (e.g. of an LLM) to the point that we could tell by checking the internalas directly\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "- Also whether an AI can feel pleasure, or any emotions\n\n- Are emotions a purely biological thing?\n\n- Part of the problem is that expressing things which for a human would indicate suffering, do not necessarily indicate suffering in an AI (For example, in an LLM it will be able to  imitate human conversation of every kind, but those words do not correlate with the question of whether it is having internal experience)\n\n- But on the other hand we aren\u2019t yet able to interpret the deep models (e.g. of an LLM) to the point that we could tell by checking the internalas directly\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7649", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:52.504+01:00", "Request Count": "", "Number of suggestions on answer doc": 23, "Total character count of suggestions on answer doc": 12769, "Helpful": ""}}, {"id": "i-ae88336bbd3394f1727bc17aa548d13ef773869ce5631427422888d3075cf2ae", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ae88336bbd3394f1727bc17aa548d13ef773869ce5631427422888d3075cf2ae", "name": "How would we align an AGI whose learning algorithms / cognition look like human brains?", "index": 92, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:37.738Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ae88336bbd3394f1727bc17aa548d13ef773869ce5631427422888d3075cf2ae", "values": {"File": "How would we align an AGI whose learning algorithms / cognition look like human brains?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How would we align an AGI whose learning algorithms / cognition look like human brains?", "Link": "https://docs.google.com/document/d/1ZL2VUNM87WcFobtZ7jqjVSUHTnsJccWeXSaaiSdG32g/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:52.241+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-10T21:35:39.338+01:00", "Status": "Live on site", "Edit Answer": "How would we align an AGI whose learning algorithms / cognition look like human brains?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8324", "Source Link": "", "aisafety.info Link": "How would we align an AGI whose learning algorithms / cognition look like human brains?", "Source": "Wiki", "All Phrasings": "How would we align an AGI whose learning algorithms / cognition look like human brains?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "This is primarily Steven Brynes, a full time independent alignment researcher, working on answering the question: \"How would we align an AGI whose learning algorithms / cognition look like human brains?\"\n\nHumans seem to robustly care about things, why is that? If we understood that, could we design AGIs to do the same thing? As far as I understand it, most of this work is biology based: trying to figure out how various parts of the brain works, but then also connecting this to alignment and seeing if we can solve the alignment problem with this understanding.\n\nThere are [three other independent researchers working on related projects](https://www.lesswrong.com/posts/c2tEfqEMi6jcJ4kdg/brain-like-agi-project-aintelope) that Steven has proposed.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "This is primarily Steven Brynes, a full time independent alignment researcher, working on answering the question: \"How would we align an AGI whose learning algorithms / cognition look like human brains?\"\n\nHumans seem to robustly care about things, why is that? If we understood that, could we design AGIs to do the same thing? As far as I understand it, most of this work is biology based: trying to figure out how various parts of the brain works, but then also connecting this to alignment and seeing if we can solve the alignment problem with this understanding.\n\nThere are [three other independent researchers working on related projects](https://www.lesswrong.com/posts/c2tEfqEMi6jcJ4kdg/brain-like-agi-project-aintelope) that Steven has proposed.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8324", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:55.010+01:00", "Request Count": "", "Number of suggestions on answer doc": 25, "Total character count of suggestions on answer doc": 12772, "Helpful": ""}}, {"id": "i-34c31b36548772c6624a78689222d7772a29ef2d0c815f7b62a8b3d66d2135e1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-34c31b36548772c6624a78689222d7772a29ef2d0c815f7b62a8b3d66d2135e1", "name": "How would I know if AGI were imminent?", "index": 93, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:39.956Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-34c31b36548772c6624a78689222d7772a29ef2d0c815f7b62a8b3d66d2135e1", "values": {"File": "How would I know if AGI were imminent?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How would I know if AGI were imminent?", "Link": "https://docs.google.com/document/d/11gWuzcI1fB3fql1GSkhn4xCF5ZKgFPtVuCNCXDsYIno/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:48.521+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI Fire Alarm", "Doc Last Edited": "2023-02-22T22:56:20.829+01:00", "Status": "Not started", "Edit Answer": "How would I know if AGI were imminent?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7633", "Source Link": "", "aisafety.info Link": "How would I know if AGI were imminent?", "Source": "Wiki", "All Phrasings": "How would I know if AGI were imminent?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7633", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:57.356+01:00", "Request Count": "", "Number of suggestions on answer doc": 25, "Total character count of suggestions on answer doc": 12772, "Helpful": ""}}, {"id": "i-2de2963a98aeca9c1252cc4aa9c9c70683016c49e8359083bd7c70395968e803", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-2de2963a98aeca9c1252cc4aa9c9c70683016c49e8359083bd7c70395968e803", "name": "How tractable is it to get governments to play a good role (rather than a bad role) and/or to get them to play a role at all (rather than no role)?", "index": 94, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:42.450Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-2de2963a98aeca9c1252cc4aa9c9c70683016c49e8359083bd7c70395968e803", "values": {"File": "How tractable is it to get governments to play a good role (rather than a bad role) and/or to get them to play a role at all (rather than no role)?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How tractable is it to get governments to play a good role (rather than a bad role) and/or to get them to play a role at all (rather than no role)?", "Link": "https://docs.google.com/document/d/1f0JBy2SzBtLk_4tIhEjGBY5qsR0fGnGS0f7zNLG53Qs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:44.994+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Government", "Doc Last Edited": "2023-03-06T22:50:17.300+01:00", "Status": "Not started", "Edit Answer": "How tractable is it to get governments to play a good role (rather than a bad role) and/or to get them to play a role at all (rather than no role)?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7737", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "How tractable is it to get governments to play a good role (rather than a bad role) and/or to get them to play a role at all (rather than no role)?", "Source": "LessWrong", "All Phrasings": "How tractable is it to get governments to play a good role (rather than a bad role) and/or to get them to play a role at all (rather than no role)?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7737", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:55:59.446+01:00", "Request Count": "", "Number of suggestions on answer doc": 26, "Total character count of suggestions on answer doc": 13852, "Helpful": ""}}, {"id": "i-e8048ae1aabcc220b94df0f3bf88a1743d843224489eb4f754054888bde73ebd", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e8048ae1aabcc220b94df0f3bf88a1743d843224489eb4f754054888bde73ebd", "name": "How successfully have institutions managed risks from novel technology in the past?", "index": 95, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:45.317Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e8048ae1aabcc220b94df0f3bf88a1743d843224489eb4f754054888bde73ebd", "values": {"File": "How successfully have institutions managed risks from novel technology in the past?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How successfully have institutions managed risks from novel technology in the past?", "Link": "https://docs.google.com/document/d/1LdVRHHL2GWNL70OSBlsVPb7loXi6x0IU_jBYEmDmWG8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:41.606+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Institutions,Technology", "Doc Last Edited": "2023-02-22T22:56:22.898+01:00", "Status": "In progress", "Edit Answer": "How successfully have institutions managed risks from novel technology in the past?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6480", "Source Link": "", "aisafety.info Link": "How successfully have institutions managed risks from novel technology in the past?", "Source": "Wiki", "All Phrasings": "How successfully have institutions managed risks from novel technology in the past?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Usually disaster then safety after\n\n- E.g car, accidents with some specific problem kill large numbers of people before it gets fixed\n\n- Machine shop rules are written in blood\n\n- Nuclear: did fairly well, but not good enough to stop there from ever being a major disaster. \n\n- \n\n- TORESEARCH: AI impacts on reference class for past handling of risk\n\n- Reasonable record of not developing techs e.g. human cloning and blinding laser weapons\n\n- \n\n    - \n\n    - \n\n- \n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "- Usually disaster then safety after\n\n- E.g car, accidents with some specific problem kill large numbers of people before it gets fixed\n\n- Machine shop rules are written in blood\n\n- Nuclear: did fairly well, but not good enough to stop there from ever being a major disaster. \n\n- \n\n- TORESEARCH: AI impacts on reference class for past handling of risk\n\n- Reasonable record of not developing techs e.g. human cloning and blinding laser weapons\n\n- \n\n    - \n\n    - \n\n- \n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "6480", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:02.108+01:00", "Request Count": "", "Number of suggestions on answer doc": 28, "Total character count of suggestions on answer doc": 14241, "Helpful": ""}}, {"id": "i-fb94bfba14aa761ec683fb2db53a9ff1d7be6a2f02c95b854e1d042a12dc44e2", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-fb94bfba14aa761ec683fb2db53a9ff1d7be6a2f02c95b854e1d042a12dc44e2", "name": "How software- and/or hardware-bottlenecked are we on AGI?", "index": 96, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:47.967Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-fb94bfba14aa761ec683fb2db53a9ff1d7be6a2f02c95b854e1d042a12dc44e2", "values": {"File": "How software- and/or hardware-bottlenecked are we on AGI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How software- and/or hardware-bottlenecked are we on AGI?", "Link": "https://docs.google.com/document/d/1zf2-nCvx5p77iGVIzOpdk73Li4TGdnPoJeSJIjGrIII/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:38.117+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI,Compute,Algorithmic Progress", "Doc Last Edited": "2023-03-06T22:50:48.521+01:00", "Status": "Not started", "Edit Answer": "How software- and/or hardware-bottlenecked are we on AGI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7728", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "How software- and/or hardware-bottlenecked are we on AGI?", "Source": "LessWrong", "All Phrasings": "How software- and/or hardware-bottlenecked are we on AGI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 3, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7728", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:04.581+01:00", "Request Count": "", "Number of suggestions on answer doc": 29, "Total character count of suggestions on answer doc": 15174, "Helpful": ""}}, {"id": "i-38db9b9e27acf61d448fd929d5e4d28d3694b466524bad01bc1e17939cb007bc", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-38db9b9e27acf61d448fd929d5e4d28d3694b466524bad01bc1e17939cb007bc", "name": "How should I personally prepare for when transformative AI arrives?", "index": 97, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:51.126Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-38db9b9e27acf61d448fd929d5e4d28d3694b466524bad01bc1e17939cb007bc", "values": {"File": "How should I personally prepare for when transformative AI arrives?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How should I personally prepare for when transformative AI arrives?", "Link": "https://docs.google.com/document/d/15agijiAV3Dn0wkJv4Z5MlQh2-0PZoYMaX93dR8u3pAg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:35.036+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Transformative AI,Personal Action", "Doc Last Edited": "2023-02-22T22:56:25.039+01:00", "Status": "In progress", "Edit Answer": "How should I personally prepare for when transformative AI arrives?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7625", "Source Link": "", "aisafety.info Link": "How should I personally prepare for when transformative AI arrives?", "Source": "Wiki", "All Phrasings": "How should I personally prepare for when transformative AI arrives?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Don't train in careers which are about to become fully automated (but being a human-computer Centaur might be a good idea).\n\n- \n\n- Don\u2019t be a wheat harvester, become someone who can operate a combine harvester.\n\n- \n\n- We may go extinct. If so, investments with a longer payback time than our timelines don\u2019t make a whole lot of sense. \u201cLive every day like it\u2019s your last\u201d might be relevant.\n\n- \n\n- We may reach a post-scarcity society, and property may or may not be maintained. Focus on resources you expect to be valuable even so.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Don't train in careers which are about to become fully automated (but being a human-computer Centaur might be a good idea).\n\n- \n\n- Don\u2019t be a wheat harvester, become someone who can operate a combine harvester.\n\n- \n\n- We may go extinct. If so, investments with a longer payback time than our timelines don\u2019t make a whole lot of sense. \u201cLive every day like it\u2019s your last\u201d might be relevant.\n\n- \n\n- We may reach a post-scarcity society, and property may or may not be maintained. Focus on resources you expect to be valuable even so.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7625", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:06.835+01:00", "Request Count": "", "Number of suggestions on answer doc": 29, "Total character count of suggestions on answer doc": 15174, "Helpful": ""}}, {"id": "i-7db450ba6a813c590a6cb607f5c65707eabebb425b8f87df503dd93bfc9db356", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-7db450ba6a813c590a6cb607f5c65707eabebb425b8f87df503dd93bfc9db356", "name": "How should I decide which quality level to attribute to a proposed question?", "index": 98, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:53.918Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-7db450ba6a813c590a6cb607f5c65707eabebb425b8f87df503dd93bfc9db356", "values": {"File": "How should I decide which quality level to attribute to a proposed question?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How should I decide which quality level to attribute to a proposed question?", "Link": "https://docs.google.com/document/d/1O-P6Pp6ovkg0aIPe6vp4gP9WlXMnxTkOpbLOyNNEWkg/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:31.591+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Stampy", "Doc Last Edited": "2023-02-22T22:56:26.063+01:00", "Status": "Not started", "Edit Answer": "How should I decide which quality level to attribute to a proposed question?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6632", "Source Link": "", "aisafety.info Link": "How should I decide which quality level to attribute to a proposed question?", "Source": "Wiki", "All Phrasings": "How should I decide which quality level to attribute to a proposed question?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "6632", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:09.104+01:00", "Request Count": "", "Number of suggestions on answer doc": 29, "Total character count of suggestions on answer doc": 15174, "Helpful": ""}}, {"id": "i-5a8a25ff5b15e5afba74f271f25e9e6cbb71b9f22f6286a9f2edd7f5665b46df", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5a8a25ff5b15e5afba74f271f25e9e6cbb71b9f22f6286a9f2edd7f5665b46df", "name": "How should I change my financial investments in response to the possibility of transformative AI?", "index": 99, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:56.617Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5a8a25ff5b15e5afba74f271f25e9e6cbb71b9f22f6286a9f2edd7f5665b46df", "values": {"File": "How should I change my financial investments in response to the possibility of transformative AI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How should I change my financial investments in response to the possibility of transformative AI?", "Link": "https://docs.google.com/document/d/1IBKoq2Q8fISvMB6u2UCJvHf0XY7tppR-A_si1AnP3-g/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:27.967+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Transformative AI,Investmenting", "Doc Last Edited": "2023-03-12T00:32:12.147+01:00", "Status": "Not started", "Edit Answer": "How should I change my financial investments in response to the possibility of transformative AI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7635", "Source Link": "", "aisafety.info Link": "How should I change my financial investments in response to the possibility of transformative AI?", "Source": "Wiki", "All Phrasings": "How should I change my financial investments in response to the possibility of transformative AI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- [Zvi\u2019s take](https://thezvi.substack.com/p/on-ai-and-interest-rates?utm_source=post-email-title&publication_id=573100&post_id=97061392&isFreemail=true&utm_medium=email)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "- [Zvi\u2019s take](https://thezvi.substack.com/p/on-ai-and-interest-rates?utm_source=post-email-title&publication_id=573100&post_id=97061392&isFreemail=true&utm_medium=email)\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7635", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:11.292+01:00", "Request Count": -1, "Number of suggestions on answer doc": 29, "Total character count of suggestions on answer doc": 15174, "Helpful": ""}}, {"id": "i-87b044534e6ce53a314b59201da0b178c06d69dc395eda4a0b61877db1414793", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-87b044534e6ce53a314b59201da0b178c06d69dc395eda4a0b61877db1414793", "name": "How quickly would the AI capabilities ecosystem adopt promising new advances in AI alignment?", "index": 100, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:12:59.562Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-87b044534e6ce53a314b59201da0b178c06d69dc395eda4a0b61877db1414793", "values": {"File": "How quickly would the AI capabilities ecosystem adopt promising new advances in AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How quickly would the AI capabilities ecosystem adopt promising new advances in AI alignment?", "Link": "https://docs.google.com/document/d/1bnxJIy_iXOSjFw5UJUW1wfwwMAg5hUFNqDMq1T7Vrc0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:23.982+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AI Takeoff", "Doc Last Edited": "2023-02-22T22:56:28.062+01:00", "Status": "In progress", "Edit Answer": "How quickly would the AI capabilities ecosystem adopt promising new advances in AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7751", "Source Link": "", "aisafety.info Link": "How quickly would the AI capabilities ecosystem adopt promising new advances in AI alignment?", "Source": "Wiki", "All Phrasings": "How quickly would the AI capabilities ecosystem adopt promising new advances in AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "A lot of narrow AI alignment advances will improve capabilities too, make it easier for human users to work with the AI tools. Those are going to be adopted almost instantly, for example [interpretability](http://interpretability) might be considered a desirable property by all AI researchers.\n\nHowever, with our current research methods for search in highly dimensional spaces, it seems exponentially more likely to find a capable AGI than to find a capable and aligned AGI. So even if capabilities research will adopt all new advances in AI alignment as soon as they come along, it is likely capability research will happen faster than alignment research. We need to find ways to incentivize the corporations who will would profit from more capability research to also focus on alignment.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "A lot of narrow AI alignment advances will improve capabilities too, make it easier for human users to work with the AI tools. Those are going to be adopted almost instantly, for example [interpretability](http://interpretability) might be considered a desirable property by all AI researchers.\n\nHowever, with our current research methods for search in highly dimensional spaces, it seems exponentially more likely to find a capable AGI than to find a capable and aligned AGI. So even if capabilities research will adopt all new advances in AI alignment as soon as they come along, it is likely capability research will happen faster than alignment research. We need to find ways to incentivize the corporations who will would profit from more capability research to also focus on alignment.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7751", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:13.652+01:00", "Request Count": "", "Number of suggestions on answer doc": 29, "Total character count of suggestions on answer doc": 15174, "Helpful": ""}}, {"id": "i-656c7c6f785111afa19973fff0dd2f140b34953fc25f4e5b2440569bbf2e1bf7", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-656c7c6f785111afa19973fff0dd2f140b34953fc25f4e5b2440569bbf2e1bf7", "name": "How quickly could an AI go from the first indications of problems to an unrecoverable disaster?", "index": 101, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:02.449Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-656c7c6f785111afa19973fff0dd2f140b34953fc25f4e5b2440569bbf2e1bf7", "values": {"File": "How quickly could an AI go from the first indications of problems to an unrecoverable disaster?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How quickly could an AI go from the first indications of problems to an unrecoverable disaster?", "Link": "https://docs.google.com/document/d/113cyPrnzE0VnHMQJZYcL19KWY0tAm9kDeq2wf866q30/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:20.728+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:09:58.108+01:00", "Status": "Live on site", "Edit Answer": "How quickly could an AI go from the first indications of problems to an unrecoverable disaster?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5851", "Source Link": "", "aisafety.info Link": "How quickly could an AI go from the first indications of problems to an unrecoverable disaster?", "Source": "Wiki", "All Phrasings": "How quickly could an AI go from the first indications of problems to an unrecoverable disaster?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "If the AI system was deceptively aligned (i.e. pretending to be nice until it was in control of the situation) or had been in stealth mode while getting things in place for a takeover, quite possibly within hours. We may get more warning with weaker systems, if the AGI does not feel at all threatened by us, or if a [complex ecosystem of AI systems is built over time and we gradually lose control](https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story).\n\nPaul Christiano writes [a story of alignment failure](https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story) which shows a relatively fast transition.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "If the AI system was deceptively aligned (i.e. pretending to be nice until it was in control of the situation) or had been in stealth mode while getting things in place for a takeover, quite possibly within hours. We may get more warning with weaker systems, if the AGI does not feel at all threatened by us, or if a [complex ecosystem of AI systems is built over time and we gradually lose control](https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story).\n\nPaul Christiano writes [a story of alignment failure](https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story) which shows a relatively fast transition.\n\n", "Stamp Count": 2, "Multi Answer": "", "Stamped By": "Sophialb\nplex", "Priority": 4, "Asker": "Sophiaaa", "External Source": "", "Last Asked On Discord": "", "UI ID": "5851", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:16.271+01:00", "Request Count": "", "Number of suggestions on answer doc": 29, "Total character count of suggestions on answer doc": 15174, "Helpful": ""}}, {"id": "i-1b1f7e5029eacd358a2fb450f0f09ece7314dceacb8e4ca8365056723a240efa", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-1b1f7e5029eacd358a2fb450f0f09ece7314dceacb8e4ca8365056723a240efa", "name": "How powerful will a mature superintelligence be?", "index": 102, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-16T05:06:50.570Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-1b1f7e5029eacd358a2fb450f0f09ece7314dceacb8e4ca8365056723a240efa", "values": {"File": "How powerful will a mature superintelligence be?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How powerful will a mature superintelligence be?", "Link": "https://docs.google.com/document/d/1933mx_fXY8VBvosoonXOiuHF_WCCbceqlS9X6pONoc8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:17.522+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-03-16T03:26:48.444+01:00", "Status": "Not started", "Edit Answer": "How powerful will a mature superintelligence be?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7755", "Source Link": "", "aisafety.info Link": "How powerful will a mature superintelligence be?", "Source": "Wiki", "All Phrasings": "How powerful will a mature superintelligence be?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7755", "Related Answers": "", "Doc Last Ingested": "2023-03-16T04:12:08.330+01:00", "Request Count": "", "Number of suggestions on answer doc": 0, "Total character count of suggestions on answer doc": 0, "Helpful": ""}}, {"id": "i-28fd990884f29343025816d5a1c2d70fbdb82532ba0071b2a034e59e2a46966a", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-28fd990884f29343025816d5a1c2d70fbdb82532ba0071b2a034e59e2a46966a", "name": "How possible (and how desirable) is it to change which path humanity follows to get to AGI?", "index": 103, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:08.095Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-28fd990884f29343025816d5a1c2d70fbdb82532ba0071b2a034e59e2a46966a", "values": {"File": "How possible (and how desirable) is it to change which path humanity follows to get to AGI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How possible (and how desirable) is it to change which path humanity follows to get to AGI?", "Link": "https://docs.google.com/document/d/1hZeuVDFZeO8KsScwd4fprGti2lbeI1m6IBWeLsdajD0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:13.940+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI,Differential Technological Development", "Doc Last Edited": "2023-02-22T22:56:30.020+01:00", "Status": "Not started", "Edit Answer": "How possible (and how desirable) is it to change which path humanity follows to get to AGI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7734", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "How possible (and how desirable) is it to change which path humanity follows to get to AGI?", "Source": "LessWrong", "All Phrasings": "How possible (and how desirable) is it to change which path humanity follows to get to AGI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7734", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:20.774+01:00", "Request Count": "", "Number of suggestions on answer doc": 29, "Total character count of suggestions on answer doc": 15174, "Helpful": ""}}, {"id": "i-e56bce874a8811abcb453823173a381a476524fbed0a19880c75d0cefcfe19b8", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e56bce874a8811abcb453823173a381a476524fbed0a19880c75d0cefcfe19b8", "name": "How much resources did the processes of biological evolution use to evolve intelligent creatures?", "index": 104, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:10.643Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e56bce874a8811abcb453823173a381a476524fbed0a19880c75d0cefcfe19b8", "values": {"File": "How much resources did the processes of biological evolution use to evolve intelligent creatures?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How much resources did the processes of biological evolution use to evolve intelligent creatures?", "Link": "https://docs.google.com/document/d/1IhCb23UIODpgpuH2MX6gEWvJYmepY4e0mSYvmpJdqGo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:09.827+01:00", "Related Answers DO NOT EDIT": "When will transformative AI be created?", "Tags": "", "Doc Last Edited": "2023-02-22T23:09:59.753+01:00", "Status": "Live on site", "Edit Answer": "How much resources did the processes of biological evolution use to evolve intelligent creatures?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7598", "Source Link": "", "aisafety.info Link": "How much resources did the processes of biological evolution use to evolve intelligent creatures?", "Source": "Wiki", "All Phrasings": "How much resources did the processes of biological evolution use to evolve intelligent creatures?\n", "Initial Order": "", "Related IDs": "2398", "Rich Text DO NOT EDIT": "Ajeya Cotra attempted to calculate this number in her paper [Bio Anchors](https://docs.google.com/document/d/1k7qzzn14jgE-Gbf0CON7_Py6tQUp2QNodr_8VAoDGnY/edit#heading=h.gvc1xyxlemkd).\n\n> [...]the total amount of computation done over the course of evolution from the first animals with neurons to humans was (~1e16 seconds) * (~1e25 FLOP/s) = ~**1e41 FLOP**\n\nNu\u00f1o Sempere [argues](https://forum.effectivealtruism.org/posts/FHTyixYNnGaQfEexH/a-concern-about-the-evolutionary-anchor-of-ajeya-cotra-s) that this calculation of the computation done by neurons is insufficient as the environment would also need to be simulated, leading to a possibly much larger number.\n\nCotra posits that this amount of computation should be taken on an upper bound to the amount of computation needed to develop AGI. The actual amount of computation needed is probably many orders of magnitude lower.\n\n", "Tag Count": 0, "Related Answer Count": 1, "Rich Text": "Ajeya Cotra attempted to calculate this number in her paper [Bio Anchors](https://docs.google.com/document/d/1k7qzzn14jgE-Gbf0CON7_Py6tQUp2QNodr_8VAoDGnY/edit#heading=h.gvc1xyxlemkd).\n\n> [...]the total amount of computation done over the course of evolution from the first animals with neurons to humans was (~1e16 seconds) * (~1e25 FLOP/s) = ~**1e41 FLOP**\n\nNu\u00f1o Sempere [argues](https://forum.effectivealtruism.org/posts/FHTyixYNnGaQfEexH/a-concern-about-the-evolutionary-anchor-of-ajeya-cotra-s) that this calculation of the computation done by neurons is insufficient as the environment would also need to be simulated, leading to a possibly much larger number.\n\nCotra posits that this amount of computation should be taken on an upper bound to the amount of computation needed to develop AGI. The actual amount of computation needed is probably many orders of magnitude lower.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7598", "Related Answers": "When will transformative AI be created?", "Doc Last Ingested": "2023-03-14T23:56:23.371+01:00", "Request Count": "", "Number of suggestions on answer doc": 29, "Total character count of suggestions on answer doc": 15174, "Helpful": ""}}, {"id": "i-e70a19880148c4537b33aac637af3d4b01c596a8aa1e6fe12b67d1afbb5927be", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e70a19880148c4537b33aac637af3d4b01c596a8aa1e6fe12b67d1afbb5927be", "name": "How much can we learn about AI with interpretability tools?", "index": 105, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:12.994Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e70a19880148c4537b33aac637af3d4b01c596a8aa1e6fe12b67d1afbb5927be", "values": {"File": "How much can we learn about AI with interpretability tools?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How much can we learn about AI with interpretability tools?", "Link": "https://docs.google.com/document/d/14e9YGIel-4pah_vgFGmVgwXm-RNqkAcQZkFa8I4LeVo/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:06.265+01:00", "Related Answers DO NOT EDIT": "How might interpretability be helpful?", "Tags": "Interpretability", "Doc Last Edited": "2023-02-22T22:56:31.011+01:00", "Status": "Not started", "Edit Answer": "How much can we learn about AI with interpretability tools?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8239", "Source Link": "", "aisafety.info Link": "How much can we learn about AI with interpretability tools?", "Source": "Wiki", "All Phrasings": "How much can we learn about AI with interpretability tools?\n", "Initial Order": "", "Related IDs": "89LK", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 1, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nicky", "External Source": "", "Last Asked On Discord": "", "UI ID": "8239", "Related Answers": "How might interpretability be helpful?", "Doc Last Ingested": "2023-03-14T23:56:25.174+01:00", "Request Count": "", "Number of suggestions on answer doc": 29, "Total character count of suggestions on answer doc": 15174, "Helpful": ""}}, {"id": "i-4482186ffba1ed9175688077b06c295d7587630e5fc7e86f6acc8e465ac7bab0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-4482186ffba1ed9175688077b06c295d7587630e5fc7e86f6acc8e465ac7bab0", "name": "How might we reduce the diffusion of dangerous AI technology to insufficiently careful actors?", "index": 106, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:15.240Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-4482186ffba1ed9175688077b06c295d7587630e5fc7e86f6acc8e465ac7bab0", "values": {"File": "How might we reduce the diffusion of dangerous AI technology to insufficiently careful actors?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might we reduce the diffusion of dangerous AI technology to insufficiently careful actors?", "Link": "https://docs.google.com/document/d/1NcFzOS6v3oc3XJmqqIdWY0Ri0Vzk9-lgT7AIzmOdW7U/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:13:02.883+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Information Security", "Doc Last Edited": "2023-02-22T22:56:32.004+01:00", "Status": "Not started", "Edit Answer": "How might we reduce the diffusion of dangerous AI technology to insufficiently careful actors?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7624", "Source Link": "", "aisafety.info Link": "How might we reduce the diffusion of dangerous AI technology to insufficiently careful actors?", "Source": "Wiki", "All Phrasings": "How might we reduce the diffusion of dangerous AI technology to insufficiently careful actors?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7624", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:27.566+01:00", "Request Count": "", "Number of suggestions on answer doc": 29, "Total character count of suggestions on answer doc": 15174, "Helpful": ""}}, {"id": "i-c454f4f9b51f5669edfb96fd5773d4a8c6d2d5ceb6880fccf859dc83aadd09eb", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c454f4f9b51f5669edfb96fd5773d4a8c6d2d5ceb6880fccf859dc83aadd09eb", "name": "How might we reduce the chance of an AI arms race?", "index": 107, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:17.495Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c454f4f9b51f5669edfb96fd5773d4a8c6d2d5ceb6880fccf859dc83aadd09eb", "values": {"File": "How might we reduce the chance of an AI arms race?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might we reduce the chance of an AI arms race?", "Link": "https://docs.google.com/document/d/1NXdY73qYgBJGumgenFMUZHXmX_kN7yiz_bExAL9Oo9M/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:12:59.390+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Race Dynamics", "Doc Last Edited": "2023-02-22T22:56:33.086+01:00", "Status": "In progress", "Edit Answer": "How might we reduce the chance of an AI arms race?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7623", "Source Link": "", "aisafety.info Link": "How might we reduce the chance of an AI arms race?", "Source": "Wiki", "All Phrasings": "How might we reduce the chance of an AI arms race?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Fewer people makes it easier to reach agreement.\n\n- Unilateralism is a problem if there are many parties. Which is made worse since there is also the assessment of each party that others will break the agreement.\n\n- Increase cooperation between parties/well-meaning ness\n\n- \n\n- Carl Shulman\n\n- http://intelligence.org/files/ArmsControl.pdf\n\n- \n\n- [https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf](https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf)\n\n- \n\n- \n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "- Fewer people makes it easier to reach agreement.\n\n- Unilateralism is a problem if there are many parties. Which is made worse since there is also the assessment of each party that others will break the agreement.\n\n- Increase cooperation between parties/well-meaning ness\n\n- \n\n- Carl Shulman\n\n- http://intelligence.org/files/ArmsControl.pdf\n\n- \n\n- [https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf](https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf)\n\n- \n\n- \n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7623", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:29.234+01:00", "Request Count": "", "Number of suggestions on answer doc": 29, "Total character count of suggestions on answer doc": 15174, "Helpful": ""}}, {"id": "i-92c9403fe02a4e6cc599b945efbf87d5ba2ba6ca8d12bdb52df73a1d2f1bee02", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-92c9403fe02a4e6cc599b945efbf87d5ba2ba6ca8d12bdb52df73a1d2f1bee02", "name": "How might we get from Artificial General Intelligence to a Superintelligent system?", "index": 108, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:22.648Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-92c9403fe02a4e6cc599b945efbf87d5ba2ba6ca8d12bdb52df73a1d2f1bee02", "values": {"File": "How might we get from Artificial General Intelligence to a Superintelligent system?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might we get from Artificial General Intelligence to a Superintelligent system?", "Link": "https://docs.google.com/document/d/1Qd9PlZtTQSEYDychmvrdifRV9E52_PsQZBhZpOWeAPQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:12:55.910+01:00", "Related Answers DO NOT EDIT": "Will there be a discontinuity in AI capabilities? If so, at what stage?", "Tags": "AGI,Recursive Self-improvement,Superintelligence,Intelligence Explosion", "Doc Last Edited": "2023-02-22T23:10:01.710+01:00", "Status": "Live on site", "Edit Answer": "How might we get from Artificial General Intelligence to a Superintelligent system?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8158", "Source Link": "", "aisafety.info Link": "How might we get from Artificial General Intelligence to a Superintelligent system?", "Source": "Wiki", "All Phrasings": "How might we get from Artificial General Intelligence to a Superintelligent system?\n", "Initial Order": "", "Related IDs": "7729", "Rich Text DO NOT EDIT": "Once a system is at least as capable as top human at AI research, it would tend to become the driver of its own development and initiate a process of recursive self-improvement known as the [intelligence explosion](https://docs.google.com/document/d/1o_CnFOMGmL1hniT3dasrh6id67buwOwjU6R8zHrhMbk/edit), leading to an extremely powerful system. A general framing of this process is[Open Philanthropy](https://www.openphilanthropy.org/)'s[Process for Automating Scientific and Technological Advancement](https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#:~:text=Process%20for%20Automating%20Scientific%20and%20Technological%20Advancement%2C%20or%20PASTA).\n\nThere is[much](https://sideways-view.com/2018/02/24/takeoff-speeds/)[debate](https://astralcodexten.substack.com/p/yudkowsky-contra-christiano-on-ai) about whether there would be a notable period where the AI was partially driving its own development, with humans being gradually less and less important, or whether the transition to AI automated AI capability research would be sudden. However, the core idea that there is *some* threshold of capabilities beyond which a system would begin to rapidly ascend is hard to reasonably dispute, and is a significant consideration for developing alignment strategies.\n\n", "Tag Count": 4, "Related Answer Count": 1, "Rich Text": "Once a system is at least as capable as top human at AI research, it would tend to become the driver of its own development and initiate a process of recursive self-improvement known as the [intelligence explosion](https://docs.google.com/document/d/1o_CnFOMGmL1hniT3dasrh6id67buwOwjU6R8zHrhMbk/edit), leading to an extremely powerful system. A general framing of this process is[Open Philanthropy](https://www.openphilanthropy.org/)'s[Process for Automating Scientific and Technological Advancement](https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#:~:text=Process%20for%20Automating%20Scientific%20and%20Technological%20Advancement%2C%20or%20PASTA).\n\nThere is[much](https://sideways-view.com/2018/02/24/takeoff-speeds/)[debate](https://astralcodexten.substack.com/p/yudkowsky-contra-christiano-on-ai) about whether there would be a notable period where the AI was partially driving its own development, with humans being gradually less and less important, or whether the transition to AI automated AI capability research would be sudden. However, the core idea that there is *some* threshold of capabilities beyond which a system would begin to rapidly ascend is hard to reasonably dispute, and is a significant consideration for developing alignment strategies.\n\n", "Stamp Count": 2, "Multi Answer": "", "Stamped By": "Aprillion\nplex", "Priority": 2, "Asker": "plex", "External Source": "", "Last Asked On Discord": "", "UI ID": "8158", "Related Answers": "Will there be a discontinuity in AI capabilities? If so, at what stage?", "Doc Last Ingested": "2023-03-14T23:56:31.718+01:00", "Request Count": "", "Number of suggestions on answer doc": 29, "Total character count of suggestions on answer doc": 15174, "Helpful": ""}}, {"id": "i-acbf9edc89e03b44bca2726c1c154dafb20af2cf223697884c25374701051af2", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-acbf9edc89e03b44bca2726c1c154dafb20af2cf223697884c25374701051af2", "name": "How might things go wrong with AI even without an agentic superintelligence?", "index": 109, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:26.737Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-acbf9edc89e03b44bca2726c1c154dafb20af2cf223697884c25374701051af2", "values": {"File": "How might things go wrong with AI even without an agentic superintelligence?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might things go wrong with AI even without an agentic superintelligence?", "Link": "https://docs.google.com/document/d/107oGBBQuFE9BKVwol4c-IGIRHusDPCTLKJ2lrIkRWcQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:12:52.498+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Narrow AI,Agency,Persuasion", "Doc Last Edited": "2023-02-22T23:10:02.823+01:00", "Status": "Live on site", "Edit Answer": "How might things go wrong with AI even without an agentic superintelligence?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7774", "Source Link": "", "aisafety.info Link": "How might things go wrong with AI even without an agentic superintelligence?", "Source": "Wiki", "All Phrasings": "How might things go wrong with AI even without an agentic superintelligence?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Failures can happen with narrow non-agentic systems, mostly from humans not anticipating safety-relevant decisions made too quickly to react, much like in the [2010 flash crash](https://en.wikipedia.org/wiki/2010_flash_crash).\n\nA helpful metaphor draws on self-driving cars. By relying more and more on an automated process to make decisions, people become worse drivers as they\u2019re not training themselves to react to the unexpected; then the unexpected happens, the software system itself reacts in an unsafe way and the human is too slow to regain control.\n\nThis generalizes to broader tasks. A human using a powerful system to make better decisions (say, as the CEO of a company) might not understand those very well, get trapped into an equilibrium without realizing it and essentially losing control over the entire process.\n\nMore detailed examples in this vein are described by Paul Christiano in *[What failure looks like](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like)*.\n\nAnother source of failures is AI-mediated stable totalitarianism. The limiting factor in current pervasive surveillance, police and armed forces is manpower; the use of drones and other automated tools decreases the need for personnel to ensure security and extract resources.\n\nAs capabilities improve, political dissent could become impossible, checks and balances would break down as [a minimal number of key actors is needed to stay in power](https://www.youtube.com/watch?v=rStL7niR7gs).\n\n", "Tag Count": 3, "Related Answer Count": 0, "Rich Text": "Failures can happen with narrow non-agentic systems, mostly from humans not anticipating safety-relevant decisions made too quickly to react, much like in the [2010 flash crash](https://en.wikipedia.org/wiki/2010_flash_crash).\n\nA helpful metaphor draws on self-driving cars. By relying more and more on an automated process to make decisions, people become worse drivers as they\u2019re not training themselves to react to the unexpected; then the unexpected happens, the software system itself reacts in an unsafe way and the human is too slow to regain control.\n\nThis generalizes to broader tasks. A human using a powerful system to make better decisions (say, as the CEO of a company) might not understand those very well, get trapped into an equilibrium without realizing it and essentially losing control over the entire process.\n\nMore detailed examples in this vein are described by Paul Christiano in *[What failure looks like](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like)*.\n\nAnother source of failures is AI-mediated stable totalitarianism. The limiting factor in current pervasive surveillance, police and armed forces is manpower; the use of drones and other automated tools decreases the need for personnel to ensure security and extract resources.\n\nAs capabilities improve, political dissent could become impossible, checks and balances would break down as [a minimal number of key actors is needed to stay in power](https://www.youtube.com/watch?v=rStL7niR7gs).\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 5, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7774", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:34.155+01:00", "Request Count": "", "Number of suggestions on answer doc": 30, "Total character count of suggestions on answer doc": 15221, "Helpful": ""}}, {"id": "i-8d06bf1bd29f735af807fc996750d0fc05d17ffa83d37f82f21d018821823cb9", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8d06bf1bd29f735af807fc996750d0fc05d17ffa83d37f82f21d018821823cb9", "name": "How might non-agentic GPT-style AI cause an \"intelligence explosion\" or otherwise contribute to existential risk?", "index": 110, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:29.631Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8d06bf1bd29f735af807fc996750d0fc05d17ffa83d37f82f21d018821823cb9", "values": {"File": "How might non-agentic GPT-style AI cause an \"intelligence explosion\" or otherwise contribute to existential risk?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might non-agentic GPT-style AI cause an \"intelligence explosion\" or otherwise contribute to existential risk?", "Link": "https://docs.google.com/document/d/1HC5f8BNYJcqI7nBUTA_PjBjFcoGkPqMrtPVfxJqTwSM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:12:49.048+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Intelligence Explosion,AI Takeoff,Existential Risk,GPT,Pattern Matching", "Doc Last Edited": "2023-03-01T21:52:28.580+01:00", "Status": "Live on site", "Edit Answer": "How might non-agentic GPT-style AI cause an \"intelligence explosion\" or otherwise contribute to existential risk?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6571", "Source Link": "", "aisafety.info Link": "How might non-agentic GPT-style AI cause an \"intelligence explosion\" or otherwise contribute to existential risk?", "Source": "Wiki", "All Phrasings": "How might non-agentic GPT-style AI cause an \"intelligence explosion\" or otherwise contribute to existential risk?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "One threat model which includes a GPT component is[M](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent)[isaligned](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent) [M](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent)[odel-](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent)[B](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent)[ased RL](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent) [A](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent)[gent](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent). It suggests that a reinforcement learner attached to a GPT-style world model could lead to an existential risk, with the RL agent being the optimizer which uses the world model to be much more effective at achieving its goals.\n\nAnother possibility is that a sufficiently powerful world model[may develop mesa optimizers](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this) which could influence the world via the outputs of the model to achieve the mesa objective (perhaps by causing an optimizer to be created with goals aligned to it), though this is somewhat speculative.\n\n", "Tag Count": 5, "Related Answer Count": 0, "Rich Text": "One threat model which includes a GPT component is[M](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent)[isaligned](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent) [M](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent)[odel-](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent)[B](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent)[ased RL](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent) [A](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent)[gent](https://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent). It suggests that a reinforcement learner attached to a GPT-style world model could lead to an existential risk, with the RL agent being the optimizer which uses the world model to be much more effective at achieving its goals.\n\nAnother possibility is that a sufficiently powerful world model[may develop mesa optimizers](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this) which could influence the world via the outputs of the model to achieve the mesa objective (perhaps by causing an optimizer to be created with goals aligned to it), though this is somewhat speculative.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "^, ^", "External Source": "", "Last Asked On Discord": "", "UI ID": "6571", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:36.905+01:00", "Request Count": "", "Number of suggestions on answer doc": 34, "Total character count of suggestions on answer doc": 15645, "Helpful": ""}}, {"id": "i-c35e0f99053957851f14596249e5b9f43408462ba6ca5c63b8f7b1756947d7b1", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-c35e0f99053957851f14596249e5b9f43408462ba6ca5c63b8f7b1756947d7b1", "name": "How might an AI achieve a seemingly beneficial goal via inappropriate means?", "index": 111, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:32.393Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-c35e0f99053957851f14596249e5b9f43408462ba6ca5c63b8f7b1756947d7b1", "values": {"File": "How might an AI achieve a seemingly beneficial goal via inappropriate means?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might an AI achieve a seemingly beneficial goal via inappropriate means?", "Link": "https://docs.google.com/document/d/1rAyObyiBegtLnZ7I1lPUl-d0CANJwXNsW6EY3LTS4Y4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:12:41.731+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:10:05.197+01:00", "Status": "Live on site", "Edit Answer": "How might an AI achieve a seemingly beneficial goal via inappropriate means?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6190", "Source Link": "https://futureoflife.org/ai-faqs/", "aisafety.info Link": "How might an AI achieve a seemingly beneficial goal via inappropriate means?", "Source": "FLI's FAQ", "All Phrasings": "How might an AI achieve a seemingly beneficial goal via inappropriate means?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Imagine, for example, that you are tasked with reducing traffic congestion in San Francisco at all costs, i.e. you do not take into account any other constraints. How would you do it? You might start by just timing traffic lights better. But wouldn\u2019t there be less traffic if all the bridges closed down from 5 to 10AM, preventing all those cars from entering the city? Such a measure obviously violates common sense, and subverts the purpose of improving traffic, which is to help people get around \u2013 but it is consistent with the goal of \u201creducing traffic congestion\u201d.\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "Imagine, for example, that you are tasked with reducing traffic congestion in San Francisco at all costs, i.e. you do not take into account any other constraints. How would you do it? You might start by just timing traffic lights better. But wouldn\u2019t there be less traffic if all the bridges closed down from 5 to 10AM, preventing all those cars from entering the city? Such a measure obviously violates common sense, and subverts the purpose of improving traffic, which is to help people get around \u2013 but it is consistent with the goal of \u201creducing traffic congestion\u201d.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "FLI", "External Source": "", "Last Asked On Discord": "", "UI ID": "6190", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:38.393+01:00", "Request Count": "", "Number of suggestions on answer doc": 34, "Total character count of suggestions on answer doc": 15645, "Helpful": ""}}, {"id": "i-30b722762b4d59a800fbde92bb6bf45cbeb82f020ddf3be86bd094c4e18061c8", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-30b722762b4d59a800fbde92bb6bf45cbeb82f020ddf3be86bd094c4e18061c8", "name": "How might an \"intelligence explosion\" be dangerous?", "index": 112, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:35.025Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-30b722762b4d59a800fbde92bb6bf45cbeb82f020ddf3be86bd094c4e18061c8", "values": {"File": "How might an \"intelligence explosion\" be dangerous?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might an \"intelligence explosion\" be dangerous?", "Link": "https://docs.google.com/document/d/1ikDa8_Gy-R6C67s57fSOe79lnLXzoJnG49AOAH7mFZ8/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:12:38.399+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Intelligence Explosion", "Doc Last Edited": "2023-02-22T23:10:06.282+01:00", "Status": "Live on site", "Edit Answer": "How might an \"intelligence explosion\" be dangerous?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6607", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "How might an \"intelligence explosion\" be dangerous?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "How might an \"intelligence explosion\" be dangerous?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "If programmed with the wrong motivations, a machine could be malevolent toward humans, and intentionally exterminate our species. More likely, it could be designed with motivations that initially appeared safe (and easy to program) to its designers, but that turn out to be best fulfilled (given sufficient power) by reallocating resources from sustaining human life to [other projects](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf). As Yudkowsky writes, \u201cthe AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\u201d\n\nSince weak AIs with many different motivations could better achieve their goal by faking benevolence until they are powerful, safety testing to avoid this could be very challenging. Alternatively, competitive pressures, both economic and military, might lead AI designers to try to use other methods to control AIs with undesirable motivations. As those AIs became more sophisticated this could eventually lead to one risk too many.\n\nEven a machine successfully designed with superficially benevolent motivations could easily go awry when it discovers implications of its decision criteria unanticipated by its designers. For example, a superintelligence programmed to maximize human happiness might find it easier to rewire human neurology so that humans are happiest when sitting quietly in jars than to build and maintain a utopian world that caters to the complex and nuanced whims of current human neurology.\n\nSee also:\n\n- Yudkowsky, [Artificial intelligence as a positive and negative factor in global risk](https://intelligence.org/files/AIPosNegFactor.pdf)\n\n- Chalmers, [The Singularity: A Philosophical Analysis](http://consc.net/papers/singularity.pdf)\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "If programmed with the wrong motivations, a machine could be malevolent toward humans, and intentionally exterminate our species. More likely, it could be designed with motivations that initially appeared safe (and easy to program) to its designers, but that turn out to be best fulfilled (given sufficient power) by reallocating resources from sustaining human life to [other projects](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf). As Yudkowsky writes, \u201cthe AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\u201d\n\nSince weak AIs with many different motivations could better achieve their goal by faking benevolence until they are powerful, safety testing to avoid this could be very challenging. Alternatively, competitive pressures, both economic and military, might lead AI designers to try to use other methods to control AIs with undesirable motivations. As those AIs became more sophisticated this could eventually lead to one risk too many.\n\nEven a machine successfully designed with superficially benevolent motivations could easily go awry when it discovers implications of its decision criteria unanticipated by its designers. For example, a superintelligence programmed to maximize human happiness might find it easier to rewire human neurology so that humans are happiest when sitting quietly in jars than to build and maintain a utopian world that caters to the complex and nuanced whims of current human neurology.\n\nSee also:\n\n- Yudkowsky, [Artificial intelligence as a positive and negative factor in global risk](https://intelligence.org/files/AIPosNegFactor.pdf)\n\n- Chalmers, [The Singularity: A Philosophical Analysis](http://consc.net/papers/singularity.pdf)\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "", "UI ID": "6607", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:40.905+01:00", "Request Count": "", "Number of suggestions on answer doc": 34, "Total character count of suggestions on answer doc": 15645, "Helpful": ""}}, {"id": "i-d3f84d55665ad53bbf12e6cbd2b1696a220b4ff666dab4a164472eea340e9ca5", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-d3f84d55665ad53bbf12e6cbd2b1696a220b4ff666dab4a164472eea340e9ca5", "name": "How might a superintelligence technologically manipulate humans?", "index": 113, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:37.853Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-d3f84d55665ad53bbf12e6cbd2b1696a220b4ff666dab4a164472eea340e9ca5", "values": {"File": "How might a superintelligence technologically manipulate humans?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might a superintelligence technologically manipulate humans?", "Link": "https://docs.google.com/document/d/14oAp5XCac763f45bMjzS3jPBOg-tW3QQDgLmwEfRrd0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:12:33.778+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Superintelligence,Cognitive Superpowers,Technology", "Doc Last Edited": "2023-02-24T04:53:18.419+01:00", "Status": "In progress", "Edit Answer": "How might a superintelligence technologically manipulate humans?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6976", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "How might a superintelligence technologically manipulate humans?", "Source": "Superintelligence FAQ", "All Phrasings": "How might a superintelligence technologically manipulate humans?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "AlphaGo was connected to the Internet \u2013 why shouldn\u2019t the first superintelligence be? This gives a sufficiently clever superintelligence the opportunity to manipulate world computer networks. For example, it might program a virus that will infect every computer in the world, causing them to fill their empty memory with partial copies of the superintelligence, which when networked together become full copies of the superintelligence. Now the superintelligence controls every computer in the world, including the ones that target nuclear weapons. At this point it can force humans to bargain with it, and part of that bargain might be enough resources to establish its own industrial base, and then we\u2019re in humans vs. lions territory again.\n\nSatoshi Nakamoto is a mysterious individual who posted a design for the Bitcoin currency system to a cryptography forum. The design was so brilliant that everyone started using it, and Nakamoto \u2013 who had made sure to accumulate his own store of the currency before releasing it to the public \u2013 became a multibillionaire.\n\nIn other words, somebody with no resources except the ability to make posts to Internet forums managed to leverage that into a multibillion dollar fortune \u2013 and he wasn\u2019t even superintelligent. If Hitler is a lower-bound on how bad intelligent persuaders can be, Nakamoto should be a lower-bound on how bad intelligent programmers with Internet access can be.\n\n", "Tag Count": 3, "Related Answer Count": 0, "Rich Text": "AlphaGo was connected to the Internet \u2013 why shouldn\u2019t the first superintelligence be? This gives a sufficiently clever superintelligence the opportunity to manipulate world computer networks. For example, it might program a virus that will infect every computer in the world, causing them to fill their empty memory with partial copies of the superintelligence, which when networked together become full copies of the superintelligence. Now the superintelligence controls every computer in the world, including the ones that target nuclear weapons. At this point it can force humans to bargain with it, and part of that bargain might be enough resources to establish its own industrial base, and then we\u2019re in humans vs. lions territory again.\n\nSatoshi Nakamoto is a mysterious individual who posted a design for the Bitcoin currency system to a cryptography forum. The design was so brilliant that everyone started using it, and Nakamoto \u2013 who had made sure to accumulate his own store of the currency before releasing it to the public \u2013 became a multibillionaire.\n\nIn other words, somebody with no resources except the ability to make posts to Internet forums managed to leverage that into a multibillion dollar fortune \u2013 and he wasn\u2019t even superintelligent. If Hitler is a lower-bound on how bad intelligent persuaders can be, Nakamoto should be a lower-bound on how bad intelligent programmers with Internet access can be.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "", "UI ID": "6976", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:43.520+01:00", "Request Count": "", "Number of suggestions on answer doc": 34, "Total character count of suggestions on answer doc": 15645, "Helpful": ""}}, {"id": "i-665270b78cb73a55e08e8ac99f1c5c104831a5ad43fd8f9344c4f691068ffcee", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-665270b78cb73a55e08e8ac99f1c5c104831a5ad43fd8f9344c4f691068ffcee", "name": "How might a superintelligence socially manipulate humans?", "index": 114, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:40.793Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-665270b78cb73a55e08e8ac99f1c5c104831a5ad43fd8f9344c4f691068ffcee", "values": {"File": "How might a superintelligence socially manipulate humans?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might a superintelligence socially manipulate humans?", "Link": "https://docs.google.com/document/d/1ua4Pj2N-hcUVAS5sabv1sBCYpue_ymwSIBhjeddsh7c/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:12:06.434+01:00", "Related Answers DO NOT EDIT": "Won't humans be able to beat an unaligned AI since we have a huge advantage in numbers?", "Tags": "Deception,Superintelligence,Cognitive Superpowers", "Doc Last Edited": "2023-02-22T23:10:07.715+01:00", "Status": "Live on site", "Edit Answer": "How might a superintelligence socially manipulate humans?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6974", "Source Link": "https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq", "aisafety.info Link": "How might a superintelligence socially manipulate humans?", "Source": "Superintelligence FAQ", "All Phrasings": "How might a superintelligence socially manipulate humans?\n", "Initial Order": "", "Related IDs": "86WT", "Rich Text DO NOT EDIT": "People tend to imagine AIs as being like nerdy humans \u2013 brilliant at technology but clueless about social skills. There is no reason to expect this \u2013 persuasion and manipulation is a different kind of skill from writing mathematical proofs, but it\u2019s still a skill, and there\u2019s no reason a powerful AI system couldn\u2019t replicate or exceed the \u201ccharming sociopaths\u201d who can naturally win friends and followers despite a lack of normal human emotions.\n\nA superintelligence might be able to analyze human psychology deeply enough to understand the hopes and fears of everyone it negotiates with. Single humans using psychopathic social manipulation have done plenty of harm \u2013 Hitler leveraged his skill at oratory and his understanding of people\u2019s darkest prejudices to take over a continent. Why should we expect superintelligences to do worse than humans at this kind of task?\n\nA superintelligence would likely have an overwhelming advantage in time and attention to bring to bear, due to thinking more quickly than humans or due to having multiple copies of itself. You could imagine the AI as an organization of thousands of experts who build an extensive psychological profile on the person they want to manipulate, and then spend hours or days thinking through their strategy between every breath the human takes.\n\nMuch more speculatively,  a superintelligence might skip conventional persuasion entirely. The fact that many humans are succeptible to hypnosis suggests that there could be more effective ways to manipulate people than persuading them with arguments, and it seems likely that something with a better understanding of neuroscience could find and exploit known or unknown vulnerabilities in human brains. Ultimately, brains aren\u2019t very secure systems, and when you\u2019re dealing with something substantially more intelligent than you are, it would be wise to expect the unexpected.\n\n", "Tag Count": 3, "Related Answer Count": 1, "Rich Text": "People tend to imagine AIs as being like nerdy humans \u2013 brilliant at technology but clueless about social skills. There is no reason to expect this \u2013 persuasion and manipulation is a different kind of skill from writing mathematical proofs, but it\u2019s still a skill, and there\u2019s no reason a powerful AI system couldn\u2019t replicate or exceed the \u201ccharming sociopaths\u201d who can naturally win friends and followers despite a lack of normal human emotions.\n\nA superintelligence might be able to analyze human psychology deeply enough to understand the hopes and fears of everyone it negotiates with. Single humans using psychopathic social manipulation have done plenty of harm \u2013 Hitler leveraged his skill at oratory and his understanding of people\u2019s darkest prejudices to take over a continent. Why should we expect superintelligences to do worse than humans at this kind of task?\n\nA superintelligence would likely have an overwhelming advantage in time and attention to bring to bear, due to thinking more quickly than humans or due to having multiple copies of itself. You could imagine the AI as an organization of thousands of experts who build an extensive psychological profile on the person they want to manipulate, and then spend hours or days thinking through their strategy between every breath the human takes.\n\nMuch more speculatively,  a superintelligence might skip conventional persuasion entirely. The fact that many humans are succeptible to hypnosis suggests that there could be more effective ways to manipulate people than persuading them with arguments, and it seems likely that something with a better understanding of neuroscience could find and exploit known or unknown vulnerabilities in human brains. Ultimately, brains aren\u2019t very secure systems, and when you\u2019re dealing with something substantially more intelligent than you are, it would be wise to expect the unexpected.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Scott Alexander", "External Source": "", "Last Asked On Discord": "", "UI ID": "6974", "Related Answers": "Won't humans be able to beat an unaligned AI since we have a huge advantage in numbers?", "Doc Last Ingested": "2023-03-14T23:56:45.907+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-38123bd4e5b1b2bef652657a5dd58b8cb89dd98d2e54f64216f7522b78341347", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-38123bd4e5b1b2bef652657a5dd58b8cb89dd98d2e54f64216f7522b78341347", "name": "How might a real-world AI system that receives orders in natural language and does what you mean look?", "index": 115, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:44.605Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-38123bd4e5b1b2bef652657a5dd58b8cb89dd98d2e54f64216f7522b78341347", "values": {"File": "How might a real-world AI system that receives orders in natural language and does what you mean look?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might a real-world AI system that receives orders in natural language and does what you mean look?", "Link": "https://docs.google.com/document/d/1DQ_klodsbzvDK3_srKnxMsoCxvCfB9bfwmfG0H745hc/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:12:02.426+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Do What I Mean", "Doc Last Edited": "2023-02-22T22:56:35.082+01:00", "Status": "In progress", "Edit Answer": "How might a real-world AI system that receives orders in natural language and does what you mean look?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7611", "Source Link": "", "aisafety.info Link": "How might a real-world AI system that receives orders in natural language and does what you mean look?", "Source": "Wiki", "All Phrasings": "How might a real-world AI system that receives orders in natural language and does what you mean look?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- If it works on open-ended big tasks like \u201cmake my company successful\u201d, then you can probably just make a sovereign (I.e. tell it \u201cdo good things\u201d)\n\n    - \n\n- \n\n- \n\n- \n\n- To always do what a person means, really demands knowing it better than the person themself, since a person cant specify what they mean.\n\n- \n\n- Bostrom would call that a Genie. If it does what you *mean*, that would be an aligned genie. Or super butler (?) chapter 10 (page 149, he says \u201cThe ideal genie would be a super-butler rather than an autistic savant\u201d. He also argues that the distinction in practice would not be large)\n\n- Technology for creating a safe genie is just as hard as a sovereign and might as well build a soveireign (at least for full superintellingce) .\n\n- Example of strategy that wouldn\u2019t scale, at ~human level giving out tasks buys you some safety over open ended goals, but at radically superintelligent levels it does not buy you much safety.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "- If it works on open-ended big tasks like \u201cmake my company successful\u201d, then you can probably just make a sovereign (I.e. tell it \u201cdo good things\u201d)\n\n    - \n\n- \n\n- \n\n- \n\n- To always do what a person means, really demands knowing it better than the person themself, since a person cant specify what they mean.\n\n- \n\n- Bostrom would call that a Genie. If it does what you *mean*, that would be an aligned genie. Or super butler (?) chapter 10 (page 149, he says \u201cThe ideal genie would be a super-butler rather than an autistic savant\u201d. He also argues that the distinction in practice would not be large)\n\n- Technology for creating a safe genie is just as hard as a sovereign and might as well build a soveireign (at least for full superintellingce) .\n\n- Example of strategy that wouldn\u2019t scale, at ~human level giving out tasks buys you some safety over open ended goals, but at radically superintelligent levels it does not buy you much safety.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7611", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:48.466+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-474d718e488fcc3dfa9cf4276841e55a1bbc6c19fd478c0675b35e95e834a2db", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-474d718e488fcc3dfa9cf4276841e55a1bbc6c19fd478c0675b35e95e834a2db", "name": "How might AGI kill people?", "index": 116, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:49.775Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-474d718e488fcc3dfa9cf4276841e55a1bbc6c19fd478c0675b35e95e834a2db", "values": {"File": "How might AGI kill people?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might AGI kill people?", "Link": "https://docs.google.com/document/d/1MbvOfA8E_jRNtA6rDSBmUaiB6j_exlUIoauzY85Ofek/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:58.866+01:00", "Related Answers DO NOT EDIT": "", "Tags": "", "Doc Last Edited": "2023-02-22T23:10:09.223+01:00", "Status": "Live on site", "Edit Answer": "How might AGI kill people?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5943", "Source Link": "", "aisafety.info Link": "How might AGI kill people?", "Source": "Wiki", "All Phrasings": "How might AGI kill people?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "If we pose a serious threat, it could hack our weapons systems and turn them against us. Future militaries are much more vulnerable to this due to rapidly progressing autonomous weapons. There\u2019s also the option of creating bioweapons and distributing them to the most unstable groups you can find, tricking nations into WW3, or dozens of other things an agent many times smarter than any human with the ability to develop arbitrary technology, hack things (including communications), and manipulate people, or many other possibilities that something smarter than a human could think up. More can be found [here](https://www.lesswrong.com/posts/pxGYZs2zHJNHvWY5b/request-for-concrete-ai-takeover-mechanisms).\n\nIf we are not a threat, in the course of pursuing its goals it may consume vital resources that humans need (e.g. using land for solar panels instead of farm crops). This video goes into more detail:\n\n<iframe src=\"https://www.youtube.com/embed/ZeecOKBus3Q\" title=\"Why Would AI Want to do Bad Things? Instrumental Convergence\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n", "Tag Count": 0, "Related Answer Count": 0, "Rich Text": "If we pose a serious threat, it could hack our weapons systems and turn them against us. Future militaries are much more vulnerable to this due to rapidly progressing autonomous weapons. There\u2019s also the option of creating bioweapons and distributing them to the most unstable groups you can find, tricking nations into WW3, or dozens of other things an agent many times smarter than any human with the ability to develop arbitrary technology, hack things (including communications), and manipulate people, or many other possibilities that something smarter than a human could think up. More can be found [here](https://www.lesswrong.com/posts/pxGYZs2zHJNHvWY5b/request-for-concrete-ai-takeover-mechanisms).\n\nIf we are not a threat, in the course of pursuing its goals it may consume vital resources that humans need (e.g. using land for solar panels instead of farm crops). This video goes into more detail:\n\n<iframe src=\"https://www.youtube.com/embed/ZeecOKBus3Q\" title=\"Why Would AI Want to do Bad Things? Instrumental Convergence\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Jack Harley", "External Source": "", "Last Asked On Discord": "2023-03-06T19:38:36.196+01:00", "UI ID": "5943", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:51.151+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-5848133a647a228c67485ee603fa24a9fcd82d32df1c618b18e9725ea29ef6e0", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-5848133a647a228c67485ee603fa24a9fcd82d32df1c618b18e9725ea29ef6e0", "name": "How might AGI interface with cybersecurity?", "index": 117, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:55.091Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-5848133a647a228c67485ee603fa24a9fcd82d32df1c618b18e9725ea29ef6e0", "values": {"File": "How might AGI interface with cybersecurity?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might AGI interface with cybersecurity?", "Link": "https://docs.google.com/document/d/1YvuTzWlK2EThmzVOQyO7nOzy6R75ybapu4NBYTv0oA4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:55.420+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Cybersecurity", "Doc Last Edited": "2023-02-22T22:56:36.069+01:00", "Status": "In progress", "Edit Answer": "How might AGI interface with cybersecurity?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7606", "Source Link": "", "aisafety.info Link": "How might AGI interface with cybersecurity?", "Source": "Wiki", "All Phrasings": "How might AGI interface with cybersecurity?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- Rob doesn\u2019t know tons, more research needed\n\n- Security for AI labs, need to have good security to prevent leaks\n\n- \n\n- Trying to sandbox AI system needs trusted code\n\n- \n\n- Safer designs might look like unsafe designs with some special sauce around it\n\n- alignment tax, more compute, can remove safety stuff to make it more powerful\n\n- Security is important, because if someone steals your weights then runs a less safe copy\n\n- \n\n- Superhuman AI likely will be better than humans at hacking since they can have access to all libraries which human hacker might use and also they have greater speed.\n\n- Abilities are strengthening, for example could fake voice on phone. Or possible that it will be capability of combined human using AI systems\n\n- See roman yampolskys work on the issue [https://arxiv.org/abs/1610.07997](https://arxiv.org/abs/1610.07997)\n\n    - Not vouched for, check it first\n\n- By being very good at discovering/exploiting problems\n\n- By being very good at detecting intruders and stopping them\n\n- By inserting sophisticated but innocuous exploits into code - something like the [underhanded c contest](http://www.underhanded-c.org/_page_id_2.html), but on a massive scale\n\n- Generally speaking, imagine you have a really good hacker (or security expert - same thing). Now make them work 1000 times faster.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "- Rob doesn\u2019t know tons, more research needed\n\n- Security for AI labs, need to have good security to prevent leaks\n\n- \n\n- Trying to sandbox AI system needs trusted code\n\n- \n\n- Safer designs might look like unsafe designs with some special sauce around it\n\n- alignment tax, more compute, can remove safety stuff to make it more powerful\n\n- Security is important, because if someone steals your weights then runs a less safe copy\n\n- \n\n- Superhuman AI likely will be better than humans at hacking since they can have access to all libraries which human hacker might use and also they have greater speed.\n\n- Abilities are strengthening, for example could fake voice on phone. Or possible that it will be capability of combined human using AI systems\n\n- See roman yampolskys work on the issue [https://arxiv.org/abs/1610.07997](https://arxiv.org/abs/1610.07997)\n\n    - Not vouched for, check it first\n\n- By being very good at discovering/exploiting problems\n\n- By being very good at detecting intruders and stopping them\n\n- By inserting sophisticated but innocuous exploits into code - something like the [underhanded c contest](http://www.underhanded-c.org/_page_id_2.html), but on a massive scale\n\n- Generally speaking, imagine you have a really good hacker (or security expert - same thing). Now make them work 1000 times faster.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7606", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:53.699+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-af25a66efae23170012234649bf38173c3592ebea5b9ddc33222d047eac8cefb", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-af25a66efae23170012234649bf38173c3592ebea5b9ddc33222d047eac8cefb", "name": "How might \"acausal trade\" affect alignment?", "index": 118, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:13:59.071Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-af25a66efae23170012234649bf38173c3592ebea5b9ddc33222d047eac8cefb", "values": {"File": "How might \"acausal trade\" affect alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How might \"acausal trade\" affect alignment?", "Link": "https://docs.google.com/document/d/1cOPnJAL5k08pfKakapd6CuzEWLRAkzGvhVIFXc0aa4A/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:51.399+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Acausal Trade", "Doc Last Edited": "2023-03-13T17:28:21.906+01:00", "Status": "Not started", "Edit Answer": "How might \"acausal trade\" affect alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7607", "Source Link": "", "aisafety.info Link": "How might \"acausal trade\" affect alignment?", "Source": "Wiki", "All Phrasings": "How might \"acausal trade\" affect alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Suppose that we have two agents, and you\u2019re one of them. Unfortunately, you can\u2019t communicate or interact with the other agent. A common sense picture might assume that you should ignore this other agent, and certainly you shouldn\u2019t trade with them. After all, you can\u2019t talk or interact with them. How could trade with them be *possible*, let alone beneficial?\n\nIf you believe in acausal trade, you believe that attempting to trade with such distant agents can sometimes be both positive and beneficial. Here\u2019s the gist. Suppose that you have a representation, in your mind, of an agent you\u2019re unable to causally interact with. And let\u2019s suppose (never mind how, we\u2019ll get to that later) that they have a corresponding representation of you. Finally, we\u2019ll assume that the other agent is functionally similar to you in some way \u2014 when they make decisions, they use the same sort of process as you do.\n\nWe can represent this pictorially below, with the similarities between the two of you depicted by dotted lines.\n\nLet\u2019s tie this back to AGI. If we develop a superintelligent AI, it may care about certain outcomes that it can\u2019t causally affect. Indeed, these values aren\u2019t so unusual for humans. I want the best for my mom, even if she\u2019s stranded on a desert island and I\u2019ve got no way of reaching her \u2014 sometimes, we care about things we can\u2019t affect.\n\nLink some stuff about acausal normalcy in:\n\n- [https://www.lesswrong.com/posts/FCffGHJnYfdE2DgRe/humans-do-acausal-coordination-all-the-time](https://www.lesswrong.com/posts/FCffGHJnYfdE2DgRe/humans-do-acausal-coordination-all-the-time)\n\n- [https://www.lesswrong.com/posts/3RSq3bfnzuL3sp46J/acausal-normalcy](https://www.lesswrong.com/posts/3RSq3bfnzuL3sp46J/acausal-normalcy)\n\nSome decision theories suggest that the optimal procedure for making decisions makes no mention of what we can causally affect \u2014 instead, we should use some other algorithm.\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Suppose that we have two agents, and you\u2019re one of them. Unfortunately, you can\u2019t communicate or interact with the other agent. A common sense picture might assume that you should ignore this other agent, and certainly you shouldn\u2019t trade with them. After all, you can\u2019t talk or interact with them. How could trade with them be *possible*, let alone beneficial?\n\nIf you believe in acausal trade, you believe that attempting to trade with such distant agents can sometimes be both positive and beneficial. Here\u2019s the gist. Suppose that you have a representation, in your mind, of an agent you\u2019re unable to causally interact with. And let\u2019s suppose (never mind how, we\u2019ll get to that later) that they have a corresponding representation of you. Finally, we\u2019ll assume that the other agent is functionally similar to you in some way \u2014 when they make decisions, they use the same sort of process as you do.\n\nWe can represent this pictorially below, with the similarities between the two of you depicted by dotted lines.\n\nLet\u2019s tie this back to AGI. If we develop a superintelligent AI, it may care about certain outcomes that it can\u2019t causally affect. Indeed, these values aren\u2019t so unusual for humans. I want the best for my mom, even if she\u2019s stranded on a desert island and I\u2019ve got no way of reaching her \u2014 sometimes, we care about things we can\u2019t affect.\n\nLink some stuff about acausal normalcy in:\n\n- [https://www.lesswrong.com/posts/FCffGHJnYfdE2DgRe/humans-do-acausal-coordination-all-the-time](https://www.lesswrong.com/posts/FCffGHJnYfdE2DgRe/humans-do-acausal-coordination-all-the-time)\n\n- [https://www.lesswrong.com/posts/3RSq3bfnzuL3sp46J/acausal-normalcy](https://www.lesswrong.com/posts/3RSq3bfnzuL3sp46J/acausal-normalcy)\n\nSome decision theories suggest that the optimal procedure for making decisions makes no mention of what we can causally affect \u2014 instead, we should use some other algorithm.\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7607", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:56:56.393+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-78c6998520ac1ab86c05e2913739b5d6265f40b809e3b4f0b18cfea83f31d6e5", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-78c6998520ac1ab86c05e2913739b5d6265f40b809e3b4f0b18cfea83f31d6e5", "name": "How long will it be until superintelligent AI is created?", "index": 119, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:02.378Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-78c6998520ac1ab86c05e2913739b5d6265f40b809e3b4f0b18cfea83f31d6e5", "values": {"File": "How long will it be until superintelligent AI is created?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How long will it be until superintelligent AI is created?", "Link": "https://docs.google.com/document/d/1Oo3Ay25w6lOosi9E-idoRXAc1rQFWFPidmzxHj1usF4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:48.032+01:00", "Related Answers DO NOT EDIT": "When will transformative AI be created?,What evidence do experts usually base their timeline predictions on?", "Tags": "Timelines", "Doc Last Edited": "2023-03-12T22:01:57.187+01:00", "Status": "In progress", "Edit Answer": "How long will it be until superintelligent AI is created?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5633", "Source Link": "", "aisafety.info Link": "How long will it be until superintelligent AI is created?", "Source": "Wiki", "All Phrasings": "How long will it be until superintelligent AI is created?\n", "Initial Order": "", "Related IDs": "2398,6478", "Rich Text DO NOT EDIT": "\n\nIt\u2019s hard to have justified confidence in timelines until superintelligent AI. Attempts to forecast the arrival of artificial general intelligence (AGI) have proceeded from both: (1) expert surveys, and (2) more explicit modeling.\n\n- [A recent survey](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) of 738 researchers who published at either NeurIPS or ICML (top ML conferences) in 2021 produced an aggregate forecast of 50% by 2059.\n\n- However, more explicit attempts to model the inputs needed for the development of AGI tends to suggest more recent arrival dates.\n\n    - Ajeya Cotra\u2019s report attempts to estimate the arrival of AGI, in which she initially suggested a 50% chance of 2050. In the two years hence, recent developments in AI have pushed her estimate forwards to [2040](https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines).\n\nAs with Ajeya\u2019s report, survey responses show a trend of earlier arrival dates, in the advent of ever-increasing ML capabilities. A 2016 version of the survey above estimated a [50% chance of AGI by 2061](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/). Similarly, results from the aggregate forecasting platform Metaculus now puts the median date for a publicly known AGI at *2029*, after OpenAI\u2019s GPT-3 showed unexpected ability on [a broad range of tasks](https://gpt3examples.com).\n\nWhile forecasts are speculative, and may contain various contestable assumptions, median forecasts indicate a substantial chance of AGI within our lifetimes. What\u2019s more \u2013 if recent trends continue \u2013 forecasts for the arrival of AGI appear to be getting *shorter*.\n\n", "Tag Count": 1, "Related Answer Count": 2, "Rich Text": "\n\nIt\u2019s hard to have justified confidence in timelines until superintelligent AI. Attempts to forecast the arrival of artificial general intelligence (AGI) have proceeded from both: (1) expert surveys, and (2) more explicit modeling.\n\n- [A recent survey](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) of 738 researchers who published at either NeurIPS or ICML (top ML conferences) in 2021 produced an aggregate forecast of 50% by 2059.\n\n- However, more explicit attempts to model the inputs needed for the development of AGI tends to suggest more recent arrival dates.\n\n    - Ajeya Cotra\u2019s report attempts to estimate the arrival of AGI, in which she initially suggested a 50% chance of 2050. In the two years hence, recent developments in AI have pushed her estimate forwards to [2040](https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines).\n\nAs with Ajeya\u2019s report, survey responses show a trend of earlier arrival dates, in the advent of ever-increasing ML capabilities. A 2016 version of the survey above estimated a [50% chance of AGI by 2061](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/). Similarly, results from the aggregate forecasting platform Metaculus now puts the median date for a publicly known AGI at *2029*, after OpenAI\u2019s GPT-3 showed unexpected ability on [a broad range of tasks](https://gpt3examples.com).\n\nWhile forecasts are speculative, and may contain various contestable assumptions, median forecasts indicate a substantial chance of AGI within our lifetimes. What\u2019s more \u2013 if recent trends continue \u2013 forecasts for the arrival of AGI appear to be getting *shorter*.\n\n", "Stamp Count": 0, "Multi Answer": true, "Stamped By": "", "Priority": 4, "Asker": "Anonymous", "External Source": "", "Last Asked On Discord": "", "UI ID": "5633", "Related Answers": "When will transformative AI be created?,What evidence do experts usually base their timeline predictions on?", "Doc Last Ingested": "2023-03-14T23:56:58.394+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-959b369b51932c0d8f8a86801f597c0886438786c333952bc71416dd7d375c5f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-959b369b51932c0d8f8a86801f597c0886438786c333952bc71416dd7d375c5f", "name": "How likely is it that governments will play a significant role? What role would be desirable, if any?", "index": 120, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:05.821Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-959b369b51932c0d8f8a86801f597c0886438786c333952bc71416dd7d375c5f", "values": {"File": "How likely is it that governments will play a significant role? What role would be desirable, if any?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How likely is it that governments will play a significant role? What role would be desirable, if any?", "Link": "https://docs.google.com/document/d/1iAouDo9jY-9qtkSEfLkxtbLIOiVkdClKp-Rl6W1Oi8E/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:44.520+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Government", "Doc Last Edited": "2023-02-22T23:10:11.147+01:00", "Status": "Live on site", "Edit Answer": "How likely is it that governments will play a significant role? What role would be desirable, if any?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7736", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "How likely is it that governments will play a significant role? What role would be desirable, if any?", "Source": "LessWrong", "All Phrasings": "How likely is it that governments will play a significant role? What role would be desirable, if any?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Currently, private AI labs are greatly ahead of any known governmental efforts in the production of advanced AI, so it looks unlikely that a government will be involved directly in the creation of the first AGI.\n\nNevertheless, governments are important drivers of policies, so in order to avoid unaligned AGI, here are some suggestions that they can enact:\n\n- Choose policies that slow down rather than speed up AGI development, such as not incentivizing [AI capabilities research](https://www.alignmentforum.org/posts/tmyTb4bQQi7C47sde/safety-capabilities-tradeoff-dials-are-inevitable-in-agi).\n\n- Attempt to foster cooperation and reduce the number of players competing to create the first AGI in order to avoid race dynamics that lead to cut corners on safety.\n\n- Generally foster a stable, peaceful, connected world. Also don't start wars!\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Currently, private AI labs are greatly ahead of any known governmental efforts in the production of advanced AI, so it looks unlikely that a government will be involved directly in the creation of the first AGI.\n\nNevertheless, governments are important drivers of policies, so in order to avoid unaligned AGI, here are some suggestions that they can enact:\n\n- Choose policies that slow down rather than speed up AGI development, such as not incentivizing [AI capabilities research](https://www.alignmentforum.org/posts/tmyTb4bQQi7C47sde/safety-capabilities-tradeoff-dials-are-inevitable-in-agi).\n\n- Attempt to foster cooperation and reduce the number of players competing to create the first AGI in order to avoid race dynamics that lead to cut corners on safety.\n\n- Generally foster a stable, peaceful, connected world. Also don't start wars!\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7736", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:01.093+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-754fdef02c592cb5d999f4f19762f604060a872dd93a1e1c163d23fcbbfaa9ba", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-754fdef02c592cb5d999f4f19762f604060a872dd93a1e1c163d23fcbbfaa9ba", "name": "How likely is it that an AI would pretend to be a human to further its goals?", "index": 121, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:08.712Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-754fdef02c592cb5d999f4f19762f604060a872dd93a1e1c163d23fcbbfaa9ba", "values": {"File": "How likely is it that an AI would pretend to be a human to further its goals?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How likely is it that an AI would pretend to be a human to further its goals?", "Link": "https://docs.google.com/document/d/1WD8snJgaEfj_gKQII2R-2osagwkPMsIdaG9VCqsvXzI/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:40.748+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Deception", "Doc Last Edited": "2023-02-22T23:10:12.392+01:00", "Status": "Live on site", "Edit Answer": "How likely is it that an AI would pretend to be a human to further its goals?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "5842", "Source Link": "", "aisafety.info Link": "How likely is it that an AI would pretend to be a human to further its goals?", "Source": "Wiki", "All Phrasings": "How likely is it that an AI would pretend to be a human to further its goals?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Talking about full AGI: Fairly likely, but depends on takeoff speed. In a slow takeoff of a misaligned AGI, where it is only weakly superintelligent, manipulating humans would be one of its main options for trying to further its goals for some time. Even in a fast takeoff, it\u2019s plausible that it would at least briefly manipulate humans in order to accelerate its ascent to technological superiority, though depending on what machines are available to hack at the time it may be able to skip this stage.\n\nIf the AI's goals include reference to humans it may have reason to continue deceiving us after it attains technological superiority, but will not necessarily do so. How this unfolds would depend on the details of its goals.\n\nEliezer Yudkowsky gives [the example](https://www.lesswrong.com/posts/pxGYZs2zHJNHvWY5b/request-for-concrete-ai-takeover-mechanisms) of an AI solving protein folding, then mail-ordering synthesised DNA to a bribed or deceived human with instructions to mix the ingredients in a specific order to create [wet nanotechnology](https://en.wikipedia.org/wiki/Atomically_precise_manufacturing).\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "Talking about full AGI: Fairly likely, but depends on takeoff speed. In a slow takeoff of a misaligned AGI, where it is only weakly superintelligent, manipulating humans would be one of its main options for trying to further its goals for some time. Even in a fast takeoff, it\u2019s plausible that it would at least briefly manipulate humans in order to accelerate its ascent to technological superiority, though depending on what machines are available to hack at the time it may be able to skip this stage.\n\nIf the AI's goals include reference to humans it may have reason to continue deceiving us after it attains technological superiority, but will not necessarily do so. How this unfolds would depend on the details of its goals.\n\nEliezer Yudkowsky gives [the example](https://www.lesswrong.com/posts/pxGYZs2zHJNHvWY5b/request-for-concrete-ai-takeover-mechanisms) of an AI solving protein folding, then mail-ordering synthesised DNA to a bribed or deceived human with instructions to mix the ingredients in a specific order to create [wet nanotechnology](https://en.wikipedia.org/wiki/Atomically_precise_manufacturing).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Sophiaaa", "External Source": "", "Last Asked On Discord": "", "UI ID": "5842", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:03.979+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-79389a45ce54e6858f7d43fc8bba76ed4f071d371d81a87a8076a5bb610f2859", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-79389a45ce54e6858f7d43fc8bba76ed4f071d371d81a87a8076a5bb610f2859", "name": "How likely is it that AGI will first be developed by a large established organization, rather than a small startup, an academic group or a government?", "index": 122, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:11.173Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-79389a45ce54e6858f7d43fc8bba76ed4f071d371d81a87a8076a5bb610f2859", "values": {"File": "How likely is it that AGI will first be developed by a large established organization, rather than a small startup, an academic group or a government?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How likely is it that AGI will first be developed by a large established organization, rather than a small startup, an academic group or a government?", "Link": "https://docs.google.com/document/d/1cfItt9MqEKtWND9h8mg0t2ky8M_MAk808dqFXWtkRiM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:37.509+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Actors,Government,Academia,Tech Companies", "Doc Last Edited": "2023-02-22T22:56:39.340+01:00", "Status": "In progress", "Edit Answer": "How likely is it that AGI will first be developed by a large established organization, rather than a small startup, an academic group or a government?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7735", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "How likely is it that AGI will first be developed by a large established organization, rather than a small startup, an academic group or a government?", "Source": "LessWrong", "All Phrasings": "How likely is it that AGI will first be developed by a large established organization, rather than a small startup, an academic group or a government?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "- \n\n- Even if in principle AGI doesn\u2019t need so much compute, vast amounts can compensate for lack of insight.\n\n    - And we likely will\n\n- First one is likely to be the worst one.\n\n- Funding is key.\n\n- \n\n- It used to be much less clear, but scaling laws strongly imply that if the first AGI requires enormous amounts of compute, smaller actors would not have the funds\n\n- Manhattan-Project style thing could be possible for a government, but they\u2019d need to get lots of top scientists, which is hard as those people are likely happy where they are (Rob speculating)\n\n- Academics don\u2019t have the compute/funding\n\n", "Tag Count": 4, "Related Answer Count": 0, "Rich Text": "- \n\n- Even if in principle AGI doesn\u2019t need so much compute, vast amounts can compensate for lack of insight.\n\n    - And we likely will\n\n- First one is likely to be the worst one.\n\n- Funding is key.\n\n- \n\n- It used to be much less clear, but scaling laws strongly imply that if the first AGI requires enormous amounts of compute, smaller actors would not have the funds\n\n- Manhattan-Project style thing could be possible for a government, but they\u2019d need to get lots of top scientists, which is hard as those people are likely happy where they are (Rob speculating)\n\n- Academics don\u2019t have the compute/funding\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7735", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:08.994+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-9cf8d8be79d8d0f66441ef2ffad6fe324985396db5e38b8ea892dd7af3294be7", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-9cf8d8be79d8d0f66441ef2ffad6fe324985396db5e38b8ea892dd7af3294be7", "name": "How likely is an \"intelligence explosion\"?", "index": 123, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:13.853Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-9cf8d8be79d8d0f66441ef2ffad6fe324985396db5e38b8ea892dd7af3294be7", "values": {"File": "How likely is an \"intelligence explosion\"?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How likely is an \"intelligence explosion\"?", "Link": "https://docs.google.com/document/d/1dXYZWNHbse2tliI6OIKJzEPzFHtDXv_rVRm65lNacQs/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:34.326+01:00", "Related Answers DO NOT EDIT": "What is an \"intelligence explosion\"?", "Tags": "Plausibility,Intelligence Explosion", "Doc Last Edited": "2023-02-22T23:10:13.739+01:00", "Status": "Live on site", "Edit Answer": "How likely is an \"intelligence explosion\"?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6586", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "How likely is an \"intelligence explosion\"?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "How likely is an \"intelligence explosion\"?\n", "Initial Order": "", "Related IDs": "6306", "Rich Text DO NOT EDIT": "Conditional on technological progress continuing, it seems extremely likely that there will be an intelligence explosion, as at some point generally capable intelligent systems will tend to become the main drivers of their own development both at a software and hardware level. This would predictably create a feedback cycle of increasingly intelligent systems improving themselves more effectively. It seems like if the compute was used effectively, [computers have many large advantages over biological cognition](https://publicism.info/philosophy/superintelligence/4.html), so this scaling up might be very rapid if there is a [computational overhang](https://www.lesswrong.com/tag/computing-overhang).\n\nSome ways technological progress could stop would be global coordination to stop AI research, global catastrophes severe enough to stop hardware production and maintenance, or hardware reaching physical limits before an intelligence explosion is possible (though this last one seems unlikely, as [atomically precise manufacturing](https://en.wikipedia.org/wiki/Atomically_precise_manufacturing) promises many orders of magnitude of cost reduction and processing power increase, and we're already seeing fairly capable systems on current hardware).\n\n", "Tag Count": 2, "Related Answer Count": 1, "Rich Text": "Conditional on technological progress continuing, it seems extremely likely that there will be an intelligence explosion, as at some point generally capable intelligent systems will tend to become the main drivers of their own development both at a software and hardware level. This would predictably create a feedback cycle of increasingly intelligent systems improving themselves more effectively. It seems like if the compute was used effectively, [computers have many large advantages over biological cognition](https://publicism.info/philosophy/superintelligence/4.html), so this scaling up might be very rapid if there is a [computational overhang](https://www.lesswrong.com/tag/computing-overhang).\n\nSome ways technological progress could stop would be global coordination to stop AI research, global catastrophes severe enough to stop hardware production and maintenance, or hardware reaching physical limits before an intelligence explosion is possible (though this last one seems unlikely, as [atomically precise manufacturing](https://en.wikipedia.org/wiki/Atomically_precise_manufacturing) promises many orders of magnitude of cost reduction and processing power increase, and we're already seeing fairly capable systems on current hardware).\n\n", "Stamp Count": 1, "Multi Answer": "", "Stamped By": "plex", "Priority": 4, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "", "UI ID": "6586", "Related Answers": "What is an \"intelligence explosion\"?", "Doc Last Ingested": "2023-03-14T23:57:15.057+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-e78dd548de290e4a28e89f571e237dc0204603d32b4c8c8939d5b08f6ad816e3", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-e78dd548de290e4a28e89f571e237dc0204603d32b4c8c8939d5b08f6ad816e3", "name": "How likely are AI organizations to respond appropriately to the risks of their creations?", "index": 124, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:16.413Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-e78dd548de290e4a28e89f571e237dc0204603d32b4c8c8939d5b08f6ad816e3", "values": {"File": "How likely are AI organizations to respond appropriately to the risks of their creations?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How likely are AI organizations to respond appropriately to the risks of their creations?", "Link": "https://docs.google.com/document/d/1UKxB7chSL7JOZP7A_6h0xGwlMUVuJe23zTbXIUYe1Ic/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:30.694+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Organizations,Institutions", "Doc Last Edited": "2023-02-22T22:56:40.369+01:00", "Status": "Not started", "Edit Answer": "How likely are AI organizations to respond appropriately to the risks of their creations?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7584", "Source Link": "", "aisafety.info Link": "How likely are AI organizations to respond appropriately to the risks of their creations?", "Source": "Wiki", "All Phrasings": "How likely are AI organizations to respond appropriately to the risks of their creations?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7584", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:17.925+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-071f2f8363547fddb979e8358de57bc77d53244fc77229834971e7f56d4b24ba", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-071f2f8363547fddb979e8358de57bc77d53244fc77229834971e7f56d4b24ba", "name": "How is the Alignment Research Center (ARC) trying to solve Eliciting Latent Knowledge (ELK)?", "index": 125, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:18.902Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-071f2f8363547fddb979e8358de57bc77d53244fc77229834971e7f56d4b24ba", "values": {"File": "How is the Alignment Research Center (ARC) trying to solve Eliciting Latent Knowledge (ELK)?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How is the Alignment Research Center (ARC) trying to solve Eliciting Latent Knowledge (ELK)?", "Link": "https://docs.google.com/document/d/1r7PeJ9FFhdWMbCwcDs7z-B9rAikILk8GOwBsbIFN8J4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:27.069+01:00", "Related Answers DO NOT EDIT": "", "Tags": "ARC,ELK", "Doc Last Edited": "2023-02-22T23:10:15.127+01:00", "Status": "Live on site", "Edit Answer": "How is the Alignment Research Center (ARC) trying to solve Eliciting Latent Knowledge (ELK)?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8316", "Source Link": "", "aisafety.info Link": "How is the Alignment Research Center (ARC) trying to solve Eliciting Latent Knowledge (ELK)?", "Source": "Wiki", "All Phrasings": "How is the Alignment Research Center (ARC) trying to solve Eliciting Latent Knowledge (ELK)?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "ARC is trying to solve [Eliciting Latent Knowledge (ELK)](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit). Suppose that you are training an AI agent that predicts the state of the world and then performs some actions, called a *predictor*. This predictor is the AGI that will be acting to accomplish goals in the world. How can you create another model, called a *reporter*, that tells you what the predictor believes about the world? A key challenge in training this reporter is that training your reporter on human labeled training data, by default, incentivizes the predictor to just model what the human thinks is true, because the human is a simpler model than the AI.\n\nMotivation: At a high level, Paul's plan seems to be to produce a minimal AI that can help to do AI safety research. To do this, preventing [deception](https://www.lesswrong.com/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment) and [inner alignment failure](https://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem) are on the critical path, and the only known solution paths to this require interpretability (this is how all of Evan's [11 proposals](https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai) plan to get around this problem).\n\nIf ARC can solve ELK, this would be a very strong form of interpretability: our reporter is able to tell us what the predictor believes about the world. Some ways this could end up being useful for aligning the predictor include:\n\n- Using the reporter to find deceptive/misaligned thoughts in the predictor, and then optimizing against those interpreted thoughts. At any given point in time, SGD only updates the weights a small amount. If an AI becomes misaligned, it won't be very misaligned, and the interpretability tools will be able to figure this out and do a gradient step to make it aligned again. In this way, we can prevent deception at any point in training.\n\n- Stopping training if the AI is misaligned.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "ARC is trying to solve [Eliciting Latent Knowledge (ELK)](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit). Suppose that you are training an AI agent that predicts the state of the world and then performs some actions, called a *predictor*. This predictor is the AGI that will be acting to accomplish goals in the world. How can you create another model, called a *reporter*, that tells you what the predictor believes about the world? A key challenge in training this reporter is that training your reporter on human labeled training data, by default, incentivizes the predictor to just model what the human thinks is true, because the human is a simpler model than the AI.\n\nMotivation: At a high level, Paul's plan seems to be to produce a minimal AI that can help to do AI safety research. To do this, preventing [deception](https://www.lesswrong.com/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment) and [inner alignment failure](https://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem) are on the critical path, and the only known solution paths to this require interpretability (this is how all of Evan's [11 proposals](https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai) plan to get around this problem).\n\nIf ARC can solve ELK, this would be a very strong form of interpretability: our reporter is able to tell us what the predictor believes about the world. Some ways this could end up being useful for aligning the predictor include:\n\n- Using the reporter to find deceptive/misaligned thoughts in the predictor, and then optimizing against those interpreted thoughts. At any given point in time, SGD only updates the weights a small amount. If an AI becomes misaligned, it won't be very misaligned, and the interpretability tools will be able to figure this out and do a gradient step to make it aligned again. In this way, we can prevent deception at any point in training.\n\n- Stopping training if the AI is misaligned.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8316", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:25.461+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-716135b500cbba19ae9ced8330cbbd659117864a5369072968579d0a624eb897", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-716135b500cbba19ae9ced8330cbbd659117864a5369072968579d0a624eb897", "name": "How is metaethics relevant to AI alignment?", "index": 126, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:21.369Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-716135b500cbba19ae9ced8330cbbd659117864a5369072968579d0a624eb897", "values": {"File": "How is metaethics relevant to AI alignment?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How is metaethics relevant to AI alignment?", "Link": "https://docs.google.com/document/d/18XrwzraKQhqpzLIFdaRQmwdZ-U0AzWBFeWRdJ8-I3XY/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:23.962+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Metaethics", "Doc Last Edited": "2023-02-22T22:56:41.476+01:00", "Status": "Not started", "Edit Answer": "How is metaethics relevant to AI alignment?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7615", "Source Link": "", "aisafety.info Link": "How is metaethics relevant to AI alignment?", "Source": "Wiki", "All Phrasings": "How is metaethics relevant to AI alignment?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7615", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:27.653+01:00", "Request Count": "", "Number of suggestions on answer doc": 35, "Total character count of suggestions on answer doc": 15656, "Helpful": ""}}, {"id": "i-75fcf9edb9cc45b31ba4768a4343acd3a40a7f8318ae9988b89cb5f598bf7461", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-75fcf9edb9cc45b31ba4768a4343acd3a40a7f8318ae9988b89cb5f598bf7461", "name": "How is OpenAI planning to solve the full alignment problem?", "index": 127, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:23.847Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-75fcf9edb9cc45b31ba4768a4343acd3a40a7f8318ae9988b89cb5f598bf7461", "values": {"File": "How is OpenAI planning to solve the full alignment problem?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How is OpenAI planning to solve the full alignment problem?", "Link": "https://docs.google.com/document/d/17aXs6e7UOhx-KDmrPIgnTSaDfOYnzsTBVRehfq2hSrQ/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:20.074+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Open AI", "Doc Last Edited": "2023-03-01T22:03:48.970+01:00", "Status": "Live on site", "Edit Answer": "How is OpenAI planning to solve the full alignment problem?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8368", "Source Link": "", "aisafety.info Link": "How is OpenAI planning to solve the full alignment problem?", "Source": "Wiki", "All Phrasings": "How is OpenAI planning to solve the full alignment problem?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "The safety team at OpenAI's plan is to build a [MVP aligned AGI](https://aligned.substack.com/p/alignment-mvp) to try and help us solve the full alignment problem.\n\nThey want to do this with Reinforcement Learning from Human Feedback (RLHF): get feedback from humans about what is good, i.e. give reward to AI's based on the human feedback. Problem: what if the AI makes gigabrain 5D chess moves that humans don't understand, so can't evaluate. Jan Leike, the director of the safety team, views this ([the informed oversight problem](https://ai-alignment.com/the-informed-oversight-problem-1b51b4f66b35)) as the core difficulty of alignment. Their proposed solution: an AI assisted oversight scheme, with a recursive hierarchy of AIs bottoming out at humans. They are working on experimenting with this approach by trying to get current day AIs to do useful supporting work such as [summarizing books](https://openai.com/blog/summarizing-books/) and [criticizing itself](https://openai.com/blog/critiques/).\n\nOpenAI also published GPT-3, and are continuing to push LLM capabilities, with GPT-4 expected to be released at some point soon.\n\nSee also: [Common misconceptions about OpenAI](https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai) and [Our approach to alignment research](https://openai.com/blog/our-approach-to-alignment-research/).\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "The safety team at OpenAI's plan is to build a [MVP aligned AGI](https://aligned.substack.com/p/alignment-mvp) to try and help us solve the full alignment problem.\n\nThey want to do this with Reinforcement Learning from Human Feedback (RLHF): get feedback from humans about what is good, i.e. give reward to AI's based on the human feedback. Problem: what if the AI makes gigabrain 5D chess moves that humans don't understand, so can't evaluate. Jan Leike, the director of the safety team, views this ([the informed oversight problem](https://ai-alignment.com/the-informed-oversight-problem-1b51b4f66b35)) as the core difficulty of alignment. Their proposed solution: an AI assisted oversight scheme, with a recursive hierarchy of AIs bottoming out at humans. They are working on experimenting with this approach by trying to get current day AIs to do useful supporting work such as [summarizing books](https://openai.com/blog/summarizing-books/) and [criticizing itself](https://openai.com/blog/critiques/).\n\nOpenAI also published GPT-3, and are continuing to push LLM capabilities, with GPT-4 expected to be released at some point soon.\n\nSee also: [Common misconceptions about OpenAI](https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai) and [Our approach to alignment research](https://openai.com/blog/our-approach-to-alignment-research/).\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8368", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:29.989+01:00", "Request Count": "", "Number of suggestions on answer doc": 39, "Total character count of suggestions on answer doc": 15660, "Helpful": ""}}, {"id": "i-48c717dafcbab3bb7e4df9aa1e04a1ff6124a8967755093fea2223bcfae9c1ed", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-48c717dafcbab3bb7e4df9aa1e04a1ff6124a8967755093fea2223bcfae9c1ed", "name": "How is Beth Barnes evaluating LM power seeking?", "index": 128, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:28.019Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-48c717dafcbab3bb7e4df9aa1e04a1ff6124a8967755093fea2223bcfae9c1ed", "values": {"File": "How is Beth Barnes evaluating LM power seeking?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How is Beth Barnes evaluating LM power seeking?", "Link": "https://docs.google.com/document/d/1eWI1mu5mLJfqQhXSngvvBDgHavvvEk-a311sB9hggmk/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:16.382+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Language Models,Power Seeking", "Doc Last Edited": "2023-02-22T23:10:17.666+01:00", "Status": "Live on site", "Edit Answer": "How is Beth Barnes evaluating LM power seeking?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "8318", "Source Link": "", "aisafety.info Link": "How is Beth Barnes evaluating LM power seeking?", "Source": "Wiki", "All Phrasings": "How is Beth Barnes evaluating LM power seeking?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Beth is working on \u201cgenerating a dataset that we can use to evaluate how close models are to being able to successfully seek power\u201d. The dataset is being created through simulating situations in which an Large Language Model is trying to seek power.\n\nThe [overall goal of the project](https://docs.google.com/document/d/1tf9Diyf46jlhuy7FcfvV1Ti9DsAxDAjVojMzruEySIg/edit) is to assess how close a model is to being dangerous, e.g. so we can know if it\u2019s safe for labs to scale it up. Evaluations focus on whether models are capable enough to seek power successfully, rather than whether they are aligned. They are aiming to create an automated evaluation which takes in a model and outputs how far away from dangerous it is, approximating an idealized human evaluation.\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Beth is working on \u201cgenerating a dataset that we can use to evaluate how close models are to being able to successfully seek power\u201d. The dataset is being created through simulating situations in which an Large Language Model is trying to seek power.\n\nThe [overall goal of the project](https://docs.google.com/document/d/1tf9Diyf46jlhuy7FcfvV1Ti9DsAxDAjVojMzruEySIg/edit) is to assess how close a model is to being dangerous, e.g. so we can know if it\u2019s safe for labs to scale it up. Evaluations focus on whether models are capable enough to seek power successfully, rather than whether they are aligned. They are aiming to create an automated evaluation which takes in a model and outputs how far away from dangerous it is, approximating an idealized human evaluation.\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 2, "Asker": "RoseMcClelland", "External Source": "", "Last Asked On Discord": "", "UI ID": "8318", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:32.382+01:00", "Request Count": "", "Number of suggestions on answer doc": 39, "Total character count of suggestions on answer doc": 15660, "Helpful": ""}}, {"id": "i-0c50ea8dc7d8f722d54baace5881ad36df357afa9a8f54478512b31276d06cc6", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0c50ea8dc7d8f722d54baace5881ad36df357afa9a8f54478512b31276d06cc6", "name": "How is AGI different from current AI?", "index": 129, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:31.136Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0c50ea8dc7d8f722d54baace5881ad36df357afa9a8f54478512b31276d06cc6", "values": {"File": "How is AGI different from current AI?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How is AGI different from current AI?", "Link": "https://docs.google.com/document/d/1ljy0sNxHIeksXq80KPZIlKmmJAycJgjMXolmNHpWa18/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:12.693+01:00", "Related Answers DO NOT EDIT": "", "Tags": "AGI,Narrow AI", "Doc Last Edited": "2023-02-24T20:50:24.932+01:00", "Status": "Live on site", "Edit Answer": "How is AGI different from current AI?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "2402", "Source Link": "", "aisafety.info Link": "How is AGI different from current AI?", "Source": "Wiki", "All Phrasings": "How is AGI different from current AI?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Current narrow AI systems are much more domain-specific than AGI. We don\u2019t know what the first AGI will look like.Some people think that scaling up the [GPT-3](https://www.lesswrong.com/posts/6Hee7w2paEzHsD6mn/collection-of-gpt-3-results) architecture may be sufficient to produce AGI, while others disagree. (GPT-3 is a giant prediction model which, when trained on a vast amount of text, seems to [learn how to learn](https://www.lesswrong.com/posts/D3hP47pZwXNPRByj8/an-102-meta-learning-by-gpt-3-and-a-list-of-full-proposals) and do [all sorts of highly-impressive things](https://gpt3examples.com/); a related model can [generate pictures from text](https://openai.com/blog/dall-e/).)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Current narrow AI systems are much more domain-specific than AGI. We don\u2019t know what the first AGI will look like.Some people think that scaling up the [GPT-3](https://www.lesswrong.com/posts/6Hee7w2paEzHsD6mn/collection-of-gpt-3-results) architecture may be sufficient to produce AGI, while others disagree. (GPT-3 is a giant prediction model which, when trained on a vast amount of text, seems to [learn how to learn](https://www.lesswrong.com/posts/D3hP47pZwXNPRByj8/an-102-meta-learning-by-gpt-3-and-a-list-of-full-proposals) and do [all sorts of highly-impressive things](https://gpt3examples.com/); a related model can [generate pictures from text](https://openai.com/blog/dall-e/).)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Jack Harley", "External Source": "", "Last Asked On Discord": "", "UI ID": "2402", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:34.375+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 16071, "Helpful": ""}}, {"id": "i-ae099e9aca3ec680876f5dfcc73f823ea391a5400e6dce1c804a3074d4504e58", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-ae099e9aca3ec680876f5dfcc73f823ea391a5400e6dce1c804a3074d4504e58", "name": "How is \"intelligence\" defined?", "index": 130, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:33.617Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-ae099e9aca3ec680876f5dfcc73f823ea391a5400e6dce1c804a3074d4504e58", "values": {"File": "How is \"intelligence\" defined?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How is \"intelligence\" defined?", "Link": "https://docs.google.com/document/d/1GkphSo2tQOTzkJwuyQI1uJ6op-A3Ld7ZUAfsHRMoBCM/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:08.890+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Intelligence,Definitions", "Doc Last Edited": "2023-02-22T23:10:19.916+01:00", "Status": "Live on site", "Edit Answer": "How is \"intelligence\" defined?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6315", "Source Link": "https://intelligence.org/ie-faq/", "aisafety.info Link": "How is \"intelligence\" defined?", "Source": "MIRI's Intelligence Explosion FAQ", "All Phrasings": "How is \"intelligence\" defined?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "Artificial intelligence researcher Shane Legg[defines intelligence](http://www.vetta.org/documents/Machine_Super_Intelligence.pdf) like this:\n\nIntelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.\n\nThis is a bit vague, but serves as the working definition of \u2018intelligence\u2019. For a more in-depth exploration, see[Efficient Cross-Domain Optimization](https://www.lesswrong.com/posts/yLeEPFnnB9wE7KLx2/efficient-cross-domain-optimization).\n\nSee also:\n\n- Wikipedia,[Intelligence](http://en.wikipedia.org/wiki/Intelligence)\n\n- Neisser et al.,[Intelligence: Knowns and Unknowns](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.134.1282&rep=rep1&type=pdf)\n\n- Wasserman & Zentall (eds.),[Comparative Cognition: Experimental Explorations of Animal Intelligence](http://www.amazon.com/Comparative-Cognition-Experimental-Explorations-Intelligence/dp/019537780X/)\n\n- Legg,[Definitions of Intelligence](http://www.vetta.org/definitions-of-intelligence/)\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "Artificial intelligence researcher Shane Legg[defines intelligence](http://www.vetta.org/documents/Machine_Super_Intelligence.pdf) like this:\n\nIntelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.\n\nThis is a bit vague, but serves as the working definition of \u2018intelligence\u2019. For a more in-depth exploration, see[Efficient Cross-Domain Optimization](https://www.lesswrong.com/posts/yLeEPFnnB9wE7KLx2/efficient-cross-domain-optimization).\n\nSee also:\n\n- Wikipedia,[Intelligence](http://en.wikipedia.org/wiki/Intelligence)\n\n- Neisser et al.,[Intelligence: Knowns and Unknowns](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.134.1282&rep=rep1&type=pdf)\n\n- Wasserman & Zentall (eds.),[Comparative Cognition: Experimental Explorations of Animal Intelligence](http://www.amazon.com/Comparative-Cognition-Experimental-Explorations-Intelligence/dp/019537780X/)\n\n- Legg,[Definitions of Intelligence](http://www.vetta.org/definitions-of-intelligence/)\n\n", "Stamp Count": 0, "Multi Answer": "", "Stamped By": "", "Priority": 5, "Asker": "Luke Muehlhauser", "External Source": "", "Last Asked On Discord": "", "UI ID": "6315", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:36.856+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 16071, "Helpful": ""}}, {"id": "i-8911c3cf9fb440ee834bc10a2380b46dd2553ac0fd55b000f83a4b7cdb4857f4", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-8911c3cf9fb440ee834bc10a2380b46dd2553ac0fd55b000f83a4b7cdb4857f4", "name": "How important is research closure and OPSEC for capabilities-synergistic ideas?", "index": 131, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:36.050Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-8911c3cf9fb440ee834bc10a2380b46dd2553ac0fd55b000f83a4b7cdb4857f4", "values": {"File": "How important is research closure and OPSEC for capabilities-synergistic ideas?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How important is research closure and OPSEC for capabilities-synergistic ideas?", "Link": "https://docs.google.com/document/d/1Pn55PyYeWZ5da5d3PuGkHyZuxbcrIjwmZp0hJhePZTU/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:05.646+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Capabilities,Information Security", "Doc Last Edited": "2023-02-22T22:56:42.563+01:00", "Status": "Not started", "Edit Answer": "How important is research closure and OPSEC for capabilities-synergistic ideas?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7726", "Source Link": "https://www.greaterwrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion/comment/PmhG9Rvhu6iGP5fRn", "aisafety.info Link": "How important is research closure and OPSEC for capabilities-synergistic ideas?", "Source": "LessWrong", "All Phrasings": "How important is research closure and OPSEC for capabilities-synergistic ideas?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 2, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 4, "Asker": "Rob Bensinger", "External Source": "", "Last Asked On Discord": "", "UI ID": "7726", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:39.365+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 16071, "Helpful": ""}}, {"id": "i-cd6ba3e7c5d9f431524ac4925721965ad36b8df9567b2d77e759e48485dc811f", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-cd6ba3e7c5d9f431524ac4925721965ad36b8df9567b2d77e759e48485dc811f", "name": "How hard is it for an AGI to develop powerful nanotechnology?", "index": 132, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:38.278Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-cd6ba3e7c5d9f431524ac4925721965ad36b8df9567b2d77e759e48485dc811f", "values": {"File": "How hard is it for an AGI to develop powerful nanotechnology?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How hard is it for an AGI to develop powerful nanotechnology?", "Link": "https://docs.google.com/document/d/17qxO_pXGe_Q_UhsZOcQG9UW6KAmeJH2DeKSwJhGKfO0/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:11:02.454+01:00", "Related Answers DO NOT EDIT": "", "Tags": "Molecular Nanotechnology", "Doc Last Edited": "2023-02-22T22:56:43.577+01:00", "Status": "Not started", "Edit Answer": "How hard is it for an AGI to develop powerful nanotechnology?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "7639", "Source Link": "", "aisafety.info Link": "How hard is it for an AGI to develop powerful nanotechnology?", "Source": "Wiki", "All Phrasings": "How hard is it for an AGI to develop powerful nanotechnology?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "7639", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:41.249+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 16071, "Helpful": ""}}, {"id": "i-926d25f11c1f77f21899e962a3f123eb19063a0bb513a5e294c6b37f94b2c43c", "type": "row", "href": "https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-926d25f11c1f77f21899e962a3f123eb19063a0bb513a5e294c6b37f94b2c43c", "name": "How good is the world model of GPT-3?", "index": 133, "createdAt": "2023-01-14T12:58:12.344Z", "updatedAt": "2023-03-14T23:14:40.643Z", "browserLink": "https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-926d25f11c1f77f21899e962a3f123eb19063a0bb513a5e294c6b37f94b2c43c", "values": {"File": "How good is the world model of GPT-3?", "Synced": false, "Sync account": "stampysaisafetyinfo@gmail.com", "Question": "How good is the world model of GPT-3?", "Link": "https://docs.google.com/document/d/1sjclTYjUWj_QWbz23KI8luIfVsUNJbWEoodT46xByW4/edit?usp=drivesdk", "Thumbnail": "image.jpeg", "Doc Created": "2023-01-14T13:10:59.111+01:00", "Related Answers DO NOT EDIT": "", "Tags": "GPT", "Doc Last Edited": "2023-02-22T22:56:44.508+01:00", "Status": "In progress", "Edit Answer": "How good is the world model of GPT-3?", "Alternate Phrasings": "", "UI ID DO NOT EDIT": "6475", "Source Link": "", "aisafety.info Link": "How good is the world model of GPT-3?", "Source": "Wiki", "All Phrasings": "How good is the world model of GPT-3?\n", "Initial Order": "", "Related IDs": "", "Rich Text DO NOT EDIT": "GPT-3 is a model that is \u201ctrained with predictive loss on a self-supervised dataset, invariant to architecture or data type (natural language, code, pixels, game states, etc)\u201d. As such, GPT3 is a language model that has \u201cread\u201d the internet and is pretty good at figuring out patterns in writing/speech which is why it can imitate specific writing styles so well. GPT-3 cannot \u201csee\u201d the real world or confirm the accuracy of the text it produces. In many cases, the model is hallucinating meaning that it makes up stuff that appear to be [truthful](https://www.lesswrong.com/posts/PF58wEdztZFX2dSue/how-truthful-is-gpt-3-a-benchmark-for-language-models) but are not. At the same time, GPT/transformers can be used as [helpful research assistants](https://www.lesswrong.com/posts/spBoxzcaCrqXqyQHq/using-gpt-3-to-augment-human-intelligence).\n\nFor more, see post on **[The Cave Allegory Revisited: Understanding GPT's Worldview](https://www.alignmentforum.org/posts/kFCu3batN8k8mwtmh/the-cave-allegory-revisited-understanding-gpt-s-worldview)**\n\n", "Tag Count": 1, "Related Answer Count": 0, "Rich Text": "GPT-3 is a model that is \u201ctrained with predictive loss on a self-supervised dataset, invariant to architecture or data type (natural language, code, pixels, game states, etc)\u201d. As such, GPT3 is a language model that has \u201cread\u201d the internet and is pretty good at figuring out patterns in writing/speech which is why it can imitate specific writing styles so well. GPT-3 cannot \u201csee\u201d the real world or confirm the accuracy of the text it produces. In many cases, the model is hallucinating meaning that it makes up stuff that appear to be [truthful](https://www.lesswrong.com/posts/PF58wEdztZFX2dSue/how-truthful-is-gpt-3-a-benchmark-for-language-models) but are not. At the same time, GPT/transformers can be used as [helpful research assistants](https://www.lesswrong.com/posts/spBoxzcaCrqXqyQHq/using-gpt-3-to-augment-human-intelligence).\n\nFor more, see post on **[The Cave Allegory Revisited: Understanding GPT's Worldview](https://www.alignmentforum.org/posts/kFCu3batN8k8mwtmh/the-cave-allegory-revisited-understanding-gpt-s-worldview)**\n\n", "Stamp Count": "", "Multi Answer": "", "Stamped By": "", "Priority": 3, "Asker": "Nico Hill2", "External Source": "", "Last Asked On Discord": "", "UI ID": "6475", "Related Answers": "", "Doc Last Ingested": "2023-03-14T23:57:43.618+01:00", "Request Count": "", "Number of suggestions on answer doc": 41, "Total character count of suggestions on answer doc": 16071, "Helpful": ""}}]}